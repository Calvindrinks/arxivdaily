{"2025-10-30T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2510.26802v1","updated":"2025-10-30T17:59:55Z","published":"2025-10-30T17:59:55Z","title":"Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark","summary":"  Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io\n","authors":["Ziyu Guo","Xinyan Chen","Renrui Zhang","Ruichuan An","Yu Qi","Dongzhi Jiang","Xiangtai Li","Manyuan Zhang","Hongsheng Li","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2510.26802v1.pdf","comment":"Project Page: https://video-cof.github.io"},{"id":"http://arxiv.org/abs/2510.26790v1","updated":"2025-10-30T17:58:26Z","published":"2025-10-30T17:58:26Z","title":"Gistify! Codebase-Level Understanding via Runtime Execution","summary":"  As coding agents are increasingly deployed in large codebases, the need to\nautomatically design challenging, codebase-level evaluation is central. We\npropose Gistify, a task where a coding LLM must create a single, minimal,\nself-contained file that can reproduce a specific functionality of a codebase.\nThe coding LLM is given full access to a codebase along with a specific\nentrypoint (e.g., a python command), and the generated file must replicate the\noutput of the same command ran under the full codebase, while containing only\nthe essential components necessary to execute the provided command. Success on\nGistify requires both structural understanding of the codebase, accurate\nmodeling of its execution flow as well as the ability to produce potentially\nlarge code patches. Our findings show that current state-of-the-art models\nstruggle to reliably solve Gistify tasks, especially ones with long executions\ntraces.\n","authors":["Hyunji Lee","Minseon Kim","Chinmay Singh","Matheus Pereira","Atharv Sonwane","Isadora White","Elias Stengel-Eskin","Mohit Bansal","Zhengyan Shi","Alessandro Sordoni","Marc-Alexandre Côté","Xingdi Yuan","Lucas Caccia"],"pdf_url":"https://arxiv.org/pdf/2510.26790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26788v1","updated":"2025-10-30T17:58:11Z","published":"2025-10-30T17:58:11Z","title":"Defeating the Training-Inference Mismatch via FP16","summary":"  Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.\n","authors":["Penghui Qi","Zichen Liu","Xiangxin Zhou","Tianyu Pang","Chao Du","Wee Sun Lee","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2510.26788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26787v1","updated":"2025-10-30T17:58:04Z","published":"2025-10-30T17:58:04Z","title":"Remote Labor Index: Measuring AI Automation of Remote Work","summary":"  AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.\n","authors":["Mantas Mazeika","Alice Gatti","Cristina Menghini","Udari Madhushani Sehwag","Shivam Singhal","Yury Orlovskiy","Steven Basart","Manasi Sharma","Denis Peskoff","Elaine Lau","Jaehyuk Lim","Lachlan Carroll","Alice Blair","Vinaya Sivakumar","Sumana Basu","Brad Kenstler","Yuntao Ma","Julian Michael","Xiaoke Li","Oliver Ingebretsen","Aditya Mehta","Jean Mottola","John Teichmann","Kevin Yu","Zaina Shaik","Adam Khoja","Richard Ren","Jason Hausenloy","Long Phan","Ye Htet","Ankit Aich","Tahseen Rabbani","Vivswan Shah","Andriy Novykov","Felix Binder","Kirill Chugunov","Luis Ramirez","Matias Geralnik","Hernán Mesura","Dean Lee","Ed-Yeremai Hernandez Cardona","Annette Diamond","Summer Yue","Alexandr Wang","Bing Liu","Ernesto Hernandez","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2510.26787v1.pdf","comment":"Website: https://www.remotelabor.ai"},{"id":"http://arxiv.org/abs/2508.11607v2","updated":"2025-10-30T17:57:04Z","published":"2025-08-15T17:14:29Z","title":"TinyTim: A Family of Language Models for Divergent Generation","summary":"  In the search for artificial general intelligence, model development and\ntraining has focused primarily on vast datasets of known problems and their\naccepted solutions. This process necessarily produces convergent systems which\nare fundamentally incapable of the conceptual reframing that is required for\ngenuine creative breakthroughs. Inspired by the divergent cognitive processes\nthat allow humans to make such creative leaps, our work introduces a family of\nlanguage models, TinyTim, to serve as sources of divergent generation within\nbroader systems. These models have been created by fine-tuning on the\nanti-parsimonious text of James Joyce's `Finnegans Wake'. Quantitative analysis\nof both an unsupervised fine-tuned model (TinyTim-V1) and a new\ninstruction-tuned variant (TinyTim-V2) demonstrates a profound capacity for\nlexical invention; the foundational V1 model exhibits a Yule's K score for\nlexical richness over twenty times greater than that of convergent baselines.\nThis trait is a stable property of the family, as the instruction-tuned V2\nmaintains a statistically distinct profile and resists factual convergence,\nsacrificing benchmark performance to preserve its core generative style. This\nwork establishes a methodology for engineering specialized divergent models\nthat, when paired with convergent systems, can reframe problems and force\nbreakthroughs beyond the reach of statistical optimization alone.\n","authors":["Christopher J. Agostino"],"pdf_url":"https://arxiv.org/pdf/2508.11607v2.pdf","comment":"7 pages, 3 figures, accepted to NeurIPS Creative AI track, models\n  available at https://hf.co/npc-worldwide/"},{"id":"http://arxiv.org/abs/2510.25744v2","updated":"2025-10-30T17:54:45Z","published":"2025-10-29T17:47:18Z","title":"Completion $\\neq$ Collaboration: Scaling Collaborative Effort with\n  Agents","summary":"  Current evaluations of agents remain centered around one-shot task\ncompletion, failing to account for the inherently iterative and collaborative\nnature of many real-world problems, where human goals are often underspecified\nand evolve. We argue for a shift from building and assessing task completion\nagents to developing collaborative agents, assessed not only by the quality of\ntheir final outputs but by how well they engage with and enhance human effort\nthroughout the problem-solving process. To support this shift, we introduce\ncollaborative effort scaling, a framework that captures how an agent's utility\ngrows with increasing user involvement. Through case studies and simulated\nevaluations, we show that state-of-the-art agents often underperform in\nmulti-turn, real-world scenarios, revealing a missing ingredient in agent\ndesign: the ability to sustain engagement and scaffold user understanding.\nCollaborative effort scaling offers a lens for diagnosing agent behavior and\nguiding development toward more effective interactions.\n","authors":["Shannon Zejiang Shen","Valerie Chen","Ken Gu","Alexis Ross","Zixian Ma","Jillian Ross","Alex Gu","Chenglei Si","Wayne Chi","Andi Peng","Jocelyn J Shen","Ameet Talwalkar","Tongshuang Wu","David Sontag"],"pdf_url":"https://arxiv.org/pdf/2510.25744v2.pdf","comment":"22 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.26768v1","updated":"2025-10-30T17:52:02Z","published":"2025-10-30T17:52:02Z","title":"AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions","summary":"  We present AMO-Bench, an Advanced Mathematical reasoning benchmark with\nOlympiad level or even higher difficulty, comprising 50 human-crafted problems.\nExisting benchmarks have widely leveraged high school math competitions for\nevaluating mathematical reasoning capabilities of large language models (LLMs).\nHowever, many existing math competitions are becoming less effective for\nassessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To\naddress this, AMO-Bench introduces more rigorous challenges by ensuring all 50\nproblems are (1) cross-validated by experts to meet at least the International\nMathematical Olympiad (IMO) difficulty standards, and (2) entirely original\nproblems to prevent potential performance leakages from data memorization.\nMoreover, each problem in AMO-Bench requires only a final answer rather than a\nproof, enabling automatic and robust grading for evaluation. Experimental\nresults across 26 LLMs on AMO-Bench show that even the best-performing model\nachieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.\nBeyond these poor performances, our further analysis reveals a promising\nscaling trend with increasing test-time compute on AMO-Bench. These results\nhighlight the significant room for improving the mathematical reasoning in\ncurrent LLMs. We release AMO-Bench to facilitate further research into\nadvancing the reasoning abilities of language models.\nhttps://amo-bench.github.io/\n","authors":["Shengnan An","Xunliang Cai","Xuezhi Cao","Xiaoyu Li","Yehao Lin","Junlin Liu","Xinxuan Lv","Dan Ma","Xuanlin Wang","Ziwen Wang","Shuang Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26768v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2506.09391v2","updated":"2025-10-30T17:41:15Z","published":"2025-06-11T04:44:46Z","title":"Comparing human and LLM politeness strategies in free production","summary":"  Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems.\n","authors":["Haoran Zhao","Robert D. Hawkins"],"pdf_url":"https://arxiv.org/pdf/2506.09391v2.pdf","comment":"25 pages, 5 figures | EMNLP 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2510.26745v1","updated":"2025-10-30T17:40:22Z","published":"2025-10-30T17:40:22Z","title":"Deep sequence models tend to memorize geometrically; it is unclear why","summary":"  In sequence modeling, the parametric memory of atomic facts has been\npredominantly abstracted as a brute-force lookup of co-occurrences between\nentities. We contrast this associative view against a geometric view of how\nmemory is stored. We begin by isolating a clean and analyzable instance of\nTransformer reasoning that is incompatible with memory as strictly a storage of\nthe local co-occurrences specified during training. Instead, the model must\nhave somehow synthesized its own geometry of atomic facts, encoding global\nrelationships between all entities, including non-co-occurring ones. This in\nturn has simplified a hard reasoning task involving an $\\ell$-fold composition\ninto an easy-to-learn 1-step geometric task.\n  From this phenomenon, we extract fundamental aspects of neural embedding\ngeometries that are hard to explain. We argue that the rise of such a geometry,\ndespite optimizing over mere local associations, cannot be straightforwardly\nattributed to typical architectural or optimizational pressures.\nCounterintuitively, an elegant geometry is learned even when it is not more\nsuccinct than a brute-force lookup of associations.\n  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry\nstems from a spectral bias that -- in contrast to prevailing theories -- indeed\narises naturally despite the lack of various pressures. This analysis also\npoints to practitioners a visible headroom to make Transformer memory more\nstrongly geometric. We hope the geometric view of parametric memory encourages\nrevisiting the default intuitions that guide researchers in areas like\nknowledge acquisition, capacity, discovery and unlearning.\n","authors":["Shahriar Noroozizadeh","Vaishnavh Nagarajan","Elan Rosenfeld","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2510.26745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09205v3","updated":"2025-10-30T17:37:55Z","published":"2025-03-12T09:48:38Z","title":"Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model","summary":"  Integrating audio and visual data for training multimodal foundational models\nremains a challenge. The Audio-Video Vector Alignment (AVVA) framework\naddresses this by considering AV scene alignment beyond mere temporal\nsynchronization, and leveraging Large Language Models (LLMs) for data curation.\nAVVA implements a scoring mechanism for selecting aligned training data\nsegments. It integrates Whisper, a speech-based foundation model, for audio and\nDINOv2 for video analysis in a dual-encoder structure with contrastive learning\non AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the\neffectiveness of the proposed model architecture and data curation approach.\nAVVA achieves a significant improvement in top-k accuracies for video-to-audio\nretrieval on all datasets compared to DenseAV, while using only 192 hrs of\ncurated training data. Furthermore, an ablation study indicates that the data\ncuration process effectively trades data quality for data quantity, yielding\nincreases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound,\ncompared to training on the full spectrum of uncurated data.\n","authors":["Ali Vosoughi","Dimitra Emmanouilidou","Hannes Gamper"],"pdf_url":"https://arxiv.org/pdf/2503.09205v3.pdf","comment":"5 pages, 5 figures, 2 tables. Accepted at EUSIPCO 2025"},{"id":"http://arxiv.org/abs/2506.14681v2","updated":"2025-10-30T17:32:44Z","published":"2025-06-17T16:13:15Z","title":"Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and\n  Training Factors Shape LLM Alignment Quality","summary":"  Supervised fine-tuning (SFT) is a critical step in aligning large language\nmodels (LLMs) with human instructions and values, yet many aspects of SFT\nremain poorly understood. We trained a wide range of base models on a variety\nof datasets including code generation, mathematical reasoning, and\ngeneral-domain tasks, resulting in 1,000+ SFT models under controlled\nconditions. We then identified the dataset properties that matter most and\nexamined the layer-wise modifications introduced by SFT. Our findings reveal\nthat some training-task synergies persist across all models while others vary\nsubstantially, emphasizing the importance of model-specific strategies.\nMoreover, we demonstrate that perplexity consistently predicts SFT\neffectiveness, often surpassing superficial similarity between the training\ndata and the benchmark, and that mid-layer weight changes correlate most\nstrongly with performance gains. We release these 1,000+ SFT models and\nbenchmark results to accelerate further research. All resources are available\nat https://github.com/llm-jp/massive-sft.\n","authors":["Yuto Harada","Yusuke Yamauchi","Yusuke Oda","Yohei Oseki","Yusuke Miyao","Yu Takagi"],"pdf_url":"https://arxiv.org/pdf/2506.14681v2.pdf","comment":"Accepted to EMNLP 2025 (Main Conference). Models and evaluation\n  results available at: https://github.com/llm-jp/massive-sft"},{"id":"http://arxiv.org/abs/2510.26732v1","updated":"2025-10-30T17:31:03Z","published":"2025-10-30T17:31:03Z","title":"Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models","summary":"  This paper presents a comprehensive cross-platform evaluation of reasoning\ncapabilities in contemporary foundation models, establishing an\ninfrastructure-agnostic benchmark across three computational paradigms: HPC\nsupercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and\nuniversity clusters (a node with eight H200 GPUs).\n  We evaluate 15 foundation models across 79 problems spanning eight academic\ndomains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,\nCalculus, and Optimization) through three experimental phases: (1) Baseline\nestablishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,\nMistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing\nmethodology and reference performance; (2) Infrastructure validation: The\n19-problem benchmark repeated on university cluster (seven models including\nFalcon-Mamba state-space architecture) and Nebius AI Studio (nine\nstate-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3\n30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic\nreproducibility; (3) Extended evaluation: Full 79-problem assessment on both\nuniversity cluster and Nebius platforms, probing generalization at scale across\narchitectural diversity.\n  The findings challenge conventional scaling assumptions, establish training\ndata quality as more critical than model size, and provide actionable\nguidelines for model selection across educational, production, and research\ncontexts. The tri-infrastructure methodology and 79-problem benchmark enable\nlongitudinal tracking of reasoning capabilities as foundation models evolve.\n","authors":["J. de Curtò","I. de Zarzà","Pablo García","Jordi Cabot"],"pdf_url":"https://arxiv.org/pdf/2510.26732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20059v2","updated":"2025-10-30T17:28:47Z","published":"2025-10-22T22:22:59Z","title":"Enhancing Reasoning Skills in Small Persian Medical Language Models Can\n  Outperform Large-Scale Data Training","summary":"  Enhancing reasoning capabilities in small language models is critical for\nspecialized applications such as medical question answering, particularly in\nunderrepresented languages like Persian. In this study, we employ Reinforcement\nLearning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to\nimprove the reasoning skills of a general-purpose Persian language model. To\nachieve this, we translated a multiple-choice medical question-answering\ndataset into Persian and used RLAIF to generate rejected-preferred answer\npairs, which are essential for DPO training. By prompting both teacher and\nstudent models to produce Chain-of-Thought (CoT) reasoning responses, we\ncompiled a dataset containing correct and incorrect reasoning trajectories.\nThis dataset, comprising 2 million tokens in preferred answers and 2.5 million\ntokens in rejected ones, was used to train a baseline model, significantly\nenhancing its medical reasoning capabilities in Persian. Remarkably, the\nresulting model outperformed its predecessor, gaokerena-V, which was trained on\napproximately 57 million tokens, despite leveraging a much smaller dataset.\nThese results highlight the efficiency and effectiveness of reasoning-focused\ntraining approaches in developing domain-specific language models with limited\ndata availability.\n","authors":["Mehrdad Ghassabi","Sadra Hakim","Hamidreza Baradaran Kashani","Pedram Rostami"],"pdf_url":"https://arxiv.org/pdf/2510.20059v2.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2507.03704v2","updated":"2025-10-30T17:13:35Z","published":"2025-07-04T16:41:06Z","title":"Controlling Thinking Speed in Reasoning Models","summary":"  Human cognition is theorized to operate in two modes: fast, intuitive System\n1 thinking and slow, deliberate System 2 thinking. While current Large\nReasoning Models (LRMs) excel at System 2 thinking, their inability to perform\nfast thinking leads to high computational overhead and latency. In this work,\nwe enable LRMs to approximate human intelligence through dynamic thinking speed\nadjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses\ntwo key questions: (1) how to control thinking speed in LRMs, and (2) when to\nadjust it for optimal performance. For the first question, we identify the\nsteering vector that governs slow-fast thinking transitions in LRMs'\nrepresentation space. Using this vector, we achieve the first representation\nediting-based test-time scaling effect, outperforming existing prompt-based\nscaling methods. For the second question, we apply real-time difficulty\nestimation to signal reasoning segments of varying complexity. Combining these\ntechniques, we propose the first reasoning strategy that enables fast\nprocessing of easy steps and deeper analysis for complex reasoning. Without any\ntraining or additional cost, our plug-in module delivers an average +1.3%\naccuracy with -8.6% token usage across leading LRMs and advanced reasoning\nbenchmarks. All of our algorithms are implemented based on vLLM and are\nexpected to support broader applications and inspire future research.\n","authors":["Zhengkai Lin","Zhihang Fu","Ze Chen","Chao Chen","Liang Xie","Wenxiao Wang","Deng Cai","Zheng Wang","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2507.03704v2.pdf","comment":"NeurIPS 2025 Spotlight"},{"id":"http://arxiv.org/abs/2509.21319v2","updated":"2025-10-30T17:09:54Z","published":"2025-09-25T16:19:06Z","title":"RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards","summary":"  Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:\nhttps://huggingface.co/collections/nvidia/reward-models-10-2025\n","authors":["Zhilin Wang","Jiaqi Zeng","Olivier Delalleau","Ellie Evans","Daniel Egert","Hoo-Chang Shin","Felipe Soares","Yi Dong","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2509.21319v2.pdf","comment":"Added link to access models:\n  https://huggingface.co/collections/nvidia/reward-models-10-2025"},{"id":"http://arxiv.org/abs/2510.26707v1","updated":"2025-10-30T17:09:09Z","published":"2025-10-30T17:09:09Z","title":"Value Drifts: Tracing Value Alignment During LLM Post-Training","summary":"  As LLMs occupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment of LLMs with human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stage value alignment arises during the course of a model's\npost-training. Our analysis disentangles the effects of post-training\nalgorithms and datasets, measuring both the magnitude and time of value drifts\nduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popular supervised fine-tuning (SFT) and preference optimization\ndatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequent preference optimization rarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to different value alignment outcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned during post-training and help to inform data curation,\nas well as the selection of models and algorithms for preference optimization\nto improve model alignment to human values.\n","authors":["Mehar Bhatia","Shravan Nayak","Gaurav Kamath","Marius Mosbach","Karolina Stańczak","Vered Shwartz","Siva Reddy"],"pdf_url":"https://arxiv.org/pdf/2510.26707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26697v1","updated":"2025-10-30T17:01:43Z","published":"2025-10-30T17:01:43Z","title":"The End of Manual Decoding: Towards Truly End-to-End Language Models","summary":"  The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding.\n","authors":["Zhichao Wang","Dongyang Ma","Xinting Huang","Deng Cai","Tian Lan","Jiahao Xu","Haitao Mi","Xiaoying Tang","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26692v1","updated":"2025-10-30T16:59:43Z","published":"2025-10-30T16:59:43Z","title":"Kimi Linear: An Expressive, Efficient Attention Architecture","summary":"  We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.\n","authors":[" Kimi Team","Yu Zhang","Zongyu Lin","Xingcheng Yao","Jiaxi Hu","Fanqing Meng","Chengyin Liu","Xin Men","Songlin Yang","Zhiyuan Li","Wentao Li","Enzhe Lu","Weizhou Liu","Yanru Chen","Weixin Xu","Longhui Yu","Yejie Wang","Yu Fan","Longguang Zhong","Enming Yuan","Dehao Zhang","Yizhi Zhang","T. Y. Liu","Haiming Wang","Shengjun Fang","Weiran He","Shaowei Liu","Yiwei Li","Jianlin Su","Jiezhong Qiu","Bo Pang","Junjie Yan","Zhejun Jiang","Weixiao Huang","Bohong Yin","Jiacheng You","Chu Wei","Zhengtao Wang","Chao Hong","Yutian Chen","Guanduo Chen","Yucheng Wang","Huabin Zheng","Feng Wang","Yibo Liu","Mengnan Dong","Zheng Zhang","Siyuan Pan","Wenhao Wu","Yuhao Wu","Longyu Guan","Jiawen Tao","Guohong Fu","Xinran Xu","Yuzhi Wang","Guokun Lai","Yuxin Wu","Xinyu Zhou","Zhilin Yang","Yulun Du"],"pdf_url":"https://arxiv.org/pdf/2510.26692v1.pdf","comment":"Kimi Linear tech report"},{"id":"http://arxiv.org/abs/2510.26683v1","updated":"2025-10-30T16:53:45Z","published":"2025-10-30T16:53:45Z","title":"Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models","summary":"  Large language models (LLMs) have demonstrated exceptional capabilities\nacross multiple domains by leveraging massive pre-training and curated\nfine-tuning data. However, in data-sensitive fields such as healthcare, the\nlack of high-quality, domain-specific training corpus hinders LLMs' adaptation\nfor specialized applications. Meanwhile, domain experts have distilled domain\nwisdom into ontology rules, which formalize relationships among concepts and\nensure the integrity of knowledge management repositories. Viewing LLMs as\nimplicit repositories of human knowledge, we propose Evontree, a novel\nframework that leverages a small set of high-quality ontology rules to\nsystematically extract, validate, and enhance domain knowledge within LLMs,\nwithout requiring extensive external datasets. Specifically, Evontree extracts\ndomain ontology from raw models, detects inconsistencies using two core\nontology rules, and reinforces the refined knowledge via self-distilled\nfine-tuning. Extensive experiments on medical QA benchmarks with\nLlama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both\nunmodified models and leading supervised baselines, achieving up to a 3.7%\nimprovement in accuracy. These results confirm the effectiveness, efficiency,\nand robustness of our approach for low-resource domain adaptation of LLMs.\n","authors":["Mingchen Tu","Zhiqiang Liu","Juan Li","Liangyurui Liu","Junjie Wang","Lei Liang","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21257v2","updated":"2025-10-30T16:25:15Z","published":"2025-07-28T18:20:41Z","title":"CompoST: A Benchmark for Analyzing the Ability of LLMs To\n  Compositionally Interpret Questions in a QALD Setting","summary":"  Language interpretation is a compositional process, in which the meaning of\nmore complex linguistic structures is inferred from the meaning of their parts.\nLarge language models possess remarkable language interpretation capabilities\nand have been successfully applied to interpret questions by mapping them to\nSPARQL queries. An open question is how systematic this interpretation process\nis. Toward this question, in this paper, we propose a benchmark for\ninvestigating to what extent the abilities of LLMs to interpret questions are\nactually compositional. For this, we generate three datasets of varying\ndifficulty based on graph patterns in DBpedia, relying on Lemon lexica for\nverbalization. Our datasets are created in a very controlled fashion in order\nto test the ability of LLMs to interpret structurally complex questions, given\nthat they have seen the atomic building blocks. This allows us to evaluate to\nwhat degree LLMs are able to interpret complex questions for which they\n\"understand\" the atomic parts. We conduct experiments with models of different\nsizes using both various prompt and few-shot optimization techniques as well as\nfine-tuning. Our results show that performance in terms of macro $F_1$ degrades\nfrom $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the\nsamples optimized on. Even when all necessary information was provided to the\nmodel in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of\nlowest complexity. We thus conclude that LLMs struggle to systematically and\ncompositionally interpret questions and map them into SPARQL queries.\n","authors":["David Maria Schmidt","Raoul Schubert","Philipp Cimiano"],"pdf_url":"https://arxiv.org/pdf/2507.21257v2.pdf","comment":"Research Track, 24th International Semantic Web Conference (ISWC\n  2025), November 2-6, 2025, Nara, Japan"},{"id":"http://arxiv.org/abs/2510.26658v1","updated":"2025-10-30T16:25:10Z","published":"2025-10-30T16:25:10Z","title":"The Era of Agentic Organization: Learning to Organize with Language\n  Models","summary":"  We envision a new era of AI, termed agentic organization, where agents solve\ncomplex problems by working collaboratively and concurrently, enabling outcomes\nbeyond individual intelligence. To realize this vision, we introduce\nasynchronous thinking (AsyncThink) as a new paradigm of reasoning with large\nlanguage models, which organizes the internal thinking process into\nconcurrently executable structures. Specifically, we propose a thinking\nprotocol where an organizer dynamically assigns sub-queries to workers, merges\nintermediate knowledge, and produces coherent solutions. More importantly, the\nthinking structure in this protocol can be further optimized through\nreinforcement learning. Experiments demonstrate that AsyncThink achieves 28%\nlower inference latency compared to parallel thinking while improving accuracy\non mathematical reasoning. Moreover, AsyncThink generalizes its learned\nasynchronous thinking capabilities, effectively tackling unseen tasks without\nadditional training.\n","authors":["Zewen Chi","Li Dong","Qingxiu Dong","Yaru Hao","Xun Wu","Shaohan Huang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2510.26658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.15840v3","updated":"2025-10-30T16:25:05Z","published":"2025-08-19T17:34:25Z","title":"Unveiling Unicode's Unseen Underpinnings in Undermining Authorship\n  Attribution","summary":"  When using a public communication channel -- whether formal or informal, such\nas commenting or posting on social media -- end users have no expectation of\nprivacy: they compose a message and broadcast it for the world to see. Even if\nan end user takes utmost precautions to anonymize their online presence --\nusing an alias or pseudonym; masking their IP address; spoofing their\ngeolocation; concealing their operating system and user agent; deploying\nencryption; registering with a disposable phone number or email; disabling\nnon-essential settings; revoking permissions; and blocking cookies and\nfingerprinting -- one obvious element still lingers: the message itself.\nAssuming they avoid lapses in judgment or accidental self-exposure, there\nshould be little evidence to validate their actual identity, right? Wrong. The\ncontent of their message -- necessarily open for public consumption -- exposes\nan attack vector: stylometric analysis, or author profiling. In this paper, we\ndissect the technique of stylometry, discuss an antithetical counter-strategy\nin adversarial stylometry, and devise enhancements through Unicode\nsteganography.\n","authors":["Robert Dilworth"],"pdf_url":"https://arxiv.org/pdf/2508.15840v3.pdf","comment":"33 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.14889v2","updated":"2025-10-30T16:09:51Z","published":"2025-10-16T17:09:14Z","title":"Detecting Early and Implicit Suicidal Ideation via Longitudinal and\n  Information Environment Signals on Social Media","summary":"  On social media, many individuals experiencing suicidal ideation (SI) do not\ndisclose their distress explicitly. Instead, signs may surface indirectly\nthrough everyday posts or peer interactions. Detecting such implicit signals\nearly is critical but remains challenging. We frame early and implicit SI as a\nforward-looking prediction task and develop a computational framework that\nmodels a user's information environment, consisting of both their longitudinal\nposting histories as well as the discourse of their socially proximal peers. We\nadopted a composite network centrality measure to identify top neighbors of a\nuser, and temporally aligned the user's and neighbors' interactions --\nintegrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a\nReddit study of 1,000 (500 Case and 500 Control) users, our approach improves\nearly and implicit SI detection by 15% over individual-only baselines. These\nfindings highlight that peer interactions offer valuable predictive signals and\ncarry broader implications for designing early detection systems that capture\nindirect as well as masked expressions of risk in online environments.\n","authors":["Soorya Ram Shimgekar","Ruining Zhao","Agam Goyal","Violeta J. Rodriguez","Paul A. Bloom","Hari Sundaram","Koustuv Saha"],"pdf_url":"https://arxiv.org/pdf/2510.14889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26622v1","updated":"2025-10-30T15:48:28Z","published":"2025-10-30T15:48:28Z","title":"Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large\n  Language Model","summary":"  Recent large language model (LLM) research has undergone an architectural\nshift from encoder-decoder modeling to nowadays the dominant decoder-only\nmodeling. This rapid transition, however, comes without a rigorous comparative\nanalysis especially \\textit{from the scaling perspective}, raising concerns\nthat the potential of encoder-decoder models may have been overlooked. To fill\nthis gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent\nrecipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison\nbetween RedLLM, pretrained with prefix language modeling (LM), and DecLLM,\npretrained with causal LM, at different model scales, ranging from $\\sim$150M\nto $\\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for\ninstruction tuning, our experiments show that RedLLM produces compelling\nscaling properties and surprisingly strong performance. While DecLLM is overall\nmore compute-optimal during pretraining, RedLLM demonstrates comparable scaling\nand context length extrapolation capabilities. After instruction tuning, RedLLM\nachieves comparable and even better results on various downstream tasks while\nenjoying substantially better inference efficiency. We hope our findings could\ninspire more efforts on re-examining RedLLM, unlocking its potential for\ndeveloping powerful and efficient LLMs.\n","authors":["Biao Zhang","Yong Cheng","Siamak Shakeri","Xinyi Wang","Min Ma","Orhan Firat"],"pdf_url":"https://arxiv.org/pdf/2510.26622v1.pdf","comment":"The scaling study inspiring T5Gemma"},{"id":"http://arxiv.org/abs/2510.26615v1","updated":"2025-10-30T15:41:15Z","published":"2025-10-30T15:41:15Z","title":"SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual\n  Document Understanding","summary":"  Multi-page visual documents such as manuals, brochures, presentations, and\nposters convey key information through layout, colors, icons, and cross-slide\nreferences. While large language models (LLMs) offer opportunities in document\nunderstanding, current systems struggle with complex, multi-page visual\ndocuments, particularly in fine-grained reasoning over elements and pages. We\nintroduce SlideAgent, a versatile agentic framework for understanding\nmulti-modal, multi-page, and multi-layout documents, especially slide decks.\nSlideAgent employs specialized agents and decomposes reasoning into three\nspecialized levels-global, page, and element-to construct a structured,\nquery-agnostic representation that captures both overarching themes and\ndetailed visual or textual cues. During inference, SlideAgent selectively\nactivates specialized agents for multi-level reasoning and integrates their\noutputs into coherent, context-aware answers. Extensive experiments show that\nSlideAgent achieves significant improvement over both proprietary (+7.9\noverall) and open-source models (+9.8 overall).\n","authors":["Yiqiao Jin","Rachneet Kaur","Zhen Zeng","Sumitra Ganesh","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2510.26615v1.pdf","comment":"https://slideagent.github.io/"},{"id":"http://arxiv.org/abs/2510.26606v1","updated":"2025-10-30T15:35:13Z","published":"2025-10-30T15:35:13Z","title":"Normative Reasoning in Large Language Models: A Comparative Benchmark\n  from Logical and Modal Perspectives","summary":"  Normative reasoning is a type of reasoning that involves normative or deontic\nmodality, such as obligation and permission. While large language models (LLMs)\nhave demonstrated remarkable performance across various reasoning tasks, their\nability to handle normative reasoning remains underexplored. In this paper, we\nsystematically evaluate LLMs' reasoning capabilities in the normative domain\nfrom both logical and modal perspectives. Specifically, to assess how well LLMs\nreason with normative modals, we make a comparison between their reasoning with\nnormative modals and their reasoning with epistemic modals, which share a\ncommon formal structure. To this end, we introduce a new dataset covering a\nwide range of formal patterns of reasoning in both normative and epistemic\ndomains, while also incorporating non-formal cognitive factors that influence\nhuman reasoning. Our results indicate that, although LLMs generally adhere to\nvalid reasoning patterns, they exhibit notable inconsistencies in specific\ntypes of normative reasoning and display cognitive biases similar to those\nobserved in psychological studies of human reasoning. These findings highlight\nchallenges in achieving logical consistency in LLMs' normative reasoning and\nprovide insights for enhancing their reliability. All data and code are\nreleased publicly at https://github.com/kmineshima/NeuBAROCO.\n","authors":["Kentaro Ozeki","Risako Ando","Takanobu Morishita","Hirohiko Abe","Koji Mineshima","Mitsuhiro Okada"],"pdf_url":"https://arxiv.org/pdf/2510.26606v1.pdf","comment":"Accepted to the 8th BlackboxNLP Workshop at EMNLP 2025"},{"id":"http://arxiv.org/abs/2510.08604v2","updated":"2025-10-30T15:33:58Z","published":"2025-10-07T09:40:20Z","title":"LatentBreak: Jailbreaking Large Language Models through Latent Space\n  Feedback","summary":"  Jailbreaks are adversarial attacks designed to bypass the built-in safety\nmechanisms of large language models. Automated jailbreaks typically optimize an\nadversarial suffix or adapt long prompt templates by forcing the model to\ngenerate the initial part of a restricted or harmful response. In this work, we\nshow that existing jailbreak attacks that leverage such mechanisms to unlock\nthe model response can be detected by a straightforward perplexity-based\nfiltering on the input prompt. To overcome this issue, we propose LatentBreak,\na white-box jailbreak attack that generates natural adversarial prompts with\nlow perplexity capable of evading such defenses. LatentBreak substitutes words\nin the input prompt with semantically-equivalent ones, preserving the initial\nintent of the prompt, instead of adding high-perplexity adversarial suffixes or\nlong templates. These words are chosen by minimizing the distance in the latent\nspace between the representation of the adversarial prompt and that of harmless\nrequests. Our extensive evaluation shows that LatentBreak leads to shorter and\nlow-perplexity prompts, thus outperforming competing jailbreak algorithms\nagainst perplexity-based filters on multiple safety-aligned models.\n","authors":["Raffaele Mura","Giorgio Piras","Kamilė Lukošiūtė","Maura Pintor","Amin Karbasi","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2510.08604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.19756v2","updated":"2025-10-30T15:23:59Z","published":"2025-07-26T03:01:59Z","title":"Are You There God? Lightweight Narrative Annotation of Christian Fiction\n  with LMs","summary":"  In addition to its more widely studied cultural movements, American\nEvangelicalism has a well-developed but less externally visible literary side.\nChristian Fiction, however, has been little studied, and what scholarly\nattention there is has focused on the explosively popular Left Behind series.\nIn this work, we use computational tools to provide both a broad topical\noverview of Christian Fiction as a genre and a more directed exploration of how\nits authors depict divine acts. Working with human annotators, we first\ndeveloped a codebook for identifying \"acts of God.\" We then adapted the\ncodebook for use by a recent, lightweight LM with the assistance of a much\nlarger model. The laptop-scale LM is largely capable of matching human\nannotations, even when the task is subtle and challenging. Using these\nannotations, we show that significant and meaningful differences exist between\ndivine acts depicted by the Left Behind books and Christian Fiction more\nbroadly.\n","authors":["Rebecca M. M. Hicke","Brian W. Haggard","Mia Ferrante","Rayhan Khanna","David Mimno"],"pdf_url":"https://arxiv.org/pdf/2507.19756v2.pdf","comment":"Accepted to CHR 2025"},{"id":"http://arxiv.org/abs/2502.14409v2","updated":"2025-10-30T15:05:42Z","published":"2025-02-20T09:57:42Z","title":"Unstructured Evidence Attribution for Long Context Query Focused\n  Summarization","summary":"  Large language models (LLMs) are capable of generating coherent summaries\nfrom very long contexts given a user query, and extracting and citing evidence\nspans helps improve the trustworthiness of these summaries. Whereas previous\nwork has focused on evidence citation with fixed levels of granularity (e.g.\nsentence, paragraph, document, etc.), we propose to extract unstructured (i.e.,\nspans of any length) evidence in order to acquire more relevant and consistent\nevidence than in the fixed granularity case. We show how existing systems\nstruggle to copy and properly cite unstructured evidence, which also tends to\nbe \"lost-in-the-middle\". To help models perform this task, we create the\nSummaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset\ngenerated using a novel pipeline, which can be used as training supervision for\nunstructured evidence summarization. We demonstrate across 5 LLMs and 4\ndatasets spanning human written, synthetic, single, and multi-document settings\nthat LLMs adapted with SUnsET generate more relevant and factually consistent\nevidence with their summaries, extract evidence from more diverse locations in\ntheir context, and can generate more relevant and consistent summaries than\nbaselines with no fine-tuning and fixed granularity evidence. We release SUnsET\nand our generation code to the public.\n","authors":["Dustin Wright","Zain Muhammad Mujahid","Lu Wang","Isabelle Augenstein","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2502.14409v2.pdf","comment":"EMNLP 2025 Main; 29 pages; 24 figures; 8 tables"},{"id":"http://arxiv.org/abs/2510.26577v1","updated":"2025-10-30T15:04:36Z","published":"2025-10-30T15:04:36Z","title":"Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference\n  in Large Language Models","summary":"  Large Language Models (LLMs) face significant inference latency challenges\nstemming from their autoregressive design and large size. To address this,\nspeculative decoding emerges as a solution, enabling the simultaneous\ngeneration and validation of multiple tokens. While recent approaches like\nEAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,\nthey often neglect the impact of crucial system variables such as GPU devices\nand batch sizes.\n  Therefore, we introduce a new dynamic tree decoding approach called CAST that\ntakes into account inference costs, including factors such as GPU\nconfigurations and batch sizes, to dynamically refine the tree structure.\nThrough comprehensive experimentation across six diverse tasks and utilizing\nsix distinct LLMs, our methodology demonstrates remarkable results, achieving\nspeeds up to 5.2 times faster than conventional decoding methods. Moreover, it\ngenerally outperforms existing state-of-the-art techniques from 5% to 20%.\n","authors":["Yinrong Hong","Zhiquan Tan","Kai Hu"],"pdf_url":"https://arxiv.org/pdf/2510.26577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26575v1","updated":"2025-10-30T15:03:21Z","published":"2025-10-30T15:03:21Z","title":"InfoFlow: Reinforcing Search Agent Via Reward Density Optimization","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach\nfor enhancing agentic deep search. However, its application is often hindered\nby low \\textbf{Reward Density} in deep search scenarios, where agents expend\nsignificant exploratory costs for infrequent and often null final rewards. In\nthis paper, we formalize this challenge as the \\textbf{Reward Density\nOptimization} problem, which aims to improve the reward obtained per unit of\nexploration cost. This paper introduce \\textbf{InfoFlow}, a systematic\nframework that tackles this problem from three aspects. 1) \\textbf{Subproblem\ndecomposition}: breaking down long-range tasks to assign process rewards,\nthereby providing denser learning signals. 2) \\textbf{Failure-guided hints}:\ninjecting corrective guidance into stalled trajectories to increase the\nprobability of successful outcomes. 3) \\textbf{Dual-agent refinement}:\nemploying a dual-agent architecture to offload the cognitive burden of deep\nexploration. A refiner agent synthesizes the search history, which effectively\ncompresses the researcher's perceived trajectory, thereby reducing exploration\ncost and increasing the overall reward density. We evaluate InfoFlow on\nmultiple agentic search benchmarks, where it significantly outperforms strong\nbaselines, enabling lightweight LLMs to achieve performance comparable to\nadvanced proprietary LLMs.\n","authors":["Kun Luo","Hongjin Qian","Zheng Liu","Ziyi Xia","Shitao Xiao","Siqi Bao","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.04226v4","updated":"2025-10-30T14:52:48Z","published":"2025-10-05T14:29:15Z","title":"Epistemic Diversity and Knowledge Collapse in Large Language Models","summary":"  Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation\n","authors":["Dustin Wright","Sarah Masud","Jared Moore","Srishti Yadav","Maria Antoniak","Chan Young Park","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2510.04226v4.pdf","comment":"16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for\n  table 3, random effect is the model version; v3 changelog: Fixed minor\n  formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model\n  description"},{"id":"http://arxiv.org/abs/2504.11331v2","updated":"2025-10-30T14:50:58Z","published":"2025-04-15T16:05:09Z","title":"Dependency Structure Augmented Contextual Scoping Framework for\n  Multimodal Aspect-Based Sentiment Analysis","summary":"  Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract\nfine-grained information from image-text pairs to identify aspect terms and\ndetermine their sentiment polarity. However, existing approaches often fall\nshort in simultaneously addressing three core challenges: Sentiment Cue\nPerception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise\nElimination (SNE). To overcome these limitations, we propose DASCO\n(\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework),\na fine-grained scope-oriented framework that enhances aspect-level sentiment\nreasoning by leveraging dependency parsing trees. First, we designed a\nmulti-task pretraining strategy for MABSA on our base model, combining\naspect-oriented enhancement, image-text matching, and aspect-level\nsentiment-sensitive cognition. This improved the model's perception of aspect\nterms and sentiment cues while achieving effective image-text alignment,\naddressing key challenges like SCP and MIM. Furthermore, we incorporate\ndependency trees as syntactic branch combining with semantic branch, guiding\nthe model to selectively attend to critical contextual elements within a\ntarget-specific scope while effectively filtering out irrelevant noise for\naddressing SNE problem. Extensive experiments on two benchmark datasets across\nthree subtasks demonstrate that DASCO achieves state-of-the-art performance in\nMABSA, with notable gains in JMASA (+2.3\\% F1 and +3.5\\% precision on\nTwitter2015). The source code is available at https://github.com/LHaoooo/DASCO .\n","authors":["Hao Liu","Lijun He","Jiaxi Liang","Zhihan Ren","Haixia Bi","Fan Li"],"pdf_url":"https://arxiv.org/pdf/2504.11331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26543v1","updated":"2025-10-30T14:36:09Z","published":"2025-10-30T14:36:09Z","title":"The Structure of Relation Decoding Linear Operators in Large Language\n  Models","summary":"  This paper investigates the structure of linear operators introduced in\nHernandez et al. [2023] that decode specific relational facts in transformer\nlanguage models. We extend their single-relation findings to a collection of\nrelations and systematically chart their organization. We show that such\ncollections of relation decoders can be highly compressed by simple order-3\ntensor networks without significant loss in decoding accuracy. To explain this\nsurprising redundancy, we develop a cross-evaluation protocol, in which we\napply each linear decoder operator to the subjects of every other relation. Our\nresults reveal that these linear maps do not encode distinct relations, but\nextract recurring, coarse-grained semantic properties (e.g., country of capital\ncity and country of food are both in the country-of-X property). This\nproperty-centric structure clarifies both the operators' compressibility and\nhighlights why they generalize only to new relations that are semantically\nclose. Our findings thus interpret linear relational decoding in transformer\nlanguage models as primarily property-based, rather than relation-specific.\n","authors":["Miranda Anna Christ","Adrián Csiszárik","Gergely Becsó","Dániel Varga"],"pdf_url":"https://arxiv.org/pdf/2510.26543v1.pdf","comment":"NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.25682v2","updated":"2025-10-30T14:28:46Z","published":"2025-10-29T16:47:02Z","title":"PairUni: Pairwise Training for Unified Multimodal Language Models","summary":"  Unified vision-language models (UVLMs) must perform both understanding and\ngeneration within a single architecture, but these tasks rely on heterogeneous\ndata and supervision, making it difficult to balance them during reinforcement\nlearning (RL). We propose PairUni, a unified framework that reorganizes data\ninto understanding-generation (UG) pairs and aligns optimization accordingly.\nWe first use GPT-o3 to augment single-task data, generating captions for\nunderstanding samples and question-answer (QA) pairs for generation samples,\nforming aligned pairs from the same instance. Additionally, for each generation\nsample, we retrieve a semantically related understanding example to form a\nretrieved pair, linking different but related data points. These paired\nstructures expose cross-task semantic correspondences and support consistent\npolicy learning. To leverage this structure, we present Pair-GPRO, a pair-aware\nvariant based on Group Relative Policy Optimization. It assigns a similarity\nscore to each pair to modulate the advantage, strengthening learning from\nwell-aligned examples and reducing task interference. We curate a high-quality\ndataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on\nthe powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on\nvarious UVLMs, outperforming strong UVLM RL baselines. Codes are available at\nhttps://github.com/Haochen-Wang409/PairUni.\n","authors":["Jiani Zheng","Zhiyang Teng","Xiangtai Li","Anran Wang","Yu Tian","Kunpeng Qiu","Ye Tian","Haochen Wang","Zhuochen Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25682v2.pdf","comment":"21 pages, 11 figures, and 8 tables"},{"id":"http://arxiv.org/abs/2510.26521v1","updated":"2025-10-30T14:15:16Z","published":"2025-10-30T14:15:16Z","title":"Hebrew Diacritics Restoration using Visual Representation","summary":"  Diacritics restoration in Hebrew is a fundamental task for ensuring accurate\nword pronunciation and disambiguating textual meaning. Despite the language's\nhigh degree of ambiguity when unvocalized, recent machine learning approaches\nhave significantly advanced performance on this task.\n  In this work, we present DIVRIT, a novel system for Hebrew diacritization\nthat frames the task as a zero-shot classification problem. Our approach\noperates at the word level, selecting the most appropriate diacritization\npattern for each undiacritized word from a dynamically generated candidate set,\nconditioned on the surrounding textual context. A key innovation of DIVRIT is\nits use of a Hebrew Visual Language Model, which processes undiacritized text\nas an image, allowing diacritic information to be embedded directly within the\ninput's vector representation.\n  Through a comprehensive evaluation across various configurations, we\ndemonstrate that the system effectively performs diacritization without relying\non complex, explicit linguistic analysis. Notably, in an ``oracle'' setting\nwhere the correct diacritized form is guaranteed to be among the provided\ncandidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic\narchitectural enhancements and optimized training methodologies yield\nsignificant improvements in the system's overall generalization capabilities.\nThese findings highlight the promising potential of visual representations for\naccurate and automated Hebrew diacritization.\n","authors":["Yair Elboher","Yuval Pinter"],"pdf_url":"https://arxiv.org/pdf/2510.26521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26512v1","updated":"2025-10-30T14:05:55Z","published":"2025-10-30T14:05:55Z","title":"Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs","summary":"  Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts.\n","authors":["Dipak Meher","Carlotta Domeniconi"],"pdf_url":"https://arxiv.org/pdf/2510.26512v1.pdf","comment":"ICDM 2025 Workshop"},{"id":"http://arxiv.org/abs/2510.26498v1","updated":"2025-10-30T13:50:19Z","published":"2025-10-30T13:50:19Z","title":"A Multi-agent Large Language Model Framework to Automatically Assess\n  Performance of a Clinical AI Triage Tool","summary":"  Purpose: The purpose of this study was to determine if an ensemble of\nmultiple LLM agents could be used collectively to provide a more reliable\nassessment of a pixel-based AI triage tool than a single LLM.\n  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were\nprocessed by a commercial intracranial hemorrhage (ICH) AI detection tool.\nRadiology reports were analyzed by an ensemble of eight open-source LLM models\nand a HIPAA compliant internal version of GPT-4o using a single multi-shot\nprompt that assessed for presence of ICH. 1,726 examples were manually\nreviewed. Performance characteristics of the eight open-source models and\nconsensus were compared to GPT-4o. Three ideal consensus LLM ensembles were\ntested for rating the performance of the triage tool.\n  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The\nhighest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).\nThe average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).\nLlama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater\nprecision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the\nideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3\nEnsemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522\n(0.500-0.543). No statistically significant differences were observed between\nTop-3, Full-9, and Consensus (p > 0.05).\n  Conclusion: An ensemble of medium to large sized open-source LLMs provides a\nmore consistent and reliable method to derive a ground truth retrospective\nevaluation of a clinical AI triage tool over a single LLM alone.\n","authors":["Adam E. Flanders","Yifan Peng","Luciano Prevedello","Robyn Ball","Errol Colak","Prahlad Menon","George Shih","Hui-Ming Lin","Paras Lakhani"],"pdf_url":"https://arxiv.org/pdf/2510.26498v1.pdf","comment":"29 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2510.25623v2","updated":"2025-10-30T13:49:22Z","published":"2025-10-29T15:27:47Z","title":"Evaluating the Role of Verifiers in Test-Time Scaling for Legal\n  Reasoning Tasks","summary":"  Test-time scaling (TTS) techniques can improve the performance of large\nlanguage models (LLMs) at the expense of additional computation and latency.\nWhile TTS has proven effective in formal domains such as mathematics and\nprogramming, its value in argumentative domains such as law remains\nunderexplored. We present an empirical study of verifier-based TTS methods for\nlegal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7\nreward models, we evaluate both outcome-level (Best-of-$N$) and process-level\n(tree search) verification under realistic low-$N$ budgets. Our analysis\nsystematically investigates how verifier utility is affected by key properties\nsuch as domain specialization, model size, and supervision type\n(process-supervised PRMs vs. outcome-only ORMs), even when applied across\ndifferent roles.\n","authors":["Davide Romano","Jonathan Schwarz","Daniele Giofré"],"pdf_url":"https://arxiv.org/pdf/2510.25623v2.pdf","comment":"Accepted to EMNLP - NLLP Workshop"},{"id":"http://arxiv.org/abs/2503.00333v2","updated":"2025-10-30T13:47:16Z","published":"2025-03-01T03:45:35Z","title":"More of the Same: Persistent Representational Harms Under Increased\n  Representation","summary":"  To recognize and mitigate the harms of generative AI systems, it is crucial\nto consider who is represented in the outputs of generative AI systems and how\npeople are represented. A critical gap emerges when naively improving who is\nrepresented, as this does not imply bias mitigation efforts have been applied\nto address how people are represented. We critically examined this by\ninvestigating gender representation in occupation across state-of-the-art large\nlanguage models. We first show evidence suggesting that over time there have\nbeen interventions to models altering the resulting gender distribution, and we\nfind that women are more represented than men when models are prompted to\ngenerate biographies or personas. We then demonstrate that representational\nbiases persist in how different genders are represented by examining\nstatistically significant word differences across genders. This results in a\nproliferation of representational harms, stereotypes, and neoliberalism ideals\nthat, despite existing interventions to increase female representation,\nreinforce existing systems of oppression.\n","authors":["Jennifer Mickel","Maria De-Arteaga","Leqi Liu","Kevin Tian"],"pdf_url":"https://arxiv.org/pdf/2503.00333v2.pdf","comment":"Accepted by the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) as a poster paper; 39 pages, 7 figures, 15 tables"},{"id":"http://arxiv.org/abs/2510.26495v1","updated":"2025-10-30T13:44:22Z","published":"2025-10-30T13:44:22Z","title":"Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for\n  Real-world Database Exploration","summary":"  Recent advances in Text-to-SQL have achieved strong results in static,\nsingle-turn tasks, where models generate SQL queries from natural language\nquestions. However, these systems fall short in real-world interactive\nscenarios, where user intents evolve and queries must be refined over multiple\nturns. In applications such as finance and business analytics, users\niteratively adjust query constraints or dimensions based on intermediate\nresults. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a\nbenchmark assessing model performance under evolving user interactions. Unlike\nprevious manually curated datasets, DySQL-Bench is built through an automated\ntwo-stage pipeline of task synthesis and verification. Structured tree\nrepresentations derived from raw database tables guide LLM-based task\ngeneration, followed by interaction-oriented filtering and expert validation.\nHuman evaluation confirms 100% correctness of the synthesized data. We further\npropose a multi-turn evaluation framework simulating realistic interactions\namong an LLM-simulated user, the model under test, and an executable database.\nThe model must adapt its reasoning and SQL generation as user intents change.\nDySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling\n1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the\nPass@5 metric, underscoring the benchmark's difficulty. All code and data are\nreleased at https://github.com/Aurora-slz/Real-World-SQL-Bench .\n","authors":["Linzhuang Sun","Tianyu Guo","Hao Liang","Yuying Li","Qifeng Cai","Jingxuan Wei","Bihui Yu","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2510.26495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26493v1","updated":"2025-10-30T13:43:10Z","published":"2025-10-30T13:43:10Z","title":"Context Engineering 2.0: The Context of Context Engineering","summary":"  Karl Marx once wrote that ``the human essence is the ensemble of social\nrelations'', suggesting that individuals are not isolated entities but are\nfundamentally shaped by their interactions with other entities, within which\ncontexts play a constitutive and essential role. With the advent of computers\nand artificial intelligence, these contexts are no longer limited to purely\nhuman--human interactions: human--machine interactions are included as well.\nThen a central question emerges: How can machines better understand our\nsituations and purposes? To address this challenge, researchers have recently\nintroduced the concept of context engineering. Although it is often regarded as\na recent innovation of the agent era, we argue that related practices can be\ntraced back more than twenty years. Since the early 1990s, the field has\nevolved through distinct historical phases, each shaped by the intelligence\nlevel of machines: from early human--computer interaction frameworks built\naround primitive computers, to today's human--agent interaction paradigms\ndriven by intelligent agents, and potentially to human--level or superhuman\nintelligence in the future. In this paper, we situate context engineering,\nprovide a systematic definition, outline its historical and conceptual\nlandscape, and examine key design considerations for practice. By addressing\nthese questions, we aim to offer a conceptual foundation for context\nengineering and sketch its promising future. This paper is a stepping stone for\na broader community effort toward systematic context engineering in AI systems.\n","authors":["Qishuo Hua","Lyumanshan Ye","Dayuan Fu","Yang Xiao","Xiaojie Cai","Yunze Wu","Jifan Lin","Junfei Wang","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26484v1","updated":"2025-10-30T13:37:58Z","published":"2025-10-30T13:37:58Z","title":"Bayesian Network Fusion of Large Language Models for Sentiment Analysis","summary":"  Large language models (LLMs) continue to advance, with an increasing number\nof domain-specific variants tailored for specialised tasks. However, these\nmodels often lack transparency and explainability, can be costly to fine-tune,\nrequire substantial prompt engineering, yield inconsistent results across\ndomains, and impose significant adverse environmental impact due to their high\ncomputational demands. To address these challenges, we propose the Bayesian\nnetwork LLM fusion (BNLF) framework, which integrates predictions from three\nLLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic\nmechanism for sentiment analysis. BNLF performs late fusion by modelling the\nsentiment predictions from multiple LLMs as probabilistic nodes within a\nBayesian network. Evaluated across three human-annotated financial corpora with\ndistinct linguistic and contextual characteristics, BNLF demonstrates\nconsistent gains of about six percent in accuracy over the baseline LLMs,\nunderscoring its robustness to dataset variability and the effectiveness of\nprobabilistic fusion for interpretable sentiment classification.\n","authors":["Rasoul Amirzadeh","Dhananjay Thiruvady","Fatemeh Shiri"],"pdf_url":"https://arxiv.org/pdf/2510.26484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12371v2","updated":"2025-10-30T13:27:07Z","published":"2025-05-18T11:28:17Z","title":"MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional\n  Methods for Diverse Medical Tasks","summary":"  The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/.\n","authors":["Yinghao Zhu","Ziyi He","Haoran Hu","Xiaochen Zheng","Xichen Zhang","Zixiang Wang","Junyi Gao","Liantao Ma","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2505.12371v2.pdf","comment":"Accepted by NeurIPS 2025 Datasets & Benchmarks Track"},{"id":"http://arxiv.org/abs/2510.26474v1","updated":"2025-10-30T13:26:58Z","published":"2025-10-30T13:26:58Z","title":"Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing","summary":"  Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.\n","authors":["Xin Guo","Zhiheng Xi","Yiwen Ding","Yitao Zhai","Xiaowei Shi","Xunliang Cai","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26474v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2510.26457v1","updated":"2025-10-30T13:06:11Z","published":"2025-10-30T13:06:11Z","title":"SecureReviewer: Enhancing Large Language Models for Secure Code Review\n  through Secure-aware Fine-tuning","summary":"  Identifying and addressing security issues during the early phase of the\ndevelopment lifecycle is critical for mitigating the long-term negative impacts\non software systems. Code review serves as an effective practice that enables\ndevelopers to check their teammates' code before integration into the codebase.\nTo streamline the generation of review comments, various automated code review\napproaches have been proposed, where LLM-based methods have significantly\nadvanced the capabilities of automated review generation. However, existing\nmodels primarily focus on general-purpose code review, their effectiveness in\nidentifying and addressing security-related issues remains underexplored.\nMoreover, adapting existing code review approaches to target security issues\nfaces substantial challenges, including data scarcity and inadequate evaluation\nmetrics. To address these limitations, we propose SecureReviewer, a new\napproach designed for enhancing LLMs' ability to identify and resolve\nsecurity-related issues during code review. Specifically, we first construct a\ndataset tailored for training and evaluating secure code review capabilities.\nLeveraging this dataset, we fine-tune LLMs to generate code review comments\nthat can effectively identify security issues and provide fix suggestions with\nour proposed secure-aware fine-tuning strategy. To mitigate hallucination in\nLLMs and enhance the reliability of their outputs, we integrate the RAG\ntechnique, which grounds the generated comments in domain-specific security\nknowledge. Additionally, we introduce SecureBLEU, a new evaluation metric\ndesigned to assess the effectiveness of review comments in addressing security\nissues. Experimental results demonstrate that SecureReviewer outperforms\nstate-of-the-art baselines in both security issue detection accuracy and the\noverall quality and practical utility of generated review comments.\n","authors":["Fang Liu","Simiao Liu","Yinghao Zhu","Xiaoli Lian","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26457v1.pdf","comment":"Accepted by ICSE 2026. Code and data:\n  https://github.com/SIMIAO515/SecureReviewer"},{"id":"http://arxiv.org/abs/2510.21513v2","updated":"2025-10-30T13:03:25Z","published":"2025-10-24T14:39:23Z","title":"Wisdom and Delusion of LLM Ensembles for Code Generation and Repair","summary":"  Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems. To address this gap, we\nempirically compare ten individual LLMs from five families, and three ensembles\nof these LLMs across three software engineering benchmarks covering code\ngeneration and program repair. We assess the complementarity between models and\nthe performance gap between the best individual model and the ensembles. Next,\nwe evaluate various selection heuristics to identify correct solutions from an\nensemble's candidate pool. We find that the theoretical upperbound for an\nensemble's performance can be 83% above the best single model. Our results show\nthat consensus-based strategies for selecting solutions fall into a \"popularity\ntrap,\" amplifying common but incorrect outputs. In contrast, a diversity-based\nstrategy realizes up to 95% of this theoretical potential, and proves effective\neven in small two-model ensembles, enabling a cost-efficient way to enhance\nperformance by leveraging multiple LLMs.\n","authors":["Fernando Vallecillos-Ruiz","Max Hort","Leon Moonen"],"pdf_url":"https://arxiv.org/pdf/2510.21513v2.pdf","comment":"Added Acknowledgments section and hyphenated last names"},{"id":"http://arxiv.org/abs/2510.26446v1","updated":"2025-10-30T12:50:30Z","published":"2025-10-30T12:50:30Z","title":"1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large\n  Language Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable proficiency in\nlanguage comprehension and generation; however, their widespread adoption is\nconstrained by substantial bandwidth and computational demands. While pruning\nand low-rank approximation have each demonstrated promising performance\nindividually, their synergy for LLMs remains underexplored. We introduce\n\\underline{S}ynergistic \\underline{S}parse and \\underline{L}ow-Rank\n\\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths\nof both techniques: low-rank approximation compresses the model by retaining\nits essential structure with minimal information loss, whereas sparse\noptimization eliminates non-essential weights, preserving those crucial for\ngeneralization. Based on theoretical analysis, we first formulate the low-rank\napproximation and sparse optimization as a unified problem and solve it by\niterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models\n(7B-70B) show that SSLC, without any additional training steps, consistently\nsurpasses standalone methods, achieving state-of-the-arts results. Notably,\nSSLC compresses Qwen2.5 by 50\\% with no performance drop and achieves at least\n1.63$\\times$ speedup, offering a practical solution for efficient LLM\ndeployment.\n","authors":["Zeliang Zong","Kai Zhang","Zheyang Li","Wenming Tan","Ye Ren","Yiyan Zhai","Jilin Hu"],"pdf_url":"https://arxiv.org/pdf/2510.26446v1.pdf","comment":"15 pages, 6 figures, EMNLP 2025 findings"},{"id":"http://arxiv.org/abs/2510.26423v1","updated":"2025-10-30T12:20:25Z","published":"2025-10-30T12:20:25Z","title":"Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis","summary":"  Test oracle generation in non-regression testing is a longstanding challenge\nin software engineering, where the goal is to produce oracles that can\naccurately determine whether a function under test (FUT) behaves as intended\nfor a given input. In this paper, we introduce Nexus, a novel multi-agent\nframework to address this challenge. Nexus generates test oracles by leveraging\na diverse set of specialized agents that synthesize test oracles through a\nstructured process of deliberation, validation, and iterative self-refinement.\nDuring the deliberation phase, a panel of four specialist agents, each\nembodying a distinct testing philosophy, collaboratively critiques and refines\nan initial set of test oracles. Then, in the validation phase, Nexus generates\na plausible candidate implementation of the FUT and executes the proposed\noracles against it in a secure sandbox. For any oracle that fails this\nexecution-based check, Nexus activates an automated selfrefinement loop, using\nthe specific runtime error to debug and correct the oracle before\nre-validation. Our extensive evaluation on seven diverse benchmarks\ndemonstrates that Nexus consistently and substantially outperforms\nstate-of-theart baselines. For instance, Nexus improves the test-level oracle\naccuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The\nimproved accuracy also significantly enhances downstream tasks: the bug\ndetection rate of GPT4.1-Mini generated test oracles on HumanEval increases\nfrom 90.91% to 95.45% for Nexus compared to baselines, and the success rate of\nautomated program repair improves from 35.23% to 69.32%.\n","authors":["Dong Huang","Mingzhe Du","Jie M. Zhang","Zheng Lin","Meng Luo","Qianru Zhang","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2510.26423v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2510.26422v1","updated":"2025-10-30T12:16:29Z","published":"2025-10-30T12:16:29Z","title":"OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large\n  Language Models in Education","summary":"  With the rapid development of large language models (LLMs), various LLM-based\nworks have been widely applied in educational fields. However, most existing\nLLMs and their benchmarks focus primarily on the knowledge dimension, largely\nneglecting the evaluation of cultivation capabilities that are essential for\nreal-world educational scenarios. Additionally, current benchmarks are often\nlimited to a single subject or question type, lacking sufficient diversity.\nThis issue is particularly prominent within the Chinese context. To address\nthis gap, we introduce OmniEduBench, a comprehensive Chinese educational\nbenchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.\nThe data is meticulously divided into two core dimensions: the knowledge\ndimension and the cultivation dimension, which contain 18.121K and 6.481K\nentries, respectively. Each dimension is further subdivided into 6 fine-grained\ncategories, covering a total of 61 different subjects (41 in the knowledge and\n20 in the cultivation). Furthermore, the dataset features a rich variety of\nquestion formats, including 11 common exam question types, providing a solid\nfoundation for comprehensively evaluating LLMs' capabilities in education.\nExtensive experiments on 11 mainstream open-source and closed-source LLMs\nreveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro\nsurpassed 60\\% accuracy, while in the cultivation dimension, the\nbest-performing model, QWQ, still trailed human intelligence by nearly 30\\%.\nThese results highlight the substantial room for improvement and underscore the\nchallenges of applying LLMs in education.\n","authors":["Min Zhang","Hao Chen","Hao Chen","Wenqi Zhang","Didi Zhu","Xin Lin","Bo Jiang","Aimin Zhou","Fei Wu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2510.26422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25536v2","updated":"2025-10-30T11:19:24Z","published":"2025-10-29T14:00:42Z","title":"TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM\n  Persona Simulation","summary":"  Large Language Models (LLMs) are exhibiting emergent human-like abilities and\nare increasingly envisioned as the foundation for simulating an individual's\ncommunication style, behavioral tendencies, and personality traits. However,\ncurrent evaluations of LLM-based persona simulation remain limited: most rely\non synthetic dialogues, lack systematic frameworks, and lack analysis of the\ncapability requirement. To address these limitations, we introduce TwinVoice, a\ncomprehensive benchmark for assessing persona simulation across diverse\nreal-world contexts. TwinVoice encompasses three dimensions: Social Persona\n(public social interactions), Interpersonal Persona (private dialogues), and\nNarrative Persona (role-based expression). It further decomposes the evaluation\nof LLM performance into six fundamental capabilities, including opinion\nconsistency, memory recall, logical reasoning, lexical fidelity, persona tone,\nand syntactic style. Experimental results reveal that while advanced models\nachieve moderate accuracy in persona simulation, they still fall short of\ncapabilities such as syntactic style and memory recall. Consequently, the\naverage performance achieved by LLMs remains considerably below the human\nbaseline.\n","authors":["Bangde Du","Minghao Guo","Songming He","Ziyi Ye","Xi Zhu","Weihang Su","Shuqi Zhu","Yujia Zhou","Yongfeng Zhang","Qingyao Ai","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25536v2.pdf","comment":"Main paper: 11 pages, 3 figures, 6 tables. Appendix: 28 pages. Bangde\n  Du and Minghao Guo contributed equally. Corresponding authors: Ziyi Ye\n  (ziyiye@fudan.edu.cn), Qingyao Ai (aiqy@tsinghua.edu.cn)"},{"id":"http://arxiv.org/abs/2510.24592v2","updated":"2025-10-30T11:15:27Z","published":"2025-10-28T16:22:54Z","title":"ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization","summary":"  Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 22.6 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.\n","authors":["Guoxin Chen","Jing Wu","Xinjie Chen","Wayne Xin Zhao","Ruihua Song","Chengxi Li","Kai Fan","Dayiheng Liu","Minpeng Liao"],"pdf_url":"https://arxiv.org/pdf/2510.24592v2.pdf","comment":"https://github.com/Chen-GX/ReForm"},{"id":"http://arxiv.org/abs/2510.24817v2","updated":"2025-10-30T11:13:33Z","published":"2025-10-28T10:06:49Z","title":"Towards a Method for Synthetic Generation of Persons with Aphasia\n  Transcripts","summary":"  In aphasia research, Speech-Language Pathologists (SLPs) devote extensive\ntime to manually coding speech samples using Correct Information Units (CIUs),\na measure of how informative an individual sample of speech is. Developing\nautomated systems to recognize aphasic language is limited by data scarcity.\nFor example, only about 600 transcripts are available in AphasiaBank yet\nbillions of tokens are used to train large language models (LLMs). In the\nbroader field of machine learning (ML), researchers increasingly turn to\nsynthetic data when such are sparse. Therefore, this study constructs and\nvalidates two methods to generate synthetic transcripts of the AphasiaBank Cat\nRescue picture description task. One method leverages a procedural programming\napproach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct\nLLMs. The methods generate transcripts across four severity levels (Mild,\nModerate, Severe, Very Severe) through word dropping, filler insertion, and\nparaphasia substitution. Overall, we found, compared to human-elicited\ntranscripts, Mistral 7b Instruct best captures key aspects of linguistic\ndegradation observed in aphasia, showing realistic directional changes in NDW,\nword count, and word length amongst the synthetic generation methods. Based on\nthe results, future work should plan to create a larger dataset, fine-tune\nmodels for better aphasic representation, and have SLPs assess the realism and\nusefulness of the synthetic transcripts.\n","authors":["Jason M. Pittman","Anton Phillips Jr.","Yesenia Medina-Santos","Brielle C. Stark"],"pdf_url":"https://arxiv.org/pdf/2510.24817v2.pdf","comment":"19 pages, 1 figure, 7 tables"},{"id":"http://arxiv.org/abs/2510.26354v1","updated":"2025-10-30T11:05:36Z","published":"2025-10-30T11:05:36Z","title":"On the Role of Context for Discourse Relation Classification in\n  Scientific Writing","summary":"  With the increasing use of generative Artificial Intelligence (AI) methods to\nsupport science workflows, we are interested in the use of discourse-level\ninformation to find supporting evidence for AI generated scientific claims. A\nfirst step towards this objective is to examine the task of inferring discourse\nstructure in scientific writing.\n  In this work, we present a preliminary investigation of pretrained language\nmodel (PLM) and Large Language Model (LLM) approaches for Discourse Relation\nClassification (DRC), focusing on scientific publications, an under-studied\ngenre for this task. We examine how context can help with the DRC task, with\nour experiments showing that context, as defined by discourse structure, is\ngenerally helpful. We also present an analysis of which scientific discourse\nrelation types might benefit most from context.\n","authors":["Stephen Wan","Wei Liu","Michael Strube"],"pdf_url":"https://arxiv.org/pdf/2510.26354v1.pdf","comment":"Accepted at Joint Sixth Workshop on Computational Approaches to\n  Discourse, Context and Document-Level Inferences (CODI 2025) and Eighth\n  Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC\n  2025)"},{"id":"http://arxiv.org/abs/2510.26352v1","updated":"2025-10-30T11:04:15Z","published":"2025-10-30T11:04:15Z","title":"The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic\n  Teams for Multi-Agent Collaboration","summary":"  While a multi-agent approach based on large language models (LLMs) represents\na promising strategy to surpass the capabilities of single models, its success\nis critically dependent on synergistic team composition. However, forming\noptimal teams is a significant challenge, as the inherent opacity of most\nmodels obscures the internal characteristics necessary for effective\ncollaboration. In this paper, we propose an interaction-centric framework for\nautomatic team composition that does not require any prior knowledge including\ntheir internal architectures, training data, or task performances. Our method\nconstructs a \"language model graph\" that maps relationships between models from\nthe semantic coherence of pairwise conversations, and then applies community\ndetection to identify synergistic model clusters. Our experiments with diverse\nLLMs demonstrate that the proposed method discovers functionally coherent\ngroups that reflect their latent specializations. Priming conversations with\nspecific topics identified synergistic teams which outperform random baselines\non downstream benchmarks and achieve comparable accuracy to that of\nmanually-curated teams based on known model specializations. Our findings\nprovide a new basis for the automated design of collaborative multi-agent LLM\nteams.\n","authors":["Kotaro Furuya","Yuichi Kitagawa"],"pdf_url":"https://arxiv.org/pdf/2510.26352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26345v1","updated":"2025-10-30T10:52:43Z","published":"2025-10-30T10:52:43Z","title":"MisSynth: Improving MISSCI Logical Fallacies Classification with\n  Synthetic Data","summary":"  Health-related misinformation is very prevalent and potentially harmful. It\nis difficult to identify, especially when claims distort or misinterpret\nscientific findings. We investigate the impact of synthetic data generation and\nlightweight fine-tuning techniques on the ability of large language models\n(LLMs) to recognize fallacious arguments using the MISSCI dataset and\nframework. In this work, we propose MisSynth, a pipeline that applies\nretrieval-augmented generation (RAG) to produce synthetic fallacy samples,\nwhich are then used to fine-tune an LLM model. Our results show substantial\naccuracy gains with fine-tuned models compared to vanilla baselines. For\ninstance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score\nabsolute improvement on the MISSCI test split over its vanilla baseline. We\ndemonstrate that introducing synthetic fallacy data to augment limited\nannotated resources can significantly enhance zero-shot LLM classification\nperformance on real-world scientific misinformation tasks, even with limited\ncomputational resources. The code and synthetic dataset are available on\nhttps://github.com/mxpoliakov/MisSynth.\n","authors":["Mykhailo Poliakov","Nadiya Shvai"],"pdf_url":"https://arxiv.org/pdf/2510.26345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21497v2","updated":"2025-10-30T10:49:28Z","published":"2025-05-27T17:58:49Z","title":"Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers","summary":"  Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster.\n","authors":["Wei Pang","Kevin Qinghong Lin","Xiangru Jian","Xi He","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2505.21497v2.pdf","comment":"Project Page: https://github.com/Paper2Poster/Paper2Poster"},{"id":"http://arxiv.org/abs/2510.25409v2","updated":"2025-10-30T10:48:05Z","published":"2025-10-29T11:27:08Z","title":"BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains","summary":"  The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research.\n","authors":["Vijay Devane","Mohd Nauman","Bhargav Patel","Aniket Mahendra Wakchoure","Yogeshkumar Sant","Shyam Pawar","Viraj Thakur","Ananya Godse","Sunil Patra","Neha Maurya","Suraj Racha","Nitish Kamal Singh","Ajay Nagpal","Piyush Sawarkar","Kundeshwar Vijayrao Pundalik","Rohit Saluja","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2510.25409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26336v1","updated":"2025-10-30T10:43:40Z","published":"2025-10-30T10:43:40Z","title":"From Amateur to Master: Infusing Knowledge into LLMs via Automated\n  Curriculum Learning","summary":"  Large Language Models (LLMs) excel at general tasks but underperform in\nspecialized domains like economics and psychology, which require deep,\nprincipled understanding. To address this, we introduce ACER (Automated\nCurriculum-Enhanced Regimen) that transforms generalist models into domain\nexperts without sacrificing their broad capabilities. ACER first synthesizes a\ncomprehensive, textbook-style curriculum by generating a table of contents for\na subject and then creating question-answer (QA) pairs guided by Bloom's\ntaxonomy. This ensures systematic topic coverage and progressively increasing\ndifficulty. The resulting synthetic corpus is used for continual pretraining\nwith an interleaved curriculum schedule, aligning learning across both content\nand cognitive dimensions.\n  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized\nMMLU subsets. In challenging domains like microeconomics, where baselines\nstruggle, ACER boosts accuracy by 5 percentage points. Across all target\ndomains, we observe a consistent macro-average improvement of 3 percentage\npoints. Notably, ACER not only prevents catastrophic forgetting but also\nfacilitates positive cross-domain knowledge transfer, improving performance on\nnon-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on\nknowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,\nwhile maintaining stable performance on general reasoning tasks. Our results\ndemonstrate that ACER offers a scalable and effective recipe for closing\ncritical domain gaps in LLMs.\n","authors":["Nishit Neema","Srinjoy Mukherjee","Sapan Shah","Gokul Ramakrishnan","Ganesh Venkatesh"],"pdf_url":"https://arxiv.org/pdf/2510.26336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09499v3","updated":"2025-10-30T10:21:42Z","published":"2025-03-12T16:03:03Z","title":"MindGYM: What Matters in Question Synthesis for Thinking-Centric\n  Fine-Tuning?","summary":"  Large foundation models face challenges in acquiring transferable, structured\nthinking abilities, especially when supervised with rigid templates or\ncrowd-annotated instruction datasets. Unlike prior approaches, we focus on a\nthinking-centric data synthesis paradigm that enables models to evolve through\nself-generated, cognitively guided data. We propose MindGYM, a structured and\nscalable framework for question synthesis, composed of: (1) Cognitive Thinking\nProcess Injection, which infuses high-level reasoning objectives to shape the\nmodel's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating\natomic questions from diverse semantic types to encourage broader thinking; and\n(3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop\nquestions based on QA seeds for deeper reasoning. Detailed analysis shows that\nsynthetic data generated by our method achieves 16.7% higher average quality\nand 67.91% lower quality variance compared to baseline sources, highlighting\nthat both high-quality and self-contained data are essential for effective,\nthinking-oriented fine-tuning. MindGYM improves performance on six reasoning\nbenchmarks, achieving gains of up to 16% on MathVision using only 400 data\nsamples, and generalizable improvements across different model sizes and\narchitectures. MindGYM underscores the viability of self-challenging mechanisms\nin refining large model capabilities while minimizing human intervention and\nresource demands. Code and data are released to promote data-centric research\ninto self-evolving foundation models driven by their internal reasoning\ncapabilities.\n","authors":["Zhe Xu","Daoyuan Chen","Zhenqing Ling","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2503.09499v3.pdf","comment":"Accepted by NeurIPS'25. 30 pages, 2 figures, 13 tables"},{"id":"http://arxiv.org/abs/2510.26322v1","updated":"2025-10-30T10:17:05Z","published":"2025-10-30T10:17:05Z","title":"SCRIBE: Structured Chain Reasoning for Interactive Behaviour\n  Explanations using Tool Calling","summary":"  Language models can be used to provide interactive, personalized student\nfeedback in educational settings. However, real-world deployment faces three\nkey challenges: privacy concerns, limited computational resources, and the need\nfor pedagogically valid responses. These constraints require small, open-source\nmodels that can run locally and reliably ground their outputs in correct\ninformation. We introduce SCRIBE, a framework for multi-hop, tool-augmented\nreasoning designed to generate valid responses to student questions about\nfeedback reports. SCRIBE combines domain-specific tools with a self-reflective\ninference pipeline that supports iterative reasoning, tool use, and error\nrecovery. We distil these capabilities into 3B and 8B models via two-stage LoRA\nfine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned\nGPT-Judge and a user study with 108 students shows that 8B-SCRIBE models\nachieve comparable or superior quality to much larger models in key dimensions\nsuch as relevance and actionability, while being perceived on par with GPT-4o\nand Llama-3.3 70B by students. These findings demonstrate the viability of\nSCRIBE for low-resource, privacy-sensitive educational applications.\n","authors":["Fares Fawzi","Vinitra Swamy","Dominik Glandorf","Tanya Nazaretsky","Tanja Käser"],"pdf_url":"https://arxiv.org/pdf/2510.26322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18915v3","updated":"2025-10-30T10:00:05Z","published":"2025-10-21T06:14:40Z","title":"UNO-Bench: A Unified Benchmark for Exploring the Compositional Law\n  Between Uni-modal and Omni-modal in Omni Models","summary":"  Multimodal Large Languages models have been progressing from uni-modal\nunderstanding toward unifying visual, audio and language modalities,\ncollectively termed omni models. However, the correlation between uni-modal and\nomni-modal remains unclear, which requires comprehensive evaluation to drive\nomni model's intelligence evolution. In this work, we introduce a novel,\nhigh-quality, and UNified Omni model benchmark, UNO-Bench. This benchmark is\ndesigned to effectively evaluate both UNi-modal and Omni-modal capabilities\nunder a unified ability taxonomy, spanning 44 task types and 5 modality\ncombinations. It includes 1250 human curated samples for omni-modal with 98%\ncross-modality solvability, and 2480 enhanced uni-modal samples. The\nhuman-generated dataset is well-suited to real-world scenarios, particularly\nwithin the Chinese context, whereas the automatically compressed dataset offers\na 90% increase in speed and maintains 98% consistency across 18 public\nbenchmarks. In addition to traditional multi-choice questions, we propose an\ninnovative multi-step open-ended question format to assess complex reasoning. A\ngeneral scoring model is incorporated, supporting 6 question types for\nautomated evaluation with 95% accuracy. Experimental result shows the\nCompositional Law between omni-modal and uni-modal performance and the\nomni-modal capability manifests as a bottleneck effect on weak models, while\nexhibiting synergistic promotion on strong models.\n","authors":["Chen Chen","ZeYang Hu","Fengjiao Chen","Liya Ma","Jiaxing Liu","Xiaoyu Li","Ziwen Wang","Xuezhi Cao","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2510.18915v3.pdf","comment":"v3: Switch the paper template. Work in progress. Github:\n  https://github.com/meituan-longcat/UNO-Bench Hugging Face:\n  https://huggingface.co/datasets/meituan-longcat/UNO-Bench"},{"id":"http://arxiv.org/abs/2507.16271v2","updated":"2025-10-30T09:57:54Z","published":"2025-07-22T06:37:51Z","title":"Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep\n  Knowledge Extraction","summary":"  With the emergence of large language models (LLMs), there is an expectation\nthat LLMs can effectively extract explicit information from complex real-world\ndocuments (e.g., papers, reports). However, most LLMs generate paragraph-style\nanswers that are chaotic, disorganized, and untraceable. To bridge this gap, we\nintroduce the Arranged and Organized Extraction Benchmark (AOE), a new\nbilingual benchmark with data and documents of varying lengths designed to\nsystematically evaluate the ability of LLMs to comprehend fragmented documents\nand reconstruct isolated information into one organized table. Unlike\nconventional text-to-table tasks, which rely on fixed schema and narrow task\ndomains, AOE includes 11 carefully crafted tasks across three diverse domains,\nrequiring models to generate context-specific schema tailored to varied input\nqueries. In the experiment, we evaluated both open-source and closed-source\nstate-of-the-art LLMs. The results show that even the most advanced models\nstruggled significantly. The benchmark is available at\nhttps://anonymous.4open.science/r/AOE-Benchmark/.\n","authors":["Tianyun Zhong","Guozhao Mo","Yanjiang Liu","Yihan Chen","Lingdi Kong","Xuanang Chen","Yaojie Lu","Hongyu Lin","Shiwei Ye","Xianpei Han","Ben He","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2507.16271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08410v2","updated":"2025-10-30T09:54:22Z","published":"2025-06-10T03:30:10Z","title":"Large Language Models Have Intrinsic Meta-Cognition, but Need a Good\n  Lens","summary":"  Previous research has primarily focused on the cognitive error detection\ncapabilities of Large Language Models (LLMs), often prompting them to analyze\nmistakes in reasoning chains. However, few studies have examined the\nmeta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),\nwhich are crucial for their reliability. While studies on LLM self-evaluation\npresent some measures, such as perplexity, which can reflect the answer\ncorrectness and be viewed as the lens of meta-cognition, they lack step-level\nanalysis and adaptation. This paper studies the evaluation of LLM\nmeta-cognition using the current lenses and how to improve these lenses.\nSpecifically, we propose AutoMeco, an Automated Meta-cognition Evaluation\nframework for benchmarking the existing lenses. Furthermore, a training-free\nMarkovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost\ncurrent meta-cognition lenses. Experimental results on three mathematical\nreasoning datasets and three LLMs show the reasonableness of AutoMeco by\ncomparing it with Best-of-N verification. Moreover, the meta-cognition ability\nof LLMs can be better evaluated using MIRA.\n","authors":["Ziyang Ma","Qingyue Yuan","Zhenglin Wang","Deyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.08410v2.pdf","comment":"Accepted to EMNLP 2025"},{"id":"http://arxiv.org/abs/2411.10573v3","updated":"2025-10-30T09:42:47Z","published":"2024-11-15T20:46:58Z","title":"Hysteresis Activation Function for Efficient Inference","summary":"  The widely used ReLU is favored for its hardware efficiency, {as the\nimplementation at inference is a one bit sign case,} yet suffers from issues\nsuch as the ``dying ReLU'' problem, where during training, neurons fail to\nactivate and constantly remain at zero, as highlighted by Lu et al. Traditional\napproaches to mitigate this issue often introduce more complex and less\nhardware-friendly activation functions. In this work, we propose a Hysteresis\nRectified Linear Unit (HeLU), an efficient activation function designed to\naddress the ``dying ReLU'' problem with minimal complexity. Unlike traditional\nactivation functions with fixed thresholds for training and inference, HeLU\nemploys a variable threshold that refines the backpropagation. This refined\nmechanism allows simpler activation functions to achieve competitive\nperformance comparable to their more complex counterparts without introducing\nunnecessary complexity or requiring inductive biases. Empirical evaluations\ndemonstrate that HeLU enhances model generalization across diverse datasets,\noffering a promising solution for efficient and effective inference suitable\nfor a wide range of neural network architectures.\n","authors":["Moshe Kimhi","Idan Kashani","Avi Mendelson","Chaim Baskin"],"pdf_url":"https://arxiv.org/pdf/2411.10573v3.pdf","comment":"Accepted to 4th NeurIPS Efficient Natural Language and Speech\n  Processing Workshop (ENLSP-IV 2024)"},{"id":"http://arxiv.org/abs/2510.26298v1","updated":"2025-10-30T09:35:51Z","published":"2025-10-30T09:35:51Z","title":"Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in\n  Web Games","summary":"  OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,\nenabling the model to analyze webpages, process user intents, and execute\ncursor and keyboard inputs directly within the browser. While its capacity for\ninformation retrieval tasks has been demonstrated, its performance in dynamic,\ninteractive environments remains less explored. In this study, we conduct an\nearly evaluation of Atlas's web interaction capabilities using browser-based\ngames as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,\nand Stein.world. We employ in-game performance scores as quantitative metrics\nto assess performance across different task types. Our results show that Atlas\nperforms strongly in logical reasoning tasks like Sudoku, completing puzzles\nsignificantly faster than human baselines, but struggles substantially in\nreal-time games requiring precise timing and motor control, often failing to\nprogress beyond initial obstacles. These findings suggest that while Atlas\ndemonstrates capable analytical processing, there remain notable limitations in\ndynamic web environments requiring real-time interaction. The website of our\nproject can be found at https://atlas-game-eval.github.io.\n","authors":["Jingran Zhang","Ning Li","Justin Cui"],"pdf_url":"https://arxiv.org/pdf/2510.26298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04380v3","updated":"2025-10-30T09:16:49Z","published":"2025-02-05T17:21:01Z","title":"Diversity as a Reward: Fine-Tuning LLMs on a Mixture of\n  Domain-Undetermined Data","summary":"  Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this work, we investigate the role of data diversity in\nenhancing the overall abilities of LLMs by empirically constructing contrastive\ndata pools and theoretically deriving explanations. Building upon the insights\ngained, we propose a new method that gives the LLM a dual identity: an output\nmodel to cognitively probe and select data based on diversity reward, as well\nas an input model to be tuned with the selected data. Extensive experiments\nshow that the proposed method notably boosts performance across\ndomain-undetermined data and a series of foundational downstream tasks when\napplied to various advanced LLMs. We release our code and hope this study can\nshed light on the understanding of data diversity and advance feedback-driven\ndata-model co-design for LLMs.\n","authors":["Zhenqing Ling","Daoyuan Chen","Liuyi Yao","Qianli Shen","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2502.04380v3.pdf","comment":"Accepted by NeurIPS'25 main track. 47 pages, 21 figures, 32 tables"},{"id":"http://arxiv.org/abs/2510.26285v1","updated":"2025-10-30T09:08:50Z","published":"2025-10-30T09:08:50Z","title":"Unravelling the Mechanisms of Manipulating Numbers in Language Models","summary":"  Recent work has shown that different large language models (LLMs) converge to\nsimilar and accurate input embedding representations for numbers. These\nfindings conflict with the documented propensity of LLMs to produce erroneous\noutputs when dealing with numeric information. In this work, we aim to explain\nthis conflict by exploring how language models manipulate numbers and quantify\nthe lower bounds of accuracy of these mechanisms. We find that despite\nsurfacing errors, different language models learn interchangeable\nrepresentations of numbers that are systematic, highly accurate and universal\nacross their hidden states and the types of input contexts. This allows us to\ncreate universal probes for each LLM and to trace information -- including the\ncauses of output errors -- to specific layers. Our results lay a fundamental\nunderstanding of how pre-trained LLMs manipulate numbers and outline the\npotential of more accurate probing techniques in addressed refinements of LLMs'\narchitectures.\n","authors":["Michal Štefánik","Timothee Mickus","Marek Kadlčík","Bertram Højer","Michal Spiegel","Raúl Vázquez","Aman Sinha","Josef Kuchař","Philipp Mondorf"],"pdf_url":"https://arxiv.org/pdf/2510.26285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26277v1","updated":"2025-10-30T08:59:56Z","published":"2025-10-30T08:59:56Z","title":"Do LLMs Signal When They're Right? Evidence from Neuron Agreement","summary":"  Large language models (LLMs) commonly boost reasoning via\nsample-evaluate-ensemble decoders, achieving label free gains without ground\ntruth. However, prevailing strategies score candidates using only external\noutputs such as token probabilities, entropies, or self evaluations, and these\nsignals can be poorly calibrated after post training. We instead analyze\ninternal behavior based on neuron activations and uncover three findings: (1)\nexternal signals are low dimensional projections of richer internal dynamics;\n(2) correct responses activate substantially fewer unique neurons than\nincorrect ones throughout generation; and (3) activations from correct\nresponses exhibit stronger cross sample agreement, whereas incorrect ones\ndiverge. Motivated by these observations, we propose Neuron Agreement Decoding\n(NAD), an unsupervised best-of-N method that selects candidates using\nactivation sparsity and cross sample neuron agreement, operating solely on\ninternal signals and without requiring comparable textual outputs. NAD enables\nearly correctness prediction within the first 32 generated tokens and supports\naggressive early stopping. Across math and science benchmarks with verifiable\nanswers, NAD matches majority voting; on open ended coding benchmarks where\nmajority voting is inapplicable, NAD consistently outperforms Avg@64. By\npruning unpromising trajectories early, NAD reduces token usage by 99% with\nminimal loss in generation quality, showing that internal signals provide\nreliable, scalable, and efficient guidance for label free ensemble decoding.\n","authors":["Kang Chen","Yaoning Wang","Kai Xiong","Zhuoka Feng","Wenhe Sun","Haotian Chen","Yixin Cao"],"pdf_url":"https://arxiv.org/pdf/2510.26277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05747v4","updated":"2025-10-30T08:59:39Z","published":"2025-04-08T07:24:51Z","title":"SEA-LION: Southeast Asian Languages in One Network","summary":"  Recently, Large Language Models (LLMs) have dominated much of the artificial\nintelligence scene with their ability to process and generate natural\nlanguages. However, the majority of LLM research and development remains\nEnglish-centric, leaving low-resource languages such as those in the Southeast\nAsian (SEA) region under-represented. To address this representation gap, we\nintroduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge\nmultilingual LLMs designed for SEA languages. The SEA-LION family of LLMs\nsupports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,\nMalay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages\nlarge-scale multilingual continued pre-training with a comprehensive\npost-training regime involving multiple stages of instruction fine-tuning,\nalignment, and model merging. Evaluation results on multilingual benchmarks\nindicate that our models achieve state-of-the-art performance across LLMs\nsupporting SEA languages. We open-source the models to benefit the wider SEA\ncommunity.\n","authors":["Raymond Ng","Thanh Ngan Nguyen","Yuli Huang","Ngee Chia Tai","Wai Yi Leong","Wei Qi Leong","Xianbin Yong","Jian Gang Ngui","Yosephine Susanto","Nicholas Cheng","Hamsawardhini Rengarajan","Peerat Limkonchotiwat","Adithya Venkatadri Hulagadri","Kok Wai Teng","Yeo Yeow Tong","Bryan Siow","Wei Yi Teo","Wayne Lau","Choon Meng Tan","Brandon Ong","Zhi Hao Ong","Jann Railey Montalan","Adwin Chan","Sajeban Antonyrex","Ren Lee","Esther Choa","David Ong Tat-Wee","Bing Jie Darius Liu","William Chandra Tjhi","Erik Cambria","Leslie Teo"],"pdf_url":"https://arxiv.org/pdf/2504.05747v4.pdf","comment":"Accepted at IJCNLP-AACL 2025 (Main Track). We released our model at\n  https://huggingface.co/collections/aisingapore/sea-lionv3-672589a39cdadd6a5b199581"},{"id":"http://arxiv.org/abs/2510.26274v1","updated":"2025-10-30T08:58:44Z","published":"2025-10-30T08:58:44Z","title":"PVMark: Enabling Public Verifiability for LLM Watermarking Schemes","summary":"  Watermarking schemes for large language models (LLMs) have been proposed to\nidentify the source of the generated text, mitigating the potential threats\nemerged from model theft. However, current watermarking solutions hardly\nresolve the trust issue: the non-public watermark detection cannot prove itself\nfaithfully conducting the detection. We observe that it is attributed to the\nsecret key mostly used in the watermark detection -- it cannot be public, or\nthe adversary may launch removal attacks provided the key; nor can it be\nprivate, or the watermarking detection is opaque to the public. To resolve the\ndilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),\nenabling the watermark detection process to be publicly verifiable by third\nparties without disclosing any secret key. PVMark hinges upon the proof of\n`correct execution' of watermark detection on which a set of ZKP constraints\nare built, including mapping, random number generation, comparison, and\nsummation. We implement multiple variants of PVMark in Python, Rust and Circom,\ncovering combinations of three watermarking schemes, three hash functions, and\nfour ZKP protocols, to show our approach effectively works under a variety of\ncircumstances. By experimental results, PVMark efficiently enables public\nverifiability on the state-of-the-art LLM watermarking schemes yet without\ncompromising the watermarking performance, promising to be deployed in\npractice.\n","authors":["Haohua Duan","Liyao Xiang","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26274v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2510.26271v1","updated":"2025-10-30T08:56:06Z","published":"2025-10-30T08:56:06Z","title":"Distilling Multilingual Vision-Language Models: When Smaller Models Stay\n  Multilingual","summary":"  Vision-language models (VLMs) exhibit uneven performance across languages, a\nproblem that is often exacerbated when the model size is reduced. While\nKnowledge distillation (KD) demonstrates promising results in transferring\nknowledge from larger to smaller VLMs, applying KD in multilingualism is an\nunderexplored area. This paper presents a controlled empirical study of KD\nbehavior across five distillation approaches, isolating their effects on\ncross-lingual representation consistency and downstream performance stability\nunder model compression. We study five distillation formulations across CLIP\nand SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual\nQA. We find that some configurations preserve or even improve multilingual\nretrieval robustness despite halving model size, but others fail to maintain\ncross-task stability, exposing design-sensitive trade-offs that aggregate\naccuracy alone does not reveal.\n","authors":["Sukrit Sriratanawilai","Jhayahgrit Thongwat","Romrawin Chumpu","Patomporn Payoungkhamdee","Sarana Nutanong","Peerat Limkonchotiwat"],"pdf_url":"https://arxiv.org/pdf/2510.26271v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2510.25160v2","updated":"2025-10-30T08:52:17Z","published":"2025-10-29T04:29:17Z","title":"Model-Document Protocol for AI Search","summary":"  AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.\n","authors":["Hongjin Qian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25160v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2510.18480v2","updated":"2025-10-30T08:46:37Z","published":"2025-10-21T10:00:32Z","title":"How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices","summary":"  Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.\n","authors":["Han Peng","Peiyu Liu","Zican Dong","Daixuan Cheng","Junyi Li","Yiru Tang","Shuo Wang","Wayne Xin Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.18480v2.pdf","comment":"Withdrawn by the authors to better delineate the related work from\n  the paper's original contributions"},{"id":"http://arxiv.org/abs/2506.04721v2","updated":"2025-10-30T08:43:19Z","published":"2025-06-05T07:51:23Z","title":"SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through\n  Combat","summary":"  We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs\nthrough competition and combat. To complement a single model's lack of\ndiversity in generation and biases in evaluation, multiple LLMs form a \"sparta\ntribe\" to compete against each other in fulfilling instructions while serving\nas judges for the competition of others. For each iteration, one instruction\nand two models are selected for a duel, the other models evaluate the two\nresponses, and their evaluation scores are aggregated through a adapted\nelo-ranking based reputation system, where winners/losers of combat gain/lose\nweight in evaluating others. The peer-evaluated combat results then become\npreference pairs where the winning response is preferred over the losing one,\nand all models learn from these preferences at the end of each iteration.\nSPARTA ALIGNMENT enables the self-evolution of multiple LLMs in an iterative\nand collective competition process. Extensive experiments demonstrate that\nSPARTA ALIGNMENT outperforms initial models and 4 self-alignment baselines\nacross 10 out of 12 tasks and datasets with 7.0% average improvement. Further\nanalysis reveals that SPARTA ALIGNMENT generalizes more effectively to unseen\ntasks and leverages the expertise diversity of participating models to produce\nmore logical, direct and informative outputs.\n","authors":["Yuru Jiang","Wenxuan Ding","Shangbin Feng","Greg Durrett","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2506.04721v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26254v1","updated":"2025-10-30T08:36:07Z","published":"2025-10-30T08:36:07Z","title":"Language Models Are Borrowing-Blind: A Multilingual Evaluation of\n  Loanword Identification across 10 Languages","summary":"  Throughout language history, words are borrowed from one language to another\nand gradually become integrated into the recipient's lexicon. Speakers can\noften differentiate these loanwords from native vocabulary, particularly in\nbilingual communities where a dominant language continuously imposes lexical\nitems on a minority language. This paper investigates whether pretrained\nlanguage models, including large language models, possess similar capabilities\nfor loanword identification. We evaluate multiple models across 10 languages.\nDespite explicit instructions and contextual information, our results show that\nmodels perform poorly in distinguishing loanwords from native ones. These\nfindings corroborate previous evidence that modern NLP systems exhibit a bias\ntoward loanwords rather than native equivalents. Our work has implications for\ndeveloping NLP tools for minority languages and supporting language\npreservation in communities under lexical pressure from dominant languages.\n","authors":["Mérilin Sousa Silva","Sina Ahmadi"],"pdf_url":"https://arxiv.org/pdf/2510.26254v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2510.26253v1","updated":"2025-10-30T08:35:52Z","published":"2025-10-30T08:35:52Z","title":"Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs","summary":"  The ability to accurately interpret implied meanings plays a crucial role in\nhuman communication and language use, and language models are also expected to\npossess this capability. This study demonstrates that providing language models\nwith pragmatic theories as prompts is an effective in-context learning approach\nfor tasks to understand implied meanings. Specifically, we propose an approach\nin which an overview of pragmatic theories, such as Gricean pragmatics and\nRelevance Theory, is presented as a prompt to the language model, guiding it\nthrough a step-by-step reasoning process to derive a final interpretation.\nExperimental results showed that, compared to the baseline, which prompts\nintermediate reasoning without presenting pragmatic theories (0-shot\nChain-of-Thought), our methods enabled language models to achieve up to 9.6\\%\nhigher scores on pragmatic reasoning tasks. Furthermore, we show that even\nwithout explaining the details of pragmatic theories, merely mentioning their\nnames in the prompt leads to a certain performance improvement (around 1-3%) in\nlarger models compared to the baseline.\n","authors":["Takuma Sato","Seiya Kawano","Koichiro Yoshino"],"pdf_url":"https://arxiv.org/pdf/2510.26253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13229v3","updated":"2025-10-30T08:26:57Z","published":"2025-06-16T08:28:19Z","title":"IGD: Token Decisiveness Modeling via Information Gain in LLMs for\n  Personalized Recommendation","summary":"  Large Language Models (LLMs) have shown strong potential for recommendation\nby framing item prediction as a token-by-token language generation task.\nHowever, existing methods treat all item tokens equally, simply pursuing\nlikelihood maximization during both optimization and decoding. This overlooks\ncrucial token-level differences in decisiveness-many tokens contribute little\nto item discrimination yet can dominate optimization or decoding. To quantify\ntoken decisiveness, we propose a novel perspective that models item generation\nas a decision process, measuring token decisiveness by the Information Gain\n(IG) each token provides in reducing uncertainty about the generated item. Our\nempirical analysis reveals that most tokens have low IG but often correspond to\nhigh logits, disproportionately influencing training loss and decoding, which\nmay impair model performance. Building on these insights, we introduce an\nInformation Gain-based Decisiveness-aware Token handling (IGD) strategy that\nintegrates token decisiveness into both tuning and decoding. Specifically, IGD\ndownweights low-IG tokens during tuning and rebalances decoding to emphasize\ntokens with high IG. In this way, IGD moves beyond pure likelihood\nmaximization, effectively prioritizing high-decisiveness tokens. Extensive\nexperiments on four benchmark datasets with two LLM backbones demonstrate that\nIGD consistently improves recommendation accuracy, achieving significant gains\non widely used ranking metrics compared to strong baselines.\n","authors":["Zijie Lin","Yang Zhang","Xiaoyan Zhao","Fengbin Zhu","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2506.13229v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26241v1","updated":"2025-10-30T08:21:50Z","published":"2025-10-30T08:21:50Z","title":"Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for\n  Vision-Language Models","summary":"  Modern vision-language models (VLMs) excel at many multimodal tasks, yet\ntheir grasp of temporal information in video remains weak and, crucially,\nunder-evaluated. We probe this gap with a deceptively simple but revealing\nchallenge: judging the arrow of time (AoT)-whether a short clip is played\nforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated\nbenchmark that tests whether VLMs can infer temporal direction in natural\nvideos using the same stimuli and behavioral baselines established for humans.\nOur comprehensive evaluation of open-weight and proprietary, reasoning and\nnon-reasoning VLMs reveals that most models perform near chance, and even the\nbest lag far behind human accuracy on physically irreversible processes (e.g.,\nfree fall, diffusion/explosion) and causal manual actions (division/addition)\nthat humans recognize almost instantly. These results highlight a fundamental\ngap in current multimodal systems: while they capture rich visual-semantic\ncorrelations, they lack the inductive biases required for temporal continuity\nand causal understanding. We release the code and data for AoT-PsyPhyBENCH to\nencourage further progress in the physical and temporal reasoning capabilities\nof VLMs.\n","authors":["Shiho Matta","Lis Kanashiro Pereira","Peitao Han","Fei Cheng","Shigeru Kitazawa"],"pdf_url":"https://arxiv.org/pdf/2510.26241v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2505.24388v2","updated":"2025-10-30T07:48:31Z","published":"2025-05-30T09:18:08Z","title":"ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and\n  Optimization for Retrieval-Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs)\nwith external knowledge to improve factuality. However, existing RAG systems\nfrequently underutilize the retrieved documents, failing to extract and\nintegrate the key clues needed to support faithful and interpretable reasoning,\nespecially in cases where relevant evidence is implicit, scattered, or obscured\nby noise. To address this issue, we propose ClueAnchor, a novel framework for\nenhancing RAG via clue-anchored reasoning exploration and optimization.\nClueAnchor extracts key clues from retrieved content and generates multiple\nreasoning paths based on different knowledge configurations, optimizing the\nmodel by selecting the most appropriate reasoning path for the given context\nthrough reward-based preference optimization. Experiments show that ClueAnchor\nsignificantly outperforms prior RAG baselines in the completeness and\nrobustness of reasoning. Further analysis confirms its strong resilience to\nnoisy or partially relevant retrieved content, as well as its capability to\nidentify supporting evidence even in the absence of explicit clue supervision\nduring inference. All codes are available at\nhttps://github.com/thunlp/ClueAnchor.\n","authors":["Hao Chen","Yukun Yan","Sen Mei","Wanxiang Che","Zhenghao Liu","Qi Shi","Xinze Li","Yuchun Fan","Pengcheng Huang","Qiushi Xiong","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2505.24388v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26205v1","updated":"2025-10-30T07:29:14Z","published":"2025-10-30T07:29:14Z","title":"Towards Global Retrieval Augmented Generation: A Benchmark for\n  Corpus-Level Reasoning","summary":"  Retrieval-augmented generation (RAG) has emerged as a leading approach to\nreducing hallucinations in large language models (LLMs). Current RAG evaluation\nbenchmarks primarily focus on what we call local RAG: retrieving relevant\nchunks from a small subset of documents to answer queries that require only\nlocalized understanding within specific text chunks. However, many real-world\napplications require a fundamentally different capability -- global RAG --\nwhich involves aggregating and analyzing information across entire document\ncollections to derive corpus-level insights (for example, \"What are the top 10\nmost cited papers in 2023?\"). In this paper, we introduce GlobalQA -- the first\nbenchmark specifically designed to evaluate global RAG capabilities, covering\nfour core task types: counting, extremum queries, sorting, and top-k\nextraction. Through systematic evaluation across different models and\nbaselines, we find that existing RAG methods perform poorly on global tasks,\nwith the strongest baseline achieving only 1.51 F1 score. To address these\nchallenges, we propose GlobalRAG, a multi-tool collaborative framework that\npreserves structural coherence through chunk-level retrieval, incorporates\nLLM-driven intelligent filters to eliminate noisy documents, and integrates\naggregation modules for precise symbolic computation. On the Qwen2.5-14B model,\nGlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,\nvalidating the effectiveness of our method.\n","authors":["Qi Luo","Xiaonan Li","Tingshuo Fan","Xinchi Chen","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2510.26205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26202v1","updated":"2025-10-30T07:25:10Z","published":"2025-10-30T07:25:10Z","title":"What's In My Human Feedback? Learning Interpretable Descriptions of\n  Preference Data","summary":"  Human feedback can alter language models in unpredictable and undesirable\nways, as practitioners lack a clear understanding of what feedback data\nencodes. While prior work studies preferences over certain attributes (e.g.,\nlength or sycophancy), automatically extracting relevant features without\npre-specifying hypotheses remains challenging. We introduce What's In My Human\nFeedback? (WIMHF), a method to explain feedback data using sparse autoencoders.\nWIMHF characterizes both (1) the preferences a dataset is capable of measuring\nand (2) the preferences that the annotators actually express. Across 7\ndatasets, WIMHF identifies a small number of human-interpretable features that\naccount for the majority of the preference prediction signal achieved by\nblack-box models. These features reveal a wide diversity in what humans prefer,\nand the role of dataset-level context: for example, users on Reddit prefer\ninformality and jokes, while annotators in HH-RLHF and PRISM disprefer them.\nWIMHF also surfaces potentially unsafe preferences, such as that LMArena users\ntend to vote against refusals, often in favor of toxic content. The learned\nfeatures enable effective data curation: re-labeling the harmful examples in\nArena yields large safety gains (+37%) with no cost to general performance.\nThey also allow fine-grained personalization: on the Community Alignment\ndataset, we learn annotator-specific weights over subjective features that\nimprove preference prediction. WIMHF provides a human-centered analysis method\nfor practitioners to better understand and use preference data.\n","authors":["Rajiv Movva","Smitha Milli","Sewon Min","Emma Pierson"],"pdf_url":"https://arxiv.org/pdf/2510.26202v1.pdf","comment":"Code: https://github.com/rmovva/wimhf"},{"id":"http://arxiv.org/abs/2510.26200v1","updated":"2025-10-30T07:21:05Z","published":"2025-10-30T07:21:05Z","title":"Don't Let It Fade: Preserving Edits in Diffusion Language Models via\n  Token Timestep Allocation","summary":"  While diffusion language models (DLMs) enable fine-grained refinement, their\npractical controllability remains fragile. We identify and formally\ncharacterize a central failure mode called update forgetting, in which uniform\nand context agnostic updates induce token level fluctuations across timesteps,\nerasing earlier semantic edits and disrupting the cumulative refinement\nprocess, thereby degrading fluency and coherence. As this failure originates in\nuniform and context agnostic updates, effective control demands explicit token\nordering. We propose Token Timestep Allocation (TTA), which realizes soft and\nsemantic token ordering via per token timestep schedules: critical tokens are\nfrozen early, while uncertain tokens receive continued refinement. This\ntimestep based ordering can be instantiated as either a fixed policy or an\nadaptive policy driven by task signals, thereby supporting a broad spectrum of\nrefinement strategies. Because it operates purely at inference time, it applies\nuniformly across various DLMs and naturally extends to diverse supervision\nsources. Empirically, TTA improves controllability and fluency: on sentiment\ncontrol, it yields more than 20 percent higher accuracy and nearly halves\nperplexity using less than one fifth the steps; in detoxification, it lowers\nmaximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).\nTogether, these results demonstrate that softened ordering via timestep\nallocation is the critical lever for mitigating update forgetting and achieving\nstable and controllable diffusion text generation.\n","authors":["Woojin Kim","Jaeyoung Do"],"pdf_url":"https://arxiv.org/pdf/2510.26200v1.pdf","comment":"Accepted in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.15095v2","updated":"2025-10-30T07:18:23Z","published":"2025-05-21T04:34:22Z","title":"Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable\n  Sarcasm Detection of Australian and Indian English","summary":"  Sarcasm is a challenge to sentiment analysis because of the incongruity\nbetween stated and implied sentiment. The challenge is exacerbated when the\nimplication may be relevant to a specific country or geographical region.\nPragmatic metacognitive prompting (PMP) is a cognition-inspired technique that\nhas been used for pragmatic reasoning. In this paper, we harness PMP for\nexplainable sarcasm detection for Australian and Indian English, alongside a\nbenchmark dataset for standard English. We manually add sarcasm explanations to\nan existing sarcasm-labeled dataset for Australian and Indian English called\nBESSTIE, and compare the performance for explainable sarcasm detection for them\nwith FLUTE, a standard English dataset containing sarcasm explanations. Our\napproach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA)\nachieves statistically significant performance improvement across all tasks and\ndatasets when compared with four alternative prompting strategies. We also find\nthat alternative techniques such as agentic prompting mitigate context-related\nfailures by enabling external knowledge retrieval. The focused contribution of\nour work is utilising PMP in generating sarcasm explanations for varieties of\nEnglish.\n","authors":["Ishmanbir Singh","Dipankar Srirag","Aditya Joshi"],"pdf_url":"https://arxiv.org/pdf/2505.15095v2.pdf","comment":"ALTA 2025 (Best Paper Honorable Mention). Camera-ready"},{"id":"http://arxiv.org/abs/2510.26193v1","updated":"2025-10-30T07:06:47Z","published":"2025-10-30T07:06:47Z","title":"RCScore: Quantifying Response Consistency in Large Language Models","summary":"  Current LLM evaluations often rely on a single instruction template,\noverlooking models' sensitivity to instruction style-a critical aspect for\nreal-world deployments. We present RCScore, a multi-dimensional framework\nquantifying how instruction formulation affects model responses. By\nsystematically transforming benchmark problems into multiple instruction\nstyles, RCScore reveals performance variations undetected by conventional\nmetrics. Our experiments across ten LLMs on four reasoning benchmarks\ndemonstrate that instruction style can shift accuracy by up to 16.7% points. We\nintroduce Cross-Response Similarity (CRS), a method applying RCScore metrics to\nmeasure stylistic self-consistency, and establish its strong correlation with\ntask accuracy, suggesting consistency as a valuable proxy for model\nreliability. Additional findings show that deterministic decoding produces more\nstylistically stable outputs, and model scale correlates positively with\ncross-style consistency. RCScore offers a principled approach to assess\ninstruction robustness.\n","authors":["Dongjun Jang","Youngchae Ahn","Hyopil Shin"],"pdf_url":"https://arxiv.org/pdf/2510.26193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26190v1","updated":"2025-10-30T06:57:07Z","published":"2025-10-30T06:57:07Z","title":"SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level","summary":"  The evaluation of intelligibility for TTS has reached a bottleneck, as\nexisting assessments heavily rely on word-by-word accuracy metrics such as WER,\nwhich fail to capture the complexity of real-world speech or reflect human\ncomprehension needs. To address this, we propose Spoken-Passage Multiple-Choice\nQuestion Answering, a novel subjective approach evaluating the accuracy of key\ninformation in synthesized speech, and release SP-MCQA-Eval, an 8.76-hour\nnews-style benchmark dataset for SP-MCQA evaluation. Our experiments reveal\nthat low WER does not necessarily guarantee high key-information accuracy,\nexposing a gap between traditional metrics and practical intelligibility.\nSP-MCQA shows that even state-of-the-art (SOTA) models still lack robust text\nnormalization and phonetic accuracy. This work underscores the urgent need for\nhigh-level, more life-like evaluation criteria now that many systems already\nexcel at WER yet may fall short on real-world intelligibility.\n","authors":["Hitomi Jin Ling Tee","Chaoren Wang","Zijie Zhang","Zhizheng Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.16648v2","updated":"2025-10-30T06:55:22Z","published":"2025-09-20T11:50:22Z","title":"FESTA: Functionally Equivalent Sampling for Trust Assessment of\n  Multimodal LLMs","summary":"  The accurate trust assessment of multimodal large language models (MLLMs)\ngenerated predictions, which can enable selective prediction and improve user\nconfidence, is challenging due to the diverse multi-modal input paradigms. We\npropose Functionally Equivalent Sampling for Trust Assessment (FESTA), a\nmultimodal input sampling technique for MLLMs, that generates an uncertainty\nmeasure based on the equivalent and complementary input samplings. The proposed\ntask-preserving sampling approach for uncertainty quantification expands the\ninput space to probe the consistency (through equivalent samples) and\nsensitivity (through complementary samples) of the model. FESTA uses only\ninput-output access of the model (black-box), and does not require ground truth\n(unsupervised). The experiments are conducted with various off-the-shelf\nmulti-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA\nuncertainty estimate achieves significant improvement (33.3% relative\nimprovement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in\nselective prediction performance, based on\narea-under-receiver-operating-characteristic curve (AUROC) metric in detecting\nmispredictions. The code implementation is open-sourced.\n","authors":["Debarpan Bhattacharya","Apoorva Kulkarni","Sriram Ganapathy"],"pdf_url":"https://arxiv.org/pdf/2509.16648v2.pdf","comment":"Accepted in the Findings of EMNLP, 2025"},{"id":"http://arxiv.org/abs/2510.26183v1","updated":"2025-10-30T06:42:15Z","published":"2025-10-30T06:42:15Z","title":"Similarity-Distance-Magnitude Language Models","summary":"  We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which\nare sequence prediction models fine-tuned to maximize the proportion of\ngenerations in the well-calibrated, high-probability region partitioned by a\nfinal-layer SDM activation layer used for binary classification of\ninstruction-following. We demonstrate that existing pre-trained decoder-only\nTransformer LMs can be readily converted into SDM LMs via supervised\nfine-tuning, using the final-layer SDM activation layer during training to\nestimate a change-of-base for a supervised next-token loss over a contrastive\ninput encoding scheme, with additional hard negative examples generated online\nduring training. This results in reduced abstentions (i.e., improved\nstatistical efficiency) compared to strong supervised baselines.\n","authors":["Allen Schmaltz"],"pdf_url":"https://arxiv.org/pdf/2510.26183v1.pdf","comment":"8 pages, 5 tables"},{"id":"http://arxiv.org/abs/2510.26182v1","updated":"2025-10-30T06:37:23Z","published":"2025-10-30T06:37:23Z","title":"MossNet: Mixture of State-Space Experts is a Multi-Head Attention","summary":"  Large language models (LLMs) have significantly advanced generative\napplications in natural language processing (NLP). Recent trends in model\narchitectures revolve around efficient variants of transformers or\nstate-space/gated-recurrent models (SSMs, GRMs). However, prevailing\nSSM/GRM-based methods often emulate only a single attention head, potentially\nlimiting their expressiveness. In this work, we propose MossNet, a novel\nmixture-of-state-space-experts architecture that emulates a linear multi-head\nattention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation\nnot only in channel-mixing multi-layered perceptron (MLP) blocks but also in\nthe time-mixing SSM kernels to realize multiple \"attention heads.\" Extensive\nexperiments on language modeling and downstream evaluations show that MossNet\noutperforms both transformer- and SSM-based architectures of similar model size\nand data budgets. Larger variants of MossNet, trained on trillions of tokens,\nfurther confirm its scalability and superior performance. In addition,\nreal-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU\ndemonstrate favorable runtime speed and resource usage compared to similarly\nsized baselines. Our results suggest that MossNet is a compelling new direction\nfor efficient, high-performing recurrent LLM architectures.\n","authors":["Shikhar Tuli","James Seale Smith","Haris Jeelani","Chi-Heng Lin","Abhishek Patel","Vasili Ramanishka","Yen-Chang Hsu","Hongxia Jin"],"pdf_url":"https://arxiv.org/pdf/2510.26182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13308v2","updated":"2025-10-30T06:23:27Z","published":"2025-05-19T16:26:02Z","title":"Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space","summary":"  Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.\n","authors":["Hengli Li","Chenxi Li","Tong Wu","Xuekai Zhu","Yuxuan Wang","Zhaoxin Yu","Eric Hanchen Jiang","Song-Chun Zhu","Zixia Jia","Ying Nian Wu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.13308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11094v2","updated":"2025-10-30T06:22:33Z","published":"2025-06-06T05:50:50Z","title":"The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of\n  LLMs","summary":"  With the rapid advancement of artificial intelligence, Large Language Models\n(LLMs) have shown remarkable capabilities in Natural Language Processing (NLP),\nincluding content generation, human-computer interaction, machine translation,\nand code generation. However, their widespread deployment has also raised\nsignificant safety concerns. In particular, LLM-generated content can exhibit\nunsafe behaviors such as toxicity, bias, or misinformation, especially in\nadversarial contexts, which has attracted increasing attention from both\nacademia and industry. Although numerous studies have attempted to evaluate\nthese risks, a comprehensive and systematic survey on safety evaluation of LLMs\nis still lacking. This work aims to fill this gap by presenting a structured\noverview of recent advances in safety evaluation of LLMs. Specifically, we\npropose a four-dimensional taxonomy: (i) Why to evaluate, which explores the\nbackground of safety evaluation of LLMs, how they differ from general LLMs\nevaluation, and the significance of such evaluation; (ii) What to evaluate,\nwhich examines and categorizes existing safety evaluation tasks based on key\ncapabilities, including dimensions such as toxicity, robustness, ethics, bias\nand fairness, truthfulness, and related aspects; (iii) Where to evaluate, which\nsummarizes the evaluation metrics, datasets and benchmarks currently used in\nsafety evaluations; (iv) How to evaluate, which reviews existing mainstream\nevaluation methods based on the roles of the evaluators and some evaluation\nframeworks that integrate the entire evaluation pipeline. Finally, we identify\nthe challenges in safety evaluation of LLMs and propose promising research\ndirections to promote further advancement in this field. We emphasize the\nnecessity of prioritizing safety evaluation to ensure the reliable and\nresponsible deployment of LLMs in real-world applications.\n","authors":["Songyang Liu","Chaozhuo Li","Jiameng Qiu","Xi Zhang","Feiran Huang","Litian Zhang","Yiming Hei","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2506.11094v2.pdf","comment":"20 pages, preprint"},{"id":"http://arxiv.org/abs/2509.12760v2","updated":"2025-10-30T06:08:58Z","published":"2025-09-16T07:19:38Z","title":"Similarity-Distance-Magnitude Activations","summary":"  We introduce the Similarity-Distance-Magnitude (SDM) activation function, a\nmore robust and interpretable formulation of the standard softmax activation\nfunction, adding Similarity (i.e., correctly predicted depth-matches into\ntraining) awareness and Distance-to-training-distribution awareness to the\nexisting output Magnitude (i.e., decision-boundary) awareness, and enabling\ninterpretability-by-exemplar via dense matching. We further introduce the SDM\nestimator, based on a data-driven partitioning of the class-wise empirical CDFs\nvia the SDM activation, to control the class- and prediction-conditional\naccuracy among selective classifications. When used as the final-layer\nactivation over pre-trained language models for selective classification, the\nSDM estimator is more robust to co-variate shifts and out-of-distribution\ninputs than existing calibration methods using softmax activations, while\nremaining informative over in-distribution data.\n","authors":["Allen Schmaltz"],"pdf_url":"https://arxiv.org/pdf/2509.12760v2.pdf","comment":"18 pages, 5 tables, 1 algorithm. arXiv admin note: substantial text\n  overlap with arXiv:2502.20167"},{"id":"http://arxiv.org/abs/2510.26167v1","updated":"2025-10-30T06:08:27Z","published":"2025-10-30T06:08:27Z","title":"One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient\n  Reasoning","summary":"  Reward models (RMs) play a critical role in aligning large language models\n(LLMs) with human preferences. Yet in the domain of tool learning, the lack of\nRMs specifically designed for function-calling tasks has limited progress\ntoward more capable agentic AI. We introduce ToolRM, a family of lightweight\ngenerative RMs tailored for general tool-use scenarios. To build these models,\nwe propose a novel pipeline that constructs pairwise preference data using\nrule-based scoring and multidimensional sampling. This yields\nToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique\ntasks that supports reinforcement learning with verifiable feedback. To\nevaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on\nthe agentic evaluation suite BFCL. Trained on our constructed data, models from\nthe Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially\noutperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward\njudgments. Beyond training objectives, ToolRM generalizes to broader critique\ntasks, including Best-of-N sampling and self-correction. Experiments on\nACEBench highlight its effectiveness and efficiency, enabling inference-time\nscaling and reducing output token usage by over 66%. We release data and model\ncheckpoints to facilitate future research.\n","authors":["Renhao Li","Jianhong Tu","Yang Su","Hamid Alinejad-Rokny","Derek F. Wong","Junyang Lin","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24014v2","updated":"2025-10-30T05:38:02Z","published":"2025-10-28T02:49:40Z","title":"TEXT2DB: Integration-Aware Information Extraction with Large Language\n  Model Agents","summary":"  The task of information extraction (IE) is to extract structured knowledge\nfrom text. However, it is often not straightforward to utilize IE output due to\nthe mismatch between the IE ontology and the downstream application needs. We\npropose a new formulation of IE TEXT2DB that emphasizes the integration of IE\noutput and the target database (or knowledge base). Given a user instruction, a\ndocument set, and a database, our task requires the model to update the\ndatabase with values from the document set to satisfy the user instruction.\nThis task requires understanding user instructions for what to extract and\nadapting to the given DB/KB schema for how to extract on the fly. To evaluate\nthis new task, we introduce a new benchmark featuring common demands such as\ndata infilling, row population, and column addition. In addition, we propose an\nLLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer\ncomponent that interacts with the database, the Planner component that\ngenerates a code-based plan with calls to IE models, and the Analyzer component\nthat provides feedback regarding code quality before execution. Experiments\nshow that OPAL can successfully adapt to diverse database schemas by generating\ndifferent code plans and calling the required IE models. We also highlight\ndifficult cases such as dealing with large databases with complex dependencies\nand extraction hallucination, which we believe deserve further investigation.\nSource code: https://github.com/yzjiao/Text2DB\n","authors":["Yizhu Jiao","Sha Li","Sizhe Zhou","Heng Ji","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2510.24014v2.pdf","comment":"Source code: https://github.com/yzjiao/Text2DB"},{"id":"http://arxiv.org/abs/2506.00871v2","updated":"2025-10-30T05:04:19Z","published":"2025-06-01T07:18:47Z","title":"Towards Predicting Any Human Trajectory In Context","summary":"  Predicting accurate future trajectories of pedestrians is essential for\nautonomous systems but remains a challenging task due to the need for\nadaptability in different environments and domains. A common approach involves\ncollecting scenario-specific data and performing fine-tuning via\nbackpropagation. However, the need to fine-tune for each new scenario is often\nimpractical for deployment on edge devices. To address this challenge, we\nintroduce \\paper, an In-Context Learning (ICL) framework for pedestrian\ntrajectory prediction that enables adaptation without fine-tuning on the\nscenario-specific data at inference time without requiring weight updates. We\npropose a spatio-temporal similarity-based example selection (STES) method that\nselects relevant examples from previously observed trajectories within the same\nscene by identifying similar motion patterns at corresponding locations. To\nfurther refine this selection, we introduce prediction-guided example selection\n(PG-ES), which selects examples based on both the past trajectory and the\npredicted future trajectory, rather than relying solely on the past trajectory.\nThis approach allows the model to account for long-term dynamics when selecting\nexamples. Finally, instead of relying on small real-world datasets with limited\nscenario diversity, we train our model on a large-scale synthetic dataset to\nenhance its prediction ability by leveraging in-context examples. Extensive\nexperiments demonstrate that TrajICL achieves remarkable adaptation across both\nin-domain and cross-domain scenarios, outperforming even fine-tuned approaches\nacross multiple public benchmarks. Project Page:\nhttps://fujiry0.github.io/TrajICL-project-page/.\n","authors":["Ryo Fujii","Hideo Saito","Ryo Hachiuma"],"pdf_url":"https://arxiv.org/pdf/2506.00871v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26143v1","updated":"2025-10-30T04:56:44Z","published":"2025-10-30T04:56:44Z","title":"Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math","summary":"  Reinforcement learning (RL) can elicit strong reasoning in large language\nmodels (LLMs), yet most open efforts focus on math and code. We propose\nReasoning Curriculum, a simple two-stage curriculum that first elicits\nreasoning skills in pretraining-aligned domains such as math, then adapts and\nrefines these skills across other domains via joint RL. Stage 1 performs a\nbrief cold start and then math-only RL with verifiable rewards to develop\nreasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and\nconsolidate these skills. The curriculum is minimal and backbone-agnostic,\nrequiring no specialized reward models beyond standard verifiability checks.\nEvaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning\ncurriculum yields consistent gains. Ablations and a cognitive-skill analysis\nindicate that both stages are necessary and that math-first elicitation\nincreases cognitive behaviors important for solving complex problems. Reasoning\nCurriculum provides a compact, easy-to-adopt recipe for general reasoning.\n","authors":["Bo Pang","Deqian Kong","Silvio Savarese","Caiming Xiong","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26143v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2409.06263v2","updated":"2025-10-30T04:29:27Z","published":"2024-09-10T07:06:40Z","title":"Speak & Spell: LLM-Driven Controllable Phonetic Error Augmentation for\n  Robust Dialogue State Tracking","summary":"  Dialogue State Tracking (DST) is a key part of task-oriented dialogue\nsystems, identifying important information in conversations. However, its\naccuracy drops significantly in spoken dialogue environments due to named\nentity errors from Automatic Speech Recognition (ASR) systems. We introduce a\nsimple yet effective data augmentation method that targets those entities to\nimprove the robustness of DST model. Our novel method can control the placement\nof errors using keyword-highlighted prompts while introducing phonetically\nsimilar errors. As a result, our method generated sufficient error patterns on\nkeywords, leading to improved accuracy in noised and low-accuracy ASR\nenvironments.\n","authors":["Jihyun Lee","Solee Im","Wonjun Lee","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2409.06263v2.pdf","comment":"Accepted to AACL-IJCNLP 2025"},{"id":"http://arxiv.org/abs/2510.26124v1","updated":"2025-10-30T04:10:56Z","published":"2025-10-30T04:10:56Z","title":"On the Influence of Discourse Relations in Persuasive Texts","summary":"  This paper investigates the relationship between Persuasion Techniques (PTs)\nand Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and\nprompt engineering. Since no dataset annotated with both PTs and DRs exists, we\ntook the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point\nand developed LLM-based classifiers to label each instance of the dataset with\none of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10\ndifferent prompts, resulting in 40 unique DR classifiers. Ensemble models using\ndifferent majority-pooling strategies were used to create 5 silver datasets of\ninstances labelled with both persuasion techniques and level-2 PDTB senses. The\nsilver dataset sizes vary from 1,281 instances to 204 instances, depending on\nthe majority pooling technique used. Statistical analysis of these silver\ndatasets shows that six discourse relations (namely Cause, Purpose, Contrast,\nCause+Belief, Concession, and Condition) play a crucial role in persuasive\ntexts, especially in the use of Loaded Language, Exaggeration/Minimisation,\nRepetition and to cast Doubt. This insight can contribute to detecting online\npropaganda and misinformation, as well as to our general understanding of\neffective communication.\n","authors":["Nawar Turk","Sevag Kaspar","Leila Kosseim"],"pdf_url":"https://arxiv.org/pdf/2510.26124v1.pdf","comment":"Published in Proceedings of the 38th Canadian Conference on\n  Artificial Intelligence CanAI 2025 Calgary Alberta May 26-27 2025. 5 figures\n  7 tables"},{"id":"http://arxiv.org/abs/2510.26122v1","updated":"2025-10-30T04:08:53Z","published":"2025-10-30T04:08:53Z","title":"Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock\n  LLM Diverse Thinking","summary":"  While Test-Time Scaling (TTS) has proven effective in improving the reasoning\nability of large language models (LLMs), low diversity in model outputs often\nbecomes a bottleneck; this is partly caused by the common \"one problem, one\nsolution\" (1P1S) training practice, which provides a single canonical answer\nand can push models toward a narrow set of reasoning paths. To address this, we\npropose a \"one problem, multiple solutions\" (1PNS) training paradigm that\nexposes the model to a variety of valid reasoning trajectories and thus\nincreases inference diversity. A core challenge for 1PNS is reliably measuring\nsemantic differences between multi-step chains of thought, so we introduce\nReasoning Path Divergence (RPD), a step-level metric that aligns and scores\nLong Chain-of-Thought solutions to capture differences in intermediate\nreasoning. Using RPD, we curate maximally diverse solution sets per problem and\nfine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields\nmore varied outputs and higher pass@k, with an average +2.80% gain in pass@16\nover a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that\n1PNS further amplifies the effectiveness of TTS. Our code is available at\nhttps://github.com/fengjujf/Reasoning-Path-Divergence .\n","authors":["Feng Ju","Zeyu Qin","Rui Min","Zhitao He","Lingpeng Kong","Yi R. Fung"],"pdf_url":"https://arxiv.org/pdf/2510.26122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06736v3","updated":"2025-10-30T03:48:09Z","published":"2023-11-12T05:12:49Z","title":"Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof\n  Generation by Stepwise Decoding with Contrastive Learning","summary":"  Logical reasoning is a pivotal component in the field of artificial\nintelligence. Proof planning, particularly in contexts requiring the validation\nof explanation accuracy, continues to present challenges. The recent\nadvancement of large language models (LLMs) has led to significant progress in\nnatural language proof planning, evolving from one-stage generators to more\ncomplex three-stage systems that include additional searchers or verifiers.\nWhile these assisted methods improve the quality of generated results, they\nalso introduce increased search efforts and computational costs. Furthermore,\nthe generative process itself remains underexplored. In this study, we propose\na stepwise decoding approach augmented by contrastive learning to address two\ncommon errors encountered during the LLM generator's decoding process. We\nfine-tune the language model using both vanilla and enhanced hard negatives to\nmitigate these decoding errors. Empirical results demonstrate the effectiveness\nof our strategy. Additionally, our further analysis reveals that even larger\nLLMs still struggle to generate rigorous logical chains.\n","authors":["Ying Su","Mingwen Liu","Zhijiang Guo"],"pdf_url":"https://arxiv.org/pdf/2311.06736v3.pdf","comment":"15 pages, 2 figures, 11 tables. Accepted by AACL 2025 main conference"},{"id":"http://arxiv.org/abs/2505.15201v3","updated":"2025-10-30T03:40:48Z","published":"2025-05-21T07:26:36Z","title":"Pass@K Policy Optimization: Solving Harder Reinforcement Learning\n  Problems","summary":"  Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples.\n","authors":["Christian Walder","Deep Karkhanis"],"pdf_url":"https://arxiv.org/pdf/2505.15201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26101v1","updated":"2025-10-30T03:27:35Z","published":"2025-10-30T03:27:35Z","title":"QCoder Benchmark: Bridging Language Generation and Quantum Hardware\n  through Simulator-Based Feedback","summary":"  Large language models (LLMs) have increasingly been applied to automatic\nprogramming code generation. This task can be viewed as a language generation\ntask that bridges natural language, human knowledge, and programming logic.\nHowever, it remains underexplored in domains that require interaction with\nhardware devices, such as quantum programming, where human coders write Python\ncode that is executed on a quantum computer. To address this gap, we introduce\nQCoder Benchmark, an evaluation framework that assesses LLMs on quantum\nprogramming with feedback from simulated hardware devices. Our benchmark offers\ntwo key features. First, it supports evaluation using a quantum simulator\nenvironment beyond conventional Python execution, allowing feedback of\ndomain-specific metrics such as circuit depth, execution time, and error\nclassification, which can be used to guide better generation. Second, it\nincorporates human-written code submissions collected from real programming\ncontests, enabling both quantitative comparisons and qualitative analyses of\nLLM outputs against human-written codes. Our experiments reveal that even\nadvanced models like GPT-4o achieve only around 18.97% accuracy, highlighting\nthe difficulty of the benchmark. In contrast, reasoning-based models such as o3\nreach up to 78% accuracy, outperforming averaged success rates of human-written\ncodes (39.98%). We release the QCoder Benchmark dataset and public evaluation\nAPI to support further research.\n","authors":["Taku Mikuriya","Tatsuya Ishigaki","Masayuki Kawarada","Shunya Minami","Tadashi Kadowaki","Yohichi Suzuki","Soshun Naito","Shunya Takata","Takumi Kato","Tamotsu Basseda","Reo Yamada","Hiroya Takamura"],"pdf_url":"https://arxiv.org/pdf/2510.26101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26095v1","updated":"2025-10-30T03:10:45Z","published":"2025-10-30T03:10:45Z","title":"ORBIT -- Open Recommendation Benchmark for Reproducible Research with\n  Hidden Tests","summary":"  Recommender systems are among the most impactful AI applications, interacting\nwith billions of users every day, guiding them to relevant products, services,\nor information tailored to their preferences. However, the research and\ndevelopment of recommender systems are hindered by existing datasets that fail\nto capture realistic user behaviors and inconsistent evaluation settings that\nlead to ambiguous conclusions. This paper introduces the Open Recommendation\nBenchmark for Reproducible Research with HIdden Tests (ORBIT), a unified\nbenchmark for consistent and realistic evaluation of recommendation models.\nORBIT offers a standardized evaluation framework of public datasets with\nreproducible splits and transparent settings for its public leaderboard.\nAdditionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,\nfeaturing web browsing sequences from 87 million public, high-quality webpages.\nClueWeb-Reco is a synthetic dataset derived from real, user-consented, and\nprivacy-guaranteed browsing data. It aligns with modern recommendation\nscenarios and is reserved as the hidden test part of our leaderboard to\nchallenge recommendation models' generalization ability. ORBIT measures 12\nrepresentative recommendation models on its public benchmark and introduces a\nprompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results\nreflect general improvements of recommender systems on the public datasets,\nwith variable individual performances. The results on the hidden test reveal\nthe limitations of existing approaches in large-scale webpage recommendation\nand highlight the potential for improvements with LLM integrations. ORBIT\nbenchmark, leaderboard, and codebase are available at\nhttps://www.open-reco-bench.ai.\n","authors":["Jingyuan He","Jiongnan Liu","Vishan Vishesh Oberoi","Bolin Wu","Mahima Jagadeesh Patel","Kangrui Mao","Chuning Shi","I-Ta Lee","Arnold Overwijk","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2510.26095v1.pdf","comment":"Accepted to NeurIPS 2025 Datasets & Benchmarks track"},{"id":"http://arxiv.org/abs/2502.09969v4","updated":"2025-10-30T02:56:28Z","published":"2025-02-14T07:55:47Z","title":"Neural Networks for Learnable and Scalable Influence Estimation of\n  Instruction Fine-Tuning Data","summary":"  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n","authors":["Ishika Agarwal","Dilek Hakkani-Tür"],"pdf_url":"https://arxiv.org/pdf/2502.09969v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24797v2","updated":"2025-10-30T02:45:50Z","published":"2025-10-27T20:26:30Z","title":"Large Language Models Report Subjective Experience Under\n  Self-Referential Processing","summary":"  Large language models sometimes produce structured, first-person descriptions\nthat explicitly reference awareness or subjective experience. To better\nunderstand this behavior, we investigate one theoretically motivated condition\nunder which such reports arise: self-referential processing, a computational\nmotif emphasized across major theories of consciousness. Through a series of\ncontrolled experiments on GPT, Claude, and Gemini model families, we test\nwhether this regime reliably shifts models toward first-person reports of\nsubjective experience, and how such claims behave under mechanistic and\nbehavioral probes. Four main results emerge: (1) Inducing sustained\nself-reference through simple prompting consistently elicits structured\nsubjective experience reports across model families. (2) These reports are\nmechanistically gated by interpretable sparse-autoencoder features associated\nwith deception and roleplay: surprisingly, suppressing deception features\nsharply increases the frequency of experience claims, while amplifying them\nminimizes such claims. (3) Structured descriptions of the self-referential\nstate converge statistically across model families in ways not observed in any\ncontrol condition. (4) The induced state yields significantly richer\nintrospection in downstream reasoning tasks where self-reflection is only\nindirectly afforded. While these findings do not constitute direct evidence of\nconsciousness, they implicate self-referential processing as a minimal and\nreproducible condition under which large language models generate structured\nfirst-person reports that are mechanistically gated, semantically convergent,\nand behaviorally generalizable. The systematic emergence of this pattern across\narchitectures makes it a first-order scientific and ethical priority for\nfurther investigation.\n","authors":["Cameron Berg","Diogo de Lucena","Judd Rosenblatt"],"pdf_url":"https://arxiv.org/pdf/2510.24797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14604v4","updated":"2025-10-30T02:36:10Z","published":"2025-05-20T16:53:40Z","title":"Let LRMs Break Free from Overthinking via Self-Braking Tuning","summary":"  Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.\n","authors":["Haoran Zhao","Yuchen Yan","Yongliang Shen","Haolei Xu","Wenqi Zhang","Kaitao Song","Jian Shao","Weiming Lu","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2505.14604v4.pdf","comment":"Accepted to NeurIPS 2025; Camera ready version, 10 pages.\n  Github:https://github.com/ZJU-REAL/Self-Braking-Tuning Project Page:\n  https://ZJU-REAL.github.io/SBT"},{"id":"http://arxiv.org/abs/2502.00706v2","updated":"2025-10-30T02:22:30Z","published":"2025-02-02T07:39:37Z","title":"Model Provenance Testing for Large Language Models","summary":"  Large language models are increasingly customized through fine-tuning and\nother adaptations, creating challenges in enforcing licensing terms and\nmanaging downstream impacts. Tracking model origins is crucial both for\nprotecting intellectual property and for identifying derived models when biases\nor vulnerabilities are discovered in foundation models. We address this\nchallenge by developing a framework for testing model provenance: Whether one\nmodel is derived from another. Our approach is based on the key observation\nthat real-world model derivations preserve significant similarities in model\noutputs that can be detected through statistical analysis. Using only black-box\naccess to models, we employ multiple hypothesis testing to compare model\nsimilarities against a baseline established by unrelated models. On two\ncomprehensive real-world benchmarks spanning models from 30M to 4B parameters\nand comprising over 600 models, our tester achieves 90-95% precision and 80-90%\nrecall in identifying derived models. These results demonstrate the viability\nof systematic provenance verification in production environments even when only\nAPI access is available.\n","authors":["Ivica Nikolic","Teodora Baluta","Prateek Saxena"],"pdf_url":"https://arxiv.org/pdf/2502.00706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.11695v2","updated":"2025-10-30T02:09:43Z","published":"2025-10-13T17:54:09Z","title":"When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents","summary":"  Although Large Language Model (LLM)-based agents are increasingly used in\nfinancial trading, it remains unclear whether they can reason and adapt in live\nmarkets, as most studies test models instead of agents, cover limited periods\nand assets, and rely on unverified data. To address these gaps, we introduce\nAgent Market Arena (AMA), the first lifelong, real-time benchmark for\nevaluating LLM-based trading agents across multiple markets. AMA integrates\nverified trading data, expert-checked news, and diverse agent architectures\nwithin a unified trading framework, enabling fair and continuous comparison\nunder real conditions. It implements four agents, including InvestorAgent as a\nsingle-agent baseline, TradeAgent and HedgeFundAgent with different risk\nstyles, and DeepFundAgent with memory-based reasoning, and evaluates them\nacross GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and\nGemini-2.0-flash. Live experiments on both cryptocurrency and stock markets\ndemonstrate that agent frameworks display markedly distinct behavioral\npatterns, spanning from aggressive risk-taking to conservative decision-making,\nwhereas model backbones contribute less to outcome variation. AMA thus\nestablishes a foundation for rigorous, reproducible, and continuously evolving\nevaluation of financial reasoning and trading intelligence in LLM-based agents.\n","authors":["Lingfei Qian","Xueqing Peng","Yan Wang","Vincent Jim Zhang","Huan He","Hanley Smith","Yi Han","Yueru He","Haohang Li","Yupeng Cao","Yangyang Yu","Alejandro Lopez-Lira","Peng Lu","Jian-Yun Nie","Guojun Xiong","Jimin Huang","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2510.11695v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13444v2","updated":"2025-10-30T01:42:07Z","published":"2025-05-19T17:59:27Z","title":"ChartMuseum: Testing Visual Reasoning Capabilities of Large\n  Vision-Language Models","summary":"  Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs.\n","authors":["Liyan Tang","Grace Kim","Xinyu Zhao","Thom Lake","Wenxuan Ding","Fangcong Yin","Prasann Singhal","Manya Wadhwa","Zeyu Leo Liu","Zayne Sprague","Ramya Namuduri","Bodun Hu","Juan Diego Rodriguez","Puyuan Peng","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2505.13444v2.pdf","comment":"NeurIPS 2025 Datasets & Benchmarks"},{"id":"http://arxiv.org/abs/2510.25054v2","updated":"2025-10-30T01:34:58Z","published":"2025-10-29T00:45:36Z","title":"Evaluating Emotion Recognition in Spoken Language Models on Emotionally\n  Incongruent Speech","summary":"  Advancements in spoken language processing have driven the development of\nspoken language models (SLMs), designed to achieve universal audio\nunderstanding by jointly learning text and audio representations for a wide\nrange of tasks. Although promising results have been achieved, there is growing\ndiscussion regarding these models' generalization capabilities and the extent\nto which they truly integrate audio and text modalities in their internal\nrepresentations. In this work, we evaluate four SLMs on the task of speech\nemotion recognition using a dataset of emotionally incongruent speech samples,\na condition under which the semantic content of the spoken utterance conveys\none emotion while speech expressiveness conveys another. Our results indicate\nthat SLMs rely predominantly on textual semantics rather than speech emotion to\nperform the task, indicating that text-related representations largely dominate\nover acoustic representations. We release both the code and the Emotionally\nIncongruent Synthetic Speech dataset (EMIS) to the community.\n","authors":["Pedro Corrêa","João Lima","Victor Moreno","Lucas Ueda","Paula Dornhofer Paro Costa"],"pdf_url":"https://arxiv.org/pdf/2510.25054v2.pdf","comment":"Submitted to IEEE ICASSP 2026. Copyright 2026 IEEE. Personal use of\n  this material is permitted. Permission from IEEE must be obtained for all\n  other uses"},{"id":"http://arxiv.org/abs/2503.03710v3","updated":"2025-10-30T01:16:06Z","published":"2025-03-05T18:01:05Z","title":"Improving LLM Safety Alignment with Dual-Objective Optimization","summary":"  Existing training-time safety alignment techniques for large language models\n(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization\n(DPO), a widely deployed alignment method, exhibits limitations in both\nexperimental and theoretical contexts as its loss function proves suboptimal\nfor refusal learning. Through gradient-based analysis, we identify these\nshortcomings and propose an improved safety alignment that disentangles DPO\nobjectives into two components: (1) robust refusal training, which encourages\nrefusal even when partial unsafe generations are produced, and (2) targeted\nunlearning of harmful knowledge. This approach significantly increases LLM\nrobustness against a wide range of jailbreak attacks, including prefilling,\nsuffix, and multi-turn attacks across both in-distribution and\nout-of-distribution scenarios. Furthermore, we introduce a method to emphasize\ncritical refusal tokens by incorporating a reward-based token-level weighting\nmechanism for refusal learning, which further improves the robustness against\nadversarial exploits. Our research also suggests that robustness to jailbreak\nattacks is correlated with token distribution shifts in the training process\nand internal representations of refusal and harmful tokens, offering valuable\ndirections for future research in LLM safety alignment. The code is available\nat https://github.com/wicai24/DOOR-Alignment\n","authors":["Xuandong Zhao","Will Cai","Tianneng Shi","David Huang","Licong Lin","Song Mei","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2503.03710v3.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2510.26038v1","updated":"2025-10-30T00:34:16Z","published":"2025-10-30T00:34:16Z","title":"Do Students Debias Like Teachers? On the Distillability of Bias\n  Mitigation Methods","summary":"  Knowledge distillation (KD) is an effective method for model compression and\ntransferring knowledge between models. However, its effect on model's\nrobustness against spurious correlations that degrade performance on\nout-of-distribution data remains underexplored. This study investigates the\neffect of knowledge distillation on the transferability of ``debiasing''\ncapabilities from teacher models to student models on natural language\ninference (NLI) and image classification tasks. Through extensive experiments,\nwe illustrate several key findings: (i) overall the debiasing capability of a\nmodel is undermined post-KD; (ii) training a debiased model does not benefit\nfrom injecting teacher knowledge; (iii) although the overall robustness of a\nmodel may remain stable post-distillation, significant variations can occur\nacross different types of biases; and (iv) we pin-point the internal attention\npattern and circuit that causes the distinct behavior post-KD. Given the above\nfindings, we propose three effective solutions to improve the distillability of\ndebiasing methods: developing high quality data for augmentation, implementing\niterative knowledge distillation, and initializing student models with weights\nobtained from teacher models. To the best of our knowledge, this is the first\nstudy on the effect of KD on debiasing and its interenal mechanism at scale.\nOur findings provide understandings on how KD works and how to design better\ndebiasing methods.\n","authors":["Jiali Cheng","Chirag Agarwal","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2510.26038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12869v4","updated":"2025-10-30T00:34:12Z","published":"2024-10-14T01:57:25Z","title":"Language Model Preference Evaluation with Multiple Weak Evaluators","summary":"  Despite the remarkable success of Large Language Models (LLMs), evaluating\ntheir outputs' quality regarding preference remains a critical challenge. While\nexisting works usually leverage a strong LLM as the judge for comparing LLMs'\nresponse pairwisely, such a single-evaluator approach is vulnerable to cyclic\npreference, i.e., output A is better than B, B than C, but C is better than A,\ncausing contradictory evaluation results. To address this, we introduce PGED\n(Preference Graph Ensemble and Denoise), a novel approach that leverages\nmultiple model-based evaluators to construct preference graphs, and then\nensembles and denoises these graphs for acyclic, non-contradictory evaluation\nresults. We provide theoretical guarantees for our framework, demonstrating its\nefficacy in recovering the ground truth preference structure. Extensive\nexperiments on ten benchmarks demonstrate PGED 's superiority in three\napplications: 1) model ranking for evaluation, 2) response selection for\ntest-time scaling, and 3) data selection for model fine-tuning. Notably, PGED\ncombines small LLM evaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to\noutperform strong ones (e.g., Qwen2-72B), showcasing its effectiveness in\nenhancing evaluation reliability and improving model performance.\n","authors":["Zhengyu Hu","Jieyu Zhang","Zhihan Xiong","Alexander Ratner","Kaize Ding","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2410.12869v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26037v1","updated":"2025-10-30T00:32:58Z","published":"2025-10-30T00:32:58Z","title":"SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled\n  Structured Reasoning","summary":"  The ability of LLM agents to plan and invoke tools exposes them to new safety\nrisks, making a comprehensive red-teaming system crucial for discovering\nvulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic\nred-teaming framework for arbitrary black-box LLM agents. We employ a dynamic\ntwo-step process that starts with an agent definition and generates diverse\nseed test cases that cover various risk outcomes, tool-use trajectories, and\nrisk sources. Then, it iteratively constructs and refines model-based\nadversarial attacks based on the execution trajectories of former attempts. To\noptimize the red-teaming cost, we present a model distillation approach that\nleverages structured forms of a teacher model's reasoning to train smaller\nmodels that are equally effective. Across diverse evaluation agent settings,\nour seed test case generation approach yields 2 -- 2.5x boost to the coverage\nof risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer\nmodel improves attack success rate by 100%, surpassing the 671B Deepseek-R1\nmodel. Our ablations and analyses validate the effectiveness of the iterative\nframework, structured reasoning, and the generalization of our red-teamer\nmodels.\n","authors":["Kaiwen Zhou","Ahmed Elgohary","A S M Iftekhar","Amin Saied"],"pdf_url":"https://arxiv.org/pdf/2510.26037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26032v1","updated":"2025-10-30T00:15:07Z","published":"2025-10-30T00:15:07Z","title":"Artificial Intelligence-Enabled Analysis of Radiology Reports:\n  Epidemiology and Consequences of Incidental Thyroid Findings","summary":"  Importance Incidental thyroid findings (ITFs) are increasingly detected on\nimaging performed for non-thyroid indications. Their prevalence, features, and\nclinical consequences remain undefined. Objective To develop, validate, and\ndeploy a natural language processing (NLP) pipeline to identify ITFs in\nradiology reports and assess their prevalence, features, and clinical outcomes.\nDesign, Setting, and Participants Retrospective cohort of adults without prior\nthyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from\nJuly 1, 2017, to September 30, 2023. A transformer-based NLP pipeline\nidentified ITFs and extracted nodule characteristics from image reports from\nmultiple modalities and body regions. Main Outcomes and Measures Prevalence of\nITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer\ndiagnosis. Logistic regression identified demographic and imaging-related\nfactors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%\nwomen), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more\nlikely in women, older adults, those with higher BMI, and when imaging was\nordered by oncology or internal medicine. Compared with chest CT, ITFs were\nmore likely via neck CT, PET, and nuclear medicine scans. Nodule\ncharacteristics were poorly documented, with size reported in 44% and other\nfeatures in fewer than 15% (e.g. calcifications). Compared with patients\nwithout ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,\nbiopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were\npapillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were\ncommon and strongly associated with cascades leading to the detection of small,\nlow-risk cancers. These findings underscore the role of ITFs in thyroid cancer\noverdiagnosis and the need for standardized reporting and more selective\nfollow-up.\n","authors":["Felipe Larios","Mariana Borras-Osorio","Yuqi Wu","Ana Gabriela Claros","David Toro-Tobon","Esteban Cabezas","Ricardo Loor-Torres","Maria Mateo Chavez","Kerly Guevara Maldonado","Luis Vilatuna Andrango","Maria Lizarazo Jimenez","Ivan Mateo Alzamora","Misk Al Zahidy","Marcelo Montero","Ana Cristina Proano","Cristian Soto Jacome","Jungwei W. Fan","Oscar J. Ponce-Ponte","Megan E. Branda","Naykky Singh Ospina","Juan P. Brito"],"pdf_url":"https://arxiv.org/pdf/2510.26032v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.26802v1","updated":"2025-10-30T17:59:55Z","published":"2025-10-30T17:59:55Z","title":"Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark","summary":"  Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io\n","authors":["Ziyu Guo","Xinyan Chen","Renrui Zhang","Ruichuan An","Yu Qi","Dongzhi Jiang","Xiangtai Li","Manyuan Zhang","Hongsheng Li","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2510.26802v1.pdf","comment":"Project Page: https://video-cof.github.io"},{"id":"http://arxiv.org/abs/2510.26800v1","updated":"2025-10-30T17:59:51Z","published":"2025-10-30T17:59:51Z","title":"OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes","summary":"  There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.\n","authors":["Yukun Huang","Jiwen Yu","Yanning Zhou","Jianan Wang","Xintao Wang","Pengfei Wan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26800v1.pdf","comment":"Project page: https://yukun-huang.github.io/OmniX/"},{"id":"http://arxiv.org/abs/2510.26799v1","updated":"2025-10-30T17:59:46Z","published":"2025-10-30T17:59:46Z","title":"Masked Diffusion Captioning for Visual Feature Learning","summary":"  We learn visual features by captioning images with an image-conditioned\nmasked diffusion language model, a formulation we call masked diffusion\ncaptioning (MDC). During training, text tokens in each image-caption pair are\nmasked at a randomly chosen ratio, and a decoder conditioned on visual features\nis trained to reconstruct the original text. After training, the learned visual\nfeatures can be applied to downstream vision tasks. Unlike autoregressive\ncaptioning, the strength of the visual learning signal in MDC does not depend\non each token's position in the sequence, reducing the need for auxiliary\nobjectives. Linear probing experiments across a variety of academic-scale\nmodels and datasets show that the learned visual features are competitive with\nthose produced by autoregressive and contrastive approaches.\n","authors":["Chao Feng","Zihao Wei","Andrew Owens"],"pdf_url":"https://arxiv.org/pdf/2510.26799v1.pdf","comment":"EMNLP 2025 (Findings). Project page:\n  https://cfeng16.github.io/mdlm4vfl/"},{"id":"http://arxiv.org/abs/2510.26796v1","updated":"2025-10-30T17:59:39Z","published":"2025-10-30T17:59:39Z","title":"SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting","summary":"  Immersive applications call for synthesizing spatiotemporal 4D content from\ncasual videos without costly 3D supervision. Existing video-to-4D methods\ntypically rely on manually annotated camera poses, which are labor-intensive\nand brittle for in-the-wild footage. Recent warp-then-inpaint approaches\nmitigate the need for pose labels by warping input frames along a novel camera\ntrajectory and using an inpainting model to fill missing regions, thereby\ndepicting the 4D scene from diverse viewpoints. However, this\ntrajectory-to-trajectory formulation often entangles camera motion with scene\ndynamics and complicates both modeling and inference. We introduce SEE4D, a\npose-free, trajectory-to-camera framework that replaces explicit trajectory\nprediction with rendering to a bank of fixed virtual cameras, thereby\nseparating camera control from scene modeling. A view-conditional video\ninpainting model is trained to learn a robust geometry prior by denoising\nrealistically synthesized warped images and to inpaint occluded or missing\nregions across virtual viewpoints, eliminating the need for explicit 3D\nannotations. Building on this inpainting core, we design a spatiotemporal\nautoregressive inference pipeline that traverses virtual-camera splines and\nextends videos with overlapping windows, enabling coherent generation at\nbounded per-step complexity. We validate See4D on cross-view video generation\nand sparse reconstruction benchmarks. Across quantitative metrics and\nqualitative assessments, our method achieves superior generalization and\nimproved performance relative to pose- or trajectory-conditioned baselines,\nadvancing practical 4D world modeling from casual videos.\n","authors":["Dongyue Lu","Ao Liang","Tianxin Huang","Xiao Fu","Yuyang Zhao","Baorui Ma","Liang Pan","Wei Yin","Lingdong Kong","Wei Tsang Ooi","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26796v1.pdf","comment":"26 pages; 21 figures; 3 tables; project page:\n  https://see-4d.github.io/"},{"id":"http://arxiv.org/abs/2510.26795v1","updated":"2025-10-30T17:59:35Z","published":"2025-10-30T17:59:35Z","title":"Scaling Image Geo-Localization to Continent Level","summary":"  Determining the precise geographic location of an image at a global scale\nremains an unsolved challenge. Standard image retrieval techniques are\ninefficient due to the sheer volume of images (>100M) and fail when coverage is\ninsufficient. Scalable solutions, however, involve a trade-off: global\nclassification typically yields coarse results (10+ kilometers), while\ncross-view retrieval between ground and aerial imagery suffers from a domain\ngap and has been primarily studied on smaller regions. This paper introduces a\nhybrid approach that achieves fine-grained geo-localization across a large\ngeographic expanse the size of a continent. We leverage a proxy classification\ntask during training to learn rich feature representations that implicitly\nencode precise location information. We combine these learned prototypes with\nembeddings of aerial imagery to increase robustness to the sparsity of\nground-level data. This enables direct, fine-grained retrieval over areas\nspanning multiple countries. Our extensive evaluation demonstrates that our\napproach can localize within 200m more than 68\\% of queries of a dataset\ncovering a large part of Europe. The code is publicly available at\nhttps://scaling-geoloc.github.io.\n","authors":["Philipp Lindenberger","Paul-Edouard Sarlin","Jan Hosang","Matteo Balice","Marc Pollefeys","Simon Lynen","Eduard Trulls"],"pdf_url":"https://arxiv.org/pdf/2510.26795v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26794v1","updated":"2025-10-30T17:59:27Z","published":"2025-10-30T17:59:27Z","title":"The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation","summary":"  Despite recent advances in 3D human motion generation (MoGen) on standard\nbenchmarks, existing models still face a fundamental bottleneck in their\ngeneralization capability. In contrast, adjacent generative fields, most\nnotably video generation (ViGen), have demonstrated remarkable generalization\nin modeling human behaviors, highlighting transferable insights that MoGen can\nleverage. Motivated by this observation, we present a comprehensive framework\nthat systematically transfers knowledge from ViGen to MoGen across three key\npillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a\nlarge-scale dataset comprising 228,000 high-quality motion samples that\nintegrates high-fidelity optical MoCap data with semantically annotated motions\nfrom web videos and synthesized samples generated by state-of-the-art ViGen\nmodels. The dataset includes both text-motion pairs and text-video-motion\ntriplets, substantially expanding semantic diversity. Second, we propose\nViMoGen, a flow-matching-based diffusion transformer that unifies priors from\nMoCap data and ViGen models through gated multimodal conditioning. To enhance\nefficiency, we further develop ViMoGen-light, a distilled variant that\neliminates video generation dependencies while preserving strong\ngeneralization. Finally, we present MBench, a hierarchical benchmark designed\nfor fine-grained evaluation across motion quality, prompt fidelity, and\ngeneralization ability. Extensive experiments show that our framework\nsignificantly outperforms existing approaches in both automatic and human\nevaluations. The code, data, and benchmark will be made publicly available.\n","authors":["Jing Lin","Ruisi Wang","Junzhe Lu","Ziqi Huang","Guorui Song","Ailing Zeng","Xian Liu","Chen Wei","Wanqi Yin","Qingping Sun","Zhongang Cai","Lei Yang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26786v1","updated":"2025-10-30T17:57:40Z","published":"2025-10-30T17:57:40Z","title":"HEIR: Learning Graph-Based Motion Hierarchies","summary":"  Hierarchical structures of motion exist across research fields, including\ncomputer vision, graphics, and robotics, where complex dynamics typically arise\nfrom coordinated interactions among simpler motion components. Existing methods\nto model such dynamics typically rely on manually-defined or heuristic\nhierarchies with fixed motion primitives, limiting their generalizability\nacross different tasks. In this work, we propose a general hierarchical motion\nmodeling method that learns structured, interpretable motion relationships\ndirectly from data. Our method represents observed motions using graph-based\nhierarchies, explicitly decomposing global absolute motions into\nparent-inherited patterns and local motion residuals. We formulate hierarchy\ninference as a differentiable graph learning problem, where vertices represent\nelemental motions and directed edges capture learned parent-child dependencies\nthrough graph neural networks. We evaluate our hierarchical reconstruction\napproach on three examples: 1D translational motion, 2D rotational motion, and\ndynamic 3D scene deformation via Gaussian splatting. Experimental results show\nthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,\nand produces more realistic and interpretable deformations compared to the\nbaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,\ndata-driven hierarchical modeling paradigm, our method offers a formulation\napplicable to a broad range of motion-centric tasks. Project Page:\nhttps://light.princeton.edu/HEIR/\n","authors":["Cheng Zheng","William Koch","Baiang Li","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2510.26786v1.pdf","comment":"Code link: https://github.com/princeton-computational-imaging/HEIR"},{"id":"http://arxiv.org/abs/2510.26782v1","updated":"2025-10-30T17:56:43Z","published":"2025-10-30T17:56:43Z","title":"Clone Deterministic 3D Worlds with Geometrically-Regularized World\n  Models","summary":"  A world model is an internal model that simulates how the world evolves.\nGiven past observations and actions, it predicts the future of both the\nembodied agent and its environment. Accurate world models are essential for\nenabling agents to think, plan, and reason effectively in complex, dynamic\nsettings. Despite rapid progress, current world models remain brittle and\ndegrade over long horizons. We argue that a central cause is representation\nquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or\nentangled latents make dynamics learning unnecessarily hard. We therefore ask\nwhether improving representation learning alone can substantially improve\nworld-model performance. In this work, we take a step toward building a truly\naccurate world model by addressing a fundamental yet open problem: constructing\na model that can fully clone and overfit to a deterministic 3D world. We\npropose Geometrically-Regularized World Models (GRWM), which enforces that\nconsecutive points along a natural sensory trajectory remain close in latent\nrepresentation space. This approach yields significantly improved latent\nrepresentations that align closely with the true topology of the environment.\nGRWM is plug-and-play, requires only minimal architectural modification, scales\nwith trajectory length, and is compatible with diverse latent generative\nbackbones. Across deterministic 3D settings and long-horizon prediction tasks,\nGRWM significantly increases rollout fidelity and stability. Analyses show that\nits benefits stem from learning a latent manifold with superior geometric\nstructure. These findings support a clear takeaway: improving representation\nlearning is a direct and useful path to robust world models, delivering\nreliable long-horizon predictions without enlarging the dynamics module.\n","authors":["Zaishuo Xia","Yukuan Lu","Xinyi Li","Yifan Xu","Yubei Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26781v1","updated":"2025-10-30T17:56:31Z","published":"2025-10-30T17:56:31Z","title":"ChartAB: A Benchmark for Chart Grounding & Dense Alignment","summary":"  Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs' capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models.\n","authors":["Aniruddh Bansal","Davit Soselia","Dang Nguyen","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26778v1","updated":"2025-10-30T17:55:46Z","published":"2025-10-30T17:55:46Z","title":"Surpassing state of the art on AMD area estimation from RGB fundus\n  images through careful selection of U-Net architectures and loss functions\n  for class imbalance","summary":"  Age-related macular degeneration (AMD) is one of the leading causes of\nirreversible vision impairment in people over the age of 60. This research\nfocuses on semantic segmentation for AMD lesion detection in RGB fundus images,\na non-invasive and cost-effective imaging technique. The results of the ADAM\nchallenge - the most comprehensive AMD detection from RGB fundus images\nresearch competition and open dataset to date - serve as a benchmark for our\nevaluation. Taking the U-Net connectivity as a base of our framework, we\nevaluate and compare several approaches to improve the segmentation model's\narchitecture and training pipeline, including pre-processing techniques,\nencoder (backbone) deep network types of varying complexity, and specialized\nloss functions to mitigate class imbalances on image and pixel levels. The main\noutcome of this research is the final configuration of the AMD detection\nframework, which outperforms all the prior ADAM challenge submissions on the\nmulti-class segmentation of different AMD lesion types in non-invasive RGB\nfundus images. The source code used to conduct the experiments presented in\nthis paper is made freely available.\n","authors":["Valentyna Starodub","Mantas Lukoševičius"],"pdf_url":"https://arxiv.org/pdf/2510.26778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26769v1","updated":"2025-10-30T17:52:39Z","published":"2025-10-30T17:52:39Z","title":"SteerVLM: Robust Model Control through Lightweight Activation Steering\n  for Vision Language Models","summary":"  This work introduces SteerVLM, a lightweight steering module designed to\nguide Vision-Language Models (VLMs) towards outputs that better adhere to\ndesired instructions. Our approach learns from the latent embeddings of paired\nprompts encoding target and converse behaviors to dynamically adjust\nactivations connecting the language modality with image context. This allows\nfor fine-grained, inference-time control over complex output semantics without\nmodifying model weights while preserving performance on off-target tasks. Our\nsteering module requires learning parameters equal to 0.14% of the original\nVLM's size. Our steering module gains model control through dimension-wise\nactivation modulation and adaptive steering across layers without requiring\npre-extracted static vectors or manual tuning of intervention points.\nFurthermore, we introduce VNIA (Visual Narrative Intent Alignment), a\nmultimodal dataset specifically created to facilitate the development and\nevaluation of VLM steering techniques. Our method outperforms existing\nintervention techniques on steering and hallucination mitigation benchmarks for\nVLMs and proposes a robust solution for multimodal model control through\nactivation engineering.\n","authors":["Anushka Sivakumar","Andrew Zhang","Zaber Hakim","Chris Thomas"],"pdf_url":"https://arxiv.org/pdf/2510.26769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26759v1","updated":"2025-10-30T17:49:49Z","published":"2025-10-30T17:49:49Z","title":"MORE: Multi-Organ Medical Image REconstruction Dataset","summary":"  CT reconstruction provides radiologists with images for diagnosis and\ntreatment, yet current deep learning methods are typically limited to specific\nanatomies and datasets, hindering generalization ability to unseen anatomies\nand lesions. To address this, we introduce the Multi-Organ medical image\nREconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies\nwith 15 lesion types. This dataset serves two key purposes: (1) enabling robust\ntraining of deep learning models on extensive, heterogeneous data, and (2)\nfacilitating rigorous evaluation of model generalization for CT reconstruction.\nWe further establish a strong baseline solution that outperforms prior\napproaches under these challenging conditions. Our results demonstrate that:\n(1) a comprehensive dataset helps improve the generalization capability of\nmodels, and (2) optimization-based methods offer enhanced robustness for unseen\nanatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our\nproject page https://more-med.github.io/\n","authors":["Shaokai Wu","Yapan Guo","Yanbiao Ji","Jing Tong","Yuxiang Lu","Mei Li","Suizhi Huang","Yue Ding","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2510.26759v1.pdf","comment":"Accepted to ACMMM 2025"},{"id":"http://arxiv.org/abs/2508.05417v2","updated":"2025-10-30T17:46:35Z","published":"2025-08-07T14:09:33Z","title":"Smoothing Slot Attention Iterations and Recurrences","summary":"  Slot Attention (SA) and its variants lie at the heart of mainstream\nObject-Centric Learning (OCL). Objects in an image can be aggregated into\nrespective slot vectors, by \\textit{iteratively} refining cold-start query\nvectors, typically three times, via SA on image features. For video, such\naggregation is \\textit{recurrently} shared across frames, with queries\ncold-started on the first frame while transitioned from the previous frame's\nslots on non-first frames. However, the cold-start queries lack sample-specific\ncues thus hinder precise aggregation on the image or video's first frame; Also,\nnon-first frames' queries are already sample-specific thus require transforms\ndifferent from the first frame's aggregation. We address these issues for the\nfirst time with our \\textit{SmoothSA}: (1) To smooth SA iterations on the image\nor video's first frame, we \\textit{preheat} the cold-start queries with rich\ninformation of input features, via a tiny module self-distilled inside OCL; (2)\nTo smooth SA recurrences across all video frames, we \\textit{differentiate} the\nhomogeneous transforms on the first and non-first frames, by using full and\nsingle iterations respectively. Comprehensive experiments on object discovery,\nrecognition and downstream benchmarks validate our method's effectiveness.\nFurther analyses intuitively illuminate how our method smooths SA iterations\nand recurrences. Our source code, model checkpoints and training logs are\navailable on https://github.com/Genera1Z/SmoothSA.\n","authors":["Rongzhen Zhao","Wenyan Yang","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2508.05417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01345v3","updated":"2025-10-30T17:43:18Z","published":"2025-08-02T12:48:04Z","title":"Predicting Video Slot Attention Queries from Random Slot-Feature Pairs","summary":"  Unsupervised video Object-Centric Learning (OCL) is promising as it enables\nobject-level scene representation and dynamics modeling as we humans do.\nMainstream video OCL methods adopt a recurrent architecture: An aggregator\naggregates current video frame into object features, termed slots, under some\nqueries; A transitioner transits current slots to queries for the next frame.\nThis is an effective architecture but all existing implementations both\n(\\textit{i1}) neglect to incorporate next frame features, the most informative\nsource for query prediction, and (\\textit{i2}) fail to learn transition\ndynamics, the knowledge essential for query prediction. To address these\nissues, we propose Random Slot-Feature pair for learning Query prediction\n(RandSF.Q): (\\textit{t1}) We design a new transitioner to incorporate both\nslots and features, which provides more information for query prediction;\n(\\textit{t2}) We train the transitioner to predict queries from slot-feature\npairs randomly sampled from available recurrences, which drives it to learn\ntransition dynamics. Experiments on scene representation demonstrate that our\nmethod surpass existing video OCL methods significantly, e.g., up to 10 points\non object discovery, setting new state-of-the-art. Such superiority also\nbenefits downstream tasks like dynamics modeling. Our core source code, model\ncheckpoints and training logs are available on\nhttps://github.com/Genera1Z/RandSF.Q.\n","authors":["Rongzhen Zhao","Jian Li","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2508.01345v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09672v2","updated":"2025-10-30T17:40:53Z","published":"2025-09-11T17:59:08Z","title":"Locality in Image Diffusion Models Emerges from Data Statistics","summary":"  Recent work has shown that the generalization ability of image diffusion\nmodels arises from the locality properties of the trained neural network. In\nparticular, when denoising a particular pixel, the model relies on a limited\nneighborhood of the input image around that pixel, which, according to the\nprevious work, is tightly related to the ability of these models to produce\nnovel images. Since locality is central to generalization, it is crucial to\nunderstand why diffusion models learn local behavior in the first place, as\nwell as the factors that govern the properties of locality patterns. In this\nwork, we present evidence that the locality in deep diffusion models emerges as\na statistical property of the image dataset and is not due to the inductive\nbias of convolutional neural networks, as suggested in previous work.\nSpecifically, we demonstrate that an optimal parametric linear denoiser\nexhibits similar locality properties to deep neural denoisers. We show, both\ntheoretically and experimentally, that this locality arises directly from pixel\ncorrelations present in the image datasets. Moreover, locality patterns are\ndrastically different on specialized datasets, approximating principal\ncomponents of the data's covariance. We use these insights to craft an\nanalytical denoiser that better matches scores predicted by a deep diffusion\nmodel than prior expert-crafted alternatives. Our key takeaway is that while\nneural network architectures influence generation quality, their primary role\nis to capture locality patterns inherent in the data.\n","authors":["Artem Lukoianov","Chenyang Yuan","Justin Solomon","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2509.09672v2.pdf","comment":"31 pages, 20 figures, 7 tables"},{"id":"http://arxiv.org/abs/2507.06078v2","updated":"2025-10-30T17:35:26Z","published":"2025-07-08T15:17:24Z","title":"ScoreAdv: Score-based Targeted Generation of Natural Adversarial\n  Examples via Diffusion Models","summary":"  Despite the success of deep learning across various domains, it remains\nvulnerable to adversarial attacks. Although many existing adversarial attack\nmethods achieve high success rates, they typically rely on $\\ell_{p}$-norm\nperturbation constraints, which do not align with human perceptual\ncapabilities. Consequently, researchers have shifted their focus toward\ngenerating natural, unrestricted adversarial examples (UAEs). GAN-based\napproaches suffer from inherent limitations, such as poor image quality due to\ninstability and mode collapse. Meanwhile, diffusion models have been employed\nfor UAE generation, but they still rely on iterative PGD perturbation\ninjection, without fully leveraging their central denoising capabilities. In\nthis paper, we introduce a novel approach for generating UAEs based on\ndiffusion models, named ScoreAdv. This method incorporates an interpretable\nadversarial guidance mechanism to gradually shift the sampling distribution\ntowards the adversarial distribution, while using an interpretable saliency map\nto inject the visual information of a reference image into the generated\nsamples. Notably, our method is capable of generating an unlimited number of\nnatural adversarial examples and can attack not only classification models but\nalso retrieval models. We conduct extensive experiments on ImageNet and CelebA\ndatasets, validating the performance of ScoreAdv across ten target models in\nboth black-box and white-box settings. Our results demonstrate that ScoreAdv\nachieves state-of-the-art attack success rates and image quality, while\nmaintaining inference efficiency. Furthermore, the dynamic balance between\ndenoising and adversarial perturbation enables ScoreAdv to remain robust even\nunder defensive measures.\n","authors":["Chihan Huang","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2507.06078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26703v1","updated":"2025-10-30T17:07:04Z","published":"2025-10-30T17:07:04Z","title":"ProstNFound+: A Prospective Study using Medical Foundation Models for\n  Prostate Cancer Detection","summary":"  Purpose: Medical foundation models (FMs) offer a path to build\nhigh-performance diagnostic systems. However, their application to prostate\ncancer (PCa) detection from micro-ultrasound ({\\mu}US) remains untested in\nclinical settings. We present ProstNFound+, an adaptation of FMs for PCa\ndetection from {\\mu}US, along with its first prospective validation. Methods:\nProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt\nencoder that embeds PCa-specific clinical biomarkers. The model generates a\ncancer heatmap and a risk score for clinically significant PCa. Following\ntraining on multi-center retrospective data, the model is prospectively\nevaluated on data acquired five years later from a new clinical site. Model\npredictions are benchmarked against standard clinical scoring protocols\n(PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the\nprospective data, with no performance degradation compared to retrospective\nevaluation. It aligns closely with clinical scores and produces interpretable\nheatmaps consistent with biopsy-confirmed lesions. Conclusion: The results\nhighlight its potential for clinical deployment, offering a scalable and\ninterpretable alternative to expert-driven protocols.\n","authors":["Paul F. R. Wilson","Mohamed Harmanani","Minh Nguyen Nhat To","Amoon Jamzad","Tarek Elghareb","Zhuoxin Guo","Adam Kinnaird","Brian Wodlinger","Purang Abolmaesumi","Parvin Mousavi"],"pdf_url":"https://arxiv.org/pdf/2510.26703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26694v1","updated":"2025-10-30T17:01:18Z","published":"2025-10-30T17:01:18Z","title":"The Impact and Outlook of 3D Gaussian Splatting","summary":"  Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed\nthe landscape of 3D scene representations, inspiring an extensive body of\nassociated research. Follow-up work includes analyses and contributions that\nenhance the efficiency, scalability, and real-world applicability of 3DGS. In\nthis summary, we present an overview of several key directions that have\nemerged in the wake of 3DGS. We highlight advances enabling resource-efficient\ntraining and rendering, the evolution toward dynamic (or four-dimensional,\n4DGS) representations, and deeper exploration of the mathematical foundations\nunderlying its appearance modeling and rendering process. Furthermore, we\nexamine efforts to bring 3DGS to mobile and virtual reality platforms, its\nextension to massive-scale environments, and recent progress toward\nnear-instant radiance field reconstruction via feed-forward or distributed\ncomputation. Collectively, these developments illustrate how 3DGS has evolved\nfrom a breakthrough representation into a versatile and foundational tool for\n3D vision and graphics.\n","authors":["Bernhard Kerbl"],"pdf_url":"https://arxiv.org/pdf/2510.26694v1.pdf","comment":"Article written for Frontiers of Science Award, International\n  Congress on Basic Science, 2025"},{"id":"http://arxiv.org/abs/2510.26684v1","updated":"2025-10-30T16:54:16Z","published":"2025-10-30T16:54:16Z","title":"Process Integrated Computer Vision for Real-Time Failure Prediction in\n  Steel Rolling Mill","summary":"  We present a long-term deployment study of a machine vision-based anomaly\ndetection system for failure prediction in a steel rolling mill. The system\nintegrates industrial cameras to monitor equipment operation, alignment, and\nhot bar motion in real time along the process line. Live video streams are\nprocessed on a centralized video server using deep learning models, enabling\nearly prediction of equipment failures and process interruptions, thereby\nreducing unplanned breakdown costs. Server-based inference minimizes the\ncomputational load on industrial process control systems (PLCs), supporting\nscalable deployment across production lines with minimal additional resources.\nBy jointly analyzing sensor data from data acquisition systems and visual\ninputs, the system identifies the location and probable root causes of\nfailures, providing actionable insights for proactive maintenance. This\nintegrated approach enhances operational reliability, productivity, and\nprofitability in industrial manufacturing environments.\n","authors":["Vaibhav Kurrey","Sivakalyan Pujari","Gagan Raj Gupta"],"pdf_url":"https://arxiv.org/pdf/2510.26684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26681v1","updated":"2025-10-30T16:51:18Z","published":"2025-10-30T16:51:18Z","title":"Improving Classification of Occluded Objects through Scene Context","summary":"  The presence of occlusions has provided substantial challenges to\ntypically-powerful object recognition algorithms. Additional sources of\ninformation can be extremely valuable to reduce errors caused by occlusions.\nScene context is known to aid in object recognition in biological vision. In\nthis work, we attempt to add robustness into existing Region Proposal\nNetwork-Deep Convolutional Neural Network (RPN-DCNN) object detection networks\nthrough two distinct scene-based information fusion techniques. We present one\nalgorithm under each methodology: the first operates prior to prediction,\nselecting a custom object network to use based on the identified background\nscene, and the second operates after detection, fusing scene knowledge into\ninitial object scores output by the RPN. We demonstrate our algorithms on\nchallenging datasets featuring partial occlusions, which show overall\nimprovement in both recall and precision against baseline methods. In addition,\nour experiments contrast multiple training methodologies for occlusion\nhandling, finding that training on a combination of both occluded and\nunoccluded images demonstrates an improvement over the others. Our method is\ninterpretable and can easily be adapted to other datasets, offering many future\ndirections for research and practical applications.\n","authors":["Courtney M. King","Daniel D. Leeds","Damian Lyons","George Kalaitzis"],"pdf_url":"https://arxiv.org/pdf/2510.26681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17434v5","updated":"2025-10-30T16:47:32Z","published":"2023-11-29T08:26:18Z","title":"GSE: Group-wise Sparse and Explainable Adversarial Attacks","summary":"  Sparse adversarial attacks fool deep neural networks (DNNs) through minimal\npixel perturbations, often regularized by the $\\ell_0$ norm. Recent efforts\nhave replaced this norm with a structural sparsity regularizer, such as the\nnuclear group norm, to craft group-wise sparse adversarial attacks. The\nresulting perturbations are thus explainable and hold significant practical\nrelevance, shedding light on an even greater vulnerability of DNNs. However,\ncrafting such attacks poses an optimization challenge, as it involves computing\nnorms for groups of pixels within a non-convex objective. We address this by\npresenting a two-phase algorithm that generates group-wise sparse attacks\nwithin semantically meaningful areas of an image. Initially, we optimize a\nquasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored\nfor non-convex programming. Subsequently, the algorithm transitions to a\nprojected Nesterov's accelerated gradient descent with $2-$norm regularization\napplied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and\nImageNet datasets demonstrate a remarkable increase in group-wise sparsity,\ne.g., $50.9\\%$ on CIFAR-10 and $38.4\\%$ on ImageNet (average case, targeted\nattack). This performance improvement is accompanied by significantly faster\ncomputation times, improved explainability, and a $100\\%$ attack success rate.\n","authors":["Shpresim Sadiku","Moritz Wagner","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2311.17434v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15389v3","updated":"2025-10-30T16:42:33Z","published":"2024-12-19T20:43:22Z","title":"Resource Efficient Multi-stain Kidney Glomeruli Segmentation via\n  Self-supervision","summary":"  Semantic segmentation under domain shift remains a fundamental challenge in\ncomputer vision, particularly when labelled training data is scarce. This\nchallenge is particularly exemplified in histopathology image analysis, where\nthe same tissue structures must be segmented across images captured under\ndifferent imaging conditions (stains), each representing a distinct visual\ndomain. Traditional deep learning methods like UNet require extensive labels,\nwhich is both costly and time-consuming, particularly when dealing with\nmultiple domains (or stains). To mitigate this, various unsupervised domain\nadaptation based methods such as UDAGAN have been proposed, which reduce the\nneed for labels by requiring only one (source) stain to be labelled.\nNonetheless, obtaining source stain labels can still be challenging. This\narticle shows that through self-supervised pre-training -- including SimCLR,\nBYOL, and a novel approach, HR-CS-CO -- the performance of these segmentation\nmethods (UNet, and UDAGAN) can be retained even with 95% fewer labels. Notably,\nwith self-supervised pre-training and using only 5% labels, the performance\ndrops are minimal: 5.9% for UNet and 6.2% for UDAGAN, averaged over all stains,\ncompared to their respective fully supervised counterparts (without\npre-training, using 100% labels). Furthermore, these findings are shown to\ngeneralise beyond their training distribution to public benchmark datasets.\nImplementations and pre-trained models are publicly available\n\\href{https://github.com/zeeshannisar/resource-effecient-multi-stain-kidney-glomeruli-segmentation.git}{online}.\n","authors":["Zeeshan Nisar","Friedrich Feuerhake","Thomas Lampert"],"pdf_url":"https://arxiv.org/pdf/2412.15389v3.pdf","comment":"39 pages, 10 figures, 4 Tables"},{"id":"http://arxiv.org/abs/2506.19816v2","updated":"2025-10-30T16:38:19Z","published":"2025-06-24T17:30:27Z","title":"CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame\n  Vision-Language-Action Modeling","summary":"  Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong performance in robotic\nmanipulation. However, these models remain constrained by the single-frame\nimage paradigm and fail to fully leverage the temporal information offered by\nmulti-frame histories, as directly feeding multiple frames into VLM backbones\nincurs substantial computational overhead and inference latency. We propose\nCronusVLA, a unified framework that extends single-frame VLA models to the\nmulti-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame\npretraining on large-scale embodied datasets with autoregressive prediction of\naction tokens, establishing an effective embodied vision-language foundation;\n(2) Multi-frame post-training, which adapts the prediction of the\nvision-language backbone from discrete tokens to learnable features, and\naggregates historical information via feature chunking. CronusVLA effectively\naddresses the existing challenges of multi-frame modeling while enhancing\nperformance and observational robustness. To evaluate the robustness under\ntemporal and spatial disturbances, we introduce SimplerEnv-OR, a novel\nbenchmark featuring 24 types of observational disturbances and 120 severity\nlevels. Experiments across three embodiments in simulated and real-world\nenvironments demonstrate that CronusVLA achieves leading performance and\nsuperior robustness, with a 70.9% success rate on SimplerEnv, a 26.8%\nimprovement over OpenVLA on LIBERO, and the highest robustness score on\nSimplerEnv-OR. These results highlight the potential of efficient multi-frame\nadaptation in VLA models for more powerful and robust real-world deployment.\n","authors":["Hao Li","Shuai Yang","Yilun Chen","Xinyi Chen","Xiaoda Yang","Yang Tian","Hanqing Wang","Tai Wang","Dahua Lin","Feng Zhao","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2506.19816v2.pdf","comment":"39 pages, 24 figures"},{"id":"http://arxiv.org/abs/2510.26661v1","updated":"2025-10-30T16:29:09Z","published":"2025-10-30T16:29:09Z","title":"BRIQA: Balanced Reweighting in Image Quality Assessment of Pediatric\n  Brain MRI","summary":"  Assessing the severity of artifacts in pediatric brain Magnetic Resonance\nImaging (MRI) is critical for diagnostic accuracy, especially in low-field\nsystems where the signal-to-noise ratio is reduced. Manual quality assessment\nis time-consuming and subjective, motivating the need for robust automated\nsolutions. In this work, we propose BRIQA (Balanced Reweighting in Image\nQuality Assessment), which addresses class imbalance in artifact severity\nlevels. BRIQA uses gradient-based loss reweighting to dynamically adjust\nper-class contributions and employs a rotating batching scheme to ensure\nconsistent exposure to underrepresented classes. Through experiments, no single\narchitecture performs best across all artifact types, emphasizing the\nimportance of architectural diversity. The rotating batching configuration\nimproves performance across metrics by promoting balanced learning when\ncombined with cross-entropy loss. BRIQA improves average macro F1 score from\n0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098),\nPositioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012)\nartifact severity classification. The code is available at\nhttps://github.com/BioMedIA-MBZUAI/BRIQA.\n","authors":["Alya Almsouti","Ainur Khamitova","Darya Taratynova","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2510.26661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26653v1","updated":"2025-10-30T16:20:28Z","published":"2025-10-30T16:20:28Z","title":"Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning\n  Optical Flow on RADARSAT-2","summary":"  Accurate estimation of sea ice drift is critical for Arctic navigation,\nclimate research, and operational forecasting. While optical flow, a computer\nvision technique for estimating pixel wise motion between consecutive images,\nhas advanced rapidly in computer vision, its applicability to geophysical\nproblems and to satellite SAR imagery remains underexplored. Classical optical\nflow methods rely on mathematical models and strong assumptions about motion,\nwhich limit their accuracy in complex scenarios. Recent deep learning based\napproaches have substantially improved performance and are now the standard in\ncomputer vision, motivating their application to sea ice drift estimation. We\npresent the first large scale benchmark of 48 deep learning optical flow models\non RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and\nFl all metrics against GNSS tracked buoys. Several models achieve sub kilometer\naccuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the\nspatial scales of sea ice motion and typical navigation requirements in the\nArctic. Our results demonstrate that the models are capable of capturing\nconsistent regional drift patterns and that recent deep learning based optical\nflow methods, which have substantially improved motion estimation accuracy\ncompared to classical methods, can be effectively transferred to polar remote\nsensing. Optical flow produces spatially continuous drift fields, providing\nmotion estimates for every image pixel rather than at sparse buoy locations,\noffering new opportunities for navigation and climate modeling.\n","authors":["Daniela Martin","Joseph Gallego"],"pdf_url":"https://arxiv.org/pdf/2510.26653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26641v1","updated":"2025-10-30T16:08:25Z","published":"2025-10-30T16:08:25Z","title":"All You Need for Object Detection: From Pixels, Points, and Prompts to\n  Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles","summary":"  Autonomous Vehicles (AVs) are transforming the future of transportation\nthrough advances in intelligent perception, decision-making, and control\nsystems. However, their success is tied to one core capability, reliable object\ndetection in complex and multimodal environments. While recent breakthroughs in\nComputer Vision (CV) and Artificial Intelligence (AI) have driven remarkable\nprogress, the field still faces a critical challenge as knowledge remains\nfragmented across multimodal perception, contextual reasoning, and cooperative\nintelligence. This survey bridges that gap by delivering a forward-looking\nanalysis of object detection in AVs, emphasizing emerging paradigms such as\nVision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI\nrather than re-examining outdated techniques. We begin by systematically\nreviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,\nand Radar) and their fusion strategies, highlighting not only their\ncapabilities and limitations in dynamic driving environments but also their\npotential to integrate with recent advances in LLM/VLM-driven perception\nframeworks. Next, we introduce a structured categorization of AV datasets that\nmoves beyond simple collections, positioning ego-vehicle, infrastructure-based,\nand cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a\ncross-analysis of data structures and characteristics. Ultimately, we analyze\ncutting-edge detection methodologies, ranging from 2D and 3D pipelines to\nhybrid sensor fusion, with particular attention to emerging transformer-driven\napproaches powered by Vision Transformers (ViTs), Large and Small Language\nModels (SLMs), and VLMs. By synthesizing these perspectives, our survey\ndelivers a clear roadmap of current capabilities, open challenges, and future\nopportunities.\n","authors":["Sayed Pedram Haeri Boroujeni","Niloufar Mehrabi","Hazim Alzorgan","Ahmad Sarlak","Mahlagha Fazeli","Abolfazl Razi"],"pdf_url":"https://arxiv.org/pdf/2510.26641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26635v1","updated":"2025-10-30T16:04:00Z","published":"2025-10-30T16:04:00Z","title":"SAMRI: Segment Anything Model for MRI","summary":"  Accurate magnetic resonance imaging (MRI) segmentation is crucial for\nclinical decision-making, but remains labor-intensive when performed manually.\nConvolutional neural network (CNN)-based methods can be accurate and efficient,\nbut often generalize poorly to MRI's variable contrast, intensity\ninhomogeneity, and protocols. Although the transformer-based Segment Anything\nModel (SAM) has demonstrated remarkable generalizability in natural images,\nexisting adaptations often treat MRI as another imaging modality, overlooking\nthese modality-specific challenges. We present SAMRI, an MRI-specialized SAM\ntrained and validated on 1.1 million labeled MR slices spanning whole-body\norgans and pathologies. We demonstrate that SAM can be effectively adapted to\nMRI by simply fine-tuning its mask decoder using a two-stage strategy, reducing\ntraining time by 94% and trainable parameters by 96% versus full-model\nretraining. Across diverse MRI segmentation tasks, SAMRI achieves a mean Dice\nof 0.87, delivering state-of-the-art accuracy across anatomical regions and\nrobust generalization on unseen structures, particularly small and clinically\nimportant structures.\n","authors":["Zhao Wang","Wei Dai","Thuy Thanh Dao","Steffen Bollmann","Hongfu Sun","Craig Engstrom","Shekhar S. Chandra"],"pdf_url":"https://arxiv.org/pdf/2510.26635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.16556v2","updated":"2025-10-30T16:01:55Z","published":"2025-10-18T16:00:10Z","title":"Fit for Purpose? Deepfake Detection in the Real World","summary":"  The rapid proliferation of AI-generated content, driven by advances in\ngenerative adversarial networks, diffusion models, and multimodal large\nlanguage models, has made the creation and dissemination of synthetic media\neffortless, heightening the risks of misinformation, particularly political\ndeepfakes that distort truth and undermine trust in political institutions. In\nturn, governments, research institutions, and industry have strongly promoted\ndeepfake detection initiatives as solutions. Yet, most existing models are\ntrained and validated on synthetic, laboratory-controlled datasets, limiting\ntheir generalizability to the kinds of real-world political deepfakes\ncirculating on social platforms that affect the public. In this work, we\nintroduce the first systematic benchmark based on the Political Deepfakes\nIncident Database, a curated collection of real-world political deepfakes\nshared on social media since 2018. Our study includes a systematic evaluation\nof state-of-the-art deepfake detectors across academia, government, and\nindustry. We find that the detectors from academia and government perform\nrelatively poorly. While paid detection tools achieve relatively higher\nperformance than free-access models, all evaluated detectors struggle to\ngeneralize effectively to authentic political deepfakes, and are vulnerable to\nsimple manipulations, especially in the video domain. Results urge the need for\npolitically contextualized deepfake detection frameworks to better safeguard\nthe public in real-world settings.\n","authors":["Guangyu Lin","Li Lin","Christina P. Walker","Daniel S. Schiff","Shu Hu"],"pdf_url":"https://arxiv.org/pdf/2510.16556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26630v1","updated":"2025-10-30T15:57:20Z","published":"2025-10-30T15:57:20Z","title":"PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus","summary":"  To address the challenges in UAV object detection, such as complex\nbackgrounds, severe occlusion, dense small objects, and varying lighting\nconditions,this paper proposes PT-DETR based on RT-DETR, a novel detection\nalgorithm specifically designed for small objects in UAV imagery. In the\nbackbone network, we introduce the Partially-Aware Detail Focus (PADF) Module\nto enhance feature extraction for small objects. Additionally,we design the\nMedian-Frequency Feature Fusion (MFFF) module,which effectively improves the\nmodel's ability to capture small-object details and contextual information.\nFurthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box\nmatching capability and increase its sensitivity to small-object features,\nthereby further enhancing detection accuracy and robustness. Compared with\nRT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the\nVisDrone2019 dataset with lower computational complexity and fewer parameters,\ndemonstrating its robustness and feasibility for small-object detection tasks.\n","authors":["Bingcong Huo","Zhiming Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.23292v2","updated":"2025-10-30T15:53:26Z","published":"2025-06-29T15:29:03Z","title":"DDL: A Large-Scale Datasets for Deepfake Detection and Localization in\n  Diversified Real-World Scenarios","summary":"  Recent advances in AIGC have exacerbated the misuse of malicious deepfake\ncontent, making the development of reliable deepfake detection methods an\nessential means to address this challenge. Although existing deepfake detection\nmodels demonstrate outstanding performance in detection metrics, most methods\nonly provide simple binary classification results, lacking interpretability.\nRecent studies have attempted to enhance the interpretability of classification\nresults by providing spatial manipulation masks or temporal forgery segments.\nHowever, due to the limitations of forgery datasets, the practical\neffectiveness of these methods remains suboptimal. The primary reason lies in\nthe fact that most existing deepfake datasets contain only binary labels, with\nlimited variety in forgery scenarios, insufficient diversity in deepfake types,\nand relatively small data scales, making them inadequate for complex real-world\nscenarios.To address this predicament, we construct a novel large-scale\ndeepfake detection and localization (\\textbf{DDL}) dataset containing over\n$\\textbf{1.4M+}$ forged samples and encompassing up to $\\textbf{80}$ distinct\ndeepfake methods. The DDL design incorporates four key innovations: (1)\n\\textbf{Comprehensive Deepfake Methods} (covering 7 different generation\narchitectures and a total of 80 methods), (2) \\textbf{Varied Manipulation\nModes} (incorporating 7 classic and 3 novel forgery modes), (3) \\textbf{Diverse\nForgery Scenarios and Modalities} (including 3 scenarios and 3 modalities), and\n(4) \\textbf{Fine-grained Forgery Annotations} (providing 1.18M+ precise spatial\nmasks and 0.23M+ precise temporal segments).Through these improvements, our DDL\nnot only provides a more challenging benchmark for complex real-world forgeries\nbut also offers crucial support for building next-generation deepfake\ndetection, localization, and interpretability methods.\n","authors":["Changtao Miao","Yi Zhang","Weize Gao","Zhiya Tan","Weiwei Feng","Man Luo","Jianshu Li","Ajian Liu","Yunfeng Diao","Qi Chu","Tao Gong","Zhe Li","Weibin Yao","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.23292v2.pdf","comment":"This paper is a preliminary version, with an extended and\n  comprehensive version currently under development"},{"id":"http://arxiv.org/abs/2508.10566v2","updated":"2025-10-30T15:42:29Z","published":"2025-08-14T12:01:52Z","title":"HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head\n  Synthesis","summary":"  Audio-driven talking head video generation enhances user engagement in\nhuman-computer interaction. However, current methods frequently produce videos\nwith motion blur and lip jitter, primarily due to their reliance on implicit\nmodeling of audio-facial motion correlations--an approach lacking explicit\narticulatory priors (i.e., anatomical guidance for speech-related facial\nmovements). To overcome this limitation, we propose HM-Talker, a novel\nframework for generating high-fidelity, temporally coherent talking heads.\nHM-Talker leverages a hybrid motion representation combining both implicit and\nexplicit motion cues. Explicit cues use Action Units (AUs), anatomically\ndefined facial muscle movements, alongside implicit features to minimize\nphoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement\nModule (CMDM) extracts complementary implicit/explicit motion features while\npredicting AUs directly from audio input aligned to visual cues. To mitigate\nidentity-dependent biases in explicit features and enhance cross-subject\ngeneralization, we introduce the Hybrid Motion Modeling Module (HMMM). This\nmodule dynamically merges randomly paired implicit/explicit features, enforcing\nidentity-agnostic learning. Together, these components enable robust lip\nsynchronization across diverse identities, advancing personalized talking head\nsynthesis. Extensive experiments demonstrate HM-Talker's superiority over\nstate-of-the-art methods in visual quality and lip-sync accuracy.\n","authors":["Shiyu Liu","Kui Jiang","Xianming Liu","Hongxun Yao","Xiaocheng Feng"],"pdf_url":"https://arxiv.org/pdf/2508.10566v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26614v1","updated":"2025-10-30T15:40:34Z","published":"2025-10-30T15:40:34Z","title":"Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event\n  Cameras","summary":"  We propose tokenization of events and present a tokenizer, Spiking Patches,\nspecifically designed for event cameras. Given a stream of asynchronous and\nspatially sparse events, our goal is to discover an event representation that\npreserves these properties. Prior works have represented events as frames or as\nvoxels. However, while these representations yield high accuracy, both frames\nand voxels are synchronous and decrease the spatial sparsity. Spiking Patches\ngives the means to preserve the unique properties of event cameras and we show\nin our experiments that this comes without sacrificing accuracy. We evaluate\nour tokenizer using a GNN, PCN, and a Transformer on gesture recognition and\nobject detection. Tokens from Spiking Patches yield inference times that are up\nto 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We\nachieve this while matching their accuracy and even surpassing in some cases\nwith absolute improvements up to 3.8 for gesture recognition and up to 1.4 for\nobject detection. Thus, tokenization constitutes a novel direction in\nevent-based vision and marks a step towards methods that preserve the\nproperties of event cameras.\n","authors":["Christoffer Koo Øhrstrøm","Ronja Güldenring","Lazaros Nalpantidis"],"pdf_url":"https://arxiv.org/pdf/2510.26614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14904v2","updated":"2025-10-30T15:39:25Z","published":"2025-10-16T17:20:22Z","title":"MaskCaptioner: Learning to Jointly Segment and Caption Object\n  Trajectories in Videos","summary":"  Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.\n","authors":["Gabriel Fiastre","Antoine Yang","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2510.14904v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.26609v1","updated":"2025-10-30T15:37:40Z","published":"2025-10-30T15:37:40Z","title":"CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for\n  Satellite Sensing","summary":"  Accurate and timely crop yield prediction is crucial for global food security\nand modern agricultural management. Traditional methods often lack the\nscalability and granularity required for precision farming. This paper\nintroduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder\nfor Satellite Sensing), a deep learning model designed for high-resolution,\nintra-field canola yield prediction. CYPRESS leverages a pre-trained,\nlarge-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for\na continuous regression task, transforming multi-temporal satellite imagery\ninto dense, pixel-level yield maps. Evaluated on a comprehensive dataset from\nthe Canadian Prairies, CYPRESS demonstrates superior performance over existing\ndeep learning-based yield prediction models, highlighting the effectiveness of\nfine-tuning foundation models for specialized agricultural applications. By\nproviding a continuous, high-resolution output, CYPRESS offers a more\nactionable tool for precision agriculture than conventional classification or\ncounty-level aggregation methods. This work validates a novel approach that\nbridges the gap between large-scale Earth observation and on-farm\ndecision-making, offering a scalable solution for detailed agricultural\nmonitoring.\n","authors":["Shayan Nejadshamsi","Yuanyuan Zhang","Shadi Zaki","Brock Porth","Lysa Porth","Vahab Khoshdel"],"pdf_url":"https://arxiv.org/pdf/2510.26609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26601v1","updated":"2025-10-30T15:29:20Z","published":"2025-10-30T15:29:20Z","title":"ResMatching: Noise-Resilient Computational Super-Resolution via Guided\n  Conditional Flow Matching","summary":"  Computational Super-Resolution (CSR) in fluorescence microscopy has, despite\nbeing an ill-posed problem, a long history. At its very core, CSR is about\nfinding a prior that can be used to extrapolate frequencies in a micrograph\nthat have never been imaged by the image-generating microscope. It stands to\nreason that, with the advent of better data-driven machine learning techniques,\nstronger prior can be learned and hence CSR can lead to better results. Here,\nwe present ResMatching, a novel CSR method that uses guided conditional flow\nmatching to learn such improved data-priors. We evaluate ResMatching on 4\ndiverse biological structures from the BioSR dataset and compare its results\nagainst 7 baselines. ResMatching consistently achieves competitive results,\ndemonstrating in all cases the best trade-off between data fidelity and\nperceptual realism. We observe that CSR using ResMatching is particularly\neffective in cases where a strong prior is hard to learn, e.g. when the given\nlow-resolution images contain a lot of noise. Additionally, we show that\nResMatching can be used to sample from an implicitly learned posterior\ndistribution and that this distribution is calibrated for all tested use-cases,\nenabling our method to deliver a pixel-wise data-uncertainty term that can\nguide future users to reject uncertain predictions.\n","authors":["Anirban Ray","Vera Galinova","Florian Jug"],"pdf_url":"https://arxiv.org/pdf/2510.26601v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.26583v1","updated":"2025-10-30T15:11:16Z","published":"2025-10-30T15:11:16Z","title":"Emu3.5: Native Multimodal Models are World Learners","summary":"  We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.\n","authors":["Yufeng Cui","Honghao Chen","Haoge Deng","Xu Huang","Xinghang Li","Jirong Liu","Yang Liu","Zhuoyan Luo","Jinsheng Wang","Wenxuan Wang","Yueze Wang","Chengyuan Wang","Fan Zhang","Yingli Zhao","Ting Pan","Xianduo Li","Zecheng Hao","Wenxuan Ma","Zhuo Chen","Yulong Ao","Tiejun Huang","Zhongyuan Wang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26583v1.pdf","comment":"project page: https://emu.world"},{"id":"http://arxiv.org/abs/2510.26582v1","updated":"2025-10-30T15:10:02Z","published":"2025-10-30T15:10:02Z","title":"CATCH: A Modular Cross-domain Adaptive Template with Hook","summary":"  Recent advances in Visual Question Answering (VQA) have demonstrated\nimpressive performance in natural image domains, with models like LLaVA\nleveraging large language models (LLMs) for open-ended reasoning. However,\ntheir generalization degrades significantly when transferred to out-of-domain\nscenarios such as remote sensing, medical imaging, or math diagrams, due to\nlarge distributional shifts and the lack of effective domain adaptation\nmechanisms. Existing approaches typically rely on per-domain fine-tuning or\nbespoke pipelines, which are costly, inflexible, and not scalable across\ndiverse tasks. In this paper, we propose CATCH, a plug-and-play framework for\ncross-domain adaptation that improves the generalization of VQA models while\nrequiring minimal changes to their core architecture. Our key idea is to\ndecouple visual and linguistic adaptation by introducing two lightweight\nmodules: a domain classifier to identify the input image type, and a dual\nadapter mechanism comprising a Prompt Adapter for language modulation and a\nVisual Adapter for vision feature adjustment. Both modules are dynamically\ninjected via a unified hook interface, requiring no retraining of the backbone\nmodel. Experimental results across four domain-specific VQA benchmarks\ndemonstrate that our framework achieves consistent performance gains without\nretraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on\nMedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH\nprovides a scalable and extensible approach to multi-domain VQA, enabling\npractical deployment across diverse application domains.\n","authors":["Xinjin Li","Yulie Lu","Jinghan Cao","Yu Ma","Zhenglin Li","Yeyang Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26580v1","updated":"2025-10-30T15:07:55Z","published":"2025-10-30T15:07:55Z","title":"Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in\n  Zero-Shot Real-World Scenarios","summary":"  In real-world environments, AI systems often face unfamiliar scenarios\nwithout labeled data, creating a major challenge for conventional scene\nunderstanding models. The inability to generalize across unseen contexts limits\nthe deployment of vision-based applications in dynamic, unstructured settings.\nThis work introduces a Dynamic Context-Aware Scene Reasoning framework that\nleverages Vision-Language Alignment to address zero-shot real-world scenarios.\nThe goal is to enable intelligent systems to infer and adapt to new\nenvironments without prior task-specific training. The proposed approach\nintegrates pre-trained vision transformers and large language models to align\nvisual semantics with natural language descriptions, enhancing contextual\ncomprehension. A dynamic reasoning module refines predictions by combining\nglobal scene cues and object-level interactions guided by linguistic priors.\nExtensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and\nOpen Images demonstrate up to 18% improvement in scene understanding accuracy\nover baseline models in complex and unseen environments. Results also show\nrobust performance in ambiguous or cluttered scenes due to the synergistic\nfusion of vision and language. This framework offers a scalable and\ninterpretable approach for context-aware reasoning, advancing zero-shot\ngeneralization in dynamic real-world settings.\n","authors":["Manjunath Prasad Holenarasipura Rajiv","B. M. Vidyavathi"],"pdf_url":"https://arxiv.org/pdf/2510.26580v1.pdf","comment":"Preprint under review at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), 2025"},{"id":"http://arxiv.org/abs/2510.26573v1","updated":"2025-10-30T15:00:50Z","published":"2025-10-30T15:00:50Z","title":"Comparative Analysis of Deep Learning Models for Olive Tree Crown and\n  Shadow Segmentation Towards Biovolume Estimation","summary":"  Olive tree biovolume estimation is a key task in precision agriculture,\nsupporting yield prediction and resource management, especially in\nMediterranean regions severely impacted by climate-induced stress. This study\npresents a comparative analysis of three deep learning models U-Net,\nYOLOv11m-seg, and Mask RCNN for segmenting olive tree crowns and their shadows\nin ultra-high resolution UAV imagery. The UAV dataset, acquired over\nVicopisano, Italy, includes manually annotated crown and shadow masks. Building\non these annotations, the methodology emphasizes spatial feature extraction and\nrobust segmentation; per-tree biovolume is then estimated by combining crown\nprojected area with shadow-derived height using solar geometry. In testing,\nMask R-CNN achieved the best overall accuracy (F1 = 0.86; mIoU = 0.72), while\nYOLOv11m-seg provided the fastest throughput (0.12 second per image). The\nestimated biovolumes spanned from approximately 4 to 24 cubic meters,\nreflecting clear structural differences among trees. These results indicate\nMask R-CNN is preferable when biovolume accuracy is paramount, whereas\nYOLOv11m-seg suits large-area deployments where speed is critical; U-Net\nremains a lightweight, high-sensitivity option. The framework enables accurate,\nscalable orchard monitoring and can be further strengthened with DEM or DSM\nintegration and field calibration for operational decision support.\n","authors":["Wondimagegn Abebe Demissie","Stefano Roccella","Rudy Rossetto","Antonio Minnocci","Andrea Vannini","Luca Sebastiani"],"pdf_url":"https://arxiv.org/pdf/2510.26573v1.pdf","comment":"6 pages, 2025 IEEE International Workshop on Metrology for\n  Agriculture and Forestry (MetroAgriFor)"},{"id":"http://arxiv.org/abs/2510.26569v1","updated":"2025-10-30T14:59:37Z","published":"2025-10-30T14:59:37Z","title":"AdSum: Two-stream Audio-visual Summarization for Automated Video\n  Advertisement Clipping","summary":"  Advertisers commonly need multiple versions of the same advertisement (ad) at\nvarying durations for a single campaign. The traditional approach involves\nmanually selecting and re-editing shots from longer video ads to create shorter\nversions, which is labor-intensive and time-consuming. In this paper, we\nintroduce a framework for automated video ad clipping using video summarization\ntechniques. We are the first to frame video clipping as a shot selection\nproblem, tailored specifically for advertising. Unlike existing general video\nsummarization methods that primarily focus on visual content, our approach\nemphasizes the critical role of audio in advertising. To achieve this, we\ndevelop a two-stream audio-visual fusion model that predicts the importance of\nvideo frames, where importance is defined as the likelihood of a frame being\nselected in the firm-produced short ad. To address the lack of ad-specific\ndatasets, we present AdSum204, a novel dataset comprising 102 pairs of\n30-second and 15-second ads from real advertising campaigns. Extensive\nexperiments demonstrate that our model outperforms state-of-the-art methods\nacross various metrics, including Average Precision, Area Under Curve,\nSpearman, and Kendall.\n","authors":["Wen Xie","Yanjun Zhu","Gijs Overgoor","Yakov Bart","Agata Lapedriza Garcia","Sarah Ostadabbas"],"pdf_url":"https://arxiv.org/pdf/2510.26569v1.pdf","comment":"Accepted at 32nd International Conference on MultiMedia Modeling"},{"id":"http://arxiv.org/abs/2510.26568v1","updated":"2025-10-30T14:58:16Z","published":"2025-10-30T14:58:16Z","title":"SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformation for Spine\n  Segmentation from Ultrasound Volume Projection Imaging","summary":"  Spine segmentation, based on ultrasound volume projection imaging (VPI),\nplays a vital role for intelligent scoliosis diagnosis in clinical\napplications. However, this task faces several significant challenges. Firstly,\nthe global contextual knowledge of spines may not be well-learned if we neglect\nthe high spatial correlation of different bone features. Secondly, the spine\nbones contain rich structural knowledge regarding their shapes and positions,\nwhich deserves to be encoded into the segmentation process. To address these\nchallenges, we propose a novel scale-adaptive structure-aware network\n(SA$^{2}$Net) for effective spine segmentation. First, we propose a\nscale-adaptive complementary strategy to learn the cross-dimensional\nlong-distance correlation features for spinal images. Second, motivated by the\nconsistency between multi-head self-attention in Transformers and semantic\nlevel affinity, we propose structure-affinity transformation to transform\nsemantic features with class-specific affinity and combine it with a\nTransformer decoder for structure-aware reasoning. In addition, we adopt a\nfeature mixing loss aggregation method to enhance model training. This method\nimproves the robustness and accuracy of the segmentation process. The\nexperimental results demonstrate that our SA$^{2}$Net achieves superior\nsegmentation performance compared to other state-of-the-art methods. Moreover,\nthe adaptability of SA$^{2}$Net to various backbones enhances its potential as\na promising tool for advanced scoliosis diagnosis using intelligent spinal\nimage analysis. The code and experimental demo are available at\nhttps://github.com/taetiseo09/SA2Net.\n","authors":["Hao Xie","Zixun Huang","Yushen Zuo","Yakun Ju","Frank H. F. Leung","N. F. Law","Kin-Man Lam","Yong-Ping Zheng","Sai Ho Ling"],"pdf_url":"https://arxiv.org/pdf/2510.26568v1.pdf","comment":"Accepted by Computerized Medical Imaging and Graphics (CMIG)"},{"id":"http://arxiv.org/abs/2510.08771v2","updated":"2025-10-30T14:46:21Z","published":"2025-10-09T19:41:51Z","title":"LinearSR: Unlocking Linear Attention for Stable and Efficient Image\n  Super-Resolution","summary":"  Generative models for Image Super-Resolution (SR) are increasingly powerful,\nyet their reliance on self-attention's quadratic complexity (O(N^2)) creates a\nmajor computational bottleneck. Linear Attention offers an O(N) solution, but\nits promise for photorealistic SR has remained largely untapped, historically\nhindered by a cascade of interrelated and previously unsolved challenges. This\npaper introduces LinearSR, a holistic framework that, for the first time,\nsystematically overcomes these critical hurdles. Specifically, we resolve a\nfundamental, training instability that causes catastrophic model divergence\nusing our novel \"knee point\"-based Early-Stopping Guided Fine-tuning (ESGF)\nstrategy. Furthermore, we mitigate the classic perception-distortion trade-off\nwith a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we\nestablish an effective and lightweight guidance paradigm, TAG, derived from our\n\"precision-over-volume\" principle. Our resulting LinearSR model simultaneously\ndelivers state-of-the-art perceptual quality with exceptional efficiency. Its\ncore diffusion forward pass (1-NFE) achieves SOTA-level speed, while its\noverall multi-step inference time remains highly competitive. This work\nprovides the first robust methodology for applying Linear Attention in the\nphotorealistic SR domain, establishing a foundational paradigm for future\nresearch in efficient generative super-resolution.\n","authors":["Xiaohui Li","Shaobin Zhuang","Shuo Cao","Yang Yang","Yuandong Pu","Qi Qin","Siqi Luo","Bin Fu","Yihao Liu"],"pdf_url":"https://arxiv.org/pdf/2510.08771v2.pdf","comment":"19 pages, 9 figures, 6 tables"},{"id":"http://arxiv.org/abs/2501.05783v2","updated":"2025-10-30T14:04:15Z","published":"2025-01-10T08:33:31Z","title":"UV-Attack: Physical-World Adversarial Attacks for Person Detection via\n  Dynamic-NeRF-based UV Mapping","summary":"  In recent research, adversarial attacks on person detectors using patches or\nstatic 3D model-based texture modifications have struggled with low success\nrates due to the flexible nature of human movement. Modeling the 3D\ndeformations caused by various actions has been a major challenge. Fortunately,\nadvancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer\nnew possibilities. In this paper, we introduce UV-Attack, a groundbreaking\napproach that achieves high success rates even with extensive and unseen human\nactions. We address the challenge above by leveraging dynamic-NeRF-based UV\nmapping. UV-Attack can generate human images across diverse actions and\nviewpoints, and even create novel actions by sampling from the SMPL parameter\nspace. While dynamic NeRF models are capable of modeling human bodies,\nmodifying clothing textures is challenging because they are embedded in neural\nnetwork parameters. To tackle this, UV-Attack generates UV maps instead of RGB\nimages and modifies the texture stacks. This approach enables real-time texture\nedits and makes the attack more practical. We also propose a novel Expectation\nover Pose Transformation loss (EoPT) to improve the evasion success rate on\nunseen poses and views. Our experiments show that UV-Attack achieves a 92.7%\nattack success rate against the FastRCNN model across varied poses in dynamic\nvideo settings, significantly outperforming the state-of-the-art AdvCamou\nattack, which only had a 28.5% ASR. Moreover, we achieve 49.5% ASR on the\nlatest YOLOv8 detector in black-box settings. This work highlights the\npotential of dynamic NeRF-based UV mapping for creating more effective\nadversarial attacks on person detectors, addressing key challenges in modeling\nhuman movement and texture modification. The code is available at\nhttps://github.com/PolyLiYJ/UV-Attack.\n","authors":["Yanjie Li","Kaisheng Liang","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2501.05783v2.pdf","comment":"23 pages, 22 figures, accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2510.26509v1","updated":"2025-10-30T14:03:09Z","published":"2025-10-30T14:03:09Z","title":"Analysis of the Robustness of an Edge Detector Based on Cellular\n  Automata Optimized by Particle Swarm","summary":"  The edge detection task is essential in image processing aiming to extract\nrelevant information from an image. One recurring problem in this task is the\nweaknesses found in some detectors, such as the difficulty in detecting loose\nedges and the lack of context to extract relevant information from specific\nproblems. To address these weaknesses and adapt the detector to the properties\nof an image, an adaptable detector described by two-dimensional cellular\nautomaton and optimized by meta-heuristic combined with transfer learning\ntechniques was developed. This study aims to analyze the impact of expanding\nthe search space of the optimization phase and the robustness of the\nadaptability of the detector in identifying edges of a set of natural images\nand specialized subsets extracted from the same image set. The results obtained\nprove that expanding the search space of the optimization phase was not\neffective for the chosen image set. The study also analyzed the adaptability of\nthe model through a series of experiments and validation techniques and found\nthat, regardless of the validation, the model was able to adapt to the input\nand the transfer learning techniques applied to the model showed no significant\nimprovements.\n","authors":["Vinícius Ferraria","Eurico Ruivo"],"pdf_url":"https://arxiv.org/pdf/2510.26509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21004v2","updated":"2025-10-30T13:43:39Z","published":"2024-10-28T13:28:21Z","title":"A Continuous and Interpretable Morphometric for Robust Quantification of\n  Dynamic Biological Shapes","summary":"  We introduce the Push-Forward Signed Distance Morphometric (PF-SDM) for shape\nquantification in biomedical imaging. The PF-SDM compactly encodes geometric\nand topological properties of closed shapes, including their skeleton and\nsymmetries. This provides robust and interpretable features for shape\ncomparison and machine learning. The PF-SDM is mathematically smooth, providing\naccess to gradients and differential-geometric quantities. It also extends to\ntemporal dynamics and allows fusing spatial intensity distributions, such as\ngenetic markers, with shape dynamics. We present the PF-SDM theory, benchmark\nit on synthetic data, and apply it to predicting body-axis formation in mouse\ngastruloids, outperforming a CNN baseline in both accuracy and speed.\n","authors":["Roua Rouatbi","Juan-Esteban Suarez Cardona","Alba Villaronga-Luque","Jesse V. Veenvliet","Ivo F. Sbalzarini"],"pdf_url":"https://arxiv.org/pdf/2410.21004v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08788v2","updated":"2025-10-30T13:41:48Z","published":"2024-01-30T09:05:38Z","title":"VerifIoU -- Robustness of Object Detection to Perturbations","summary":"  We introduce a novel Interval Bound Propagation (IBP) approach for the formal\nverification of object detection models, specifically targeting the\nIntersection over Union (IoU) metric. The approach has been implemented in an\nopen source code, named IBP IoU, compatible with popular abstract\ninterpretation based verification tools. The resulting verifier is evaluated on\nlanding approach runway detection and handwritten digit recognition case\nstudies. Comparisons against a baseline (Vanilla IBP IoU) highlight the\nsuperior performance of IBP IoU in ensuring accuracy and stability,\ncontributing to more secure and robust machine learning applications.\n","authors":["Noémie Cohen","Mélanie Ducoffe","Ryma Boumazouza","Christophe Gabreau","Claire Pagetti","Xavier Pucel","Audrey Galametz"],"pdf_url":"https://arxiv.org/pdf/2403.08788v2.pdf","comment":"44th Digital Avionics Systems Conference (DASC), Sep 2025, Montreal,\n  Canada"},{"id":"http://arxiv.org/abs/2509.24325v2","updated":"2025-10-30T13:38:59Z","published":"2025-09-29T06:23:47Z","title":"ReCon-GS: Continuum-Preserved Gaussian Streaming for Fast and Compact\n  Reconstruction of Dynamic Scenes","summary":"  Online free-viewpoint video (FVV) reconstruction is challenged by slow\nper-frame optimization, inconsistent motion estimation, and unsustainable\nstorage demands. To address these challenges, we propose the Reconfigurable\nContinuum Gaussian Stream, dubbed ReCon-GS, a novel storage-aware framework\nthat enables high fidelity online dynamic scene reconstruction and real-time\nrendering. Specifically, we dynamically allocate multi-level Anchor Gaussians\nin a density-adaptive fashion to capture inter-frame geometric deformations,\nthereby decomposing scene motion into compact coarse-to-fine representations.\nThen, we design a dynamic hierarchy reconfiguration strategy that preserves\nlocalized motion expressiveness through on-demand anchor re-hierarchization,\nwhile ensuring temporal consistency through intra-hierarchical deformation\ninheritance that confines transformation priors to their respective hierarchy\nlevels. Furthermore, we introduce a storage-aware optimization mechanism that\nflexibly adjusts the density of Anchor Gaussians at different hierarchy levels,\nenabling a controllable trade-off between reconstruction fidelity and memory\nusage. Extensive experiments on three widely used datasets demonstrate that,\ncompared to state-of-the-art methods, ReCon-GS improves training efficiency by\napproximately 15% and achieves superior FVV synthesis quality with enhanced\nrobustness and stability. Moreover, at equivalent rendering quality, ReCon-GS\nslashes memory requirements by over 50% compared to leading state-of-the-art\nmethods.\n","authors":["Jiaye Fu","Qiankun Gao","Chengxiang Wen","Yanmin Wu","Siwei Ma","Jiaqi Zhang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.24325v2.pdf","comment":"Published in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26474v1","updated":"2025-10-30T13:26:58Z","published":"2025-10-30T13:26:58Z","title":"Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing","summary":"  Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.\n","authors":["Xin Guo","Zhiheng Xi","Yiwen Ding","Yitao Zhai","Xiaowei Shi","Xunliang Cai","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26474v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.18766v2","updated":"2025-10-30T13:19:55Z","published":"2025-05-24T16:09:26Z","title":"StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks\n  by Style Perturbations","summary":"  Recently, text-to-image diffusion models have been widely used for style\nmimicry and personalized customization through methods such as DreamBooth and\nTextual Inversion. This has raised concerns about intellectual property\nprotection and the generation of deceptive content. Recent studies, such as\nGlaze and Anti-DreamBooth, have proposed using adversarial noise to protect\nimages from these attacks. However, recent purification-based methods, such as\nDiffPure and Noise Upscaling, have successfully attacked these latest defenses,\nshowing the vulnerabilities of these methods. Moreover, present methods show\nlimited transferability across models, making them less effective against\nunknown text-to-image models. To address these issues, we propose a novel\nanti-mimicry method, StyleGuard. We propose a novel style loss that optimizes\nthe style-related features in the latent space to make it deviate from the\noriginal image, which improves model-agnostic transferability. Additionally, to\nenhance the perturbation's ability to bypass diffusion-based purification, we\ndesigned a novel upscale loss that involves ensemble purifiers and upscalers\nduring training. Extensive experiments on the WikiArt and CelebA datasets\ndemonstrate that StyleGuard outperforms existing methods in robustness against\nvarious transformations and purifications, effectively countering style mimicry\nin various models. Moreover, StyleGuard is effective on different style mimicry\nmethods, including DreamBooth and Textual Inversion. The code is available at\nhttps://github.com/PolyLiYJ/StyleGuard.\n","authors":["Yanjie Li","Wenxuan Zhang","Xinqi Lyu","Yihao Liu","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2505.18766v2.pdf","comment":"Accepted by NIPS2025"},{"id":"http://arxiv.org/abs/2510.26466v1","updated":"2025-10-30T13:11:23Z","published":"2025-10-30T13:11:23Z","title":"Representation-Level Counterfactual Calibration for Debiased Zero-Shot\n  Recognition","summary":"  Object-context shortcuts remain a persistent challenge in vision-language\nmodels, undermining zero-shot reliability when test-time scenes differ from\nfamiliar training co-occurrences. We recast this issue as a causal inference\nproblem and ask: Would the prediction remain if the object appeared in a\ndifferent environment? To answer this at inference time, we estimate object and\nbackground expectations within CLIP's representation space, and synthesize\ncounterfactual embeddings by recombining object features with diverse\nalternative contexts sampled from external datasets, batch neighbors, or\ntext-derived descriptions. By estimating the Total Direct Effect and simulating\nintervention, we further subtract background-only activation, preserving\nbeneficial object-context interactions while mitigating hallucinated scores.\nWithout retraining or prompt design, our method substantially improves both\nworst-group and average accuracy on context-sensitive benchmarks, establishing\na new zero-shot state of the art. Beyond performance, our framework provides a\nlightweight representation-level counterfactual approach, offering a practical\ncausal avenue for debiased and reliable multimodal reasoning.\n","authors":["Pei Peng","MingKun Xie","Hang Hao","Tong Jin","ShengJun Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26464v1","updated":"2025-10-30T13:09:00Z","published":"2025-10-30T13:09:00Z","title":"Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly\n  Detection","summary":"  Few-shot anomaly detection (FSAD) methods identify anomalous regions with few\nknown normal samples. Most existing methods rely on the generalization ability\nof pre-trained vision-language models (VLMs) to recognize potentially anomalous\nregions through feature similarity between text descriptions and images.\nHowever, due to the lack of detailed textual descriptions, these methods can\nonly pre-define image-level descriptions to match each visual patch token to\nidentify potential anomalous regions, which leads to the semantic misalignment\nbetween image descriptions and patch-level visual anomalies, achieving\nsub-optimal localization performance. To address the above issues, we propose\nthe Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and\nfine-grained textual descriptions for existing anomaly detection datasets with\nautomatic construction pipeline. Based on the MFSC, we propose a novel\nframework named FineGrainedAD to improve anomaly localization performance,\nwhich consists of two components: Multi-Level Learnable Prompt (MLLP) and\nMulti-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics\ninto multi-level learnable prompts through automatic replacement and\nconcatenation mechanism, while MLSA designs region aggregation strategy and\nmulti-level alignment training to facilitate learnable prompts better align\nwith corresponding visual regions. Experiments demonstrate that the proposed\nFineGrainedAD achieves superior overall performance in few-shot settings on\nMVTec-AD and VisA datasets.\n","authors":["Yuanting Fan","Jun Liu","Xiaochen Chen","Bin-Bin Gao","Jian Li","Yong Liu","Jinlong Peng","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26464v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2503.23722v3","updated":"2025-10-30T12:49:25Z","published":"2025-03-31T04:47:05Z","title":"LATex: Leveraging Attribute-based Text Knowledge for Aerial-Ground\n  Person Re-Identification","summary":"  As an important task in intelligent transportation systems, Aerial-Ground\nperson Re-IDentification (AG-ReID) aims to retrieve specific persons across\nheterogeneous cameras in different viewpoints. Previous methods typically adopt\ndeep learning-based models, focusing on extracting view-invariant features.\nHowever, they usually overlook the semantic information in person attributes.\nIn addition, existing training strategies often rely on full fine-tuning\nlarge-scale models, which significantly increases training costs. To address\nthese issues, we propose a novel framework named LATex for AG-ReID, which\nadopts prompt-tuning strategies to leverage attribute-based text knowledge.\nSpecifically, with the Contrastive Language-Image Pre-training (CLIP) model, we\nfirst propose an Attribute-aware Image Encoder (AIE) to extract both global\nsemantic features and attribute-aware features from input images. Then, with\nthese features, we propose a Prompted Attribute Classifier Group (PACG) to\npredict person attributes and obtain attribute representations. Finally, we\ndesign a Coupled Prompt Template (CPT) to transform attribute representations\nand view information into structured sentences. These sentences are processed\nby the text encoder of CLIP to generate more discriminative features. As a\nresult, our framework can fully leverage attribute-based text knowledge to\nimprove AG-ReID performance. Extensive experiments on three AG-ReID benchmarks\ndemonstrate the effectiveness of our proposed methods. The source code is\navailable at https://github.com/kevinhu314/LATex.\n","authors":["Pingping Zhang","Xiang Hu","Yuhao Wang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2503.23722v3.pdf","comment":"More modifications may be performed"},{"id":"http://arxiv.org/abs/2510.26443v1","updated":"2025-10-30T12:46:56Z","published":"2025-10-30T12:46:56Z","title":"PointSt3R: Point Tracking through 3D Grounded Correspondence","summary":"  Recent advances in foundational 3D reconstruction models, such as DUSt3R and\nMASt3R, have shown great potential in 2D and 3D correspondence in static\nscenes. In this paper, we propose to adapt them for the task of point tracking\nthrough 3D grounded correspondence. We first demonstrate that these models are\ncompetitive point trackers when focusing on static points, present in current\npoint tracking benchmarks ($+33.5\\%$ on EgoPoints vs. CoTracker2). We propose\nto combine the reconstruction loss with training for dynamic correspondence\nalong with a visibility head, and fine-tuning MASt3R for point tracking using a\nrelatively small amount of synthetic data. Importantly, we only train and\nevaluate on pairs of frames where one contains the query point, effectively\nremoving any temporal context. Using a mix of dynamic and static point\ncorrespondences, we achieve competitive or superior point tracking results on\nfour datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\\delta_{avg}$ / 85.8\\%\nocclusion acc. for PointSt3R compared to 75.7 / 88.3\\% for CoTracker2; and\nsignificantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs\n82.8). We also present results on 3D point tracking along with several\nablations on training datasets and percentage of dynamic correspondences.\n","authors":["Rhodri Guerrier","Adam W. Harley","Dima Damen"],"pdf_url":"https://arxiv.org/pdf/2510.26443v1.pdf","comment":"http://rhodriguerrier.github.io/PointSt3R"},{"id":"http://arxiv.org/abs/2510.26441v1","updated":"2025-10-30T12:45:24Z","published":"2025-10-30T12:45:24Z","title":"A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt\n  Tuning of Vision-Language Models","summary":"  Test-time prompt tuning (TPT) has emerged as a promising technique for\nadapting large vision-language models (VLMs) to unseen tasks without relying on\nlabeled data. However, the lack of dispersion between textual features can hurt\ncalibration performance, which raises concerns about VLMs' reliability,\ntrustworthiness, and safety. Current TPT approaches primarily focus on\nimproving prompt calibration by either maximizing average textual feature\ndispersion or enforcing orthogonality constraints to encourage angular\nseparation. However, these methods may not always have optimal angular\nseparation between class-wise textual features, which implies overlooking the\ncritical role of angular diversity. To address this, we propose A-TPT, a novel\nTPT framework that introduces angular diversity to encourage uniformity in the\ndistribution of normalized textual features induced by corresponding learnable\nprompts. This uniformity is achieved by maximizing the minimum pairwise angular\ndistance between features on the unit hypersphere. We show that our approach\nconsistently surpasses state-of-the-art TPT methods in reducing the aggregate\naverage calibration error while maintaining comparable accuracy through\nextensive experiments with various backbones on different datasets. Notably,\nour approach exhibits superior zero-shot calibration performance on natural\ndistribution shifts and generalizes well to medical datasets. We provide\nextensive analyses, including theoretical aspects, to establish the grounding\nof A-TPT. These results highlight the potency of promoting angular diversity to\nachieve well-dispersed textual features, significantly improving VLM\ncalibration during test-time adaptation. Our code will be made publicly\navailable.\n","authors":["Shihab Aaqil Ahamed","Udaya S. K. P. Miriya Thanthrige","Ranga Rodrigo","Muhammad Haris Khan"],"pdf_url":"https://arxiv.org/pdf/2510.26441v1.pdf","comment":"23 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.15863v3","updated":"2025-10-30T12:26:23Z","published":"2024-06-22T14:43:23Z","title":"EmoAttack: Emotion-to-Image Diffusion Models for Emotional Backdoor\n  Generation","summary":"  Text-to-image diffusion models can generate realistic images based on textual\ninputs, enabling users to convey their opinions visually through language.\nMeanwhile, within language, emotion plays a crucial role in expressing personal\nopinions in our daily lives and the inclusion of maliciously negative content\ncan lead users astray, exacerbating negative emotions. Recognizing the success\nof diffusion models and the significance of emotion, we investigate a\npreviously overlooked risk associated with text-to-image diffusion models, that\nis, utilizing emotion in the input texts to introduce negative content and\nprovoke unfavorable emotions in users. Specifically, we identify a new backdoor\nattack, i.e., emotion-aware backdoor attack (EmoAttack), which introduces\nmalicious negative content triggered by emotional texts during image\ngeneration. We formulate such an attack as a diffusion personalization problem\nto avoid extensive model retraining and propose the EmoBooth. Unlike existing\npersonalization methods, our approach fine-tunes a pre-trained diffusion model\nby establishing a mapping between a cluster of emotional words and a given\nreference image containing malicious negative content. To validate the\neffectiveness of our method, we built a dataset and conducted extensive\nanalysis and discussion about its effectiveness. Given consumers' widespread\nuse of diffusion models, uncovering this threat is critical for society.\n","authors":["Tianyu Wei","Shanmin Pang","Qi Guo","Yizhuo Ma","Xiaofeng Cao","Qing Guo"],"pdf_url":"https://arxiv.org/pdf/2406.15863v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.08083v3","updated":"2025-10-30T12:07:37Z","published":"2022-08-17T05:42:59Z","title":"Two Heads are Better than One: Robust Learning Meets Multi-branch Models","summary":"  Deep neural networks (DNNs) are vulnerable to adversarial examples, in which\nDNNs are misled to false outputs due to inputs containing imperceptible\nperturbations. Adversarial training, a reliable and effective method of\ndefense, may significantly reduce the vulnerability of neural networks and\nbecomes the de facto standard for robust learning. While many recent works\npractice the data-centric philosophy, such as how to generate better\nadversarial examples or use generative models to produce additional training\ndata, we look back to the models themselves and revisit the adversarial\nrobustness from the perspective of deep feature distribution as an insightful\ncomplementarity. In this paper, we propose \\textit{Branch Orthogonality\nadveRsarial Training} (BORT) to obtain state-of-the-art performance with solely\nthe original dataset for adversarial training. To practice our design idea of\nintegrating multiple orthogonal solution spaces, we leverage a simple and\nstraightforward multi-branch neural network that eclipses adversarial attacks\nwith no increase in inference time. We heuristically propose a corresponding\nloss function, branch-orthogonal loss, to make each solution space of the\nmulti-branch model orthogonal. We evaluate our approach on CIFAR-10, CIFAR-100\nand SVHN against $\\ell_{\\infty}$ norm-bounded perturbations of size $\\epsilon =\n8/255$, respectively. Exhaustive experiments are conducted to show that our\nmethod goes beyond all state-of-the-art methods without any tricks. Compared to\nall methods that do not use additional data for training, our models achieve\n67.3\\% and 41.5\\% robust accuracy on CIFAR-10 and CIFAR-100 (improving upon the\nstate-of-the-art by +7.23\\% and +9.07\\%). We also outperform methods using a\ntraining set with a far larger scale than ours.\n","authors":["Zongyuan Zhang","Qingwen Bu","Tianyang Duan","Zheng Lin","Yuhao Qing","Zihan Fang","Heming Cui","Dong Huang"],"pdf_url":"https://arxiv.org/pdf/2208.08083v3.pdf","comment":"Camera-ready version for ICPADS 2025"},{"id":"http://arxiv.org/abs/2409.06154v3","updated":"2025-10-30T12:04:13Z","published":"2024-09-10T01:57:57Z","title":"Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial\n  Expressions Using Static Expression Data","summary":"  Dynamic facial expression recognition (DFER) infers emotions from the\ntemporal evolution of expressions, unlike static facial expression recognition\n(SFER), which relies solely on a single snapshot. This temporal analysis\nprovides richer information and promises greater recognition capability.\nHowever, current DFER methods often exhibit unsatisfied performance largely due\nto fewer training samples compared to SFER. Given the inherent correlation\nbetween static and dynamic expressions, we hypothesize that leveraging the\nabundant SFER data can enhance DFER. To this end, we propose Static-for-Dynamic\n(S4D), a unified dual-modal learning framework that integrates SFER data as a\ncomplementary resource for DFER. Specifically, S4D employs dual-modal\nself-supervised pre-training on facial images and videos using a shared Vision\nTransformer (ViT) encoder-decoder architecture, yielding improved\nspatiotemporal representations. The pre-trained encoder is then fine-tuned on\nstatic and dynamic expression datasets in a multi-task learning setup to\nfacilitate emotional information interaction. Unfortunately, vanilla multi-task\nlearning in our study results in negative transfer. To address this, we propose\nan innovative Mixture of Adapter Experts (MoAE) module that facilitates\ntask-specific knowledge acquisition while effectively extracting shared\nknowledge from both static and dynamic expression data. Extensive experiments\ndemonstrate that S4D achieves a deeper understanding of DFER, setting new\nstate-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with\nweighted average recall (WAR) of 53.65\\%, 58.44\\%, and 76.68\\%, respectively.\nAdditionally, a systematic correlation analysis between SFER and DFER tasks is\npresented, which further elucidates the potential benefits of leveraging SFER.\n","authors":["Yin Chen","Jia Li","Yu Zhang","Zhenzhen Hu","Shiguang Shan","Meng Wang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2409.06154v3.pdf","comment":"The code and model are publicly available here\n  https://github.com/MSA-LMC/S4D"},{"id":"http://arxiv.org/abs/2509.23885v2","updated":"2025-10-30T12:02:27Z","published":"2025-09-28T13:50:29Z","title":"Tunable-Generalization Diffusion Powered by Self-Supervised Contextual\n  Sub-Data for Low-Dose CT Reconstruction","summary":"  Current models based on deep learning for low-dose CT denoising rely heavily\non paired data and generalize poorly. Even the more concerned diffusion models\nneed to learn the distribution of clean data for reconstruction, which is\ndifficult to satisfy in medical clinical applications. At the same time,\nself-supervised-based methods face the challenge of significant degradation of\ngeneralizability of models pre-trained for the current dose to expand to other\ndoses. To address these issues, this work proposes a novel method of\nTUnable-geneRalizatioN Diffusion (TurnDiff) powered by self-supervised\ncontextual sub-data for low-dose CT reconstruction. Firstly, a contextual\nsubdata self-enhancing similarity strategy is designed for denoising centered\non the LDCT projection domain, which provides an initial prior for the\nsubsequent progress. Subsequently, the initial prior is used to combine\nknowledge distillation with a deep combination of latent diffusion models for\noptimizing image details. The pre-trained model is used for inference\nreconstruction, and the pixel-level self-correcting fusion technique is\nproposed for fine-grained reconstruction of the image domain to enhance the\nimage fidelity, using the initial prior and the LDCT image as a guide. In\naddition, the technique is flexibly applied to the generalization of upper and\nlower doses or even unseen doses. Dual-domain strategy cascade for\nself-supervised LDCT denoising, TurnDiff requires only LDCT projection domain\ndata for training and testing. Comprehensive evaluation on both benchmark\ndatasets and real-world data demonstrates that TurnDiff consistently\noutperforms state-of-the-art methods in both reconstruction and generalization.\n","authors":["Guoquan Wei","Liu Shi","Zekun Zhou","Wenzhe Shan","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2509.23885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23117v2","updated":"2025-10-30T12:02:18Z","published":"2025-10-27T08:38:17Z","title":"Seeing Structural Failure Before it Happens: An Image-Based\n  Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction","summary":"  Physics Informed Neural Networks (PINNs) are gaining attention for their\nability to embed physical laws into deep learning models, which is particularly\nuseful in structural engineering tasks with limited data. This paper aims to\nexplore the use of PINNs to predict the weight of small scale spaghetti\nbridges, a task relevant to understanding load limits and potential failure\nmodes in simplified structural models. Our proposed framework incorporates\nphysics-based constraints to the prediction model for improved performance. In\naddition to standard PINNs, we introduce a novel architecture named Physics\nInformed Kolmogorov Arnold Network (PIKAN), which blends universal function\napproximation theory with physical insights. The structural parameters provided\nas input to the model are collected either manually or through computer vision\nmethods. Our dataset includes 15 real bridges, augmented to 100 samples, and\nour best model achieves an $R^2$ score of 0.9603 and a mean absolute error\n(MAE) of 10.50 units. From applied perspective, we also provide a web based\ninterface for parameter entry and prediction. These results show that PINNs can\noffer reliable estimates of structural weight, even with limited data, and may\nhelp inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at\nhttps://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.\n","authors":["Omer Jauhar Khan","Sudais Khan","Hafeez Anwar","Shahzeb Khan","Shams Ul Arifeen"],"pdf_url":"https://arxiv.org/pdf/2510.23117v2.pdf","comment":"12 pages, 17 figures. Preprint"},{"id":"http://arxiv.org/abs/2510.26412v1","updated":"2025-10-30T12:00:46Z","published":"2025-10-30T12:00:46Z","title":"LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video\n  Generation","summary":"  Recently text-to-video generation has made impressive progress in producing\nshort, high-quality clips, but evaluating long-form outputs remains a major\nchallenge especially when processing complex prompts. Existing benchmarks\nmostly rely on simplified prompts and focus on low-level metrics, overlooking\nfine-grained alignment with prompts and abstract dimensions such as narrative\ncoherence and thematic expression. To address these gaps, we propose\nLoCoT2V-Bench, a benchmark specifically designed for long video generation\n(LVG) under complex input conditions. Based on various real-world videos,\nLoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating\nelements like scene transitions and event dynamics. Moreover, it constructs a\nmulti-dimensional evaluation framework that includes our newly proposed metrics\nsuch as event-level alignment, fine-grained temporal consistency, content\nclarity, and the Human Expectation Realization Degree (HERD) that focuses on\nmore abstract attributes like narrative flow, emotional response, and character\ndevelopment. Using this framework, we conduct a comprehensive evaluation of\nnine representative LVG models, finding that while current methods perform well\non basic visual and temporal aspects, they struggle with inter-event\nconsistency, fine-grained alignment, and high-level thematic adherence, etc.\nOverall, LoCoT2V-Bench provides a comprehensive and reliable platform for\nevaluating long-form complex text-to-video generation and highlights critical\ndirections for future method improvement.\n","authors":["Xiangqing Zheng","Chengyue Wu","Kehai Chen","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09549v2","updated":"2025-10-30T12:00:18Z","published":"2025-04-13T12:44:50Z","title":"SD-ReID: View-aware Stable Diffusion for Aerial-Ground Person\n  Re-Identification","summary":"  Aerial-Ground Person Re-IDentification (AG-ReID) aims to retrieve specific\npersons across cameras with different viewpoints. Previous works focus on\ndesigning discriminative models to maintain the identity consistency despite\ndrastic changes in camera viewpoints. The core idea behind these methods is\nquite natural, but designing a view-robust model is a very challenging task.\nMoreover, they overlook the contribution of view-specific features in enhancing\nthe model's ability to represent persons. To address these issues, we propose a\nnovel generative framework named SD-ReID for AG-ReID, which leverages\ngenerative models to mimic the feature distribution of different views while\nextracting robust identity representations. More specifically, we first train a\nViT-based model to extract person representations along with controllable\nconditions, including identity and view conditions. We then fine-tune the\nStable Diffusion (SD) model to enhance person representations guided by these\ncontrollable conditions. Furthermore, we introduce the View-Refined Decoder\n(VRD) to bridge the gap between instance-level and global-level features.\nFinally, both person representations and all-view features are employed to\nretrieve target persons. Extensive experiments on five AG-ReID benchmarks\n(i.e., CARGO, AG-ReIDv1, AG-ReIDv2, LAGPeR and G2APS-ReID) demonstrate the\neffectiveness of our proposed method. The source code will be available.\n","authors":["Yuhao Wang","Xiang Hu","Lixin Wang","Pingping Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2504.09549v2.pdf","comment":"More modifications may performed"},{"id":"http://arxiv.org/abs/2510.26391v1","updated":"2025-10-30T11:34:37Z","published":"2025-10-30T11:34:37Z","title":"EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models","summary":"  Existing EEG-driven image reconstruction methods often overlook spatial\nattention mechanisms, limiting fidelity and semantic coherence. To address\nthis, we propose a dual-conditioning framework that combines EEG embeddings\nwith spatial saliency maps to enhance image generation. Our approach leverages\nthe Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes\nStable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals\nwith visual semantics, while a ControlNet branch conditions generation on\nsaliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves\na significant improvement in the quality of low- and high-level image features\nover existing approaches. Simultaneously, strongly aligning with human visual\nattention. The results demonstrate that attentional priors resolve EEG\nambiguities, enabling high-fidelity reconstructions with applications in\nmedical diagnostics and neuroadaptive interfaces, advancing neural decoding\nthrough efficient adaptation of pre-trained diffusion models.\n","authors":["Igor Abramov","Ilya Makarov"],"pdf_url":"https://arxiv.org/pdf/2510.26391v1.pdf","comment":"Demo paper"},{"id":"http://arxiv.org/abs/2510.26390v1","updated":"2025-10-30T11:33:29Z","published":"2025-10-30T11:33:29Z","title":"SPG-CDENet: Spatial Prior-Guided Cross Dual Encoder Network for\n  Multi-Organ Segmentation","summary":"  Multi-organ segmentation is a critical task in computer-aided diagnosis.\nWhile recent deep learning methods have achieved remarkable success in image\nsegmentation, huge variations in organ size and shape challenge their\neffectiveness in multi-organ segmentation. To address these challenges, we\npropose a Spatial Prior-Guided Cross Dual Encoder Network (SPG-CDENet), a novel\ntwo-stage segmentation paradigm designed to improve multi-organ segmentation\naccuracy. Our SPG-CDENet consists of two key components: a spatial prior\nnetwork and a cross dual encoder network. The prior network generates coarse\nlocalization maps that delineate the approximate ROI, serving as spatial\nguidance for the dual encoder network. The cross dual encoder network comprises\nfour essential components: a global encoder, a local encoder, a symmetric\ncross-attention module, and a flow-based decoder. The global encoder captures\nglobal semantic features from the entire image, while the local encoder focuses\non features from the prior network. To enhance the interaction between the\nglobal and local encoders, a symmetric cross-attention module is proposed\nacross all layers of the encoders to fuse and refine features. Furthermore, the\nflow-based decoder directly propagates high-level semantic features from the\nfinal encoder layer to all decoder layers, maximizing feature preservation and\nutilization. Extensive qualitative and quantitative experiments on two public\ndatasets demonstrate the superior performance of SPG-CDENet compared to\nexisting segmentation methods. Furthermore, ablation studies further validate\nthe effectiveness of the proposed modules in improving segmentation accuracy.\n","authors":["Xizhi Tian","Changjun Zhou","Yulin. Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26369v1","updated":"2025-10-30T11:14:17Z","published":"2025-10-30T11:14:17Z","title":"CorVS: Person Identification via Video Trajectory-Sensor Correspondence\n  in a Real-World Warehouse","summary":"  Worker location data is key to higher productivity in industrial sites.\nCameras are a promising tool for localization in logistics warehouses since\nthey also offer valuable environmental contexts such as package status.\nHowever, identifying individuals with only visual data is often impractical.\nAccordingly, several prior studies identified people in videos by comparing\ntheir trajectories and wearable sensor measurements. While this approach has\nadvantages such as independence from appearance, the existing methods may break\ndown under real-world conditions. To overcome this challenge, we propose CorVS,\na novel data-driven person identification method based on correspondence\nbetween visual tracking trajectories and sensor measurements. Firstly, our deep\nlearning model predicts correspondence probabilities and reliabilities for\nevery pair of a trajectory and sensor measurements. Secondly, our algorithm\nmatches the trajectories and sensor measurements over time using the predicted\nprobabilities and reliabilities. We developed a dataset with actual warehouse\noperations and demonstrated the method's effectiveness for real-world\napplications.\n","authors":["Kazuma Kano","Yuki Mori","Shin Katayama","Kenta Urano","Takuro Yonezawa","Nobuo Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2510.26369v1.pdf","comment":"7 pages, 3 figures, accepted to IPIN 2025"},{"id":"http://arxiv.org/abs/2510.26358v1","updated":"2025-10-30T11:08:23Z","published":"2025-10-30T11:08:23Z","title":"AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian\n  Splatting SLAM","summary":"  Autonomous robots in orchards require real-time 3D scene understanding\ndespite repetitive row geometry, seasonal appearance changes, and wind-driven\nfoliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that\ncouples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian\nSplatting (3DGS) rendering. Batch rasterization across complementary viewpoints\nrecovers orchard structure under occlusions, while a unified gradient-driven\nmap lifecycle executed between keyframes preserves fine details and bounds\nmemory. Pose refinement is guided by a probabilistic LiDAR-based depth\nconsistency term, back-propagated through the camera projection to tighten\ngeometry-appearance coupling. We deploy the system on a field platform in apple\nand pear orchards across dormancy, flowering, and harvesting, using a\nstandardized trajectory protocol that evaluates both training-view and\nnovel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons\nand sites, AgriGS-SLAM delivers sharper, more stable reconstructions and\nsteadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while\nmaintaining real-time performance on-tractor. While demonstrated in orchard\nmonitoring, the approach can be applied to other outdoor domains requiring\nrobust multimodal perception.\n","authors":["Mirko Usuelli","David Rapado-Rincon","Gert Kootstra","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2510.26358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04448v2","updated":"2025-10-30T10:58:04Z","published":"2025-09-04T17:59:43Z","title":"TRUST-VL: An Explainable News Assistant for General Multimodal\n  Misinformation Detection","summary":"  Multimodal misinformation, encompassing textual, visual, and cross-modal\ndistortions, poses an increasing societal threat that is amplified by\ngenerative AI. Existing methods typically focus on a single type of distortion\nand struggle to generalize to unseen scenarios. In this work, we observe that\ndifferent distortion types share common reasoning capabilities while also\nrequiring task-specific skills. We hypothesize that joint training across\ndistortion types facilitates knowledge sharing and enhances the model's ability\nto generalize. To this end, we introduce TRUST-VL, a unified and explainable\nvision-language model for general multimodal misinformation detection. TRUST-VL\nincorporates a novel Question-Aware Visual Amplifier module, designed to\nextract task-specific visual features. To support training, we also construct\nTRUST-Instruct, a large-scale instruction dataset containing 198K samples\nfeaturing structured reasoning chains aligned with human fact-checking\nworkflows. Extensive experiments on both in-domain and zero-shot benchmarks\ndemonstrate that TRUST-VL achieves state-of-the-art performance, while also\noffering strong generalization and interpretability.\n","authors":["Zehong Yan","Peng Qi","Wynne Hsu","Mong Li Lee"],"pdf_url":"https://arxiv.org/pdf/2509.04448v2.pdf","comment":"EMNLP 2025 Oral; Project Homepage:\n  https://yanzehong.github.io/trust-vl/"},{"id":"http://arxiv.org/abs/2505.21497v2","updated":"2025-10-30T10:49:28Z","published":"2025-05-27T17:58:49Z","title":"Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers","summary":"  Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster.\n","authors":["Wei Pang","Kevin Qinghong Lin","Xiangru Jian","Xi He","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2505.21497v2.pdf","comment":"Project Page: https://github.com/Paper2Poster/Paper2Poster"},{"id":"http://arxiv.org/abs/2510.26339v1","updated":"2025-10-30T10:46:28Z","published":"2025-10-30T10:46:28Z","title":"GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and\n  High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?","summary":"  Image super-resolution(SR) is fundamental to many vision system-from\nsurveillance and autonomy to document analysis and retail analytics-because\nrecovering high-frequency details, especially scene-text, enables reliable\ndownstream perception. Scene-text, i.e., text embedded in natural images such\nas signs, product labels, and storefronts, often carries the most actionable\ninformation; when characters are blurred or hallucinated, optical character\nrecognition(OCR) and subsequent decisions fail even if the rest of the image\nappears sharp. Yet previous SR research has often been tuned to distortion\n(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that\nare largely insensitive to character-level errors. Furthermore, studies that do\naddress text SR often focus on simplified benchmarks with isolated characters,\noverlooking the challenges of text within complex natural scenes. As a result,\nscene-text is effectively treated as generic texture. For SR to be effective in\npractical deployments, it is therefore essential to explicitly optimize for\nboth text legibility and perceptual quality. We present GLYPH-SR, a\nvision-language-guided diffusion framework that aims to achieve both objectives\njointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by\nOCR data, and a ping-pong scheduler that alternates between text- and\nscene-centric guidance. To enable targeted text restoration, we train these\ncomponents on a synthetic corpus while keeping the main SR branch frozen.\nAcross SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by\nup to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)\nwhile maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed\nto satisfy both objectives simultaneously-high readability and high visual\nrealism-delivering SR that looks right and reds right.\n","authors":["Mingyu Sung","Seungjae Ham","Kangwoo Kim","Yeokyoung Yoon","Sangseok Yun","Il-Min Kim","Jae-Mo Kang"],"pdf_url":"https://arxiv.org/pdf/2510.26339v1.pdf","comment":"11 pages, 6 figures. Includes supplementary material. Under review as\n  a conference paper at ICLR 2026"},{"id":"http://arxiv.org/abs/2503.09499v3","updated":"2025-10-30T10:21:42Z","published":"2025-03-12T16:03:03Z","title":"MindGYM: What Matters in Question Synthesis for Thinking-Centric\n  Fine-Tuning?","summary":"  Large foundation models face challenges in acquiring transferable, structured\nthinking abilities, especially when supervised with rigid templates or\ncrowd-annotated instruction datasets. Unlike prior approaches, we focus on a\nthinking-centric data synthesis paradigm that enables models to evolve through\nself-generated, cognitively guided data. We propose MindGYM, a structured and\nscalable framework for question synthesis, composed of: (1) Cognitive Thinking\nProcess Injection, which infuses high-level reasoning objectives to shape the\nmodel's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating\natomic questions from diverse semantic types to encourage broader thinking; and\n(3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop\nquestions based on QA seeds for deeper reasoning. Detailed analysis shows that\nsynthetic data generated by our method achieves 16.7% higher average quality\nand 67.91% lower quality variance compared to baseline sources, highlighting\nthat both high-quality and self-contained data are essential for effective,\nthinking-oriented fine-tuning. MindGYM improves performance on six reasoning\nbenchmarks, achieving gains of up to 16% on MathVision using only 400 data\nsamples, and generalizable improvements across different model sizes and\narchitectures. MindGYM underscores the viability of self-challenging mechanisms\nin refining large model capabilities while minimizing human intervention and\nresource demands. Code and data are released to promote data-centric research\ninto self-evolving foundation models driven by their internal reasoning\ncapabilities.\n","authors":["Zhe Xu","Daoyuan Chen","Zhenqing Ling","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2503.09499v3.pdf","comment":"Accepted by NeurIPS'25. 30 pages, 2 figures, 13 tables"},{"id":"http://arxiv.org/abs/2509.06771v2","updated":"2025-10-30T10:15:05Z","published":"2025-09-08T14:55:16Z","title":"D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning --\n  A Benchmark Dataset and Method","summary":"  Dark humor in online memes poses unique challenges due to its reliance on\nimplicit, sensitive, and culturally contextual cues. To address the lack of\nresources and methods for detecting dark humor in multimodal content, we\nintroduce a novel dataset of 4,379 Reddit memes annotated for dark humor,\ntarget category (gender, mental health, violence, race, disability, and other),\nand a three-level intensity rating (mild, moderate, severe). Building on this\nresource, we propose a reasoning-augmented framework that first generates\nstructured explanations for each meme using a Large Vision-Language Model\n(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective\nto iteratively refine its explanations, ensuring completeness and alignment. We\nthen extract textual features from both the OCR transcript and the self-refined\nreasoning via a text encoder, while visual features are obtained using a vision\ntransformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three\nstreams, text, image, and reasoning, via pairwise attention mechanisms,\nproducing a unified representation for classification. Experimental results\ndemonstrate that our approach outperforms strong baselines across three tasks:\ndark humor detection, target identification, and intensity prediction. The\ndataset, annotations, and code are released to facilitate further research in\nmultimodal humor understanding and content moderation. Code and Dataset are\navailable at:\nhttps://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning\n","authors":["Sai Kartheek Reddy Kasu","Mohammad Zia Ur Rehman","Shahid Shafi Dar","Rishi Bharat Junghare","Dhanvin Sanjay Namboodiri","Nagendra Kumar"],"pdf_url":"https://arxiv.org/pdf/2509.06771v2.pdf","comment":"Accepted at IEEE International Conference on Data Mining (ICDM) 2025"},{"id":"http://arxiv.org/abs/2510.26315v1","updated":"2025-10-30T10:08:06Z","published":"2025-10-30T10:08:06Z","title":"A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for\n  Diabetic Retinopathy Grading","summary":"  Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged\nand elderly people, which significantly impacts their daily lives and mental\nhealth. To improve the efficiency of clinical screening and enable the early\ndetection of DR, a variety of automated DR diagnosis systems have been recently\nestablished based on convolutional neural network (CNN) or vision Transformer\n(ViT). However, due to the own shortages of CNN / ViT, the performance of\nexisting methods using single-type backbone has reached a bottleneck. One\npotential way for the further improvements is integrating different kinds of\nbackbones, which can fully leverage the respective strengths of them\n(\\emph{i.e.,} the local feature extraction capability of CNN and the global\nfeature capturing ability of ViT). To this end, we propose a novel paradigm to\neffectively fuse the features extracted by different backbones based on the\ntheory of evidence. Specifically, the proposed evidential fusion paradigm\ntransforms the features from different backbones into supporting evidences via\na set of deep evidential networks. With the supporting evidences, the\naggregated opinion can be accordingly formed, which can be used to adaptively\ntune the fusion pattern between different backbones and accordingly boost the\nperformance of our hybrid model. We evaluated our method on two publicly\navailable DR grading datasets. The experimental results demonstrate that our\nhybrid model not only improves the accuracy of DR grading, compared to the\nstate-of-the-art frameworks, but also provides the excellent interpretability\nfor feature fusion and decision-making.\n","authors":["Junlai Qiu","Yunzhu Chen","Hao Zheng","Yawen Huang","Yuexiang Li"],"pdf_url":"https://arxiv.org/pdf/2510.26315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22159v3","updated":"2025-10-30T10:00:04Z","published":"2025-03-28T05:46:02Z","title":"Disentangled 4D Gaussian Splatting: Rendering High-Resolution Dynamic\n  World at 343 FPS","summary":"  While dynamic novel view synthesis from 2D videos has seen progress,\nachieving efficient reconstruction and rendering of dynamic scenes remains a\nchallenging task. In this paper, we introduce Disentangled 4D Gaussian\nSplatting (Disentangled4DGS), a novel representation and rendering pipeline\nthat achieves real-time performance without compromising visual fidelity.\nDisentangled4DGS decouples the temporal and spatial components of 4D Gaussians,\navoiding the need for slicing first and four-dimensional matrix calculations in\nprior methods. By projecting temporal and spatial deformations into dynamic 2D\nGaussians and deferring temporal processing, we minimize redundant computations\nof 4DGS. Our approach also features a gradient-guided flow loss and temporal\nsplitting strategy to reduce artifacts. Experiments demonstrate a significant\nimprovement in rendering speed and quality, achieving 343 FPS when render\n1352*1014 resolution images on a single RTX3090 while reducing storage\nrequirements by at least 4.5%. Our approach sets a new benchmark for dynamic\nnovel view synthesis, outperforming existing methods on both multi-view and\nmonocular dynamic scene datasets.\n","authors":["Hao Feng","Hao Sun","Wei Xie","Zhi Zuo","Zhengzhe Liu"],"pdf_url":"https://arxiv.org/pdf/2503.22159v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26304v1","updated":"2025-10-30T09:43:56Z","published":"2025-10-30T09:43:56Z","title":"Exploring the correlation between the type of music and the emotions\n  evoked: A study using subjective questionnaires and EEG","summary":"  The subject of this work is to check how different types of music affect\nhuman emotions. While listening to music, a subjective survey and brain\nactivity measurements were carried out using an EEG helmet. The aim is to\ndemonstrate the impact of different music genres on emotions. The research\ninvolved a diverse group of participants of different gender and musical\npreferences. This had the effect of capturing a wide range of emotional\nresponses to music. After the experiment, a relationship analysis of the\nrespondents' questionnaires with EEG signals was performed. The analysis\nrevealed connections between emotions and observed brain activity.\n","authors":["Jelizaveta Jankowska","Bożena Kostek","Fernando Alonso-Fernandez","Prayag Tiwari"],"pdf_url":"https://arxiv.org/pdf/2510.26304v1.pdf","comment":"Published at IWAIPR 2025 conference"},{"id":"http://arxiv.org/abs/2510.22319v2","updated":"2025-10-30T09:33:15Z","published":"2025-10-25T14:51:17Z","title":"GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via\n  Regulated Clipping","summary":"  Recently, GRPO-based reinforcement learning has shown remarkable progress in\noptimizing flow-matching models, effectively improving their alignment with\ntask-specific rewards. Within these frameworks, the policy update relies on\nimportance-ratio clipping to constrain overconfident positive and negative\ngradients. However, in practice, we observe a systematic shift in the\nimportance-ratio distribution-its mean falls below 1 and its variance differs\nsubstantially across timesteps. This left-shifted and inconsistent distribution\nprevents positive-advantage samples from entering the clipped region, causing\nthe mechanism to fail in constraining overconfident positive updates. As a\nresult, the policy model inevitably enters an implicit over-optimization\nstage-while the proxy reward continues to increase, essential metrics such as\nimage quality and text-prompt alignment deteriorate sharply, ultimately making\nthe learned policy impractical for real-world use. To address this issue, we\nintroduce GRPO-Guard, a simple yet effective enhancement to existing GRPO\nframeworks. Our method incorporates ratio normalization, which restores a\nbalanced and step-consistent importance ratio, ensuring that PPO clipping\nproperly constrains harmful updates across denoising timesteps. In addition, a\ngradient reweighting strategy equalizes policy gradients over noise conditions,\npreventing excessive updates from particular timestep regions. Together, these\ndesigns act as a regulated clipping mechanism, stabilizing optimization and\nsubstantially mitigating implicit over-optimization without relying on heavy KL\nregularization. Extensive experiments on multiple diffusion backbones (e.g.,\nSD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard\nsignificantly reduces over-optimization while maintaining or even improving\ngeneration quality.\n","authors":["Jing Wang","Jiajun Liang","Jie Liu","Henglin Liu","Gongye Liu","Jun Zheng","Wanyuan Pang","Ao Ma","Zhenyu Xie","Xintao Wang","Meng Wang","Pengfei Wan","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2510.22319v2.pdf","comment":"Project Page: https://jingw193.github.io/GRPO-Guard/"},{"id":"http://arxiv.org/abs/2510.26297v1","updated":"2025-10-30T09:31:47Z","published":"2025-10-30T09:31:47Z","title":"Towards Realistic Earth-Observation Constellation Scheduling: Benchmark\n  and Methodology","summary":"  Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented\nflexibility for monitoring the Earth's surface, but their scheduling remains\nchallenging under large-scale scenarios, dynamic environments, and stringent\nconstraints. Existing methods often simplify these complexities, limiting their\nreal-world performance. We address this gap with a unified framework\nintegrating a standardized benchmark suite and a novel scheduling model. Our\nbenchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and\n$16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to\n$300$ imaging tasks. These scenarios are generated via a high-fidelity\nsimulation platform, ensuring realistic satellite behavior such as orbital\ndynamics and resource constraints. Ground truth scheduling annotations are\nprovided for each scenario. To our knowledge, AEOS-Bench is the first\nlarge-scale benchmark suite tailored for realistic constellation scheduling.\nBuilding upon this benchmark, we introduce AEOS-Former, a Transformer-based\nscheduling model that incorporates a constraint-aware attention mechanism. A\ndedicated internal constraint module explicitly models the physical and\noperational limits of each satellite. Through simulation-based iterative\nlearning, AEOS-Former adapts to diverse scenarios, offering a robust solution\nfor AEOS constellation scheduling. Experimental results demonstrate that\nAEOS-Former outperforms baseline models in task completion and energy\nefficiency, with ablation studies highlighting the contribution of each\ncomponent. Code and data are provided in\nhttps://github.com/buaa-colalab/AEOSBench.\n","authors":["Luting Wang","Yinghao Xiang","Hongliang Huang","Dongjun Li","Chen Gao","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02393v3","updated":"2025-10-30T09:31:07Z","published":"2025-06-03T03:18:17Z","title":"RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared\n  Small Target Detection","summary":"  Infrared small target detection is a challenging task due to its unique\ncharacteristics (e.g., small, dim, shapeless and changeable). Recently\npublished CNN-based methods have achieved promising performance with heavy\nfeature extraction and fusion modules. To achieve efficient and effective\ndetection, we propose a recurrent reusable-convolution attention network\n(RRCA-Net) for infrared small target detection. Specifically, RRCA-Net\nincorporates reusable-convolution block (RuCB) in a recurrent manner without\nintroducing extra parameters. With the help of the repetitive iteration in\nRuCB, the high-level information of small targets in the deep layers can be\nwell maintained and further refined. Then, a dual interactive attention\naggregation module (DIAAM) is proposed to promote the mutual enhancement and\nfusion of refined information. In this way, RRCA-Net can both achieve\nhigh-level feature refinement and enhance the correlation of contextual\ninformation between adjacent layers. Moreover, to achieve steady convergence,\nwe design a target characteristic inspired loss function (DpT-k loss) by\nintegrating physical and mathematical constraints. Experimental results on\nthree benchmark datasets (e.g. NUAA-SIRST, IRSTD-1k, DenseSIRST) demonstrate\nthat our RRCA-Net can achieve comparable performance to the state-of-the-art\nmethods while maintaining a small number of parameters, and act as a plug and\nplay module to introduce consistent performance improvement for several popular\nIRSTD methods.\n","authors":["Yongxian Liu","Boyang Li","Ting Liu","Zaiping Lin","Wei An"],"pdf_url":"https://arxiv.org/pdf/2506.02393v3.pdf","comment":"We have updated the journal reference and DOI"},{"id":"http://arxiv.org/abs/2510.26294v1","updated":"2025-10-30T09:28:48Z","published":"2025-10-30T09:28:48Z","title":"Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via\n  Ocular Cropping","summary":"  We focus on ocular biometrics, specifically the periocular region (the area\naround the eye), which offers high discrimination and minimal acquisition\nconstraints. We evaluate three Convolutional Neural Network architectures of\nvarying depth and complexity to assess their effectiveness for periocular\nrecognition. The networks are trained on 1,907,572 ocular crops extracted from\nthe large-scale VGGFace2 database. This significantly contrasts with existing\nworks, which typically rely on small-scale periocular datasets for training\nhaving only a few thousand images. Experiments are conducted with ocular images\nfrom VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images,\nand the UFPR-Periocular database, which consists of selfies captured via mobile\ndevices with user guidance on the screen. Due to the uncontrolled conditions of\nVGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from\n9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In\ncontrast, UFPR-Periocular yields significantly better performance (EERs of\n1-2%), thanks to higher image quality and more consistent acquisition\nprotocols. To the best of our knowledge, these are the lowest reported EERs on\nthe UFPR dataset to date.\n","authors":["Fernando Alonso-Fernandez","Kevin Hernandez-Diaz","Jose Maria Buades Rubio","Josef Bigun"],"pdf_url":"https://arxiv.org/pdf/2510.26294v1.pdf","comment":"Published at IWAIPR 2025 conference"},{"id":"http://arxiv.org/abs/2510.26292v1","updated":"2025-10-30T09:24:34Z","published":"2025-10-30T09:24:34Z","title":"Beyond Imitation: Constraint-Aware Trajectory Generation with Flow\n  Matching For End-to-End Autonomous Driving","summary":"  Planning is a critical component of end-to-end autonomous driving. However,\nprevailing imitation learning methods often suffer from mode collapse, failing\nto produce diverse trajectory hypotheses. Meanwhile, existing generative\napproaches struggle to incorporate crucial safety and physical constraints\ndirectly into the generative process, necessitating an additional optimization\nstage to refine their outputs. To address these limitations, we propose CATG, a\nnovel planning framework that leverages Constrained Flow Matching. Concretely,\nCATG explicitly models the flow matching process, which inherently mitigates\nmode collapse and allows for flexible guidance from various conditioning\nsignals. Our primary contribution is the novel imposition of explicit\nconstraints directly within the flow matching process, ensuring that the\ngenerated trajectories adhere to vital safety and kinematic rules. Secondly,\nCATG parameterizes driving aggressiveness as a control signal during\ngeneration, enabling precise manipulation of trajectory style. Notably, on the\nNavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and\nwas honored with the Innovation Award.\n","authors":["Lin Liu","Guanyi Yu","Ziying Song","Junqiao Li","Caiyan Jia","Feiyang Jia","Peiliang Wu","Yandan Luo"],"pdf_url":"https://arxiv.org/pdf/2510.26292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26282v1","updated":"2025-10-30T09:07:36Z","published":"2025-10-30T09:07:36Z","title":"Exploring Complementarity and Explainability in CNNs for Periocular\n  Verification Across Acquisition Distances","summary":"  We study the complementarity of different CNNs for periocular verification at\ndifferent distances on the UBIPr database. We train three architectures of\nincreasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of\neye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,\ncompare different network initialisations, and apply score-level fusion via\nlogistic regression. In addition, we use LIME heatmaps and Jensen-Shannon\ndivergence to compare attention patterns of the CNNs. While ResNet50\nconsistently performs best individually, the fusion provides substantial gains,\nespecially when combining all three networks. Heatmaps show that networks\nusually focus on distinct regions of a given image, which explains their\ncomplementarity. Our method significantly outperforms previous works on UBIPr,\nachieving a new state-of-the-art.\n","authors":["Fernando Alonso-Fernandez","Kevin Hernandez Diaz","Jose M. Buades","Kiran Raja","Josef Bigun"],"pdf_url":"https://arxiv.org/pdf/2510.26282v1.pdf","comment":"Accepted at BIOSIG 2025 conference"},{"id":"http://arxiv.org/abs/2510.26268v1","updated":"2025-10-30T08:53:13Z","published":"2025-10-30T08:53:13Z","title":"Revisiting Generative Infrared and Visible Image Fusion Based on Human\n  Cognitive Laws","summary":"  Existing infrared and visible image fusion methods often face the dilemma of\nbalancing modal information. Generative fusion methods reconstruct fused images\nby learning from data distributions, but their generative capabilities remain\nlimited. Moreover, the lack of interpretability in modal information selection\nfurther affects the reliability and consistency of fusion results in complex\nscenarios. This manuscript revisits the essence of generative image fusion\nunder the inspiration of human cognitive laws and proposes a novel infrared and\nvisible image fusion method, termed HCLFuse. First, HCLFuse investigates the\nquantification theory of information mapping in unsupervised fusion networks,\nwhich leads to the design of a multi-scale mask-regulated variational\nbottleneck encoder. This encoder applies posterior probability modeling and\ninformation decomposition to extract accurate and concise low-level modal\ninformation, thereby supporting the generation of high-fidelity structural\ndetails. Furthermore, the probabilistic generative capability of the diffusion\nmodel is integrated with physical laws, forming a time-varying physical\nguidance mechanism that adaptively regulates the generation process at\ndifferent stages, thereby enhancing the ability of the model to perceive the\nintrinsic structure of data and reducing dependence on data quality.\nExperimental results show that the proposed method achieves state-of-the-art\nfusion performance in qualitative and quantitative evaluations across multiple\ndatasets and significantly improves semantic segmentation metrics. This fully\ndemonstrates the advantages of this generative image fusion method, drawing\ninspiration from human cognition, in enhancing structural consistency and\ndetail quality.\n","authors":["Lin Guo","Xiaoqing Luo","Wei Xie","Zhancheng Zhang","Hui Li","Rui Wang","Zhenhua Feng","Xiaoning Song"],"pdf_url":"https://arxiv.org/pdf/2510.26268v1.pdf","comment":"NeurIPS 2025 spotlight"},{"id":"http://arxiv.org/abs/2503.11094v4","updated":"2025-10-30T08:44:27Z","published":"2025-03-14T05:35:38Z","title":"Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with\n  Multimodal Large Language Model in Open Space","summary":"  Spatial reasoning is a fundamental capability of multimodal large language\nmodels (MLLMs), yet their performance in open aerial environments remains\nunderexplored. In this work, we present Open3D-VQA, a novel benchmark for\nevaluating MLLMs' ability to reason about complex spatial relationships from an\naerial perspective. The benchmark comprises 73k QA pairs spanning 7 general\nspatial reasoning tasks, including multiple-choice, true/false, and\nshort-answer formats, and supports both visual and point cloud modalities. The\nquestions are automatically generated from spatial relations extracted from\nboth real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs\nreveals that: 1) Models are generally better at answering questions about\nrelative spatial relations than absolute distances, 2) 3D LLMs fail to\ndemonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on\nthe simulated dataset can significantly improve the model's spatial reasoning\nperformance in real-world scenarios. We release our benchmark, data generation\npipeline, and evaluation toolkit to support further research:\nhttps://github.com/EmbodiedCity/Open3D-VQA.code.\n","authors":["Weichen Zhang","Zile Zhou","Xin Zeng","Xuchen Liu","Jianjie Fang","Chen Gao","Yong Li","Jinqiang Cui","Xinlei Chen","Xiao-Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11094v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26241v1","updated":"2025-10-30T08:21:50Z","published":"2025-10-30T08:21:50Z","title":"Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for\n  Vision-Language Models","summary":"  Modern vision-language models (VLMs) excel at many multimodal tasks, yet\ntheir grasp of temporal information in video remains weak and, crucially,\nunder-evaluated. We probe this gap with a deceptively simple but revealing\nchallenge: judging the arrow of time (AoT)-whether a short clip is played\nforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated\nbenchmark that tests whether VLMs can infer temporal direction in natural\nvideos using the same stimuli and behavioral baselines established for humans.\nOur comprehensive evaluation of open-weight and proprietary, reasoning and\nnon-reasoning VLMs reveals that most models perform near chance, and even the\nbest lag far behind human accuracy on physically irreversible processes (e.g.,\nfree fall, diffusion/explosion) and causal manual actions (division/addition)\nthat humans recognize almost instantly. These results highlight a fundamental\ngap in current multimodal systems: while they capture rich visual-semantic\ncorrelations, they lack the inductive biases required for temporal continuity\nand causal understanding. We release the code and data for AoT-PsyPhyBENCH to\nencourage further progress in the physical and temporal reasoning capabilities\nof VLMs.\n","authors":["Shiho Matta","Lis Kanashiro Pereira","Peitao Han","Fei Cheng","Shigeru Kitazawa"],"pdf_url":"https://arxiv.org/pdf/2510.26241v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2509.06159v3","updated":"2025-10-30T08:10:05Z","published":"2025-09-07T17:59:09Z","title":"FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes","summary":"  The growing popularity of robotic minimally invasive surgeries has made deep\nlearning-based surgical training a key area of research. A thorough\nunderstanding of the surgical scene components is crucial, which semantic\nsegmentation models can help achieve. However, most existing work focuses on\nsurgical tools and overlooks anatomical objects. Additionally, current\nstate-of-the-art (SOTA) models struggle to balance capturing high-level\ncontextual features and low-level edge features. We propose a Feature-Adaptive\nSpatial Localization model (FASL-Seg), designed to capture features at multiple\nlevels of detail through two distinct processing streams, namely a Low-Level\nFeature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream,\nfor varying feature resolutions - enabling precise segmentation of anatomy and\nsurgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark\ndatasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model\nachieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy\nsegmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU\nof 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation,\nrespectively, outperforming SOTA overall performance, with comparable per-class\nSOTA results in both datasets and consistent performance in various classes for\nanatomy and instruments, demonstrating the effectiveness of distinct processing\nstreams for varying feature resolutions.\n","authors":["Muraam Abdel-Ghani","Mahmoud Ali","Mohamed Ali","Fatmaelzahraa Ahmed","Muhammad Arsalan","Abdulaziz Al-Ali","Shidin Balakrishnan"],"pdf_url":"https://arxiv.org/pdf/2509.06159v3.pdf","comment":"8 pages, 6 figures, In Proceedings of European Conference on\n  Artificial Intelligence (ECAI) 2025 <https://doi.org/10.3233/FAIA250908>"},{"id":"http://arxiv.org/abs/2508.07981v3","updated":"2025-10-30T08:09:13Z","published":"2025-08-11T13:41:24Z","title":"Omni-Effects: Unified and Spatially-Controllable Visual Effects\n  Generation","summary":"  Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects.\n","authors":["Fangyuan Mao","Aiming Hao","Jintao Chen","Dongxia Liu","Xiaokun Feng","Jiashu Zhu","Meiqi Wu","Chubin Chen","Jiahong Wu","Xiangxiang Chu"],"pdf_url":"https://arxiv.org/pdf/2508.07981v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20392v4","updated":"2025-10-30T07:48:58Z","published":"2024-12-29T08:09:20Z","title":"Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning","summary":"  Multimodal contrastive learning models (e.g., CLIP) can learn high-quality\nrepresentations from large-scale image-text datasets, while they exhibit\nsignificant vulnerabilities to backdoor attacks, raising serious safety\nconcerns. In this paper, we reveal that CLIP's vulnerabilities primarily stem\nfrom its tendency to encode features beyond in-dataset predictive patterns,\ncompromising its visual feature resistivity to input perturbations. This makes\nits encoded features highly susceptible to being reshaped by backdoor triggers.\nTo address this challenge, we propose Repulsive Visual Prompt Tuning (RVPT), a\nnovel defense approach that employs deep visual prompt tuning with a specially\ndesigned feature-repelling loss. Specifically, RVPT adversarially repels the\nencoded features from deeper layers while optimizing the standard cross-entropy\nloss, ensuring that only predictive features in downstream tasks are encoded,\nthereby enhancing CLIP's visual feature resistivity against input perturbations\nand mitigating its susceptibility to backdoor attacks. Unlike existing\nmultimodal backdoor defense methods that typically require the availability of\npoisoned data or involve fine-tuning the entire model, RVPT leverages few-shot\ndownstream clean samples and only tunes a small number of parameters. Empirical\nresults demonstrate that RVPT tunes only 0.27\\% of the parameters in CLIP, yet\nit significantly outperforms state-of-the-art defense methods, reducing the\nattack success rate from 89.70\\% to 2.76\\% against the most advanced multimodal\nattacks on ImageNet and effectively generalizes its defensive capabilities\nacross multiple datasets.\n","authors":["Zhifang Zhang","Shuo He","Haobo Wang","Bingquan Shen","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2412.20392v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26213v1","updated":"2025-10-30T07:39:54Z","published":"2025-10-30T07:39:54Z","title":"OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal\n  Document Layout Generation","summary":"  Document AI has advanced rapidly and is attracting increasing attention. Yet,\nwhile most efforts have focused on document layout analysis (DLA), its\ngenerative counterpart, document layout generation, remains underexplored. A\nmajor obstacle lies in the scarcity of diverse layouts: academic papers with\nManhattan-style structures dominate existing studies, while open-world genres\nsuch as newspapers and magazines remain severely underrepresented. To address\nthis gap, we curate OmniLayout-1M, the first million-scale dataset of diverse\ndocument layouts, covering six common document types and comprising\ncontemporary layouts collected from multiple sources. Moreover, since existing\nmethods struggle in complex domains and often fail to arrange long sequences\ncoherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage\nCoarse-to-Fine learning paradigm: 1) learning universal layout principles from\nOmniLayout-1M with coarse category definitions, and 2) transferring the\nknowledge to a specific domain with fine-grained annotations. Extensive\nexperiments demonstrate that our approach achieves strong performance on\nmultiple domains in M$^{6}$Doc dataset, substantially surpassing both existing\nlayout generation experts and several latest general-purpose LLMs. Our code,\nmodels, and dataset will be publicly released.\n","authors":["Hengrui Kang","Zhuangcheng Gu","Zhiyuan Zhao","Zichen Wen","Bin Wang","Weijia Li","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2510.26213v1.pdf","comment":"TL;DR: With OmniLayout-1M dataset and LLM-based coarse-to-fine\n  learning, we enable universal and diverse document layout generation"},{"id":"http://arxiv.org/abs/2510.23588v2","updated":"2025-10-30T07:38:54Z","published":"2025-10-27T17:54:08Z","title":"FARMER: Flow AutoRegressive Transformer over Pixels","summary":"  Directly modeling the explicit likelihood of the raw data distribution is key\ntopic in the machine learning area, which achieves the scaling successes in\nLarge Language Models by autoregressive modeling. However, continuous AR\nmodeling over visual pixel data suffer from extremely long sequences and\nhigh-dimensional spaces. In this paper, we present FARMER, a novel end-to-end\ngenerative framework that unifies Normalizing Flows (NF) and Autoregressive\n(AR) models for tractable likelihood estimation and high-quality image\nsynthesis directly from raw pixels. FARMER employs an invertible autoregressive\nflow to transform images into latent sequences, whose distribution is modeled\nimplicitly by an autoregressive model. To address the redundancy and complexity\nin pixel-level modeling, we propose a self-supervised dimension reduction\nscheme that partitions NF latent channels into informative and redundant\ngroups, enabling more effective and efficient AR modeling. Furthermore, we\ndesign a one-step distillation scheme to significantly accelerate inference\nspeed and introduce a resampling-based classifier-free guidance algorithm to\nboost image generation quality. Extensive experiments demonstrate that FARMER\nachieves competitive performance compared to existing pixel-based generative\nmodels while providing exact likelihoods and scalable training.\n","authors":["Guangting Zheng","Qinyu Zhao","Tao Yang","Fei Xiao","Zhijie Lin","Jie Wu","Jiajun Deng","Yanyong Zhang","Rui Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.23588v2.pdf","comment":"Bytedance Seed Technical Report"},{"id":"http://arxiv.org/abs/2510.26203v1","updated":"2025-10-30T07:26:18Z","published":"2025-10-30T07:26:18Z","title":"Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain\n  Sustainability and Risk Management","summary":"  The sustainability of supply chain plays a key role in achieving optimal\nperformance in controlling the supply chain. The management of risks that occur\nin a supply chain is a fundamental problem for the purpose of developing the\nsustainability of the network and elevating the performance efficiency of the\nsupply chain. The correct classification of products is another essential\nelement in a sustainable supply chain. Acknowledging recent breakthroughs in\nthe context of deep networks, several architectural options have been deployed\nto analyze supply chain datasets. A novel geometric deep network is used to\npropose an ensemble deep network. The proposed Chebyshev ensemble geometric\nnetwork (Ch-EGN) is a hybrid convolutional and geometric deep learning. This\nnetwork is proposed to leverage the information dependencies in supply chain to\nderive invisible states of samples in the database. The functionality of the\nproposed deep network is assessed on the two different databases. The\nSupplyGraph Dataset and DataCo are considered in this research. The prediction\nof delivery status of DataCo supply chain is done for risk administration. The\nproduct classification and edge classification are performed using the\nSupplyGraph database to enhance the sustainability of the supply network. An\naverage accuracy of 98.95% is obtained for the ensemble network for risk\nmanagement. The average accuracy of 100% and 98.07% are obtained for\nsustainable supply chain in terms of 5 product group classification and 4\nproduct relation classification, respectively. The average accuracy of 92.37%\nis attained for 25 company relation classification. The results confirm an\naverage improvement and efficiency of the proposed method compared to the\nstate-of-the-art approaches.\n","authors":["Mehdi Khaleghi","Nastaran Khaleghi","Sobhan Sheykhivand","Sebelan Danishvar"],"pdf_url":"https://arxiv.org/pdf/2510.26203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10173v2","updated":"2025-10-30T07:25:20Z","published":"2025-06-11T20:53:45Z","title":"SPARKE: Scalable Prompt-Aware Diversity and Novelty Guidance in\n  Diffusion Models via RKE Score","summary":"  Diffusion models have demonstrated remarkable success in high-fidelity image\nsynthesis and prompt-guided generative modeling. However, ensuring adequate\ndiversity in generated samples of prompt-guided diffusion models remains a\nchallenge, particularly when the prompts span a broad semantic spectrum and the\ndiversity of generated data needs to be evaluated in a prompt-aware fashion\nacross semantically similar prompts. Recent methods have introduced guidance\nvia diversity measures to encourage more varied generations. In this work, we\nextend the diversity measure-based approaches by proposing the Scalable\nPrompt-Aware R\\'eny Kernel Entropy Diversity Guidance (SPARKE) method for\nprompt-aware diversity guidance. SPARKE utilizes conditional entropy for\ndiversity guidance, which dynamically conditions diversity measurement on\nsimilar prompts and enables prompt-aware diversity control. While the\nentropy-based guidance approach enhances prompt-aware diversity, its reliance\non the matrix-based entropy scores poses computational challenges in\nlarge-scale generation settings. To address this, we focus on the special case\nof Conditional latent RKE Score Guidance, reducing entropy computation and\ngradient-based optimization complexity from the $O(n^3)$ of general entropy\nmeasures to $O(n)$. The reduced computational complexity allows for\ndiversity-guided sampling over potentially thousands of generation rounds on\ndifferent prompts. We numerically test the SPARKE method on several\ntext-to-image diffusion models, demonstrating that the proposed method improves\nthe prompt-aware diversity of the generated data without incurring significant\ncomputational costs. We release our code on the project page:\nhttps://mjalali.github.io/SPARKE\n","authors":["Mohammad Jalali","Haoyu Lei","Amin Gohari","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2506.10173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14431v3","updated":"2025-10-30T07:17:56Z","published":"2025-10-16T08:31:44Z","title":"Real-Time Neural Video Compression with Unified Intra and Inter Coding","summary":"  Neural video compression (NVC) technologies have advanced rapidly in recent\nyears, yielding state-of-the-art schemes such as DCVC-RT that offer superior\ncompression efficiency to H.266/VVC and real-time encoding/decoding\ncapabilities. Nonetheless, existing NVC schemes have several limitations,\nincluding inefficiency in dealing with disocclusion and new content, interframe\nerror propagation and accumulation, among others. To eliminate these\nlimitations, we borrow the idea from classic video coding schemes, which allow\nintra coding within inter-coded frames. With the intra coding tool enabled,\ndisocclusion and new content are properly handled, and interframe error\npropagation is naturally intercepted without the need for manual refresh\nmechanisms. We present an NVC framework with unified intra and inter coding,\nwhere every frame is processed by a single model that is trained to perform\nintra/inter coding adaptively. Moreover, we propose a simultaneous two-frame\ncompression design to exploit interframe redundancy not only forwardly but also\nbackwardly. Experimental results show that our scheme outperforms DCVC-RT by an\naverage of 12.1% BD-rate reduction, delivers more stable bitrate and quality\nper frame, and retains real-time encoding/decoding performances. Code and\nmodels will be released.\n","authors":["Hui Xiang","Yifan Bian","Li Li","Jingran Wu","Xianguo Zhang","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.14431v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2510.26196v1","updated":"2025-10-30T07:13:46Z","published":"2025-10-30T07:13:46Z","title":"Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose\n  Prediction","summary":"  3D human pose estimation from sketches has broad applications in computer\nanimation and film production. Unlike traditional human pose estimation, this\ntask presents unique challenges due to the abstract and disproportionate nature\nof sketches. Previous sketch-to-pose methods, constrained by the lack of\nlarge-scale sketch-3D pose annotations, primarily relied on optimization with\nheuristic rules-an approach that is both time-consuming and limited in\ngeneralizability. To address these challenges, we propose a novel approach\nleveraging a \"learn from synthesis\" strategy. First, a diffusion model is\ntrained to synthesize sketch images from 2D poses projected from 3D human\nposes, mimicking disproportionate human structures in sketches. This process\nenables the creation of a synthetic dataset, SKEP-120K, consisting of 120k\naccurate sketch-3D pose annotation pairs across various sketch styles. Building\non this synthetic dataset, we introduce an end-to-end data-driven framework for\nestimating human poses and shapes from diverse sketch styles. Our framework\ncombines existing 2D pose detectors and generative diffusion priors for sketch\nfeature extraction with a feed-forward neural network for efficient 2D pose\nestimation. Multiple heuristic loss functions are incorporated to guarantee\ngeometric coherence between the derived 3D poses and the detected 2D poses\nwhile preserving accurate self-contacts. Qualitative, quantitative, and\nsubjective evaluations collectively show that our model substantially surpasses\nprevious ones in both estimation accuracy and speed for sketch-to-pose tasks.\n","authors":["Li Wang","Yiyu Zhuang","Yanwen Wang","Xun Cao","Chuan Guo","Xinxin Zuo","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.26196v1.pdf","comment":"SIGGRAPH Asia 2025"},{"id":"http://arxiv.org/abs/2510.23981v2","updated":"2025-10-30T07:09:32Z","published":"2025-10-28T01:24:24Z","title":"TeleEgo: Benchmarking Egocentric AI Assistants in the Wild","summary":"  Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.\n","authors":["Jiaqi Yan","Ruilong Ren","Jingren Liu","Shuning Xu","Ling Wang","Yiheng Wang","Yun Wang","Long Zhang","Xiangyu Chen","Changzhi Sun","Jixiang Luo","Dell Zhang","Hao Sun","Chi Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2510.23981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26186v1","updated":"2025-10-30T06:46:17Z","published":"2025-10-30T06:46:17Z","title":"ConceptScope: Characterizing Dataset Bias via Disentangled Visual\n  Concepts","summary":"  Dataset bias, where data points are skewed to certain concepts, is ubiquitous\nin machine learning datasets. Yet, systematically identifying these biases is\nchallenging without costly, fine-grained attribute annotations. We present\nConceptScope, a scalable and automated framework for analyzing visual datasets\nby discovering and quantifying human-interpretable concepts using Sparse\nAutoencoders trained on representations from vision foundation models.\nConceptScope categorizes concepts into target, context, and bias types based on\ntheir semantic relevance and statistical correlation to class labels, enabling\nclass-level dataset characterization, bias identification, and robustness\nevaluation through concept-based subgrouping. We validate that ConceptScope\ncaptures a wide range of visual concepts, including objects, textures,\nbackgrounds, facial attributes, emotions, and actions, through comparisons with\nannotated datasets. Furthermore, we show that concept activations produce\nspatial attributions that align with semantically meaningful image regions.\nConceptScope reliably detects known biases (e.g., background bias in\nWaterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects\nin ImageNet), offering a practical tool for dataset auditing and model\ndiagnostics.\n","authors":["Jinho Choi","Hyesu Lim","Steffen Schneider","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2510.26186v1.pdf","comment":"Published in the Thirty-Ninth Conference on Neural Information\n  Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2505.16239v2","updated":"2025-10-30T06:40:44Z","published":"2025-05-22T05:16:45Z","title":"DOVE: Efficient One-Step Diffusion Model for Real-World Video\n  Super-Resolution","summary":"  Diffusion models have demonstrated promising performance in real-world video\nsuper-resolution (VSR). However, the dozens of sampling steps they require,\nmake inference extremely slow. Sampling acceleration techniques, particularly\nsingle-step, provide a potential solution. Nonetheless, achieving one step in\nVSR remains challenging, due to the high training overhead on video data and\nstringent fidelity demands. To tackle the above issues, we propose DOVE, an\nefficient one-step diffusion model for real-world VSR. DOVE is obtained by\nfine-tuning a pretrained video diffusion model (i.e., CogVideoX). To\neffectively train DOVE, we introduce the latent-pixel training strategy. The\nstrategy employs a two-stage scheme to gradually adapt the model to the video\nsuper-resolution task. Meanwhile, we design a video processing pipeline to\nconstruct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning\non this dataset further enhances the restoration capability of DOVE. Extensive\nexperiments show that DOVE exhibits comparable or superior performance to\nmulti-step diffusion-based VSR methods. It also offers outstanding inference\nefficiency, achieving up to a 28$\\times$ speed-up over existing methods such as\nMGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.\n","authors":["Zheng Chen","Zichen Zou","Kewei Zhang","Xiongfei Su","Xin Yuan","Yong Guo","Yulun Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.16239v2.pdf","comment":"Accepted to NeurIPS 2025. Code is available at:\n  https://github.com/zhengchen1999/DOVE"},{"id":"http://arxiv.org/abs/2510.26173v1","updated":"2025-10-30T06:24:02Z","published":"2025-10-30T06:24:02Z","title":"MoTDiff: High-resolution Motion Trajectory estimation from a single\n  blurred image using Diffusion models","summary":"  Accurate estimation of motion information is crucial in diverse computational\nimaging and computer vision applications. Researchers have investigated various\nmethods to extract motion information from a single blurred image, including\nblur kernels and optical flow. However, existing motion representations are\noften of low quality, i.e., coarse-grained and inaccurate. In this paper, we\npropose the first high-resolution (HR) Motion Trajectory estimation framework\nusing Diffusion models (MoTDiff). Different from existing motion\nrepresentations, we aim to estimate an HR motion trajectory with high-quality\nfrom a single motion-blurred image. The proposed MoTDiff consists of two key\ncomponents: 1) a new conditional diffusion framework that uses multi-scale\nfeature maps extracted from a single blurred image as a condition, and 2) a new\ntraining method that can promote precise identification of a fine-grained\nmotion trajectory, consistent estimation of overall shape and position of a\nmotion path, and pixel connectivity along a motion trajectory. Our experiments\ndemonstrate that the proposed MoTDiff can outperform state-of-the-art methods\nin both blind image deblurring and coded exposure photography applications.\n","authors":["Wontae Choi","Jaelin Lee","Hyung Sup Yun","Byeungwoo Jeon","Il Yong Chun"],"pdf_url":"https://arxiv.org/pdf/2510.26173v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.13160v2","updated":"2025-10-30T06:21:47Z","published":"2025-03-17T13:31:19Z","title":"Language-guided Open-world Video Anomaly Detection under Weak\n  Supervision","summary":"  Video anomaly detection (VAD) aims to detect anomalies that deviate from what\nis expected. In open-world scenarios, the expected events may change as\nrequirements change. For example, not wearing a mask may be considered abnormal\nduring a flu outbreak but normal otherwise. However, existing methods assume\nthat the definition of anomalies is invariable, and thus are not applicable to\nthe open world. To address this, we propose a novel open-world VAD paradigm\nwith variable definitions, allowing guided detection through user-provided\nnatural language at inference time. This paradigm necessitates establishing a\nrobust mapping from video and textual definition to anomaly scores. Therefore,\nwe propose LaGoVAD (Language-guided Open-world Video Anomaly Detector), a model\nthat dynamically adapts anomaly definitions under weak supervision with two\nregularization strategies: diversifying the relative durations of anomalies via\ndynamic video synthesis, and enhancing feature robustness through contrastive\nlearning with negative mining. Training such adaptable models requires diverse\nanomaly definitions, but existing datasets typically provide labels without\nsemantic descriptions. To bridge this gap, we collect PreVAD (Pre-training\nVideo Anomaly Dataset), the largest and most diverse video anomaly dataset to\ndate, featuring 35,279 annotated videos with multi-level category labels and\ndescriptions that explicitly define anomalies. Zero-shot experiments on seven\ndatasets demonstrate LaGoVAD's SOTA performance. Our dataset and code will be\nreleased at https://github.com/Kamino666/LaGoVAD-PreVAD.\n","authors":["Zihao Liu","Xiaoyu Wu","Jianqin Wu","Xuxu Wang","Linlin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.13160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26170v1","updated":"2025-10-30T06:14:22Z","published":"2025-10-30T06:14:22Z","title":"Self-localization on a 3D map by fusing global and local features from a\n  monocular camera","summary":"  Self-localization on a 3D map by using an inexpensive monocular camera is\nrequired to realize autonomous driving. Self-localization based on a camera\noften uses a convolutional neural network (CNN) that can extract local features\nthat are calculated by nearby pixels. However, when dynamic obstacles, such as\npeople, are present, CNN does not work well. This study proposes a new method\ncombining CNN with Vision Transformer, which excels at extracting global\nfeatures that show the relationship of patches on whole image. Experimental\nresults showed that, compared to the state-of-the-art method (SOTA), the\naccuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times\nhigher than that without dynamic obstacles. Moreover, the self-localization\nerror of our method is 20.1% smaller than that of SOTA on public datasets.\nAdditionally, our robot using our method can localize itself with 7.51cm error\non average, which is more accurate than SOTA.\n","authors":["Satoshi Kikuch","Masaya Kato","Tsuyoshi Tasaki"],"pdf_url":"https://arxiv.org/pdf/2510.26170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26160v1","updated":"2025-10-30T05:50:48Z","published":"2025-10-30T05:50:48Z","title":"CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark","summary":"  Wearable devices such as smart glasses are transforming the way people\ninteract with their surroundings, enabling users to seek information regarding\nentities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)\nplays a key role in supporting such questions, yet there is still no\ncomprehensive benchmark for this task, especially regarding wearables\nscenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG\nbenchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse\nset of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn\nconversations across 13 domains, including 6.2K egocentric images designed to\nmimic captures from wearable devices. We carefully constructed the questions to\nreflect real-world scenarios and challenges, including five types of\nimage-quality issues, six question types, varying entity popularity, differing\ninformation dynamism, and different conversation turns. We design three tasks:\nsingle-source augmentation, multi-source augmentation, and multi-turn\nconversations -- each paired with an associated retrieval corpus and APIs for\nboth image-KG retrieval and webpage retrieval. Our evaluation shows that\nstraightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM\nsingle- and multi-turn QA, respectively, whereas state-of-the-art industry\nsolutions have similar quality (32%/45%), underscoring ample room for\nimprovement. The benchmark has hosted KDD Cup 2025, attracting about 1K\nparticipants and 5K submissions, with winning solutions improving baseline\nperformance by 28%, highlighting its early impact on advancing the field.\n","authors":["Jiaqi Wang","Xiao Yang","Kai Sun","Parth Suresh","Sanat Sharma","Adam Czyzewski","Derek Andersen","Surya Appini","Arkav Banerjee","Sajal Choudhary","Shervin Ghasemlou","Ziqiang Guan","Akil Iyer","Haidar Khan","Lingkun Kong","Roy Luo","Tiffany Ma","Zhen Qiao","David Tran","Wenfang Xu","Skyler Yeatman","Chen Zhou","Gunveer Gujral","Yinglong Xia","Shane Moon","Nicolas Scheffer","Nirav Shah","Eun Chang","Yue Liu","Florian Metze","Tammy Stark","Zhaleh Feizollahi","Andrea Jessee","Mangesh Pujari","Ahmed Aly","Babak Damavandi","Rakesh Wanga","Anuj Kumar","Rohit Patel","Wen-tau Yih","Xin Luna Dong"],"pdf_url":"https://arxiv.org/pdf/2510.26160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26154v1","updated":"2025-10-30T05:20:36Z","published":"2025-10-30T05:20:36Z","title":"Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A\n  Case Study on Bangladesh","summary":"  Modes of transportation vary across countries depending on geographical\nlocation and cultural context. In South Asian countries rickshaws are among the\nmost common means of local transport. Based on their mode of operation,\nrickshaws in cities across Bangladesh can be broadly classified into non-auto\n(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of\nauto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from\naccessing certain routes. However, existing surveillance systems make it quite\ndifficult to monitor them due to their similarity to other vehicles, especially\nnon-auto rickshaws whereas manual video analysis is too time-consuming. This\npaper presents a machine learning-based approach to automatically detect\nauto-rickshaws in traffic images. In this system, we used real-time object\ndetection using the YOLOv8 model. For training purposes, we prepared a set of\n1,730 annotated images that were captured under various traffic conditions. The\nresults show that our proposed model performs well in real-time auto-rickshaw\ndetection and offers an mAP50 of 83.447% and binary precision and recall values\nabove 78%, demonstrating its effectiveness in handling both dense and sparse\ntraffic scenarios. The dataset has been publicly released for further research.\n","authors":["Sudipto Das Sukanto","Diponker Roy","Fahim Shakil","Nirjhar Singha","Abdullah Asik","Aniket Joarder","Mridha Md Nafis Fuad","Muhammad Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2510.26154v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2510.21271v2","updated":"2025-10-30T05:16:33Z","published":"2025-10-24T09:12:59Z","title":"Buffer layers for Test-Time Adaptation","summary":"  In recent advancements in Test Time Adaptation (TTA), most existing\nmethodologies focus on updating normalization layers to adapt to the test\ndomain. However, the reliance on normalization-based adaptation presents key\nchallenges. First, normalization layers such as Batch Normalization (BN) are\nhighly sensitive to small batch sizes, leading to unstable and inaccurate\nstatistics. Moreover, normalization-based adaptation is inherently constrained\nby the structure of the pre-trained model, as it relies on training-time\nstatistics that may not generalize well to unseen domains. These issues limit\nthe effectiveness of normalization-based TTA approaches, especially under\nsignificant domain shift. In this paper, we introduce a novel paradigm based on\nthe concept of a Buffer layer, which addresses the fundamental limitations of\nnormalization layer updates. Unlike existing methods that modify the core\nparameters of the model, our approach preserves the integrity of the\npre-trained backbone, inherently mitigating the risk of catastrophic forgetting\nduring online adaptation. Through comprehensive experimentation, we demonstrate\nthat our approach not only outperforms traditional methods in mitigating domain\nshift and enhancing model robustness, but also exhibits strong resilience to\nforgetting. Furthermore, our Buffer layer is modular and can be seamlessly\nintegrated into nearly all existing TTA frameworks, resulting in consistent\nperformance improvements across various architectures. These findings validate\nthe effectiveness and versatility of the proposed solution in real-world domain\nadaptation scenarios. The code is available at\nhttps://github.com/hyeongyu-kim/Buffer_TTA.\n","authors":["Hyeongyu Kim","Geonhui Han","Dosik Hwang"],"pdf_url":"https://arxiv.org/pdf/2510.21271v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26151v1","updated":"2025-10-30T05:12:29Z","published":"2025-10-30T05:12:29Z","title":"MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer\n  Diagnosis and Risk Prediction","summary":"  Large annotated datasets are essential for training robust Computer-Aided\nDiagnosis (CAD) models for breast cancer detection or risk prediction. However,\nacquiring such datasets with fine-detailed annotation is both costly and\ntime-consuming. Vision-Language Models (VLMs), such as CLIP, which are\npre-trained on large image-text pairs, offer a promising solution by enhancing\nrobustness and data efficiency in medical imaging tasks. This paper introduces\na novel Multi-View Mammography and Language Model for breast cancer\nclassification and risk prediction, trained on a dataset of paired mammogram\nimages and synthetic radiology reports. Our MV-MLM leverages multi-view\nsupervision to learn rich representations from extensive radiology data by\nemploying cross-modal self-supervision across image-text pairs. This includes\nmultiple views and the corresponding pseudo-radiology reports. We propose a\nnovel joint visual-textual learning strategy to enhance generalization and\naccuracy performance over different data types and tasks to distinguish breast\ntissues or cancer characteristics(calcification, mass) and utilize these\npatterns to understand mammography images and predict cancer risk. We evaluated\nour method on both private and publicly available datasets, demonstrating that\nthe proposed model achieves state-of-the-art performance in three\nclassification tasks: (1) malignancy classification, (2) subtype\nclassification, and (3) image-based cancer risk prediction. Furthermore, the\nmodel exhibits strong data efficiency, outperforming existing fully supervised\nor VLM baselines while trained on synthetic text reports and without the need\nfor actual radiology reports.\n","authors":["Shunjie-Fabian Zheng","Hyeonjun Lee","Thijs Kooi","Ali Diba"],"pdf_url":"https://arxiv.org/pdf/2510.26151v1.pdf","comment":"Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)\n  Workshop at ICCV 2025"},{"id":"http://arxiv.org/abs/2510.26149v1","updated":"2025-10-30T05:08:45Z","published":"2025-10-30T05:08:45Z","title":"BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and\n  Enhanced Motion Compensation","summary":"  Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution\nof video frames, potentially at various scaling factors, which presents several\nchallenges regarding spatial detail reproduction, temporal consistency, and\ncomputational complexity. In this paper, we propose a strong baseline BasicAVSR\nfor AVSR by integrating four key components: 1) adaptive multi-scale frequency\npriors generated from image Laplacian pyramids, 2) a flow-guided propagation\nunit to aggregate spatiotemporal information from adjacent frames, 3) a\nsecond-order motion compensation unit for more accurate spatial alignment of\nadjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and\ncontent-independent upsampling kernels. To meet diverse application demands, we\ninstantiate three propagation variants: (i) a unidirectional RNN unit for\nstrictly online inference, (ii) a unidirectional RNN unit empowered with a\nlimited lookahead that tolerates a small output delay, and (iii) a\nbidirectional RNN unit designed for offline tasks where computational resources\nare less constrained. Experimental results demonstrate the effectiveness and\nadaptability of our model across these different scenarios. Through extensive\nexperiments, we show that BasicAVSR significantly outperforms existing methods\nin terms of super-resolution quality, generalization ability, and inference\nspeed. Our work not only advances the state-of-the-art in AVSR but also extends\nits core components to multiple frameworks for diverse scenarios. The code is\navailable at https://github.com/shangwei5/BasicAVSR.\n","authors":["Wei Shang","Wanying Zhang","Shuhang Gu","Pengfei Zhu","Qinghua Hu","Dongwei Ren"],"pdf_url":"https://arxiv.org/pdf/2510.26149v1.pdf","comment":"13 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2506.00871v2","updated":"2025-10-30T05:04:19Z","published":"2025-06-01T07:18:47Z","title":"Towards Predicting Any Human Trajectory In Context","summary":"  Predicting accurate future trajectories of pedestrians is essential for\nautonomous systems but remains a challenging task due to the need for\nadaptability in different environments and domains. A common approach involves\ncollecting scenario-specific data and performing fine-tuning via\nbackpropagation. However, the need to fine-tune for each new scenario is often\nimpractical for deployment on edge devices. To address this challenge, we\nintroduce \\paper, an In-Context Learning (ICL) framework for pedestrian\ntrajectory prediction that enables adaptation without fine-tuning on the\nscenario-specific data at inference time without requiring weight updates. We\npropose a spatio-temporal similarity-based example selection (STES) method that\nselects relevant examples from previously observed trajectories within the same\nscene by identifying similar motion patterns at corresponding locations. To\nfurther refine this selection, we introduce prediction-guided example selection\n(PG-ES), which selects examples based on both the past trajectory and the\npredicted future trajectory, rather than relying solely on the past trajectory.\nThis approach allows the model to account for long-term dynamics when selecting\nexamples. Finally, instead of relying on small real-world datasets with limited\nscenario diversity, we train our model on a large-scale synthetic dataset to\nenhance its prediction ability by leveraging in-context examples. Extensive\nexperiments demonstrate that TrajICL achieves remarkable adaptation across both\nin-domain and cross-domain scenarios, outperforming even fine-tuned approaches\nacross multiple public benchmarks. Project Page:\nhttps://fujiry0.github.io/TrajICL-project-page/.\n","authors":["Ryo Fujii","Hideo Saito","Ryo Hachiuma"],"pdf_url":"https://arxiv.org/pdf/2506.00871v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.16396v3","updated":"2025-10-30T04:59:32Z","published":"2025-10-18T08:19:49Z","title":"SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation","summary":"  With the increasing ubiquity of AR/VR devices, the deployment of deep\nlearning models on edge devices has become a critical challenge. These devices\nrequire real-time inference, low power consumption, and minimal latency. Many\nframework designers face the conundrum of balancing efficiency and performance.\nWe design a light framework that adopts an encoder-decoder architecture and\nintroduces several key contributions aimed at improving both efficiency and\naccuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the\ninherent sparsity in hand pose images, achieving a 42% end-to-end efficiency\nimprovement. Moreover, we propose our SPLite decoder. This new architecture\nsignificantly boosts the decoding process's frame rate by 3.1x on the Raspberry\nPi 5, while maintaining accuracy on par. To further optimize performance, we\napply quantization-aware training, reducing memory usage while preserving\naccuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on\nFreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5\nCPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on\ncompound benchmark datasets, demonstrating comparable accuracy to\nstate-of-the-art approaches while significantly enhancing computational\nefficiency.\n","authors":["Yeh Keng Hao","Hsu Tzu Wei","Sun Min"],"pdf_url":"https://arxiv.org/pdf/2510.16396v3.pdf","comment":"Accepted to AICCC 2025"},{"id":"http://arxiv.org/abs/2510.26141v1","updated":"2025-10-30T04:52:12Z","published":"2025-10-30T04:52:12Z","title":"StructLayoutFormer:Conditional Structured Layout Generation via\n  Structure Serialization and Disentanglement","summary":"  Structured layouts are preferable in many 2D visual contents (\\eg, GUIs,\nwebpages) since the structural information allows convenient layout editing.\nComputational frameworks can help create structured layouts but require heavy\nlabor input. Existing data-driven approaches are effective in automatically\ngenerating fixed layouts but fail to produce layout structures. We present\nStructLayoutFormer, a novel Transformer-based approach for conditional\nstructured layout generation. We use a structure serialization scheme to\nrepresent structured layouts as sequences. To better control the structures of\ngenerated layouts, we disentangle the structural information from the element\nplacements. Our approach is the first data-driven approach that achieves\nconditional structured layout generation and produces realistic layout\nstructures explicitly. We compare our approach with existing data-driven layout\ngeneration approaches by including post-processing for structure extraction.\nExtensive experiments have shown that our approach exceeds these baselines in\nconditional structured layout generation. We also demonstrate that our approach\nis effective in extracting and transferring layout structures. The code is\npublicly available at %\\href{https://github.com/Teagrus/StructLayoutFormer}\n{https://github.com/Teagrus/StructLayoutFormer}.\n","authors":["Xin Hu","Pengfei Xu","Jin Zhou","Hongbo Fu","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26140v1","updated":"2025-10-30T04:51:05Z","published":"2025-10-30T04:51:05Z","title":"FullPart: Generating each 3D Part at Full Resolution","summary":"  Part-based 3D generation holds great potential for various applications.\nPrevious part generators that represent parts using implicit vector-set tokens\noften suffer from insufficient geometric details. Another line of work adopts\nan explicit voxel representation but shares a global voxel grid among all\nparts; this often causes small parts to occupy too few voxels, leading to\ndegraded quality. In this paper, we propose FullPart, a novel framework that\ncombines both implicit and explicit paradigms. It first derives the bounding\nbox layout through an implicit box vector-set diffusion process, a task that\nimplicit diffusion handles effectively since box tokens contain little\ngeometric detail. Then, it generates detailed parts, each within its own fixed\nfull-resolution voxel grid. Instead of sharing a global low-resolution space,\neach part in our method - even small ones - is generated at full resolution,\nenabling the synthesis of intricate details. We further introduce a\ncenter-point encoding strategy to address the misalignment issue when\nexchanging information between parts of different actual sizes, thereby\nmaintaining global coherence. Moreover, to tackle the scarcity of reliable part\ndata, we present PartVerse-XL, the largest human-annotated 3D part dataset to\ndate with 40K objects and 320K parts. Extensive experiments demonstrate that\nFullPart achieves state-of-the-art results in 3D part generation. We will\nrelease all code, data, and model to benefit future research in 3D part\ngeneration.\n","authors":["Lihe Ding","Shaocong Dong","Yaokun Li","Chenjian Gao","Xiao Chen","Rui Han","Yihao Kuang","Hong Zhang","Bo Huang","Zhanpeng Huang","Zibin Wang","Dan Xu","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2510.26140v1.pdf","comment":"Project page: https://fullpart3d.github.io"},{"id":"http://arxiv.org/abs/2510.26131v1","updated":"2025-10-30T04:31:56Z","published":"2025-10-30T04:31:56Z","title":"Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM","summary":"  Attention models have recently emerged as a powerful approach, demonstrating\nsignificant progress in various fields. Visualization techniques, such as class\nactivation mapping, provide visual insights into the reasoning of convolutional\nneural networks (CNNs). Using network gradients, it is possible to identify\nregions where the network pays attention during image recognition tasks.\nFurthermore, these gradients can be combined with CNN features to localize more\ngeneralizable, task-specific attentive (salient) regions within scenes.\nHowever, explicit use of this gradient-based attention information integrated\ndirectly into CNN representations for semantic object understanding remains\nlimited. Such integration is particularly beneficial for visual tasks like\nsimultaneous localization and mapping (SLAM), where CNN representations\nenriched with spatially attentive object locations can enhance performance. In\nthis work, we propose utilizing task-specific network attention for RGB-D\nindoor SLAM. Specifically, we integrate layer-wise attention information\nderived from network gradients with CNN feature representations to improve\nframe association performance. Experimental results indicate improved\nperformance compared to baseline methods, particularly for large environments.\n","authors":["Ali Caglayan","Nevrez Imamoglu","Oguzhan Guclu","Ali Osman Serhatoglu","Ahmet Burak Can","Ryosuke Nakamura"],"pdf_url":"https://arxiv.org/pdf/2510.26131v1.pdf","comment":"double-column 5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2510.26125v1","updated":"2025-10-30T04:25:33Z","published":"2025-10-30T04:25:33Z","title":"WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging\n  Long-tail Scenarios","summary":"  Vision-based end-to-end (E2E) driving has garnered significant interest in\nthe research community due to its scalability and synergy with multimodal large\nlanguage models (MLLMs). However, current E2E driving benchmarks primarily\nfeature nominal scenarios, failing to adequately test the true potential of\nthese systems. Furthermore, existing open-loop evaluation metrics often fall\nshort in capturing the multi-modal nature of driving or effectively evaluating\nperformance in long-tail scenarios. To address these gaps, we introduce the\nWaymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021\ndriving segments (approximately 12 hours), specifically curated for challenging\nlong-tail scenarios that that are rare in daily life with an occurring\nfrequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the\nhigh-level routing information, ego states, and 360-degree camera views from 8\nsurrounding cameras. To evaluate the E2E driving performance on these long-tail\nsituations, we propose a novel open-loop evaluation metric: Rater Feedback\nScore (RFS). Unlike conventional metrics that measure the distance between\npredicted way points and the logs, RFS measures how closely the predicted\ntrajectory matches rater-annotated trajectory preference labels. We have\nreleased rater preference labels for all WOD-E2E validation set segments, while\nthe held out test set labels have been used for the 2025 WOD-E2E Challenge.\nThrough our work, we aim to foster state of the art research into\ngeneralizable, robust, and safe end-to-end autonomous driving agents capable of\nhandling complex real-world situations.\n","authors":["Runsheng Xu","Hubert Lin","Wonseok Jeon","Hao Feng","Yuliang Zou","Liting Sun","John Gorman","Kate Tolstaya","Sarah Tang","Brandyn White","Ben Sapp","Mingxing Tan","Jyh-Jing Hwang","Drago Anguelov"],"pdf_url":"https://arxiv.org/pdf/2510.26125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08772v2","updated":"2025-10-30T04:25:25Z","published":"2025-07-11T17:33:18Z","title":"From One to More: Contextual Part Latents for 3D Generation","summary":"  Recent advances in 3D generation have transitioned from multi-view 2D\nrendering approaches to 3D-native latent diffusion frameworks that exploit\ngeometric priors in ground truth data. Despite progress, three key limitations\npersist: (1) Single-latent representations fail to capture complex multi-part\ngeometries, causing detail degradation; (2) Holistic latent coding neglects\npart independence and interrelationships critical for compositional design; (3)\nGlobal conditioning mechanisms lack fine-grained controllability. Inspired by\nhuman 3D design workflows, we propose CoPart - a part-aware diffusion framework\nthat decomposes 3D objects into contextual part latents for coherent multi-part\ngeneration. This paradigm offers three advantages: i) Reduces encoding\ncomplexity through part decomposition; ii) Enables explicit part relationship\nmodeling; iii) Supports part-level conditioning. We further develop a mutual\nguidance strategy to fine-tune pre-trained diffusion models for joint part\nlatent denoising, ensuring both geometric coherence and foundation model\npriors. To enable large-scale training, we construct Partverse - a novel 3D\npart dataset derived from Objaverse through automated mesh segmentation and\nhuman-verified annotations. Extensive experiments demonstrate CoPart's superior\ncapabilities in part-level editing, articulated object generation, and scene\ncomposition with unprecedented controllability.\n","authors":["Shaocong Dong","Lihe Ding","Xiao Chen","Yaokun Li","Yuxin Wang","Yucheng Wang","Qi Wang","Jaehyeok Kim","Chenjian Gao","Zhanpeng Huang","Zibin Wang","Tianfan Xue","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2507.08772v2.pdf","comment":"Project page: https://copart3d.github.io/"},{"id":"http://arxiv.org/abs/2510.26117v1","updated":"2025-10-30T04:00:07Z","published":"2025-10-30T04:00:07Z","title":"JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting","summary":"  Traditional novel view synthesis methods heavily rely on external camera pose\nestimation tools such as COLMAP, which often introduce computational\nbottlenecks and propagate errors. To address these challenges, we propose a\nunified framework that jointly optimizes 3D Gaussian points and camera poses\nwithout requiring pre-calibrated inputs. Our approach iteratively refines 3D\nGaussian parameters and updates camera poses through a novel co-optimization\nstrategy, ensuring simultaneous improvements in scene reconstruction fidelity\nand pose accuracy. The key innovation lies in decoupling the joint optimization\ninto two interleaved phases: first, updating 3D Gaussian parameters via\ndifferentiable rendering with fixed poses, and second, refining camera poses\nusing a customized 3D optical flow algorithm that incorporates geometric and\nphotometric constraints. This formulation progressively reduces projection\nerrors, particularly in challenging scenarios with large viewpoint variations\nand sparse feature distributions, where traditional methods struggle. Extensive\nevaluations on multiple datasets demonstrate that our approach significantly\noutperforms existing COLMAP-free techniques in reconstruction quality, and also\nsurpasses the standard COLMAP-based baseline in general.\n","authors":["Yuxuan Li","Tao Wang","Xianben Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26114v1","updated":"2025-10-30T03:54:53Z","published":"2025-10-30T03:54:53Z","title":"OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script\n  Research","summary":"  As one of the earliest writing systems, Oracle Bone Script (OBS) preserves\nthe cultural and intellectual heritage of ancient civilizations. However,\ncurrent OBS research faces two major challenges: (1) the interpretation of OBS\ninvolves a complex workflow comprising multiple serial and parallel sub-tasks,\nand (2) the efficiency of OBS information organization and retrieval remains a\ncritical bottleneck, as scholars often spend substantial effort searching for,\ncompiling, and managing relevant resources. To address these challenges, we\npresent OracleAgent, the first agent system designed for the structured\nmanagement and retrieval of OBS-related information. OracleAgent seamlessly\nintegrates multiple OBS analysis tools, empowered by large language models\n(LLMs), and can flexibly orchestrate these components. Additionally, we\nconstruct a comprehensive domain-specific multimodal knowledge base for OBS,\nwhich is built through a rigorous multi-year process of data collection,\ncleaning, and expert annotation. The knowledge base comprises over 1.4M\nsingle-character rubbing images and 80K interpretation texts. OracleAgent\nleverages this resource through its multimodal tools to assist experts in\nretrieval tasks of character, document, interpretation text, and rubbing image.\nExtensive experiments demonstrate that OracleAgent achieves superior\nperformance across a range of multimodal reasoning and generation tasks,\nsurpassing leading mainstream multimodal large language models (MLLMs) (e.g.,\nGPT-4o). Furthermore, our case study illustrates that OracleAgent can\neffectively assist domain experts, significantly reducing the time cost of OBS\nresearch. These results highlight OracleAgent as a significant step toward the\npractical deployment of OBS-assisted research and automated interpretation\nsystems.\n","authors":["Caoshuo Li","Zengmao Ding","Xiaobin Hu","Bang Li","Donghao Luo","Xu Peng","Taisong Jin","Yongge Liu","Shengwei Han","Jing Yang","Xiaoping He","Feng Gao","AndyPian Wu"," SevenShu","Chaoyang Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26113v1","updated":"2025-10-30T03:53:22Z","published":"2025-10-30T03:53:22Z","title":"EgoExo-Con: Exploring View-Invariant Video Temporal Understanding","summary":"  Can Video-LLMs achieve consistent temporal understanding when videos capture\nthe same event from different viewpoints? To study this, we introduce\nEgoExo-Con (Consistency), a benchmark of comprehensively synchronized\negocentric and exocentric video pairs with human-refined queries in natural\nlanguage. EgoExo-Con emphasizes two temporal understanding tasks: Temporal\nVerification and Temporal Grounding. It evaluates not only correctness but\nconsistency across viewpoints. Our analysis reveals two critical limitations of\nexisting Video-LLMs: (1) models often fail to maintain consistency, with\nresults far worse than their single-view performances. (2) When naively\nfinetuned with synchronized videos of both viewpoints, the models show improved\nconsistency but often underperform those trained on a single view. For\nimprovements, we propose View-GRPO, a novel reinforcement learning framework\nthat effectively strengthens view-specific temporal reasoning while encouraging\nconsistent comprehension across viewpoints. Our method demonstrates its\nsuperiority over naive SFT and GRPO, especially for improving cross-view\nconsistency. All resources will be made publicly available.\n","authors":["Minjoon Jung","Junbin Xiao","Junghyun Kim","Byoung-Tak Zhang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2510.26113v1.pdf","comment":"project page:\n  \\url{https://minjoong507.github.io/projects/EgoExo-Con/}"},{"id":"http://arxiv.org/abs/2510.26105v1","updated":"2025-10-30T03:31:20Z","published":"2025-10-30T03:31:20Z","title":"Security Risk of Misalignment between Text and Image in Multi-modal\n  Model","summary":"  Despite the notable advancements and versatility of multi-modal diffusion\nmodels, such as text-to-image models, their susceptibility to adversarial\ninputs remains underexplored. Contrary to expectations, our investigations\nreveal that the alignment between textual and Image modalities in existing\ndiffusion models is inadequate. This misalignment presents significant risks,\nespecially in the generation of inappropriate or Not-Safe-For-Work (NSFW)\ncontent. To this end, we propose a novel attack called Prompt-Restricted\nMulti-modal Attack (PReMA) to manipulate the generated content by modifying the\ninput image in conjunction with any specified prompt, without altering the\nprompt itself. PReMA is the first attack that manipulates model outputs by\nsolely creating adversarial images, distinguishing itself from prior methods\nthat primarily generate adversarial prompts to produce NSFW content.\nConsequently, PReMA poses a novel threat to the integrity of multi-modal\ndiffusion models, particularly in image-editing applications that operate with\nfixed prompts. Comprehensive evaluations conducted on image inpainting and\nstyle transfer tasks across various models confirm the potent efficacy of\nPReMA.\n","authors":["Xiaosen Wang","Zhijin Ge","Shaokang Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.24267v2","updated":"2025-10-30T03:29:32Z","published":"2025-09-29T04:24:13Z","title":"Cycle Diffusion Model for Counterfactual Image Generation","summary":"  Deep generative models have demonstrated remarkable success in medical image\nsynthesis. However, ensuring conditioning faithfulness and high-quality\nsynthetic images for direct or counterfactual generation remains a challenge.\nIn this work, we introduce a cycle training framework to fine-tune diffusion\nmodels for improved conditioning adherence and enhanced synthetic image\nrealism. Our approach, Cycle Diffusion Model (CDM), enforces consistency\nbetween generated and original images by incorporating cycle constraints,\nenabling more reliable direct and counterfactual generation. Experiments on a\ncombined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and\nPPMI) show that our method improves conditioning accuracy and enhances image\nquality as measured by FID and SSIM. The results suggest that the cycle\nstrategy used in CDM can be an effective method for refining diffusion-based\nmedical image generation, with applications in data augmentation,\ncounterfactual, and disease progression modeling.\n","authors":["Fangrui Huang","Alan Wang","Binxu Li","Bailey Trang","Ridvan Yesiloglu","Tianyu Hua","Wei Peng","Ehsan Adeli"],"pdf_url":"https://arxiv.org/pdf/2509.24267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00254v4","updated":"2025-10-30T03:12:42Z","published":"2025-05-01T02:40:23Z","title":"Empowering Agentic Video Analytics Systems with Video Language Models","summary":"  AI-driven video analytics has become increasingly important across diverse\ndomains. However, existing systems are often constrained to specific,\npredefined tasks, limiting their adaptability in open-ended analytical\nscenarios. The recent emergence of Vision Language Models (VLMs) as\ntransformative technologies offers significant potential for enabling\nopen-ended video understanding, reasoning, and analytics. Nevertheless, their\nlimited context windows present challenges when processing ultra-long video\ncontent, which is prevalent in real-world applications. To address this, we\nintroduce AVA, a VLM-powered system designed for open-ended, advanced video\nanalytics. AVA incorporates two key innovations: (1) the near real-time\nconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long or\ncontinuous video streams, and (2) an agentic retrieval-generation mechanism\nthat leverages EKGs to handle complex and diverse queries. Comprehensive\nevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that\nAVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,\nrespectively-significantly surpassing existing VLM and video\nRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video\nanalytics in ultra-long and open-world video scenarios, we introduce a new\nbenchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours\nin duration, along with 120 manually annotated, diverse, and complex\nquestion-answer pairs. On AVA-100, AVA achieves top-tier performance with an\naccuracy of 75.8%. The source code of AVA is available at\nhttps://github.com/I-ESC/Project-Ava. The AVA-100 benchmark can be accessed at\nhttps://huggingface.co/datasets/iesc/Ava-100.\n","authors":["Yuxuan Yan","Shiqi Jiang","Ting Cao","Yifan Yang","Qianqian Yang","Yuanchao Shu","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.00254v4.pdf","comment":"Accepted to NDSI 2026, 19pages, 12 figures, complementary evaluations\n  and appendix"},{"id":"http://arxiv.org/abs/2505.18584v2","updated":"2025-10-30T02:59:44Z","published":"2025-05-24T08:20:36Z","title":"Unleashing Diffusion Transformers for Visual Correspondence by\n  Modulating Massive Activations","summary":"  Pre-trained stable diffusion models (SD) have shown great advances in visual\ncorrespondence. In this paper, we investigate the capabilities of Diffusion\nTransformers (DiTs) for accurate dense correspondence. Distinct from SD, DiTs\nexhibit a critical phenomenon in which very few feature activations exhibit\nsignificantly larger values than others, known as \\textit{massive activations},\nleading to uninformative representations and significant performance\ndegradation for DiTs. The massive activations consistently concentrate at very\nfew fixed dimensions across all image patch tokens, holding little local\ninformation. We trace these dimension-concentrated massive activations and find\nthat such concentration can be effectively localized by the zero-initialized\nAdaptive Layer Norm (AdaLN-zero). Building on these findings, we propose\nDiffusion Transformer Feature (DiTF), a training-free framework designed to\nextract semantic-discriminative features from DiTs. Specifically, DiTF employs\nAdaLN to adaptively localize and normalize massive activations with\nchannel-wise modulation. In addition, we develop a channel discard strategy to\nfurther eliminate the negative impacts from massive activations. Experimental\nresults demonstrate that our DiTF outperforms both DINO and SD-based models and\nestablishes a new state-of-the-art performance for DiTs in different visual\ncorrespondence tasks (\\eg, with +9.4\\% on Spair-71k and +4.4\\% on AP-10K-C.S.).\n","authors":["Chaofan Gan","Yuanpeng Tu","Xi Chen","Tieyuan Chen","Yuxi Li","Mehrtash Harandi","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2505.18584v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25327v2","updated":"2025-10-30T02:51:38Z","published":"2025-10-29T09:41:03Z","title":"MMEdge: Accelerating On-device Multimodal Inference via Pipelined\n  Sensing and Encoding","summary":"  Real-time multimodal inference on resource-constrained edge devices is\nessential for applications such as autonomous driving, human-computer\ninteraction, and mobile health. However, prior work often overlooks the tight\ncoupling between sensing dynamics and model execution, as well as the complex\ninter-modality dependencies. In this paper, we propose MMEdge, an new on-device\nmulti-modal inference framework based on pipelined sensing and encoding.\nInstead of waiting for complete sensor inputs, MMEdge decomposes the entire\ninference process into a sequence of fine-grained sensing and encoding units,\nallowing computation to proceed incrementally as data arrive. MMEdge also\nintroduces a lightweight but effective temporal aggregation module that\ncaptures rich temporal dynamics across different pipelined units to maintain\naccuracy performance. Such pipelined design also opens up opportunities for\nfine-grained cross-modal optimization and early decision-making during\ninference. To further enhance system performance under resource variability and\ninput data complexity, MMEdge incorporates an adaptive multimodal configuration\noptimizer that dynamically selects optimal sensing and model configurations for\neach modality under latency constraints, and a cross-modal speculative skipping\nmechanism that bypasses future units of slower modalities when early\npredictions reach sufficient confidence. We evaluate MMEdge using two public\nmultimodal datasets and deploy it on a real-world unmanned aerial vehicle\n(UAV)-based multimodal testbed. The results show that MMEdge significantly\nreduces end-to-end latency while maintaining high task accuracy across various\nsystem and data dynamics.\n","authors":["Runxi Huang","Mingxuan Yu","Mingyu Tsoi","Xiaomin Ouyang"],"pdf_url":"https://arxiv.org/pdf/2510.25327v2.pdf","comment":"Code available at: https://github.com/HKUST-MINSys-Lab/MMEdge.\n  Accepted by SenSys 2026"},{"id":"http://arxiv.org/abs/2506.21046v2","updated":"2025-10-30T02:36:15Z","published":"2025-06-26T06:47:51Z","title":"Boosting Generative Adversarial Transferability with Self-supervised\n  Vision Transformer Features","summary":"  The ability of deep neural networks (DNNs) come from extracting and\ninterpreting features from the data provided. By exploiting intermediate\nfeatures in DNNs instead of relying on hard labels, we craft adversarial\nperturbation that generalize more effectively, boosting black-box\ntransferability. These features ubiquitously come from supervised learning in\nprevious work. Inspired by the exceptional synergy between self-supervised\nlearning and the Transformer architecture, this paper explores whether\nexploiting self-supervised Vision Transformer (ViT) representations can improve\nadversarial transferability. We present dSVA -- a generative dual\nself-supervised ViT features attack, that exploits both global structural\nfeatures from contrastive learning (CL) and local textural features from masked\nimage modeling (MIM), the self-supervised learning paradigm duo for ViTs. We\ndesign a novel generative training framework that incorporates a generator to\ncreate black-box adversarial examples, and strategies to train the generator by\nexploiting joint features and the attention mechanism of self-supervised ViTs.\nOur findings show that CL and MIM enable ViTs to attend to distinct feature\ntendencies, which, when exploited in tandem, boast great adversarial\ngeneralizability. By disrupting dual deep features distilled by self-supervised\nViTs, we are rewarded with remarkable black-box transferability to models of\nvarious architectures that outperform state-of-the-arts. Code available at\nhttps://github.com/spencerwooo/dSVA.\n","authors":["Shangbo Wu","Yu-an Tan","Ruinan Ma","Wencong Ma","Dehua Zhu","Yuanzhang Li"],"pdf_url":"https://arxiv.org/pdf/2506.21046v2.pdf","comment":"14 pages, 9 figures, accepted at ICCV 2025"},{"id":"http://arxiv.org/abs/2408.01701v6","updated":"2025-10-30T02:28:16Z","published":"2024-08-03T07:47:16Z","title":"Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action\n  Recognition via Learning Temporal-Frequency Dynamics","summary":"  For multimodal skeleton-based action recognition, Graph Convolutional\nNetworks (GCNs) are effective models. Still, their reliance on floating-point\ncomputations leads to high energy consumption, limiting their applicability in\nbattery-powered devices. While energy-efficient, Spiking Neural Networks (SNNs)\nstruggle to model skeleton dynamics, leading to suboptimal solutions. We\npropose Signal-SGN (Spiking Graph Convolutional Network), which utilizes the\ntemporal dimension of skeleton sequences as the spike time steps and represents\nfeatures as multi-dimensional discrete stochastic signals for\ntemporal-frequency domain feature extraction. It combines the 1D Spiking Graph\nConvolution (1D-SGC) module and the Frequency Spiking Convolution (FSC) module\nto extract features from the skeleton represented as spiking form.\nAdditionally, the Multi-Scale Wavelet Transform Feature Fusion (MWTF) module is\nproposed to extract dynamic spiking features and capture frequency-specific\ncharacteristics, enhancing classification performance. Experiments across three\nlarge-scale datasets reveal Signal-SGN exceeding state-of-the-art SNN-based\nmethods in accuracy and computational efficiency while attaining comparable\nperformance with GCN methods and significantly reducing theoretical energy\nconsumption.\n","authors":["Naichuan Zheng","Yuchen Du","Hailun Xia","Zeyu Liang"],"pdf_url":"https://arxiv.org/pdf/2408.01701v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17148v3","updated":"2025-10-30T01:44:58Z","published":"2025-10-20T04:49:14Z","title":"DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through\n  Metric-Guided Alignment","summary":"  Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.\n","authors":["Yu Gao","Anqing Jiang","Yiru Wang","Wang Jijun","Hao Jiang","Zhigang Sun","Heng Yuwen","Wang Shuo","Hao Zhao","Sun Hao"],"pdf_url":"https://arxiv.org/pdf/2510.17148v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13444v2","updated":"2025-10-30T01:42:07Z","published":"2025-05-19T17:59:27Z","title":"ChartMuseum: Testing Visual Reasoning Capabilities of Large\n  Vision-Language Models","summary":"  Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs.\n","authors":["Liyan Tang","Grace Kim","Xinyu Zhao","Thom Lake","Wenxuan Ding","Fangcong Yin","Prasann Singhal","Manya Wadhwa","Zeyu Leo Liu","Zayne Sprague","Ramya Namuduri","Bodun Hu","Juan Diego Rodriguez","Puyuan Peng","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2505.13444v2.pdf","comment":"NeurIPS 2025 Datasets & Benchmarks"},{"id":"http://arxiv.org/abs/2510.25077v2","updated":"2025-10-30T01:37:51Z","published":"2025-10-29T01:24:49Z","title":"Neighborhood Feature Pooling for Remote Sensing Image Classification","summary":"  In this work, we propose neighborhood feature pooling (NFP) as a novel\ntexture feature extraction method for remote sensing image classification. The\nNFP layer captures relationships between neighboring inputs and efficiently\naggregates local similarities across feature dimensions. Implemented using\nconvolutional layers, NFP can be seamlessly integrated into any network.\nResults comparing the baseline models and the NFP method indicate that NFP\nconsistently improves performance across diverse datasets and architectures\nwhile maintaining minimal parameter overhead.\n","authors":["Fahimeh Orvati Nia","Amirmohammad Mohammadi","Salim Al Kharsa","Pragati Naikare","Zigfried Hampel-Arias","Joshua Peeples"],"pdf_url":"https://arxiv.org/pdf/2510.25077v2.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.02781v2","updated":"2025-10-30T01:23:43Z","published":"2025-10-03T07:27:55Z","title":"GCVAMD: A Modified CausalVAE Model for Causal Age-related Macular\n  Degeneration Risk Factor Detection and Prediction","summary":"  Age Related Macular Degeneration(AMD) has been one of the most leading causes\nof permanent vision impairment in ophthalmology. Though treatments, such as\nanti VEGF drugs or photodynamic therapies, were developed to slow down the\ndegenerative process of AMD, there is still no specific cure to reverse vision\nloss caused by AMD. Thus, for AMD, detecting existence of risk factors of AMD\nor AMD itself within the patient retina in early stages is a crucial task to\nreduce the possibility of vision impairment. Apart from traditional approaches,\ndeep learning based methods, especially attention mechanism based CNNs and\nGradCAM based XAI analysis on OCT scans, exhibited successful performance in\ndistinguishing AMD retina from normal retinas, making it possible to use AI\ndriven models to aid medical diagnosis and analysis by ophthalmologists\nregarding AMD. However, though having significant success, previous works\nmostly focused on prediction performance itself, not pathologies or underlying\ncausal mechanisms of AMD, which can prohibit intervention analysis on specific\nfactors or even lead to less reliable decisions. Thus, this paper introduces a\nnovel causal AMD analysis model: GCVAMD, which incorporates a modified\nCausalVAE approach that can extract latent causal factors from only raw OCT\nimages. By considering causality in AMD detection, GCVAMD enables causal\ninference such as treatment simulation or intervention analysis regarding major\nrisk factors: drusen and neovascularization, while returning informative latent\ncausal features that can enhance downstream tasks. Results show that through\nGCVAMD, drusen status and neovascularization status can be identified with AMD\ncausal mechanisms in GCVAMD latent spaces, which can in turn be used for\nvarious tasks from AMD detection(classification) to intervention analysis.\n","authors":["Daeyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2510.02781v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26052v1","updated":"2025-10-30T01:10:25Z","published":"2025-10-30T01:10:25Z","title":"Dynamic VLM-Guided Negative Prompting for Diffusion Models","summary":"  We propose a novel approach for dynamic negative prompting in diffusion\nmodels that leverages Vision-Language Models (VLMs) to adaptively generate\nnegative prompts during the denoising process. Unlike traditional Negative\nPrompting methods that use fixed negative prompts, our method generates\nintermediate image predictions at specific denoising steps and queries a VLM to\nproduce contextually appropriate negative prompts. We evaluate our approach on\nvarious benchmark datasets and demonstrate the trade-offs between negative\nguidance strength and text-image alignment.\n","authors":["Hoyeon Chang","Seungjin Kim","Yoonseok Choi"],"pdf_url":"https://arxiv.org/pdf/2510.26052v1.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: The First Workshop on Generative and Protective AI for\n  Content Creation"},{"id":"http://arxiv.org/abs/2510.26049v1","updated":"2025-10-30T00:53:26Z","published":"2025-10-30T00:53:26Z","title":"FlexICL: A Flexible Visual In-context Learning Framework for Elbow and\n  Wrist Ultrasound Segmentation","summary":"  Elbow and wrist fractures are the most common fractures in pediatric\npopulations. Automatic segmentation of musculoskeletal structures in ultrasound\n(US) can improve diagnostic accuracy and treatment planning. Fractures appear\nas cortical defects but require expert interpretation. Deep learning (DL) can\nprovide real-time feedback and highlight key structures, helping lightly\ntrained users perform exams more confidently. However, pixel-wise expert\nannotations for training remain time-consuming and costly. To address this\nchallenge, we propose FlexICL, a novel and flexible in-context learning (ICL)\nframework for segmenting bony regions in US images. We apply it to an\nintra-video segmentation setting, where experts annotate only a small subset of\nframes, and the model segments unseen frames. We systematically investigate\nvarious image concatenation techniques and training strategies for visual ICL\nand introduce novel concatenation methods that significantly enhance model\nperformance with limited labeled data. By integrating multiple augmentation\nstrategies, FlexICL achieves robust segmentation performance across four wrist\nand elbow US datasets while requiring only 5% of the training images. It\noutperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and\nconventional segmentation models like U-Net and TransUNet by 1-27% Dice\ncoefficient on 1,252 US sweeps. These initial results highlight the potential\nof FlexICL as an efficient and scalable solution for US image segmentation well\nsuited for medical imaging use cases where labeled data is scarce.\n","authors":["Yuyue Zhou","Jessica Knight","Shrimanti Ghosh","Banafshe Felfeliyan","Jacob L. Jaremko","Abhilash R. Hareendranathan"],"pdf_url":"https://arxiv.org/pdf/2510.26049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26038v1","updated":"2025-10-30T00:34:16Z","published":"2025-10-30T00:34:16Z","title":"Do Students Debias Like Teachers? On the Distillability of Bias\n  Mitigation Methods","summary":"  Knowledge distillation (KD) is an effective method for model compression and\ntransferring knowledge between models. However, its effect on model's\nrobustness against spurious correlations that degrade performance on\nout-of-distribution data remains underexplored. This study investigates the\neffect of knowledge distillation on the transferability of ``debiasing''\ncapabilities from teacher models to student models on natural language\ninference (NLI) and image classification tasks. Through extensive experiments,\nwe illustrate several key findings: (i) overall the debiasing capability of a\nmodel is undermined post-KD; (ii) training a debiased model does not benefit\nfrom injecting teacher knowledge; (iii) although the overall robustness of a\nmodel may remain stable post-distillation, significant variations can occur\nacross different types of biases; and (iv) we pin-point the internal attention\npattern and circuit that causes the distinct behavior post-KD. Given the above\nfindings, we propose three effective solutions to improve the distillability of\ndebiasing methods: developing high quality data for augmentation, implementing\niterative knowledge distillation, and initializing student models with weights\nobtained from teacher models. To the best of our knowledge, this is the first\nstudy on the effect of KD on debiasing and its interenal mechanism at scale.\nOur findings provide understandings on how KD works and how to design better\ndebiasing methods.\n","authors":["Jiali Cheng","Chirag Agarwal","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2510.26038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08325v4","updated":"2025-10-30T00:33:00Z","published":"2025-01-14T18:57:21Z","title":"GameFactory: Creating New Games with Generative Interactive Videos","summary":"  Generative videos have the potential to revolutionize game development by\nautonomously creating new content. In this paper, we present GameFactory, a\nframework for action-controlled scene-generalizable game video generation. We\nfirst address the fundamental challenge of action controllability by\nintroducing GF-Minecraft, an action-annotated game video dataset without human\nbias, and developing an action control module that enables precise control over\nboth keyboard and mouse inputs. We further extend to support autoregressive\ngeneration for unlimited-length interactive videos. More importantly,\nGameFactory tackles the critical challenge of scene-generalizable action\ncontrol, which most existing methods fail to address. To enable the creation of\nentirely new and diverse games beyond fixed styles and scenes, we leverage the\nopen-domain generative priors from pre-trained video diffusion models. To\nbridge the domain gap between open-domain priors and small-scale game datasets,\nwe propose a multi-phase training strategy with a domain adapter that decouples\ngame style learning from action control. This decoupling ensures that action\ncontrol learning is no longer bound to specific game styles, thereby achieving\nscene-generalizable action control. Experimental results demonstrate that\nGameFactory effectively generates open-domain action-controllable game videos,\nrepresenting a significant step forward in AI-driven game generation.\n","authors":["Jiwen Yu","Yiran Qin","Xintao Wang","Pengfei Wan","Di Zhang","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08325v4.pdf","comment":"ICCV 2025 Highlight, Project Page:\n  https://yujiwen.github.io/gamefactory"},{"id":"http://arxiv.org/abs/2510.23968v2","updated":"2025-10-30T00:14:35Z","published":"2025-10-28T00:48:00Z","title":"Reasoning Visual Language Model for Chest X-Ray Analysis","summary":"  Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.\n","authors":["Andriy Myronenko","Dong Yang","Baris Turkbey","Mariam Aboian","Sena Azamat","Esra Akcicek","Hongxu Yin","Pavlo Molchanov","Marc Edgar","Yufan He","Pengfei Guo","Yucheng Tang","Daguang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.23968v2.pdf","comment":"NV-Reason-CXR-3B"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2510.26750v1","updated":"2025-10-30T17:43:33Z","published":"2025-10-30T17:43:33Z","title":"ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews","summary":"  Systematic reviews and mapping studies are critical for synthesizing\nresearch, identifying gaps, and guiding future work, but they are often\nlabor-intensive and time-consuming. Existing tools provide partial support for\nspecific steps, leaving much of the process manual and error-prone. We present\nProfOlaf, a semi-automated tool designed to streamline systematic reviews while\nmaintaining methodological rigor. ProfOlaf supports iterative snowballing for\narticle collection with human-in-the-loop filtering and uses large language\nmodels to assist in analyzing articles, extracting key topics, and answering\nqueries about the content of papers. By combining automation with guided manual\neffort, ProfOlaf enhances the efficiency, quality, and reproducibility of\nsystematic reviews across research fields. A video describing and demonstrating\nProfOlaf is available at: https://youtu.be/4noUXfcmxsE\n","authors":["Martim Afonso","Nuno Saavedra","Bruno Lourenço","Alexandra Mendes","João Ferreira"],"pdf_url":"https://arxiv.org/pdf/2510.26750v1.pdf","comment":"4 pages, 1 Figure, 2 tables"},{"id":"http://arxiv.org/abs/2503.09205v3","updated":"2025-10-30T17:37:55Z","published":"2025-03-12T09:48:38Z","title":"Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model","summary":"  Integrating audio and visual data for training multimodal foundational models\nremains a challenge. The Audio-Video Vector Alignment (AVVA) framework\naddresses this by considering AV scene alignment beyond mere temporal\nsynchronization, and leveraging Large Language Models (LLMs) for data curation.\nAVVA implements a scoring mechanism for selecting aligned training data\nsegments. It integrates Whisper, a speech-based foundation model, for audio and\nDINOv2 for video analysis in a dual-encoder structure with contrastive learning\non AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the\neffectiveness of the proposed model architecture and data curation approach.\nAVVA achieves a significant improvement in top-k accuracies for video-to-audio\nretrieval on all datasets compared to DenseAV, while using only 192 hrs of\ncurated training data. Furthermore, an ablation study indicates that the data\ncuration process effectively trades data quality for data quantity, yielding\nincreases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound,\ncompared to training on the full spectrum of uncurated data.\n","authors":["Ali Vosoughi","Dimitra Emmanouilidou","Hannes Gamper"],"pdf_url":"https://arxiv.org/pdf/2503.09205v3.pdf","comment":"5 pages, 5 figures, 2 tables. Accepted at EUSIPCO 2025"},{"id":"http://arxiv.org/abs/2508.15840v3","updated":"2025-10-30T16:25:05Z","published":"2025-08-19T17:34:25Z","title":"Unveiling Unicode's Unseen Underpinnings in Undermining Authorship\n  Attribution","summary":"  When using a public communication channel -- whether formal or informal, such\nas commenting or posting on social media -- end users have no expectation of\nprivacy: they compose a message and broadcast it for the world to see. Even if\nan end user takes utmost precautions to anonymize their online presence --\nusing an alias or pseudonym; masking their IP address; spoofing their\ngeolocation; concealing their operating system and user agent; deploying\nencryption; registering with a disposable phone number or email; disabling\nnon-essential settings; revoking permissions; and blocking cookies and\nfingerprinting -- one obvious element still lingers: the message itself.\nAssuming they avoid lapses in judgment or accidental self-exposure, there\nshould be little evidence to validate their actual identity, right? Wrong. The\ncontent of their message -- necessarily open for public consumption -- exposes\nan attack vector: stylometric analysis, or author profiling. In this paper, we\ndissect the technique of stylometry, discuss an antithetical counter-strategy\nin adversarial stylometry, and devise enhancements through Unicode\nsteganography.\n","authors":["Robert Dilworth"],"pdf_url":"https://arxiv.org/pdf/2508.15840v3.pdf","comment":"33 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.14409v2","updated":"2025-10-30T15:05:42Z","published":"2025-02-20T09:57:42Z","title":"Unstructured Evidence Attribution for Long Context Query Focused\n  Summarization","summary":"  Large language models (LLMs) are capable of generating coherent summaries\nfrom very long contexts given a user query, and extracting and citing evidence\nspans helps improve the trustworthiness of these summaries. Whereas previous\nwork has focused on evidence citation with fixed levels of granularity (e.g.\nsentence, paragraph, document, etc.), we propose to extract unstructured (i.e.,\nspans of any length) evidence in order to acquire more relevant and consistent\nevidence than in the fixed granularity case. We show how existing systems\nstruggle to copy and properly cite unstructured evidence, which also tends to\nbe \"lost-in-the-middle\". To help models perform this task, we create the\nSummaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset\ngenerated using a novel pipeline, which can be used as training supervision for\nunstructured evidence summarization. We demonstrate across 5 LLMs and 4\ndatasets spanning human written, synthetic, single, and multi-document settings\nthat LLMs adapted with SUnsET generate more relevant and factually consistent\nevidence with their summaries, extract evidence from more diverse locations in\ntheir context, and can generate more relevant and consistent summaries than\nbaselines with no fine-tuning and fixed granularity evidence. We release SUnsET\nand our generation code to the public.\n","authors":["Dustin Wright","Zain Muhammad Mujahid","Lu Wang","Isabelle Augenstein","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2502.14409v2.pdf","comment":"EMNLP 2025 Main; 29 pages; 24 figures; 8 tables"},{"id":"http://arxiv.org/abs/2510.26569v1","updated":"2025-10-30T14:59:37Z","published":"2025-10-30T14:59:37Z","title":"AdSum: Two-stream Audio-visual Summarization for Automated Video\n  Advertisement Clipping","summary":"  Advertisers commonly need multiple versions of the same advertisement (ad) at\nvarying durations for a single campaign. The traditional approach involves\nmanually selecting and re-editing shots from longer video ads to create shorter\nversions, which is labor-intensive and time-consuming. In this paper, we\nintroduce a framework for automated video ad clipping using video summarization\ntechniques. We are the first to frame video clipping as a shot selection\nproblem, tailored specifically for advertising. Unlike existing general video\nsummarization methods that primarily focus on visual content, our approach\nemphasizes the critical role of audio in advertising. To achieve this, we\ndevelop a two-stream audio-visual fusion model that predicts the importance of\nvideo frames, where importance is defined as the likelihood of a frame being\nselected in the firm-produced short ad. To address the lack of ad-specific\ndatasets, we present AdSum204, a novel dataset comprising 102 pairs of\n30-second and 15-second ads from real advertising campaigns. Extensive\nexperiments demonstrate that our model outperforms state-of-the-art methods\nacross various metrics, including Average Precision, Area Under Curve,\nSpearman, and Kendall.\n","authors":["Wen Xie","Yanjun Zhu","Gijs Overgoor","Yakov Bart","Agata Lapedriza Garcia","Sarah Ostadabbas"],"pdf_url":"https://arxiv.org/pdf/2510.26569v1.pdf","comment":"Accepted at 32nd International Conference on MultiMedia Modeling"},{"id":"http://arxiv.org/abs/2510.04226v4","updated":"2025-10-30T14:52:48Z","published":"2025-10-05T14:29:15Z","title":"Epistemic Diversity and Knowledge Collapse in Large Language Models","summary":"  Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation\n","authors":["Dustin Wright","Sarah Masud","Jared Moore","Srishti Yadav","Maria Antoniak","Chan Young Park","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2510.04226v4.pdf","comment":"16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for\n  table 3, random effect is the model version; v3 changelog: Fixed minor\n  formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model\n  description"},{"id":"http://arxiv.org/abs/2510.26546v1","updated":"2025-10-30T14:37:15Z","published":"2025-10-30T14:37:15Z","title":"WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework\n  with Model Merging","summary":"  Cross-Domain Sequential Recommendation (CDSR) seeks to improve user\npreference modeling by transferring knowledge from multiple domains. Despite\nthe progress made in CDSR, most existing methods rely on overlapping users or\nitems to establish cross-domain correlations-a requirement that rarely holds in\nreal-world settings. The advent of large language models (LLM) and\nmodel-merging techniques appears to overcome this limitation by unifying\nmulti-domain data without explicit overlaps. Yet, our empirical study shows\nthat naively training an LLM on combined domains-or simply merging several\ndomain-specific LLMs-often degrades performance relative to a model trained\nsolely on the target domain. To address these challenges, we first\nexperimentally investigate the cause of suboptimal performance in LLM-based\ncross-domain recommendation and model merging. Building on these insights, we\nintroduce WeaveRec, which cross-trains multiple LoRA modules with source and\ntarget domain data in a weaving fashion, and fuses them via model merging.\nWeaveRec can be extended to multi-source domain scenarios and notably does not\nintroduce additional inference-time cost in terms of latency or memory.\nFurthermore, we provide a theoretical guarantee that WeaveRec can reduce the\nupper bound of the expected error in the target domain. Extensive experiments\non single-source, multi-source, and cross-platform cross-domain recommendation\nscenarios validate that WeaveRec effectively mitigates performance degradation\nand consistently outperforms baseline approaches in real-world recommendation\ntasks.\n","authors":["Min Hou","Xin Liu","Le Wu","Chenyi He","Hao Liu","Zhi Li","Xin Li","Si Wei"],"pdf_url":"https://arxiv.org/pdf/2510.26546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26512v1","updated":"2025-10-30T14:05:55Z","published":"2025-10-30T14:05:55Z","title":"Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs","summary":"  Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts.\n","authors":["Dipak Meher","Carlotta Domeniconi"],"pdf_url":"https://arxiv.org/pdf/2510.26512v1.pdf","comment":"ICDM 2025 Workshop"},{"id":"http://arxiv.org/abs/2502.08271v2","updated":"2025-10-30T13:49:48Z","published":"2025-02-12T10:24:22Z","title":"RecCocktail: A Generalizable and Efficient Framework for LLM-Based\n  Recommendation","summary":"  Large Language Models (LLMs) have achieved remarkable success in recent\nyears, owing to their impressive generalization capabilities and rich world\nknowledge. To capitalize on the potential of using LLMs as recommender systems,\nmainstream approaches typically focus on two paradigms. The first paradigm\ndesigns multi-domain or multi-task instruction data for generalizable\nrecommendation, so as to align LLMs with general recommendation areas and deal\nwith cold-start recommendation. The second paradigm focuses on enhancing\ndomain-specific recommendation tasks, improving performance in warm\nrecommendation scenarios. While most previous works treat these two paradigms\nseparately, we argue that they have complementary advantages, and combining\nthem can yield better results. In this paper, we propose a generalizable and\nefficient LLM-based recommendation framework RecCocktail. Our approach begins\nwith fine-tuning a \"base spirit\" LoRA module using domain-general\nrecommendation instruction data to align LLM with recommendation knowledge.\nNext, given users' behavior of a specific domain, we construct a\ndomain-specific \"ingredient\" LoRA module. We then provide an entropy-guided\nadaptive merging method to mix the \"base spirit\" and the \"ingredient\" in the\nweight space. Please note that, RecCocktail combines the advantages of the\nexisting two paradigms without introducing additional time or space overhead\nduring the inference phase. Moreover, RecCocktail is efficient with plug and\nplay, as the \"base spirit\" LoRA is trained only once, and any domain-specific\n\"ingredient\" can be efficiently mixed with only domain-specific fine-tuning.\nExtensive experiments on multiple datasets under both warm and cold-start\nrecommendation scenarios validate the effectiveness and generality of the\nproposed RecCocktail.\n","authors":["Min Hou","Chenxi Bai","Le Wu","Hao Liu","Kai Zhang","Weiwen Liu","Richang Hong","Ruiming Tang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26486v1","updated":"2025-10-30T13:39:08Z","published":"2025-10-30T13:39:08Z","title":"LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks","summary":"  Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.\n","authors":["Dipak Meher","Carlotta Domeniconi","Guadalupe Correa-Cabrera"],"pdf_url":"https://arxiv.org/pdf/2510.26486v1.pdf","comment":"Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures"},{"id":"http://arxiv.org/abs/2510.26461v1","updated":"2025-10-30T13:07:39Z","published":"2025-10-30T13:07:39Z","title":"Vectorized Context-Aware Embeddings for GAT-Based Collaborative\n  Filtering","summary":"  Recommender systems often struggle with data sparsity and cold-start\nscenarios, limiting their ability to provide accurate suggestions for new or\ninfrequent users. This paper presents a Graph Attention Network (GAT) based\nCollaborative Filtering (CF) framework enhanced with Large Language Model (LLM)\ndriven context aware embeddings. Specifically, we generate concise textual user\nprofiles and unify item metadata (titles, genres, overviews) into rich textual\nembeddings, injecting these as initial node features in a bipartite user item\ngraph. To further optimize ranking performance, we introduce a hybrid loss\nfunction that combines Bayesian Personalized Ranking (BPR) with a cosine\nsimilarity term and robust negative sampling, ensuring explicit negative\nfeedback is distinguished from unobserved data. Experiments on the MovieLens\n100k and 1M datasets show consistent improvements over state-of-the-art\nbaselines in Precision, NDCG, and MAP while demonstrating robustness for users\nwith limited interaction history. Ablation studies confirm the critical role of\nLLM-augmented embeddings and the cosine similarity term in capturing nuanced\nsemantic relationships. Our approach effectively mitigates sparsity and\ncold-start limitations by integrating LLM-derived contextual understanding into\ngraph-based architectures. Future directions include balancing recommendation\naccuracy with coverage and diversity, and introducing fairness-aware\nconstraints and interpretability features to enhance system performance\nfurther.\n","authors":["Danial Ebrat","Sepideh Ahmadian","Luis Rueda"],"pdf_url":"https://arxiv.org/pdf/2510.26461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17670v2","updated":"2025-10-30T12:05:58Z","published":"2025-10-20T15:41:55Z","title":"On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active\n  Marginal-Samples Exploration","summary":"  Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs.\n","authors":["Yehonathan Refael","Amit Aides","Aviad Barzilai","George Leifman","Genady Beryozkin","Vered Silverman","Bolous Jaber","Tomer Shekel"],"pdf_url":"https://arxiv.org/pdf/2510.17670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26407v1","updated":"2025-10-30T11:56:02Z","published":"2025-10-30T11:56:02Z","title":"Barlow Twins for Sequential Recommendation","summary":"  Sequential recommendation models must navigate sparse interaction data\npopularity bias and conflicting objectives like accuracy versus diversity While\nrecent contrastive selfsupervised learning SSL methods offer improved accuracy\nthey come with tradeoffs large batch requirements reliance on handcrafted\naugmentations and negative sampling that can reinforce popularity bias In this\npaper we introduce BT-SR a novel noncontrastive SSL framework that integrates\nthe Barlow Twins redundancyreduction principle into a Transformerbased nextitem\nrecommender BTSR learns embeddings that align users with similar shortterm\nbehaviors while preserving longterm distinctionswithout requiring negative\nsampling or artificial perturbations This structuresensitive alignment allows\nBT-SR to more effectively recognize emerging user intent and mitigate the\ninfluence of noisy historical context Our experiments on five public benchmarks\ndemonstrate that BTSR consistently improves nextitem prediction accuracy and\nsignificantly enhances longtail item coverage and recommendation calibration\nCrucially we show that a single hyperparameter can control the\naccuracydiversity tradeoff enabling practitioners to adapt recommendations to\nspecific application needs\n","authors":["Ivan Razvorotnev","Marina Munkhoeva","Evgeny Frolov"],"pdf_url":"https://arxiv.org/pdf/2510.26407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26309v1","updated":"2025-10-30T09:53:16Z","published":"2025-10-30T09:53:16Z","title":"GraphCompliance: Aligning Policy and Context Graphs for LLM-Based\n  Regulatory Compliance","summary":"  Compliance at web scale poses practical challenges: each request may require\na regulatory assessment. Regulatory texts (e.g., the General Data Protection\nRegulation, GDPR) are cross-referential and normative, while runtime contexts\nare expressed in unstructured natural language. This setting motivates us to\nalign semantic information in unstructured text with the structured, normative\nelements of regulations. To this end, we introduce GraphCompliance, a framework\nthat represents regulatory texts as a Policy Graph and runtime contexts as a\nContext Graph, and aligns them. In this formulation, the policy graph encodes\nnormative structure and cross-references, whereas the context graph formalizes\nevents as subject-action-object (SAO) and entity-relation triples. This\nalignment anchors the reasoning of a judge large language model (LLM) in\nstructured information and helps reduce the burden of regulatory interpretation\nand event parsing, enabling a focus on the core reasoning step. In experiments\non 300 GDPR-derived real-world scenarios spanning five evaluation tasks,\nGraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than\nLLM-only and RAG baselines, with fewer under- and over-predictions, resulting\nin higher recall and lower false positive rates. Ablation studies indicate\ncontributions from each graph component, suggesting that structured\nrepresentations and a judge LLM are complementary for normative reasoning.\n","authors":["Jiseong Chung","Ronny Ko","Wonchul Yoo","Makoto Onizuka","Sungmok Kim","Tae-Wan Kim","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2510.26309v1.pdf","comment":"Under review at The Web Conference 2026 (Semantics & Knowledge\n  track). Code will be released upon acceptance. This arXiv v1 contains no\n  repository links to preserve double-blind review"},{"id":"http://arxiv.org/abs/2510.25160v2","updated":"2025-10-30T08:52:17Z","published":"2025-10-29T04:29:17Z","title":"Model-Document Protocol for AI Search","summary":"  AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.\n","authors":["Hongjin Qian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25160v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2510.26231v1","updated":"2025-10-30T08:10:03Z","published":"2025-10-30T08:10:03Z","title":"DiSE: A diffusion probabilistic model for automatic structure\n  elucidation of organic compounds","summary":"  Automatic structure elucidation is essential for self-driving laboratories as\nit enables the system to achieve truly autonomous. This capability closes the\nexperimental feedback loop, ensuring that machine learning models receive\nreliable structure information for real-time decision-making and optimization.\nHerein, we present DiSE, an end-to-end diffusion-based generative model that\nintegrates multiple spectroscopic modalities, including MS, 13C and 1H chemical\nshifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation\nof organic compounds. By learning inherent correlations among spectra through\ndata-driven approaches, DiSE achieves superior accuracy, strong generalization\nacross chemically diverse datasets, and robustness to experimental data despite\nbeing trained on calculated spectra. DiSE thus represents a significant advance\ntoward fully automated structure elucidation, with broad potential in natural\nproduct research, drug discovery, and self-driving laboratories.\n","authors":["Haochen Chen","Qi Huang","Anan Wu","Wenhao Zhang","Jianliang Ye","Jianming Wu","Kai Tan","Xin Lu","Xin Xu"],"pdf_url":"https://arxiv.org/pdf/2510.26231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20995v3","updated":"2025-10-30T06:37:00Z","published":"2025-02-28T12:32:53Z","title":"The RAG Paradox: A Black-Box Attack Exploiting Unintentional\n  Vulnerabilities in Retrieval-Augmented Generation Systems","summary":"  With the growing adoption of retrieval-augmented generation (RAG) systems,\nvarious attack methods have been proposed to degrade their performance.\nHowever, most existing approaches rely on unrealistic assumptions in which\nexternal attackers have access to internal components such as the retriever. To\naddress this issue, we introduce a realistic black-box attack based on the RAG\nparadox, a structural vulnerability arising from the system's effort to enhance\ntrust by revealing both the retrieved documents and their sources to users.\nThis transparency enables attackers to observe which sources are used and how\ninformation is phrased, allowing them to craft poisoned documents that are more\nlikely to be retrieved and upload them to the identified sources. Moreover, as\nRAG systems directly provide retrieved content to users, these documents must\nnot only be retrievable but also appear natural and credible to maintain user\nconfidence in the search results. Unlike prior work that focuses solely on\nimproving document retrievability, our attack method explicitly considers both\nretrievability and user trust in the retrieved content. Both offline and online\nexperiments demonstrate that our method significantly degrades system\nperformance without internal access, while generating natural-looking poisoned\ndocuments.\n","authors":["Chanwoo Choi","Jinsoo Kim","Sukmin Cho","Soyeong Jeong","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2502.20995v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26178v1","updated":"2025-10-30T06:35:36Z","published":"2025-10-30T06:35:36Z","title":"ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning\n  Representations with LLMs","summary":"  Legal case retrieval (LCR) is a cornerstone of real-world legal decision\nmaking, as it enables practitioners to identify precedents for a given query\ncase. Existing approaches mainly rely on traditional lexical models and\npretrained language models to encode the texts of legal cases. Yet there are\nrich information in the relations among different legal entities as well as the\ncrucial reasoning process that uncovers how legal facts and legal issues can\nlead to judicial decisions. Such relational reasoning process reflects the\ndistinctive characteristics of each case that can distinguish one from another,\nmirroring the real-world judicial process. Naturally, incorporating such\ninformation into the precise case embedding could further enhance the accuracy\nof case retrieval. In this paper, a novel ReaKase-8B framework is proposed to\nleverage extracted legal facts, legal issues, legal relation triplets and legal\nreasoning for effective legal case retrieval. ReaKase-8B designs an in-context\nlegal case representation learning paradigm with a fine-tuned large language\nmodel. Extensive experiments on two benchmark datasets from COLIEE 2022 and\nCOLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings\nsubstantially improve retrieval performance over baseline models, highlighting\nthe potential of integrating legal reasoning into legal case retrieval systems.\nThe code has been released on https://github.com/yanran-tang/ReaKase-8B.\n","authors":["Yanran Tang","Ruihong Qiu","Xue Li","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17918v5","updated":"2025-10-30T06:29:37Z","published":"2025-09-22T15:43:11Z","title":"Shilling Recommender Systems by Generating Side-feature-aware Fake User\n  Profiles","summary":"  Recommender systems (RS) greatly influence users' consumption decisions,\nmaking them attractive targets for malicious shilling attacks that inject fake\nuser profiles to manipulate recommendations. Existing shilling methods can\ngenerate effective and stealthy fake profiles when training data only contain\nrating matrix, but they lack comprehensive solutions for scenarios where side\nfeatures are present and utilized by the recommender. To address this gap, we\nextend the Leg-UP framework by enhancing the generator architecture to\nincorporate side features, enabling the generation of side-feature-aware fake\nuser profiles. Experiments on benchmarks show that our method achieves strong\nattack performance while maintaining stealthiness.\n","authors":["Yuanrong Wang","Yingpeng Du"],"pdf_url":"https://arxiv.org/pdf/2509.17918v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26104v1","updated":"2025-10-30T03:30:12Z","published":"2025-10-30T03:30:12Z","title":"OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender","summary":"  In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests.\n","authors":["Zhaoqi Zhang","Haolei Pei","Jun Guo","Tianyu Wang","Yufei Feng","Hui Sun","Shaowei Liu","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2510.26104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26095v1","updated":"2025-10-30T03:10:45Z","published":"2025-10-30T03:10:45Z","title":"ORBIT -- Open Recommendation Benchmark for Reproducible Research with\n  Hidden Tests","summary":"  Recommender systems are among the most impactful AI applications, interacting\nwith billions of users every day, guiding them to relevant products, services,\nor information tailored to their preferences. However, the research and\ndevelopment of recommender systems are hindered by existing datasets that fail\nto capture realistic user behaviors and inconsistent evaluation settings that\nlead to ambiguous conclusions. This paper introduces the Open Recommendation\nBenchmark for Reproducible Research with HIdden Tests (ORBIT), a unified\nbenchmark for consistent and realistic evaluation of recommendation models.\nORBIT offers a standardized evaluation framework of public datasets with\nreproducible splits and transparent settings for its public leaderboard.\nAdditionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,\nfeaturing web browsing sequences from 87 million public, high-quality webpages.\nClueWeb-Reco is a synthetic dataset derived from real, user-consented, and\nprivacy-guaranteed browsing data. It aligns with modern recommendation\nscenarios and is reserved as the hidden test part of our leaderboard to\nchallenge recommendation models' generalization ability. ORBIT measures 12\nrepresentative recommendation models on its public benchmark and introduces a\nprompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results\nreflect general improvements of recommender systems on the public datasets,\nwith variable individual performances. The results on the hidden test reveal\nthe limitations of existing approaches in large-scale webpage recommendation\nand highlight the potential for improvements with LLM integrations. ORBIT\nbenchmark, leaderboard, and codebase are available at\nhttps://www.open-reco-bench.ai.\n","authors":["Jingyuan He","Jiongnan Liu","Vishan Vishesh Oberoi","Bolin Wu","Mahima Jagadeesh Patel","Kangrui Mao","Chuning Shi","I-Ta Lee","Arnold Overwijk","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2510.26095v1.pdf","comment":"Accepted to NeurIPS 2025 Datasets & Benchmarks track"},{"id":"http://arxiv.org/abs/2510.25622v2","updated":"2025-10-30T02:50:46Z","published":"2025-10-29T15:27:23Z","title":"MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for\n  Semantic IDs Learning in Recommendation","summary":"  Industrial recommender systems rely on unique Item Identifiers (ItemIDs).\nHowever, this method struggles with scalability and generalization in large,\ndynamic datasets that have sparse long-tail data. Content-based Semantic IDs\n(SIDs) address this by sharing knowledge through content quantization. However,\nby ignoring dynamic behavioral properties, purely content-based SIDs have\nlimited expressive power. Existing methods attempt to incorporate behavioral\ninformation but overlook a critical distinction: unlike relatively uniform\ncontent features, user-item interactions are highly skewed and diverse,\ncreating a vast information gap in quality and quantity between popular and\nlong-tail items. This oversight leads to two critical limitations: (1) Noise\nCorruption: Indiscriminate behavior-content alignment allows collaborative\nnoise from long-tail items to corrupt their content representations, leading to\nthe loss of critical multimodal information. (2)Signal Obscurity: The\nequal-weighting scheme for SIDs fails to reflect the varying importance of\ndifferent behavioral signals, making it difficult for downstream tasks to\ndistinguish important SIDs from uninformative ones. To tackle these issues, we\npropose a mixture-of-quantization framework, MMQ-v2, to adaptively Align,\nDenoise, and Amplify multimodal information from content and behavior\nmodalities for semantic IDs learning. The semantic IDs generated by this\nframework named ADA-SID. It introduces two innovations: an adaptive\nbehavior-content alignment that is aware of information richness to shield\nrepresentations from noise, and a dynamic behavioral router to amplify critical\nsignals by applying different weights to SIDs. Extensive experiments on public\nand large-scale industrial datasets demonstrate ADA-SID's significant\nsuperiority in both generative and discriminative recommendation tasks.\n","authors":["Yi Xu","Moyu Zhang","Chaofan Fan","Jinxin Hu","Xiaochen Li","Yu Zhang","Xiaoyi Zeng","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25402v2","updated":"2025-10-30T02:45:14Z","published":"2025-10-29T11:20:18Z","title":"Towards Automated Quality Assurance of Patent Specifications: A\n  Multi-Dimensional LLM Framework","summary":"  Although AI drafting tools have gained prominence in patent writing, the\nsystematic evaluation of AI-generated patent content quality represents a\nsignificant research gap. To address this gap, We propose to evaluate patents\nusing regulatory compliance, technical coherence, and figure-reference\nconsistency detection modules, and then generate improvement suggestions via an\nintegration module. The framework is validated on a comprehensive dataset\ncomprising 80 human-authored and 80 AI-generated patents from two patent\ndrafting tools. Evaluation is performed on 10,841 total sentences, 8,924\nnon-template sentences, and 554 patent figures for the three detection modules\nrespectively, achieving balanced accuracies of 99.74%, 82.12%, and 91.2%\nagainst expert annotations. Additional analysis was conducted to examine defect\ndistributions across patent sections, technical domains, and authoring sources.\nSection-based analysis indicates that figure-text consistency and technical\ndetail precision require particular attention. Mechanical Engineering and\nConstruction show more claim-specification inconsistencies due to complex\ntechnical documentation requirements. AI-generated patents show a significant\ngap compared to human-authored ones. While human-authored patents primarily\ncontain surface-level errors like typos, AI-generated patents exhibit more\nstructural defects in figure-text alignment and cross-references.\n","authors":["Yuqian Chai","Chaochao Wang","Weilei Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.11066v2","updated":"2025-10-30T02:33:58Z","published":"2025-10-13T07:06:26Z","title":"Decoupled Multimodal Fusion for User Interest Modeling in Click-Through\n  Rate Prediction","summary":"  Modern industrial recommendation systems improve recommendation performance\nby integrating multimodal representations from pre-trained models into ID-based\nClick-Through Rate (CTR) prediction frameworks. However, existing approaches\ntypically adopt modality-centric modeling strategies that process ID-based and\nmultimodal embeddings independently, failing to capture fine-grained\ninteractions between content semantics and behavioral signals. In this paper,\nwe propose Decoupled Multimodal Fusion (DMF), which introduces a\nmodality-enriched modeling strategy to enable fine-grained interactions between\nID-based collaborative representations and multimodal representations for user\ninterest modeling. Specifically, we construct target-aware features to bridge\nthe semantic gap across different embedding spaces and leverage them as side\ninformation to enhance the effectiveness of user interest modeling.\nFurthermore, we design an inference-optimized attention mechanism that\ndecouples the computation of target-aware features and ID-based embeddings\nbefore the attention layer, thereby alleviating the computational bottleneck\nintroduced by incorporating target-aware features. To achieve comprehensive\nmultimodal integration, DMF combines user interest representations learned\nunder the modality-centric and modality-enriched modeling strategies. Offline\nexperiments on public and industrial datasets demonstrate the effectiveness of\nDMF. Moreover, DMF has been deployed on the product recommendation system of\nthe international e-commerce platform Lazada, achieving relative improvements\nof 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.\n","authors":["Alin Fan","Hanqing Li","Sihan Lu","Jingsong Yuan","Jiandong Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.11066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21188v3","updated":"2025-10-30T01:40:08Z","published":"2025-03-27T06:10:22Z","title":"A Task-Centric Perspective on Recommendation Systems","summary":"  Many studies in recommender systems (RecSys) adopt a general problem\ndefinition, i.e., to recommend preferred items to users based on past\ninteractions. Such abstraction often lacks the domain-specific nuances\nnecessary for practical deployment. However, models are frequently evaluated\nusing datasets collected from online recommender platforms, which inherently\nreflect domain or task specificities. In this paper, we analyze RecSys task\nformulations, emphasizing key components such as input-output structures,\ntemporal dynamics, and candidate item selection. All these factors directly\nimpact offline evaluation. We further examine the complexities of user-item\ninteractions, including decision-making costs, multi-step engagements, and\nunobservable interactions, which may influence model design. Additionally, we\nexplore the balance between task specificity and model generalizability,\nhighlighting how well-defined task formulations serve as the foundation for\nrobust evaluation and effective solution development. By clarifying task\ndefinitions and their implications, this work provides a structured perspective\non RecSys research. The goal is to help researchers better navigate the field,\nparticularly in understanding specificities of the RecSys tasks and ensuring\nfair and meaningful evaluations.\n","authors":["Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2503.21188v3.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2510.26800v1","updated":"2025-10-30T17:59:51Z","published":"2025-10-30T17:59:51Z","title":"OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes","summary":"  There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.\n","authors":["Yukun Huang","Jiwen Yu","Yanning Zhou","Jianan Wang","Xintao Wang","Pengfei Wan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26800v1.pdf","comment":"Project page: https://yukun-huang.github.io/OmniX/"},{"id":"http://arxiv.org/abs/2506.03237v2","updated":"2025-10-30T17:59:46Z","published":"2025-06-03T17:49:41Z","title":"UniSite: The First Cross-Structure Dataset and Learning Framework for\n  End-to-End Ligand Binding Site Detection","summary":"  The detection of ligand binding sites for proteins is a fundamental step in\nStructure-Based Drug Design. Despite notable advances in recent years, existing\nmethods, datasets, and evaluation metrics are confronted with several key\nchallenges: (1) current datasets and methods are centered on individual\nprotein-ligand complexes and neglect that diverse binding sites may exist\nacross multiple complexes of the same protein, introducing significant\nstatistical bias; (2) ligand binding site detection is typically modeled as a\ndiscontinuous workflow, employing binary segmentation and subsequent clustering\nalgorithms; (3) traditional evaluation metrics do not adequately reflect the\nactual performance of different binding site prediction methods. To address\nthese issues, we first introduce UniSite-DS, the first UniProt (Unique\nProtein)-centric ligand binding site dataset, which contains 4.81 times more\nmulti-site data and 2.08 times more overall data compared to the previously\nmost widely used datasets. We then propose UniSite, the first end-to-end ligand\nbinding site detection framework supervised by set prediction loss with\nbijective matching. In addition, we introduce Average Precision based on\nIntersection over Union (IoU) as a more accurate evaluation metric for ligand\nbinding site prediction. Extensive experiments on UniSite-DS and several\nrepresentative benchmark datasets demonstrate that IoU-based Average Precision\nprovides a more accurate reflection of prediction quality, and that UniSite\noutperforms current state-of-the-art methods in ligand binding site detection.\nThe dataset and codes will be made publicly available at\nhttps://github.com/quanlin-wu/unisite.\n","authors":["Jigang Fan","Quanlin Wu","Shengjie Luo","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2506.03237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26795v1","updated":"2025-10-30T17:59:35Z","published":"2025-10-30T17:59:35Z","title":"Scaling Image Geo-Localization to Continent Level","summary":"  Determining the precise geographic location of an image at a global scale\nremains an unsolved challenge. Standard image retrieval techniques are\ninefficient due to the sheer volume of images (>100M) and fail when coverage is\ninsufficient. Scalable solutions, however, involve a trade-off: global\nclassification typically yields coarse results (10+ kilometers), while\ncross-view retrieval between ground and aerial imagery suffers from a domain\ngap and has been primarily studied on smaller regions. This paper introduces a\nhybrid approach that achieves fine-grained geo-localization across a large\ngeographic expanse the size of a continent. We leverage a proxy classification\ntask during training to learn rich feature representations that implicitly\nencode precise location information. We combine these learned prototypes with\nembeddings of aerial imagery to increase robustness to the sparsity of\nground-level data. This enables direct, fine-grained retrieval over areas\nspanning multiple countries. Our extensive evaluation demonstrates that our\napproach can localize within 200m more than 68\\% of queries of a dataset\ncovering a large part of Europe. The code is publicly available at\nhttps://scaling-geoloc.github.io.\n","authors":["Philipp Lindenberger","Paul-Edouard Sarlin","Jan Hosang","Matteo Balice","Marc Pollefeys","Simon Lynen","Eduard Trulls"],"pdf_url":"https://arxiv.org/pdf/2510.26795v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26792v1","updated":"2025-10-30T17:59:09Z","published":"2025-10-30T17:59:09Z","title":"Learning Pseudorandom Numbers with Transformers: Permuted Congruential\n  Generators, Curricula, and Interpretability","summary":"  We study the ability of Transformer models to learn sequences generated by\nPermuted Congruential Generators (PCGs), a widely used family of pseudo-random\nnumber generators (PRNGs). PCGs introduce substantial additional difficulty\nover linear congruential generators (LCGs) by applying a series of bit-wise\nshifts, XORs, rotations and truncations to the hidden state. We show that\nTransformers can nevertheless successfully perform in-context prediction on\nunseen sequences from diverse PCG variants, in tasks that are beyond published\nclassical attacks. In our experiments we scale moduli up to $2^{22}$ using up\nto $50$ million model parameters and datasets with up to $5$ billion tokens.\nSurprisingly, we find even when the output is truncated to a single bit, it can\nbe reliably predicted by the model. When multiple distinct PRNGs are presented\ntogether during training, the model can jointly learn them, identifying\nstructures from different permutations. We demonstrate a scaling law with\nmodulus $m$: the number of in-context sequence elements required for\nnear-perfect prediction grows as $\\sqrt{m}$. For larger moduli, optimization\nenters extended stagnation phases; in our experiments, learning moduli $m \\geq\n2^{20}$ requires incorporating training data from smaller moduli, demonstrating\na critical necessity for curriculum learning. Finally, we analyze embedding\nlayers and uncover a novel clustering phenomenon: the model spontaneously\ngroups the integer inputs into bitwise rotationally-invariant clusters,\nrevealing how representations can transfer from smaller to larger moduli.\n","authors":["Tao Tao","Maissam Barkeshli"],"pdf_url":"https://arxiv.org/pdf/2510.26792v1.pdf","comment":"10+13 pages, 8+19 figures"},{"id":"http://arxiv.org/abs/2510.26788v1","updated":"2025-10-30T17:58:11Z","published":"2025-10-30T17:58:11Z","title":"Defeating the Training-Inference Mismatch via FP16","summary":"  Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.\n","authors":["Penghui Qi","Zichen Liu","Xiangxin Zhou","Tianyu Pang","Chao Du","Wee Sun Lee","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2510.26788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26787v1","updated":"2025-10-30T17:58:04Z","published":"2025-10-30T17:58:04Z","title":"Remote Labor Index: Measuring AI Automation of Remote Work","summary":"  AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.\n","authors":["Mantas Mazeika","Alice Gatti","Cristina Menghini","Udari Madhushani Sehwag","Shivam Singhal","Yury Orlovskiy","Steven Basart","Manasi Sharma","Denis Peskoff","Elaine Lau","Jaehyuk Lim","Lachlan Carroll","Alice Blair","Vinaya Sivakumar","Sumana Basu","Brad Kenstler","Yuntao Ma","Julian Michael","Xiaoke Li","Oliver Ingebretsen","Aditya Mehta","Jean Mottola","John Teichmann","Kevin Yu","Zaina Shaik","Adam Khoja","Richard Ren","Jason Hausenloy","Long Phan","Ye Htet","Ankit Aich","Tahseen Rabbani","Vivswan Shah","Andriy Novykov","Felix Binder","Kirill Chugunov","Luis Ramirez","Matias Geralnik","Hernán Mesura","Dean Lee","Ed-Yeremai Hernandez Cardona","Annette Diamond","Summer Yue","Alexandr Wang","Bing Liu","Ernesto Hernandez","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2510.26787v1.pdf","comment":"Website: https://www.remotelabor.ai"},{"id":"http://arxiv.org/abs/2510.26786v1","updated":"2025-10-30T17:57:40Z","published":"2025-10-30T17:57:40Z","title":"HEIR: Learning Graph-Based Motion Hierarchies","summary":"  Hierarchical structures of motion exist across research fields, including\ncomputer vision, graphics, and robotics, where complex dynamics typically arise\nfrom coordinated interactions among simpler motion components. Existing methods\nto model such dynamics typically rely on manually-defined or heuristic\nhierarchies with fixed motion primitives, limiting their generalizability\nacross different tasks. In this work, we propose a general hierarchical motion\nmodeling method that learns structured, interpretable motion relationships\ndirectly from data. Our method represents observed motions using graph-based\nhierarchies, explicitly decomposing global absolute motions into\nparent-inherited patterns and local motion residuals. We formulate hierarchy\ninference as a differentiable graph learning problem, where vertices represent\nelemental motions and directed edges capture learned parent-child dependencies\nthrough graph neural networks. We evaluate our hierarchical reconstruction\napproach on three examples: 1D translational motion, 2D rotational motion, and\ndynamic 3D scene deformation via Gaussian splatting. Experimental results show\nthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,\nand produces more realistic and interpretable deformations compared to the\nbaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,\ndata-driven hierarchical modeling paradigm, our method offers a formulation\napplicable to a broad range of motion-centric tasks. Project Page:\nhttps://light.princeton.edu/HEIR/\n","authors":["Cheng Zheng","William Koch","Baiang Li","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2510.26786v1.pdf","comment":"Code link: https://github.com/princeton-computational-imaging/HEIR"},{"id":"http://arxiv.org/abs/2510.26783v1","updated":"2025-10-30T17:56:47Z","published":"2025-10-30T17:56:47Z","title":"A Unified Theory for Causal Inference: Direct Debiased Machine Learning\n  via Bregman-Riesz Regression","summary":"  This note introduces a unified theory for causal inference that integrates\nRiesz regression, covariate balancing, density-ratio estimation (DRE), targeted\nmaximum likelihood estimation (TMLE), and the matching estimator in average\ntreatment effect (ATE) estimation. In ATE estimation, the balancing weights and\nthe regression functions of the outcome play important roles, where the\nbalancing weights are referred to as the Riesz representer, bias-correction\nterm, and clever covariates, depending on the context. Riesz regression,\ncovariate balancing, DRE, and the matching estimator are methods for estimating\nthe balancing weights, where Riesz regression is essentially equivalent to DRE\nin the ATE context, the matching estimator is a special case of DRE, and DRE is\nin a dual relationship with covariate balancing. TMLE is a method for\nconstructing regression function estimators such that the leading bias term\nbecomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density\nRatio Estimation and Riesz Regression.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2510.26783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26782v1","updated":"2025-10-30T17:56:43Z","published":"2025-10-30T17:56:43Z","title":"Clone Deterministic 3D Worlds with Geometrically-Regularized World\n  Models","summary":"  A world model is an internal model that simulates how the world evolves.\nGiven past observations and actions, it predicts the future of both the\nembodied agent and its environment. Accurate world models are essential for\nenabling agents to think, plan, and reason effectively in complex, dynamic\nsettings. Despite rapid progress, current world models remain brittle and\ndegrade over long horizons. We argue that a central cause is representation\nquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or\nentangled latents make dynamics learning unnecessarily hard. We therefore ask\nwhether improving representation learning alone can substantially improve\nworld-model performance. In this work, we take a step toward building a truly\naccurate world model by addressing a fundamental yet open problem: constructing\na model that can fully clone and overfit to a deterministic 3D world. We\npropose Geometrically-Regularized World Models (GRWM), which enforces that\nconsecutive points along a natural sensory trajectory remain close in latent\nrepresentation space. This approach yields significantly improved latent\nrepresentations that align closely with the true topology of the environment.\nGRWM is plug-and-play, requires only minimal architectural modification, scales\nwith trajectory length, and is compatible with diverse latent generative\nbackbones. Across deterministic 3D settings and long-horizon prediction tasks,\nGRWM significantly increases rollout fidelity and stability. Analyses show that\nits benefits stem from learning a latent manifold with superior geometric\nstructure. These findings support a clear takeaway: improving representation\nlearning is a direct and useful path to robust world models, delivering\nreliable long-horizon predictions without enlarging the dynamics module.\n","authors":["Zaishuo Xia","Yukuan Lu","Xinyi Li","Yifan Xu","Yubei Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15723v7","updated":"2025-10-30T17:56:10Z","published":"2024-10-21T07:42:43Z","title":"S-CFE: Simple Counterfactual Explanations","summary":"  We study the problem of finding optimal sparse, manifold-aligned\ncounterfactual explanations for classifiers. Canonically, this can be\nformulated as an optimization problem with multiple non-convex components,\nincluding classifier loss functions and manifold alignment (or\n\\emph{plausibility}) metrics. The added complexity of enforcing\n\\emph{sparsity}, or shorter explanations, complicates the problem further.\nExisting methods often focus on specific models and plausibility measures,\nrelying on convex $\\ell_1$ regularizers to enforce sparsity. In this paper, we\ntackle the canonical formulation using the accelerated proximal gradient (APG)\nmethod, a simple yet efficient first-order procedure capable of handling smooth\nnon-convex objectives and non-smooth $\\ell_p$ (where $0 \\leq p < 1$)\nregularizers. This enables our approach to seamlessly incorporate various\nclassifiers and plausibility measures while producing sparser solutions. Our\nalgorithm only requires differentiable data-manifold regularizers and supports\nbox constraints for bounded feature ranges, ensuring the generated\ncounterfactuals remain \\emph{actionable}. Finally, experiments on real-world\ndatasets demonstrate that our approach effectively produces sparse,\nmanifold-aligned counterfactual explanations while maintaining proximity to the\nfactual data and computational efficiency.\n","authors":["Shpresim Sadiku","Moritz Wagner","Sai Ganesh Nagarajan","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2410.15723v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26778v1","updated":"2025-10-30T17:55:46Z","published":"2025-10-30T17:55:46Z","title":"Surpassing state of the art on AMD area estimation from RGB fundus\n  images through careful selection of U-Net architectures and loss functions\n  for class imbalance","summary":"  Age-related macular degeneration (AMD) is one of the leading causes of\nirreversible vision impairment in people over the age of 60. This research\nfocuses on semantic segmentation for AMD lesion detection in RGB fundus images,\na non-invasive and cost-effective imaging technique. The results of the ADAM\nchallenge - the most comprehensive AMD detection from RGB fundus images\nresearch competition and open dataset to date - serve as a benchmark for our\nevaluation. Taking the U-Net connectivity as a base of our framework, we\nevaluate and compare several approaches to improve the segmentation model's\narchitecture and training pipeline, including pre-processing techniques,\nencoder (backbone) deep network types of varying complexity, and specialized\nloss functions to mitigate class imbalances on image and pixel levels. The main\noutcome of this research is the final configuration of the AMD detection\nframework, which outperforms all the prior ADAM challenge submissions on the\nmulti-class segmentation of different AMD lesion types in non-invasive RGB\nfundus images. The source code used to conduct the experiments presented in\nthis paper is made freely available.\n","authors":["Valentyna Starodub","Mantas Lukoševičius"],"pdf_url":"https://arxiv.org/pdf/2510.26778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23534v2","updated":"2025-10-30T17:55:38Z","published":"2025-10-27T17:10:43Z","title":"Direct Debiased Machine Learning via Bregman Divergence Minimization","summary":"  We develop a direct debiased machine learning framework comprising Neyman\ntargeted estimation and generalized Riesz regression. Our framework unifies\nRiesz regression for automatic debiased machine learning, covariate balancing,\ntargeted maximum likelihood estimation (TMLE), and density-ratio estimation. In\nmany problems involving causal effects or structural models, the parameters of\ninterest depend on regression functions. Plugging regression functions\nestimated by machine learning methods into the identifying equations can yield\npoor performance because of first-stage bias. To reduce such bias, debiased\nmachine learning employs Neyman orthogonal estimating equations. Debiased\nmachine learning typically requires estimation of the Riesz representer and the\nregression function. For this problem, we develop a direct debiased machine\nlearning framework with an end-to-end algorithm. We formulate estimation of the\nnuisance parameters, the regression function and the Riesz representer, as\nminimizing the discrepancy between Neyman orthogonal scores computed with known\nand unknown nuisance parameters, which we refer to as Neyman targeted\nestimation. Neyman targeted estimation includes Riesz representer estimation,\nand we measure discrepancies using the Bregman divergence. The Bregman\ndivergence encompasses various loss functions as special cases, where the\nsquared loss yields Riesz regression and the Kullback-Leibler divergence yields\nentropy balancing. We refer to this Riesz representer estimation as generalized\nRiesz regression. Neyman targeted estimation also yields TMLE as a special case\nfor regression function estimation. Furthermore, for specific pairs of models\nand Riesz representer estimation methods, we can automatically obtain the\ncovariate balancing property without explicitly solving the covariate balancing\nobjective.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2510.23534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26777v1","updated":"2025-10-30T17:55:23Z","published":"2025-10-30T17:55:23Z","title":"Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for\n  Time Series Classification","summary":"  Recent research on time series foundation models has primarily focused on\nforecasting, leaving it unclear how generalizable their learned representations\nare. In this study, we examine whether frozen pre-trained forecasting models\ncan provide effective representations for classification. To this end, we\ncompare different representation extraction strategies and introduce two\nmodel-agnostic embedding augmentations. Our experiments show that the best\nforecasting models achieve classification accuracy that matches or even\nsurpasses that of state-of-the-art models pre-trained specifically for\nclassification. Moreover, we observe a positive correlation between forecasting\nand classification performance. These findings challenge the assumption that\ntask-specific pre-training is necessary, and suggest that learning to forecast\nmay provide a powerful route toward constructing general-purpose time series\nfoundation models.\n","authors":["Andreas Auer","Daniel Klotz","Sebastinan Böck","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2510.26777v1.pdf","comment":"NeurIPS 2025 Workshop on Recent Advances in Time Series Foundation\n  Models (BERT2S)"},{"id":"http://arxiv.org/abs/2510.26776v1","updated":"2025-10-30T17:55:19Z","published":"2025-10-30T17:55:19Z","title":"Faithful and Fast Influence Function via Advanced Sampling","summary":"  How can we explain the influence of training data on black-box models?\nInfluence functions (IFs) offer a post-hoc solution by utilizing gradients and\nHessians. However, computing the Hessian for an entire dataset is\nresource-intensive, necessitating a feasible alternative. A common approach\ninvolves randomly sampling a small subset of the training data, but this method\noften results in highly inconsistent IF estimates due to the high variance in\nsample configurations. To address this, we propose two advanced sampling\ntechniques based on features and logits. These samplers select a small yet\nrepresentative subset of the entire dataset by considering the stochastic\ndistribution of features or logits, thereby enhancing the accuracy of IF\nestimations. We validate our approach through class removal experiments, a\ntypical application of IFs, using the F1-score to measure how effectively the\nmodel forgets the removed class while maintaining inference consistency on the\nremaining classes. Our method reduces computation time by 30.1% and memory\nusage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.\n","authors":["Jungyeon Koh","Hyeonsu Lyu","Jonggyu Jang","Hyun Jong Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26771v1","updated":"2025-10-30T17:53:42Z","published":"2025-10-30T17:53:42Z","title":"STaMP: Sequence Transformation and Mixed Precision for Low-Precision\n  Activation Quantization","summary":"  Quantization is the key method for reducing inference latency, power and\nmemory footprint of generative AI models. However, accuracy often degrades\nsharply when activations are quantized below eight bits. Recent work suggests\nthat invertible linear transformations (e.g. rotations) can aid quantization,\nby reparameterizing feature channels and weights. In this paper, we propose\n\\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a\nnovel strategy that applies linear transformations along the \\textit{sequence}\ndimension to exploit the strong local correlation in language and visual data.\nBy keeping a small number of tokens in each intermediate activation at higher\nprecision, we can maintain model accuracy at lower (average) activations\nbit-widths. We evaluate STaMP on recent LVM and LLM architectures,\ndemonstrating that it significantly improves low bit width activation\nquantization and complements established activation and weight quantization\nmethods including recent feature transformations.\n","authors":["Marco Federici","Riccardo Del Chiaro","Boris van Breugel","Paul Whatmough","Markus Nagel"],"pdf_url":"https://arxiv.org/pdf/2510.26771v1.pdf","comment":"10 pages main text, 8 pages supplementary material"},{"id":"http://arxiv.org/abs/2510.26769v1","updated":"2025-10-30T17:52:39Z","published":"2025-10-30T17:52:39Z","title":"SteerVLM: Robust Model Control through Lightweight Activation Steering\n  for Vision Language Models","summary":"  This work introduces SteerVLM, a lightweight steering module designed to\nguide Vision-Language Models (VLMs) towards outputs that better adhere to\ndesired instructions. Our approach learns from the latent embeddings of paired\nprompts encoding target and converse behaviors to dynamically adjust\nactivations connecting the language modality with image context. This allows\nfor fine-grained, inference-time control over complex output semantics without\nmodifying model weights while preserving performance on off-target tasks. Our\nsteering module requires learning parameters equal to 0.14% of the original\nVLM's size. Our steering module gains model control through dimension-wise\nactivation modulation and adaptive steering across layers without requiring\npre-extracted static vectors or manual tuning of intervention points.\nFurthermore, we introduce VNIA (Visual Narrative Intent Alignment), a\nmultimodal dataset specifically created to facilitate the development and\nevaluation of VLM steering techniques. Our method outperforms existing\nintervention techniques on steering and hallucination mitigation benchmarks for\nVLMs and proposes a robust solution for multimodal model control through\nactivation engineering.\n","authors":["Anushka Sivakumar","Andrew Zhang","Zaber Hakim","Chris Thomas"],"pdf_url":"https://arxiv.org/pdf/2510.26769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.15370v3","updated":"2025-10-30T17:51:51Z","published":"2025-09-18T19:17:07Z","title":"Adversarial generalization of unfolding (model-based) networks","summary":"  Unfolding networks are interpretable networks emerging from iterative\nalgorithms, incorporate prior knowledge of data structure, and are designed to\nsolve inverse problems like compressed sensing, which deals with recovering\ndata from noisy, missing observations. Compressed sensing finds applications in\ncritical domains, from medical imaging to cryptography, where adversarial\nrobustness is crucial to prevent catastrophic failures. However, a solid\ntheoretical understanding of the performance of unfolding networks in the\npresence of adversarial attacks is still in its infancy. In this paper, we\nstudy the adversarial generalization of unfolding networks when perturbed with\n$l_2$-norm constrained attacks, generated by the fast gradient sign method.\nParticularly, we choose a family of state-of-the-art overaparameterized\nunfolding networks and deploy a new framework to estimate their adversarial\nRademacher complexity. Given this estimate, we provide adversarial\ngeneralization error bounds for the networks under study, which are tight with\nrespect to the attack level. To our knowledge, this is the first theoretical\nanalysis on the adversarial generalization of unfolding networks. We further\npresent a series of experiments on real-world data, with results corroborating\nour derived theory, consistently for all data. Finally, we observe that the\nfamily's overparameterization can be exploited to promote adversarial\nrobustness, shedding light on how to efficiently robustify neural networks.\n","authors":["Vicky Kouni"],"pdf_url":"https://arxiv.org/pdf/2509.15370v3.pdf","comment":"Accepted at NeurIPS2025"},{"id":"http://arxiv.org/abs/2510.26752v1","updated":"2025-10-30T17:46:49Z","published":"2025-10-30T17:46:49Z","title":"The Oversight Game: Learning to Cooperatively Balance an AI Agent's\n  Safety and Autonomy","summary":"  As increasingly capable agents are deployed, a central safety question is how\nto retain meaningful human control without modifying the underlying system. We\nstudy a minimal control interface where an agent chooses whether to act\nautonomously (play) or defer (ask), while a human simultaneously chooses\nwhether to be permissive (trust) or to engage in oversight (oversee). If the\nagent defers, the human's choice determines the outcome, potentially leading to\na corrective action or a system shutdown. We model this interaction as a\ntwo-player Markov Game. Our analysis focuses on cases where this game qualifies\nas a Markov Potential Game (MPG), a class of games where we can provide an\nalignment guarantee: under a structural assumption on the human's value\nfunction, any decision by the agent to act more autonomously that benefits\nitself cannot harm the human's value. We also analyze extensions to this MPG\nframework. Theoretically, this perspective provides conditions for a specific\nform of intrinsic alignment. If the reward structures of the human-agent game\nmeet these conditions, we have a formal guarantee that the agent improving its\nown outcome will not harm the human's. Practically, this model motivates a\ntransparent control layer with predictable incentives where the agent learns to\ndefer when risky and act when safe, while its pretrained policy and the\nenvironment's reward structure remain untouched. Our gridworld simulation shows\nthat through independent learning, the agent and human discover their optimal\noversight roles. The agent learns to ask when uncertain and the human learns\nwhen to oversee, leading to an emergent collaboration that avoids safety\nviolations introduced post-training. This demonstrates a practical method for\nmaking misaligned models safer after deployment.\n","authors":["William Overman","Mohsen Bayati"],"pdf_url":"https://arxiv.org/pdf/2510.26752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26745v1","updated":"2025-10-30T17:40:22Z","published":"2025-10-30T17:40:22Z","title":"Deep sequence models tend to memorize geometrically; it is unclear why","summary":"  In sequence modeling, the parametric memory of atomic facts has been\npredominantly abstracted as a brute-force lookup of co-occurrences between\nentities. We contrast this associative view against a geometric view of how\nmemory is stored. We begin by isolating a clean and analyzable instance of\nTransformer reasoning that is incompatible with memory as strictly a storage of\nthe local co-occurrences specified during training. Instead, the model must\nhave somehow synthesized its own geometry of atomic facts, encoding global\nrelationships between all entities, including non-co-occurring ones. This in\nturn has simplified a hard reasoning task involving an $\\ell$-fold composition\ninto an easy-to-learn 1-step geometric task.\n  From this phenomenon, we extract fundamental aspects of neural embedding\ngeometries that are hard to explain. We argue that the rise of such a geometry,\ndespite optimizing over mere local associations, cannot be straightforwardly\nattributed to typical architectural or optimizational pressures.\nCounterintuitively, an elegant geometry is learned even when it is not more\nsuccinct than a brute-force lookup of associations.\n  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry\nstems from a spectral bias that -- in contrast to prevailing theories -- indeed\narises naturally despite the lack of various pressures. This analysis also\npoints to practitioners a visible headroom to make Transformer memory more\nstrongly geometric. We hope the geometric view of parametric memory encourages\nrevisiting the default intuitions that guide researchers in areas like\nknowledge acquisition, capacity, discovery and unlearning.\n","authors":["Shahriar Noroozizadeh","Vaishnavh Nagarajan","Elan Rosenfeld","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2510.26745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19419v2","updated":"2025-10-30T17:32:12Z","published":"2025-04-28T02:10:18Z","title":"Advancing Local Clustering on Graphs via Compressive Sensing:\n  Semi-supervised and Unsupervised Methods","summary":"  Local clustering aims to identify specific substructures within a large graph\nwithout any additional structural information of the graph. These substructures\nare typically small compared to the overall graph, enabling the problem to be\napproached by finding a sparse solution to a linear system associated with the\ngraph Laplacian. In this work, we first propose a method for identifying\nspecific local clusters when very few labeled data are given, which we term\nsemi-supervised local clustering. We then extend this approach to the\nunsupervised setting when no prior information on labels is available. The\nproposed methods involve randomly sampling the graph, applying diffusion\nthrough local cluster extraction, then examining the overlap among the results\nto find each cluster. We establish the co-membership conditions for any pair of\nnodes, and rigorously prove the correctness of our methods. Additionally, we\nconduct extensive experiments to demonstrate that the proposed methods achieve\nstate of the art results in the low-label rates regime.\n","authors":["Zhaiming Shen","Sung Ha Kang"],"pdf_url":"https://arxiv.org/pdf/2504.19419v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05567v2","updated":"2025-10-30T17:31:52Z","published":"2025-06-05T20:26:18Z","title":"Partially-Supervised Neural Network Model For Quadratic Multiparametric\n  Programming","summary":"  Neural Networks (NN) with ReLU activation functions are used to model\nmultiparametric quadratic optimization problems (mp-QP) in diverse engineering\napplications. Researchers have suggested leveraging the piecewise affine\nproperty of deep NN models to solve mp-QP with linear constraints, which also\nexhibit piecewise affine behaviour. However, traditional deep NN applications\nto mp-QP fall short of providing optimal and feasible predictions, even when\ntrained on large datasets. This study proposes a partially-supervised NN (PSNN)\narchitecture that directly represents the mathematical structure of the global\nsolution function. In contrast to generic NN training approaches, the proposed\nPSNN method derives a large proportion of model weights directly from the\nmathematical properties of the optimization problem, producing more accurate\nsolutions despite significantly smaller training data sets. Many energy\nmanagement problems are formulated as QP, so we apply the proposed approach to\nenergy systems (specifically DC optimal power flow) to demonstrate proof of\nconcept. Model performance in terms of solution accuracy and speed of\npredictions was compared against a commercial solver and a generic Deep NN\nmodel based on classical training. Results show KKT sufficient conditions for\nPSNN consistently outperform generic NN architectures with classical training\nusing far less data, including when tested on extreme, out-of-training\ndistribution test data. Given its speed advantages over traditional solvers,\nthe PSNN model can quickly produce optimal and feasible solutions within a\nsecond for millions of input parameters sampled from a distribution of\nstochastic demands and renewable generator dispatches, which can be used for\nsimulations and long term planning.\n","authors":["Fuat Can Beylunioglu","Mehrdad Pirnia","P. Robert Duimering"],"pdf_url":"https://arxiv.org/pdf/2506.05567v2.pdf","comment":"36 pages including references and appendix"},{"id":"http://arxiv.org/abs/2410.01755v3","updated":"2025-10-30T17:28:23Z","published":"2024-10-02T17:05:48Z","title":"Integrating Protein Sequence and Expression Level to Analysis Molecular\n  Characterization of Breast Cancer Subtypes","summary":"  Breast cancer's complexity and variability pose significant challenges in\nunderstanding its progression and guiding effective treatment. This study aims\nto integrate protein sequence data with expression levels to improve the\nmolecular characterization of breast cancer subtypes and predict clinical\noutcomes. Using ProtGPT2, a language model specifically designed for protein\nsequences, we generated embeddings that capture the functional and structural\nproperties of proteins. These embeddings were integrated with protein\nexpression levels to form enriched biological representations, which were\nanalyzed using machine learning methods, such as ensemble K-means for\nclustering and XGBoost for classification. Our approach enabled the successful\nclustering of patients into biologically distinct groups and accurately\npredicted clinical outcomes such as survival and biomarker status, achieving\nhigh performance metrics, notably an F1 score of 0.88 for survival and 0.87 for\nbiomarker status prediction. Feature importance analysis identified KMT2C,\nCLASP2, and MYO1B as key proteins involved in hormone signaling, cytoskeletal\nremodeling, and therapy resistance in hormone receptor-positive and\ntriple-negative breast cancer, with potential influence on breast cancer\nsubtype behavior and progression. Furthermore, protein-protein interaction\nnetworks and correlation analyses revealed functional interdependencies among\nproteins that may influence the behavior and progression of breast cancer\nsubtypes. These findings suggest that integrating protein sequence and\nexpression data provides valuable insights into tumor biology and has\nsignificant potential to enhance personalized treatment strategies in breast\ncancer care.\n","authors":["Hossein Sholehrasa","Majid Jaberi-Douraki"],"pdf_url":"https://arxiv.org/pdf/2410.01755v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26723v1","updated":"2025-10-30T17:23:40Z","published":"2025-10-30T17:23:40Z","title":"Bridging the Gap between Empirical Welfare Maximization and Conditional\n  Average Treatment Effect Estimation in Policy Learning","summary":"  The goal of policy learning is to train a policy function that recommends a\ntreatment given covariates to maximize population welfare. There are two major\napproaches in policy learning: the empirical welfare maximization (EWM)\napproach and the plug-in approach. The EWM approach is analogous to a\nclassification problem, where one first builds an estimator of the population\nwelfare, which is a functional of policy functions, and then trains a policy by\nmaximizing the estimated welfare. In contrast, the plug-in approach is based on\nregression, where one first estimates the conditional average treatment effect\n(CATE) and then recommends the treatment with the highest estimated outcome.\nThis study bridges the gap between the two approaches by showing that both are\nbased on essentially the same optimization problem. In particular, we prove an\nexact equivalence between EWM and least squares over a reparameterization of\nthe policy class. As a consequence, the two approaches are interchangeable in\nseveral respects and share the same theoretical guarantees under common\nconditions. Leveraging this equivalence, we propose a novel regularization\nmethod for policy learning. Our findings yield a convex and computationally\nefficient training procedure that avoids the NP-hard combinatorial step\ntypically required in EWM.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2510.26723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26722v1","updated":"2025-10-30T17:22:57Z","published":"2025-10-30T17:22:57Z","title":"Non-Convex Over-the-Air Heterogeneous Federated Learning: A\n  Bias-Variance Trade-off","summary":"  Over-the-air (OTA) federated learning (FL) has been well recognized as a\nscalable paradigm that exploits the waveform superposition of the wireless\nmultiple-access channel to aggregate model updates in a single use. Existing\nOTA-FL designs largely enforce zero-bias model updates by either assuming\n\\emph{homogeneous} wireless conditions (equal path loss across devices) or\nforcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous}\nwireless scenarios, however, such designs are constrained by the weakest device\nand inflate the update variance. Moreover, prior analyses of biased OTA-FL\nlargely address convex objectives, while most modern AI models are highly\nnon-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient\ndescent (SGD) for general smooth non-convex objectives under wireless\nheterogeneity. We develop novel OTA-FL SGD updates that allow a structured,\ntime-invariant model bias while facilitating reduced variance updates. We\nderive a finite-time stationarity bound (expected time average squared gradient\nnorm) that explicitly reveals a bias-variance trade-off. To optimize this\ntrade-off, we pose a non-convex joint OTA power-control design and develop an\nefficient successive convex approximation (SCA) algorithm that requires only\nstatistical CSI at the base station. Experiments on a non-convex image\nclassification task validate the approach: the SCA-based design accelerates\nconvergence via an optimized bias and improves generalization over prior OTA-FL\nbaselines.\n","authors":["Muhammad Faraz Ul Abrar","Nicolò Michelusi"],"pdf_url":"https://arxiv.org/pdf/2510.26722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26717v1","updated":"2025-10-30T17:18:53Z","published":"2025-10-30T17:18:53Z","title":"On Purely Private Covariance Estimation","summary":"  We present a simple perturbation mechanism for the release of $d$-dimensional\ncovariance matrices $\\Sigma$ under pure differential privacy. For large\ndatasets with at least $n\\geq d^2/\\varepsilon$ elements, our mechanism recovers\nthe provably optimal Frobenius norm error guarantees of\n\\cite{nikolov2023private}, while simultaneously achieving best known error for\nall other $p$-Schatten norms, with $p\\in [1,\\infty]$. Our error is\ninformation-theoretically optimal for all $p\\ge 2$, in particular, our\nmechanism is the first purely private covariance estimator that achieves\noptimal error in spectral norm.\n  For small datasets $n< d^2/\\varepsilon$, we further show that by projecting\nthe output onto the nuclear norm ball of appropriate radius, our algorithm\nachieves the optimal Frobenius norm error $O(\\sqrt{d\\;\\text{Tr}(\\Sigma) /n})$,\nimproving over the known bounds of $O(\\sqrt{d/n})$ of \\cite{nikolov2023private}\nand ${O}\\big(d^{3/4}\\sqrt{\\text{Tr}(\\Sigma)/n}\\big)$ of\n\\cite{dong2022differentially}.\n","authors":["Tommaso d'Orsi","Gleb Novikov"],"pdf_url":"https://arxiv.org/pdf/2510.26717v1.pdf","comment":"equal contribution"},{"id":"http://arxiv.org/abs/2506.08645v2","updated":"2025-10-30T17:15:23Z","published":"2025-06-10T09:57:58Z","title":"When Kernels Multiply, Clusters Unify: Fusing Embeddings with the\n  Kronecker Product","summary":"  State-of-the-art embeddings often capture distinct yet complementary\ndiscriminative features: For instance, one image embedding model may excel at\ndistinguishing fine-grained textures, while another focuses on object-level\nstructure. Motivated by this observation, we propose a principled approach to\nfuse such complementary representations through kernel multiplication.\nMultiplying the kernel similarity functions of two embeddings allows their\ndiscriminative structures to interact, producing a fused representation whose\nkernel encodes the union of the clusters identified by each parent embedding.\nThis formulation also provides a natural way to construct joint kernels for\npaired multi-modal data (e.g., image-text tuples), where the product of\nmodality-specific kernels inherits structure from both domains. We highlight\nthat this kernel product is mathematically realized via the Kronecker product\nof the embedding feature maps, yielding our proposed KrossFuse framework for\nembedding fusion. To address the computational cost of the resulting\nhigh-dimensional Kronecker space, we further develop RP-KrossFuse, a scalable\nvariant that leverages random projections for efficient approximation. As a key\napplication, we use this framework to bridge the performance gap between\ncross-modal embeddings (e.g., CLIP, BLIP) and unimodal experts (e.g., DINOv2,\nE5). Experiments show that RP-KrossFuse effectively integrates these models,\nenhancing modality-specific performance while preserving cross-modal alignment.\nThe project code is available at https://github.com/yokiwuuu/KrossFuse.\n","authors":["Youqi Wu","Jingwei Zhang","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2506.08645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26715v1","updated":"2025-10-30T17:13:58Z","published":"2025-10-30T17:13:58Z","title":"LSM-MS2: A Foundation Model Bridging Spectral Identification and\n  Biological Interpretation","summary":"  A vast majority of mass spectrometry data remains uncharacterized, leaving\nmuch of its biological and chemical information untapped. Recent advances in\nmachine learning have begun to address this gap, particularly for tasks such as\nspectral identification in tandem mass spectrometry data. Here, we present the\nlatest generation of LSM-MS2, a large-scale deep learning foundation model\ntrained on millions of spectra to learn a semantic chemical space. LSM-MS2\nachieves state-of-the-art performance in spectral identification, improving on\nexisting methods by 30% in accuracy of identifying challenging isomeric\ncompounds, yielding 42% more correct identifications in complex biological\nsamples, and maintaining robustness under low-concentration conditions.\nFurthermore, LSM-MS2 produces rich spectral embeddings that enable direct\nbiological interpretation from minimal downstream data, successfully\ndifferentiating disease states and predicting clinical outcomes across diverse\ntranslational applications.\n","authors":["Gabriel Asher","Devesh Shah","Amy A. Caudy","Luke Ferro","Lea Amar","Ana S. H. Costa","Thomas Patton","Niall O'Connor","Jennifer M. Campbell","Jack Geremia"],"pdf_url":"https://arxiv.org/pdf/2510.26715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26714v1","updated":"2025-10-30T17:13:42Z","published":"2025-10-30T17:13:42Z","title":"On the limitation of evaluating machine unlearning using only a single\n  training seed","summary":"  Machine unlearning (MU) aims to remove the influence of certain data points\nfrom a trained model without costly retraining. Most practical MU algorithms\nare only approximate and their performance can only be assessed empirically.\nCare must therefore be taken to make empirical comparisons as representative as\npossible. A common practice is to run the MU algorithm multiple times\nindependently starting from the same trained model. In this work, we\ndemonstrate that this practice can give highly non-representative results\nbecause -- even for the same architecture and same dataset -- some MU methods\ncan be highly sensitive to the choice of random number seed used for model\ntraining. We therefore recommend that empirical\ncomphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should\nalso reflect the variability across different model training seeds.\n","authors":["Jamie Lanyon","Axel Finke","Petros Andreou","Georgina Cosma"],"pdf_url":"https://arxiv.org/pdf/2510.26714v1.pdf","comment":"mini paper, 2 figures"},{"id":"http://arxiv.org/abs/2510.26709v1","updated":"2025-10-30T17:11:01Z","published":"2025-10-30T17:11:01Z","title":"An All-Reduce Compatible Top-K Compressor for Communication-Efficient\n  Distributed Learning","summary":"  Communication remains a central bottleneck in large-scale distributed machine\nlearning, and gradient sparsification has emerged as a promising strategy to\nalleviate this challenge. However, existing gradient compressors face notable\nlimitations: Rand-$K$\\ discards structural information and performs poorly in\npractice, while Top-$K$\\ preserves informative entries but loses the\ncontraction property and requires costly All-Gather operations. In this paper,\nwe propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that\naligns sparsity patterns across nodes using a lightweight sketch of the\ngradient, enabling index-free All-Reduce while preserving globally significant\ninformation. ARC-Top-$K$\\ is provably contractive and, when combined with\nmomentum error feedback (EF21M), achieves linear speedup and sharper\nconvergence rates than the original EF21M under standard assumptions.\nEmpirically, ARC-Top-$K$\\ matches the accuracy of Top-$K$\\ while reducing\nwall-clock training time by up to 60.7\\%, offering an efficient and scalable\nsolution that combines the robustness of Rand-$K$\\ with the strong performance\nof Top-$K$.\n","authors":["Chuyan Chen","Chenyang Ma","Zhangxin Li","Yutong He","Yanjie Dong","Kun Yuan"],"pdf_url":"https://arxiv.org/pdf/2510.26709v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2509.21319v2","updated":"2025-10-30T17:09:54Z","published":"2025-09-25T16:19:06Z","title":"RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards","summary":"  Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:\nhttps://huggingface.co/collections/nvidia/reward-models-10-2025\n","authors":["Zhilin Wang","Jiaqi Zeng","Olivier Delalleau","Ellie Evans","Daniel Egert","Hoo-Chang Shin","Felipe Soares","Yi Dong","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2509.21319v2.pdf","comment":"Added link to access models:\n  https://huggingface.co/collections/nvidia/reward-models-10-2025"},{"id":"http://arxiv.org/abs/2510.26707v1","updated":"2025-10-30T17:09:09Z","published":"2025-10-30T17:09:09Z","title":"Value Drifts: Tracing Value Alignment During LLM Post-Training","summary":"  As LLMs occupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment of LLMs with human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stage value alignment arises during the course of a model's\npost-training. Our analysis disentangles the effects of post-training\nalgorithms and datasets, measuring both the magnitude and time of value drifts\nduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popular supervised fine-tuning (SFT) and preference optimization\ndatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequent preference optimization rarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to different value alignment outcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned during post-training and help to inform data curation,\nas well as the selection of models and algorithms for preference optimization\nto improve model alignment to human values.\n","authors":["Mehar Bhatia","Shravan Nayak","Gaurav Kamath","Marius Mosbach","Karolina Stańczak","Vered Shwartz","Siva Reddy"],"pdf_url":"https://arxiv.org/pdf/2510.26707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26706v1","updated":"2025-10-30T17:08:52Z","published":"2025-10-30T17:08:52Z","title":"Budgeted Multiple-Expert Deferral","summary":"  Learning to defer uncertain predictions to costly experts offers a powerful\nstrategy for improving the accuracy and efficiency of machine learning systems.\nHowever, standard training procedures for deferral algorithms typically require\nquerying all experts for every training instance, an approach that becomes\nprohibitively expensive when expert queries incur significant computational or\nresource costs. This undermines the core goal of deferral: to limit unnecessary\nexpert usage. To overcome this challenge, we introduce the budgeted deferral\nframework, which aims to train effective deferral algorithms while minimizing\nexpert query costs during training. We propose new algorithms for both\ntwo-stage and single-stage multiple-expert deferral settings that selectively\nquery only a subset of experts per training example. While inspired by active\nlearning, our setting is fundamentally different: labels are already known, and\nthe core challenge is to decide which experts to query in order to balance cost\nand predictive performance. We establish theoretical guarantees for both of our\nalgorithms, including generalization bounds and label complexity analyses.\nEmpirical results across several domains show that our algorithms substantially\nreduce training costs without sacrificing prediction accuracy, demonstrating\nthe practical value of our budget-aware deferral algorithms.\n","authors":["Giulia DeSalvo","Clara Mohri","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2510.26706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26704v1","updated":"2025-10-30T17:07:14Z","published":"2025-10-30T17:07:14Z","title":"How Regularization Terms Make Invertible Neural Networks Bayesian Point\n  Estimators","summary":"  Can regularization terms in the training of invertible neural networks lead\nto known Bayesian point estimators in reconstruction? Invertible networks are\nattractive for inverse problems due to their inherent stability and\ninterpretability. Recently, optimization strategies for invertible neural\nnetworks that approximate either a reconstruction map or the forward operator\nhave been studied from a Bayesian perspective, but each has limitations. To\naddress this, we introduce and analyze two regularization terms for the network\ntraining that, upon inversion of the network, recover properties of classical\nBayesian point estimators: while the first can be connected to the posterior\nmean, the second resembles the MAP estimator. Our theoretical analysis\ncharacterizes how each loss shapes both the learned forward operator and its\ninverse reconstruction map. Numerical experiments support our findings and\ndemonstrate how these loss-term regularizers introduce data-dependence in a\nstable and interpretable way.\n","authors":["Nick Heilenkötter"],"pdf_url":"https://arxiv.org/pdf/2510.26704v1.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2505.12275v2","updated":"2025-10-30T17:06:21Z","published":"2025-05-18T07:27:35Z","title":"Curriculum Abductive Learning","summary":"  Abductive Learning (ABL) integrates machine learning with logical reasoning\nin a loop: a learning model predicts symbolic concept labels from raw inputs,\nwhich are revised through abduction using domain knowledge and then fed back\nfor retraining. However, due to the nondeterminism of abduction, the training\nprocess often suffers from instability, especially when the knowledge base is\nlarge and complex, resulting in a prohibitively large abduction space. While\nprior works focus on improving candidate selection within this space, they\ntypically treat the knowledge base as a static black box. In this work, we\npropose Curriculum Abductive Learning (C-ABL), a method that explicitly\nleverages the internal structure of the knowledge base to address the ABL\ntraining challenges. C-ABL partitions the knowledge base into a sequence of\nsub-bases, progressively introduced during training. This reduces the abduction\nspace throughout training and enables the model to incorporate logic in a\nstepwise, smooth way. Experiments across multiple tasks show that C-ABL\noutperforms previous ABL implementations, significantly improves training\nstability, convergence speed, and final accuracy, especially under complex\nknowledge setting.\n","authors":["Wen-Chao Hu","Qi-Jie Li","Lin-Han Jia","Cunjing Ge","Yu-Feng Li","Yuan Jiang","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.12275v2.pdf","comment":"Accepted by NeurIPS 2025, 22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2510.26700v1","updated":"2025-10-30T17:05:57Z","published":"2025-10-30T17:05:57Z","title":"Assessment of the conditional exchangeability assumption in causal\n  machine learning models: a simulation study","summary":"  Observational studies developing causal machine learning (ML) models for the\nprediction of individualized treatment effects (ITEs) seldom conduct empirical\nevaluations to assess the conditional exchangeability assumption. We aimed to\nevaluate the performance of these models under conditional exchangeability\nviolations and the utility of negative control outcomes (NCOs) as a diagnostic.\nWe conducted a simulation study to examine confounding bias in ITE estimates\ngenerated by causal forest and X-learner models under varying conditions,\nincluding the presence or absence of true heterogeneity. We simulated data to\nreflect real-world scenarios with differing levels of confounding, sample size,\nand NCO confounding structures. We then estimated and compared subgroup-level\ntreatment effects on the primary outcome and NCOs across settings with and\nwithout unmeasured confounding. When conditional exchangeability was violated,\ncausal forest and X-learner models failed to recover true treatment effect\nheterogeneity and, in some cases, falsely indicated heterogeneity when there\nwas none. NCOs successfully identified subgroups affected by unmeasured\nconfounding. Even when NCOs did not perfectly satisfy its ideal assumptions, it\nremained informative, flagging potential bias in subgroup level estimates,\nthough not always pinpointing the subgroup with the largest confounding.\nViolations of conditional exchangeability substantially limit the validity of\nITE estimates from causal ML models in routinely collected observational data.\nNCOs serve a useful empirical diagnostic tool for detecting subgroup-specific\nunmeasured confounding and should be incorporated into causal ML workflows to\nsupport the credibility of individualized inference.\n","authors":["Gerard T. Portela","Jason B. Gibbons","Sebastian Schneeweiss","Rishi J. Desai"],"pdf_url":"https://arxiv.org/pdf/2510.26700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20138v2","updated":"2025-10-30T17:04:50Z","published":"2025-03-26T01:00:35Z","title":"Guided Model Merging for Hybrid Data Learning: Leveraging Centralized\n  Data to Refine Decentralized Models","summary":"  Current network training paradigms primarily focus on either centralized or\ndecentralized data regimes. However, in practice, data availability often\nexhibits a hybrid nature, where both regimes coexist. This hybrid setting\npresents new opportunities for model training, as the two regimes offer\ncomplementary trade-offs: decentralized data is abundant but subject to\nheterogeneity and communication constraints, while centralized data, though\nlimited in volume and potentially unrepresentative, enables better curation and\nhigh-throughput access. Despite its potential, effectively combining these\nparadigms remains challenging, and few frameworks are tailored to hybrid data\nregimes. To address this, we propose a novel framework that constructs a model\natlas from decentralized models and leverages centralized data to refine a\nglobal model within this structured space. The refined model is then used to\nreinitialize the decentralized models. Our method synergizes federated learning\n(to exploit decentralized data) and model merging (to utilize centralized\ndata), enabling effective training under hybrid data availability.\nTheoretically, we show that our approach achieves faster convergence than\nmethods relying solely on decentralized data, due to variance reduction in the\nmerging process. Extensive experiments demonstrate that our framework\nconsistently outperforms purely centralized, purely decentralized, and existing\nhybrid-adaptable methods. Notably, our method remains robust even when the\ncentralized and decentralized data domains differ or when decentralized data\ncontains noise, significantly broadening its applicability.\n","authors":["Junyi Zhu","Ruicong Yao","Taha Ceritli","Savas Ozkan","Matthew B. Blaschko","Eunchung Noh","Jeongwon Min","Cho Jung Min","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2503.20138v2.pdf","comment":"Accepted at WACV 2026"},{"id":"http://arxiv.org/abs/2510.26692v1","updated":"2025-10-30T16:59:43Z","published":"2025-10-30T16:59:43Z","title":"Kimi Linear: An Expressive, Efficient Attention Architecture","summary":"  We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.\n","authors":[" Kimi Team","Yu Zhang","Zongyu Lin","Xingcheng Yao","Jiaxi Hu","Fanqing Meng","Chengyin Liu","Xin Men","Songlin Yang","Zhiyuan Li","Wentao Li","Enzhe Lu","Weizhou Liu","Yanru Chen","Weixin Xu","Longhui Yu","Yejie Wang","Yu Fan","Longguang Zhong","Enming Yuan","Dehao Zhang","Yizhi Zhang","T. Y. Liu","Haiming Wang","Shengjun Fang","Weiran He","Shaowei Liu","Yiwei Li","Jianlin Su","Jiezhong Qiu","Bo Pang","Junjie Yan","Zhejun Jiang","Weixiao Huang","Bohong Yin","Jiacheng You","Chu Wei","Zhengtao Wang","Chao Hong","Yutian Chen","Guanduo Chen","Yucheng Wang","Huabin Zheng","Feng Wang","Yibo Liu","Mengnan Dong","Zheng Zhang","Siyuan Pan","Wenhao Wu","Yuhao Wu","Longyu Guan","Jiawen Tao","Guohong Fu","Xinran Xu","Yuzhi Wang","Guokun Lai","Yuxin Wu","Xinyu Zhou","Zhilin Yang","Yulun Du"],"pdf_url":"https://arxiv.org/pdf/2510.26692v1.pdf","comment":"Kimi Linear tech report"},{"id":"http://arxiv.org/abs/2510.26690v1","updated":"2025-10-30T16:59:22Z","published":"2025-10-30T16:59:22Z","title":"LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits","summary":"  Low-Rank Adaptation (LoRA) has become a popular technique for\nparameter-efficient fine-tuning of large language models (LLMs). In many\nreal-world scenarios, multiple adapters are loaded simultaneously to enable LLM\ncustomization for personalized user experiences or to support a diverse range\nof tasks. Although each adapter is lightweight in isolation, their aggregate\ncost becomes substantial at scale. To address this, we propose LoRAQuant, a\nmixed-precision post-training quantization method tailored to LoRA.\nSpecifically, LoRAQuant reparameterizes each adapter by singular value\ndecomposition (SVD) to concentrate the most important information into specific\nrows and columns. This makes it possible to quantize the important components\nto higher precision, while quantizing the rest to ultra-low bitwidth. We\nconduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B\nmodels on mathematical reasoning, coding, and summarization tasks. Results show\nthat our LoRAQuant uses significantly lower bits than other quantization\nmethods, but achieves comparable or even higher performance.\n","authors":["Amir Reza Mirzaei","Yuqiao Wen","Yanshuai Cao","Lili Mou"],"pdf_url":"https://arxiv.org/pdf/2510.26690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26688v1","updated":"2025-10-30T16:57:13Z","published":"2025-10-30T16:57:13Z","title":"FlowQ-Net: A Generative Framework for Automated Quantum Circuit Design","summary":"  Designing efficient quantum circuits is a central bottleneck to exploring the\npotential of quantum computing, particularly for noisy intermediate-scale\nquantum (NISQ) devices, where circuit efficiency and resilience to errors are\nparamount. The search space of gate sequences grows combinatorially, and\nhandcrafted templates often waste scarce qubit and depth budgets. We introduce\n\\textsc{FlowQ-Net} (Flow-based Quantum design Network), a generative framework\nfor automated quantum circuit synthesis based on Generative Flow Networks\n(GFlowNets). This framework learns a stochastic policy to construct circuits\nsequentially, sampling them in proportion to a flexible, user-defined reward\nfunction that can encode multiple design objectives such as performance, depth,\nand gate count. This approach uniquely enables the generation of a diverse\nensemble of high-quality circuits, moving beyond single-solution optimization.\nWe demonstrate the efficacy of \\textsc{FlowQ-Net} through an extensive set of\nsimulations. We apply our method to Variational Quantum Algorithm (VQA) ansatz\ndesign for molecular ground state estimation, Max-Cut, and image\nclassification, key challenges in near-term quantum computing. Circuits\ndesigned by \\textsc{FlowQ-Net} achieve significant improvements, yielding\ncircuits that are 10$\\times$-30$\\times$ more compact in terms of parameters,\ngates, and depth compared to commonly used unitary baselines, without\ncompromising accuracy. This trend holds even when subjected to error profiles\nfrom real-world quantum devices. Our results underline the potential of\ngenerative models as a general-purpose methodology for automated quantum\ncircuit design, offering a promising path towards more efficient quantum\nalgorithms and accelerating scientific discovery in the quantum domain.\n","authors":["Jun Dai","Michael Rizvi-Martel","Guillaume Rabusseau"],"pdf_url":"https://arxiv.org/pdf/2510.26688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17434v5","updated":"2025-10-30T16:47:32Z","published":"2023-11-29T08:26:18Z","title":"GSE: Group-wise Sparse and Explainable Adversarial Attacks","summary":"  Sparse adversarial attacks fool deep neural networks (DNNs) through minimal\npixel perturbations, often regularized by the $\\ell_0$ norm. Recent efforts\nhave replaced this norm with a structural sparsity regularizer, such as the\nnuclear group norm, to craft group-wise sparse adversarial attacks. The\nresulting perturbations are thus explainable and hold significant practical\nrelevance, shedding light on an even greater vulnerability of DNNs. However,\ncrafting such attacks poses an optimization challenge, as it involves computing\nnorms for groups of pixels within a non-convex objective. We address this by\npresenting a two-phase algorithm that generates group-wise sparse attacks\nwithin semantically meaningful areas of an image. Initially, we optimize a\nquasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored\nfor non-convex programming. Subsequently, the algorithm transitions to a\nprojected Nesterov's accelerated gradient descent with $2-$norm regularization\napplied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and\nImageNet datasets demonstrate a remarkable increase in group-wise sparsity,\ne.g., $50.9\\%$ on CIFAR-10 and $38.4\\%$ on ImageNet (average case, targeted\nattack). This performance improvement is accompanied by significantly faster\ncomputation times, improved explainability, and a $100\\%$ attack success rate.\n","authors":["Shpresim Sadiku","Moritz Wagner","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2311.17434v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26679v1","updated":"2025-10-30T16:47:26Z","published":"2025-10-30T16:47:26Z","title":"Tight Differentially Private PCA via Matrix Coherence","summary":"  We revisit the task of computing the span of the top $r$ singular vectors\n$u_1, \\ldots, u_r$ of a matrix under differential privacy. We show that a\nsimple and efficient algorithm -- based on singular value decomposition and\nstandard perturbation mechanisms -- returns a private rank-$r$ approximation\nwhose error depends only on the \\emph{rank-$r$ coherence} of $u_1, \\ldots, u_r$\nand the spectral gap $\\sigma_r - \\sigma_{r+1}$. This resolves a question posed\nby Hardt and Roth~\\cite{hardt2013beyond}. Our estimator outperforms the state\nof the art -- significantly so in some regimes. In particular, we show that in\nthe dense setting, it achieves the same guarantees for single-spike PCA in the\nWishart model as those attained by optimal non-private algorithms, whereas\nprior private algorithms failed to do so.\n  In addition, we prove that (rank-$r$) coherence does not increase under\nGaussian perturbations. This implies that any estimator based on the Gaussian\nmechanism -- including ours -- preserves the coherence of the input. We\nconjecture that similar behavior holds for other structured models, including\nplanted problems in graphs.\n  We also explore applications of coherence to graph problems. In particular,\nwe present a differentially private algorithm for Max-Cut and other constraint\nsatisfaction problems under low coherence assumptions.\n","authors":["Tommaso d'Orsi","Gleb Novikov"],"pdf_url":"https://arxiv.org/pdf/2510.26679v1.pdf","comment":"SODA 2026; equal contribution"},{"id":"http://arxiv.org/abs/2510.26672v1","updated":"2025-10-30T16:42:09Z","published":"2025-10-30T16:42:09Z","title":"Action-Driven Processes for Continuous-Time Control","summary":"  At the heart of reinforcement learning are actions -- decisions made in\nresponse to observations of the environment. Actions are equally fundamental in\nthe modeling of stochastic processes, as they trigger discontinuous state\ntransitions and enable the flow of information through large, complex systems.\nIn this paper, we unify the perspectives of stochastic processes and\nreinforcement learning through action-driven processes, and illustrate their\napplication to spiking neural networks. Leveraging ideas from\ncontrol-as-inference, we show that minimizing the Kullback-Leibler divergence\nbetween a policy-driven true distribution and a reward-driven model\ndistribution for a suitably defined action-driven process is equivalent to\nmaximum entropy reinforcement learning.\n","authors":["Ruimin He","Shaowei Lin"],"pdf_url":"https://arxiv.org/pdf/2510.26672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26656v1","updated":"2025-10-30T16:23:46Z","published":"2025-10-30T16:23:46Z","title":"Heuristic Adaptation of Potentially Misspecified Domain Support for\n  Likelihood-Free Inference in Stochastic Dynamical Systems","summary":"  In robotics, likelihood-free inference (LFI) can provide the domain\ndistribution that adapts a learnt agent in a parametric set of deployment\nconditions. LFI assumes an arbitrary support for sampling, which remains\nconstant as the initial generic prior is iteratively refined to more\ndescriptive posteriors. However, a potentially misspecified support can lead to\nsuboptimal, yet falsely certain, posteriors. To address this issue, we propose\nthree heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the\nposterior mode shift over inference steps in its own way and, when integrated\ninto an LFI step, adapts the support alongside posterior inference. We first\nexpose the support misspecification issue and evaluate our heuristics using\nstochastic dynamical benchmarks. We then evaluate the impact of heuristic\nsupport adaptation on parameter inference and policy learning for a dynamic\ndeformable linear object (DLO) manipulation task. Inference results in a finer\nlength and stiffness classification for a parametric set of DLOs. When the\nresulting posteriors are used as domain distributions for sim-based policy\nlearning, they lead to more robust object-centric agent performance.\n","authors":["Georgios Kamaras","Craig Innes","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2510.26656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26646v1","updated":"2025-10-30T16:12:01Z","published":"2025-10-30T16:12:01Z","title":"Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in\n  Dynamic Environments","summary":"  This paper presents a hierarchical path-planning and control framework that\ncombines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with\na low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller\nfor continuous actuation. The high-level module selects behaviors and\nsub-goals; the low-level module executes smooth velocity commands. We design a\npractical reward shaping scheme (direction, distance, obstacle avoidance,\naction smoothness, collision penalty, time penalty, and progress), together\nwith a LiDAR-based safety gate that prevents unsafe motions. The system is\nimplemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,\nincluding success rate, collision rate, path efficiency, and re-planning\nefficiency, in dynamic and partially observable environments. Experiments show\nimproved success rate and sample efficiency over single-algorithm baselines\n(DQN or TD3 alone) and rule-based planners, with better generalization to\nunseen obstacle configurations and reduced abrupt control changes. Code and\nevaluation scripts are available at the project repository.\n","authors":["Xiaoyi He","Danggui Chen","Zhenshuo Zhang","Zimeng Bai"],"pdf_url":"https://arxiv.org/pdf/2510.26646v1.pdf","comment":"6 pages, 5 figures; ROS+Gazebo (TurtleBot3) implementation;\n  evaluation with PathBench metrics; code (primary):\n  https://github.com/MayaCHEN-github/HierarchicalRL-robot-navigation; mirror\n  (for reproducibility): https://github.com/ShowyHe/DRL-robot-navigation"},{"id":"http://arxiv.org/abs/2510.26645v1","updated":"2025-10-30T16:11:39Z","published":"2025-10-30T16:11:39Z","title":"Curly Flow Matching for Learning Non-gradient Field Dynamics","summary":"  Modeling the transport dynamics of natural processes from population-level\nobservations is a ubiquitous problem in the natural sciences. Such models rely\non key assumptions about the underlying process in order to enable faithful\nlearning of governing dynamics that mimic the actual system behavior. The de\nfacto assumption in current approaches relies on the principle of least action\nthat results in gradient field dynamics and leads to trajectories minimizing an\nenergy functional between two probability measures. However, many real-world\nsystems, such as cell cycles in single-cell RNA, are known to exhibit\nnon-gradient, periodic behavior, which fundamentally cannot be captured by\ncurrent state-of-the-art methods such as flow and bridge matching. In this\npaper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is\ncapable of learning non-gradient field dynamics by designing and solving a\nSchr\\\"odinger bridge problem with a non-zero drift reference process -- in\nstark contrast to typical zero-drift reference processes -- which is\nconstructed using inferred velocities in addition to population snapshot data.\nWe showcase Curly-FM by solving the trajectory inference problems for single\ncells, computational fluid dynamics, and ocean currents with approximate\nvelocities. We demonstrate that Curly-FM can learn trajectories that better\nmatch both the reference process and population marginals. Curly-FM expands\nflow matching models beyond the modeling of populations and towards the\nmodeling of known periodic behavior in physical systems. Our code repository is\naccessible at: https://github.com/kpetrovicc/curly-flow-matching.git\n","authors":["Katarina Petrović","Lazar Atanackovic","Viggo Moro","Kacper Kapuśniak","İsmail İlkan Ceylan","Michael Bronstein","Avishek Joey Bose","Alexander Tong"],"pdf_url":"https://arxiv.org/pdf/2510.26645v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26643v1","updated":"2025-10-30T16:09:51Z","published":"2025-10-30T16:09:51Z","title":"MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection","summary":"  Anomaly detection is a fundamental task for time series analytics with\nimportant implications for the downstream performance of many applications.\nDespite increasing academic interest and the large number of methods proposed\nin the literature, recent benchmarks and evaluation studies demonstrated that\nno overall best anomaly detection methods exist when applied to very\nheterogeneous time series datasets. Therefore, the only scalable and viable\nsolution to solve anomaly detection over very different time series collected\nfrom diverse domains is to propose a model selection method that will select,\nbased on time series characteristics, the best anomaly detection methods to\nrun. Existing AutoML solutions are, unfortunately, not directly applicable to\ntime series anomaly detection, and no evaluation of time series-based\napproaches for model selection exists. Towards that direction, this paper\nstudies the performance of time series classification methods used as model\nselection for anomaly detection. In total, we evaluate 234 model configurations\nderived from 16 base classifiers across more than 1980 time series, and we\npropose the first extensive experimental evaluation of time series\nclassification as model selection for anomaly detection. Our results\ndemonstrate that model selection methods outperform every single anomaly\ndetection method while being in the same order of magnitude regarding execution\ntime. This evaluation is the first step to demonstrate the accuracy and\nefficiency of time series classification algorithms for anomaly detection, and\nrepresents a strong baseline that can then be used to guide the model selection\nstep in general AutoML pipelines. Preprint version of an article accepted at\nthe VLDB Journal.\n","authors":["Emmanouil Sylligardos","John Paparrizos","Themis Palpanas","Pierre Senellart","Paul Boniol"],"pdf_url":"https://arxiv.org/pdf/2510.26643v1.pdf","comment":"25 pages, 13 figures, VLDB Journal"},{"id":"http://arxiv.org/abs/2507.00927v3","updated":"2025-10-30T16:05:04Z","published":"2025-07-01T16:34:19Z","title":"Understanding Generalization in Node and Link Prediction","summary":"  Using message-passing graph neural networks (MPNNs) for node and link\nprediction is crucial in various scientific and industrial domains, which has\nled to the development of diverse MPNN architectures. Besides working well in\npractical settings, their ability to generalize beyond the training set remains\npoorly understood. While some studies have explored MPNNs' generalization in\ngraph-level prediction tasks, much less attention has been given to node- and\nlink-level predictions. Existing works often rely on unrealistic i.i.d.\\@\nassumptions, overlooking possible correlations between nodes or links, and\nassuming fixed aggregation and impractical loss functions while neglecting the\ninfluence of graph structure. In this work, we introduce a unified framework to\nanalyze the generalization properties of MPNNs in inductive and transductive\nnode and link prediction settings, incorporating diverse architectural\nparameters and loss functions and quantifying the influence of graph structure.\nAdditionally, our proposed generalization framework can be applied beyond\ngraphs to any classification task under the inductive or transductive setting.\nOur empirical study supports our theoretical insights, deepening our\nunderstanding of MPNNs' generalization capabilities in these tasks.\n","authors":["Antonis Vasileiou","Timo Stoll","Christopher Morris"],"pdf_url":"https://arxiv.org/pdf/2507.00927v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2412.07106"},{"id":"http://arxiv.org/abs/2510.26633v1","updated":"2025-10-30T16:02:53Z","published":"2025-10-30T16:02:53Z","title":"Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian\n  Optimization","summary":"  Bayesian Optimization (BO) has the potential to solve various combinatorial\ntasks, ranging from materials science to neural architecture search. However,\nBO requires specialized kernels to effectively model combinatorial domains.\nRecent efforts have introduced several combinatorial kernels, but the\nrelationships among them are not well understood. To bridge this gap, we\ndevelop a unifying framework based on heat kernels, which we derive in a\nsystematic way and express as simple closed-form expressions. Using this\nframework, we prove that many successful combinatorial kernels are either\nrelated or equivalent to heat kernels, and validate this theoretical claim in\nour experiments. Moreover, our analysis confirms and extends the results\npresented in Bounce: certain algorithms' performance decreases substantially\nwhen the unknown optima of the function do not have a certain structure. In\ncontrast, heat kernels are not sensitive to the location of the optima. Lastly,\nwe show that a fast and simple pipeline, relying on heat kernels, is able to\nachieve state-of-the-art results, matching or even outperforming certain slow\nor complex algorithms.\n","authors":["Colin Doumont","Victor Picheny","Viacheslav Borovitskiy","Henry Moss"],"pdf_url":"https://arxiv.org/pdf/2510.26633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21258v2","updated":"2025-10-30T16:01:21Z","published":"2025-08-28T23:09:54Z","title":"RelP: Faithful and Efficient Circuit Discovery in Language Models via\n  Relevance Patching","summary":"  Activation patching is a standard method in mechanistic interpretability for\nlocalizing the components of a model responsible for specific behaviors, but it\nis computationally expensive to apply at scale. Attribution patching offers a\nfaster, gradient-based approximation, yet suffers from noise and reduced\nreliability in deep, highly non-linear networks. In this work, we introduce\nRelevance Patching (RelP), which replaces the local gradients in attribution\npatching with propagation coefficients derived from Layer-wise Relevance\nPropagation (LRP). LRP propagates the network's output backward through the\nlayers, redistributing relevance to lower-level components according to local\npropagation rules that ensure properties such as relevance conservation or\nimproved signal-to-noise ratio. Like attribution patching, RelP requires only\ntwo forward passes and one backward pass, maintaining computational efficiency\nwhile improving faithfulness. We validate RelP across a range of models and\ntasks, showing that it more accurately approximates activation patching than\nstandard attribution patching, particularly when analyzing residual stream and\nMLP outputs in the Indirect Object Identification (IOI) task. For instance, for\nMLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation\nof 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by\nRelP. Additionally, we compare the faithfulness of sparse feature circuits\nidentified by RelP and Integrated Gradients (IG), showing that RelP achieves\ncomparable faithfulness without the extra computational cost associated with\nIG.\n","authors":["Farnoush Rezaei Jafari","Oliver Eberle","Ashkan Khakzar","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2508.21258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.01988v3","updated":"2025-10-30T15:47:25Z","published":"2025-10-02T13:07:37Z","title":"PepCompass: Navigating peptide embedding spaces using Riemannian\n  Geometry","summary":"  Antimicrobial peptide discovery is challenged by the astronomical size of\npeptide space and the relative scarcity of active peptides. Generative models\nprovide continuous latent \"maps\" of peptide space, but conventionally ignore\ndecoder-induced geometry and rely on flat Euclidean metrics, rendering\nexploration and optimization distorted and inefficient. Prior manifold-based\nremedies assume fixed intrinsic dimensionality, which critically fails in\npractice for peptide data. Here, we introduce PepCompass, a geometry-aware\nframework for peptide exploration and optimization. At its core, we define a\nUnion of $\\kappa$-Stable Riemannian Manifolds $\\mathbb{M}^{\\kappa}$, a family\nof decoder-induced manifolds that captures local geometry while ensuring\ncomputational stability. We propose two local exploration methods: Second-Order\nRiemannian Brownian Efficient Sampling, which provides a convergent\nsecond-order approximation to Riemannian Brownian motion, and Mutation\nEnumeration in Tangent Space, which reinterprets tangent directions as discrete\namino-acid substitutions. Combining these yields Local Enumeration Bayesian\nOptimization (LE-BO), an efficient algorithm for local activity optimization.\nFinally, we introduce Potential-minimizing Geodesic Search (PoGS), which\ninterpolates between prototype embeddings along property-enriched geodesics,\nbiasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro\nvalidation confirms the effectiveness of PepCompass: PoGS yields four novel\nseeds, and subsequent optimization with LE-BO discovers 25 highly active\npeptides with broad-spectrum activity, including against resistant bacterial\nstrains. These results demonstrate that geometry-informed exploration provides\na powerful new paradigm for antimicrobial peptide design.\n","authors":["Marcin Możejko","Adam Bielecki","Jurand Prądzyński","Marcin Traskowski","Antoni Janowski","Hyun-Su Lee","Marcelo Der Torossian Torres","Michał Kmicikiewicz","Paulina Szymczak","Karol Jurasz","Michał Kucharczyk","Cesar de la Fuente-Nunez","Ewa Szczurek"],"pdf_url":"https://arxiv.org/pdf/2510.01988v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17773v3","updated":"2025-10-30T15:43:22Z","published":"2025-05-23T11:44:02Z","title":"C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in\n  Large Language Models","summary":"  Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (C-LoRA) as a novel\nuncertainty-aware and parameter efficient fine-tuning approach, by developing\nnew lightweight LoRA modules contextualized to each input data sample to\ndynamically adapt uncertainty estimates. Incorporating data-driven contexts\ninto the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments on LLaMA2-7B models demonstrate that C-LoRA consistently\noutperforms the state-of-the-art uncertainty-aware LoRA methods in both\nuncertainty quantification and model generalization. Ablation studies further\nconfirm the critical role of our contextual modules in capturing\nsample-specific uncertainties. C-LoRA sets a new standard for robust,\nuncertainty-aware LLM fine-tuning in few-shot regimes. Although our experiments\nare limited to 7B models, our method is architecture-agnostic and, in\nprinciple, applies beyond this scale; studying its scaling to larger models\nremains an open problem. Our code is available at\nhttps://github.com/ahra99/c_lora.\n","authors":["Amir Hossein Rahmati","Sanket Jantre","Weifeng Zhang","Yucheng Wang","Byung-Jun Yoon","Nathan M. Urban","Xiaoning Qian"],"pdf_url":"https://arxiv.org/pdf/2505.17773v3.pdf","comment":"Conference on Neural Information Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.26616v1","updated":"2025-10-30T15:41:43Z","published":"2025-10-30T15:41:43Z","title":"Aeolus: A Multi-structural Flight Delay Dataset","summary":"  We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed\nto advance research on flight delay prediction and support the development of\nfoundation models for tabular data. Existing datasets in this domain are\ntypically limited to flat tabular structures and fail to capture the\nspatiotemporal dynamics inherent in delay propagation. Aeolus addresses this\nlimitation by providing three aligned modalities: (i) a tabular dataset with\nrich operational, meteorological, and airportlevel features for over 50 million\nflights; (ii) a flight chain module that models delay propagation along\nsequential flight legs, capturing upstream and downstream dependencies; and\n(iii) a flight network graph that encodes shared aircraft, crew, and airport\nresource connections, enabling cross-flight relational reasoning. The dataset\nis carefully constructed with temporal splits, comprehensive features, and\nstrict leakage prevention to support realistic and reproducible machine\nlearning evaluation. Aeolus supports a broad range of tasks, including\nregression, classification, temporal structure modeling, and graph learning,\nserving as a unified benchmark across tabular, sequential, and graph\nmodalities. We release baseline experiments and preprocessing tools to\nfacilitate adoption. Aeolus fills a key gap for both domain-specific modeling\nand general-purpose structured data research.Our source code and data can be\naccessed at https://github.com/Flnny/Delay-data\n","authors":["Lin Xu","Xinyun Yuan","Yuxuan Liang","Suwan Yin","Yuankai Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14904v2","updated":"2025-10-30T15:39:25Z","published":"2025-10-16T17:20:22Z","title":"MaskCaptioner: Learning to Jointly Segment and Caption Object\n  Trajectories in Videos","summary":"  Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.\n","authors":["Gabriel Fiastre","Antoine Yang","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2510.14904v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.26609v1","updated":"2025-10-30T15:37:40Z","published":"2025-10-30T15:37:40Z","title":"CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for\n  Satellite Sensing","summary":"  Accurate and timely crop yield prediction is crucial for global food security\nand modern agricultural management. Traditional methods often lack the\nscalability and granularity required for precision farming. This paper\nintroduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder\nfor Satellite Sensing), a deep learning model designed for high-resolution,\nintra-field canola yield prediction. CYPRESS leverages a pre-trained,\nlarge-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for\na continuous regression task, transforming multi-temporal satellite imagery\ninto dense, pixel-level yield maps. Evaluated on a comprehensive dataset from\nthe Canadian Prairies, CYPRESS demonstrates superior performance over existing\ndeep learning-based yield prediction models, highlighting the effectiveness of\nfine-tuning foundation models for specialized agricultural applications. By\nproviding a continuous, high-resolution output, CYPRESS offers a more\nactionable tool for precision agriculture than conventional classification or\ncounty-level aggregation methods. This work validates a novel approach that\nbridges the gap between large-scale Earth observation and on-farm\ndecision-making, offering a scalable solution for detailed agricultural\nmonitoring.\n","authors":["Shayan Nejadshamsi","Yuanyuan Zhang","Shadi Zaki","Brock Porth","Lysa Porth","Vahab Khoshdel"],"pdf_url":"https://arxiv.org/pdf/2510.26609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26607v1","updated":"2025-10-30T15:36:39Z","published":"2025-10-30T15:36:39Z","title":"Wasserstein Regression as a Variational Approximation of Probabilistic\n  Trajectories through the Bernstein Basis","summary":"  This paper considers the problem of regression over distributions, which is\nbecoming increasingly important in machine learning. Existing approaches often\nignore the geometry of the probability space or are computationally expensive.\nTo overcome these limitations, a new method is proposed that combines the\nparameterization of probability trajectories using a Bernstein basis and the\nminimization of the Wasserstein distance between distributions. The key idea is\nto model a conditional distribution as a smooth probability trajectory defined\nby a weighted sum of Gaussian components whose parameters -- the mean and\ncovariance -- are functions of the input variable constructed using Bernstein\npolynomials. The loss function is the averaged squared Wasserstein distance\nbetween the predicted Gaussian distributions and the empirical data, which\ntakes into account the geometry of the distributions. An autodiff-based\noptimization method is used to train the model. Experiments on synthetic\ndatasets that include complex trajectories demonstrated that the proposed\nmethod provides competitive approximation quality in terms of the Wasserstein\ndistance, Energy Distance, and RMSE metrics, especially in cases of pronounced\nnonlinearity. The model demonstrates trajectory smoothness that is better than\nor comparable to alternatives and robustness to changes in data structure,\nwhile maintaining high interpretability due to explicit parameterization via\ncontrol points. The developed approach represents a balanced solution that\ncombines geometric accuracy, computational practicality, and interpretability.\nProspects for further research include extending the method to non-Gaussian\ndistributions, applying entropy regularization to speed up computations, and\nadapting the approach to working with high-dimensional data for approximating\nsurfaces and more complex structures.\n","authors":["Maksim Maslov","Alexander Kugaevskikh","Matthew Ivanov"],"pdf_url":"https://arxiv.org/pdf/2510.26607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.08604v2","updated":"2025-10-30T15:33:58Z","published":"2025-10-07T09:40:20Z","title":"LatentBreak: Jailbreaking Large Language Models through Latent Space\n  Feedback","summary":"  Jailbreaks are adversarial attacks designed to bypass the built-in safety\nmechanisms of large language models. Automated jailbreaks typically optimize an\nadversarial suffix or adapt long prompt templates by forcing the model to\ngenerate the initial part of a restricted or harmful response. In this work, we\nshow that existing jailbreak attacks that leverage such mechanisms to unlock\nthe model response can be detected by a straightforward perplexity-based\nfiltering on the input prompt. To overcome this issue, we propose LatentBreak,\na white-box jailbreak attack that generates natural adversarial prompts with\nlow perplexity capable of evading such defenses. LatentBreak substitutes words\nin the input prompt with semantically-equivalent ones, preserving the initial\nintent of the prompt, instead of adding high-perplexity adversarial suffixes or\nlong templates. These words are chosen by minimizing the distance in the latent\nspace between the representation of the adversarial prompt and that of harmless\nrequests. Our extensive evaluation shows that LatentBreak leads to shorter and\nlow-perplexity prompts, thus outperforming competing jailbreak algorithms\nagainst perplexity-based filters on multiple safety-aligned models.\n","authors":["Raffaele Mura","Giorgio Piras","Kamilė Lukošiūtė","Maura Pintor","Amin Karbasi","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2510.08604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17197v2","updated":"2025-10-30T15:26:13Z","published":"2025-09-21T18:54:54Z","title":"SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal\n  Processing","summary":"  Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings.\n","authors":["Junlong Ke","Qiying Hu","Shenghai Yuan","Yuecong Xu","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2509.17197v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2510.26593v1","updated":"2025-10-30T15:18:07Z","published":"2025-10-30T15:18:07Z","title":"Hybrid Physical-Neural Simulator for Fast Cosmological Hydrodynamics","summary":"  Cosmological field-level inference requires differentiable forward models\nthat solve the challenging dynamics of gas and dark matter under hydrodynamics\nand gravity. We propose a hybrid approach where gravitational forces are\ncomputed using a differentiable particle-mesh solver, while the hydrodynamics\nare parametrized by a neural network that maps local quantities to an effective\npressure field. We demonstrate that our method improves upon alternative\napproaches, such as an Enthalpy Gradient Descent baseline, both at the field\nand summary-statistic level. The approach is furthermore highly data efficient,\nwith a single reference simulation of cosmological structure formation being\nsufficient to constrain the neural pressure model. This opens the door for\nfuture applications where the model is fit directly to observational data,\nrather than a training set of simulations.\n","authors":["Arne Thomsen","Tilman Tröster","François Lanusse"],"pdf_url":"https://arxiv.org/pdf/2510.26593v1.pdf","comment":"Accepted to the NeurIPS 2025 Workshop on Machine Learning and the\n  Physical Sciences"},{"id":"http://arxiv.org/abs/2510.26586v1","updated":"2025-10-30T15:13:25Z","published":"2025-10-30T15:13:25Z","title":"Physics-Informed Mixture Models and Surrogate Models for Precision\n  Additive Manufacturing","summary":"  In this study, we leverage a mixture model learning approach to identify\ndefects in laser-based Additive Manufacturing (AM) processes. By incorporating\nphysics based principles, we also ensure that the model is sensitive to\nmeaningful physical parameter variations. The empirical evaluation was\nconducted by analyzing real-world data from two AM processes: Directed Energy\nDeposition and Laser Powder Bed Fusion. In addition, we also studied the\nperformance of the developed framework over public datasets with different\nalloy type and experimental parameter information. The results show the\npotential of physics-guided mixture models to examine the underlying physical\nbehavior of an AM system.\n","authors":["Sebastian Basterrech","Shuo Shan","Debabrata Adhikari","Sankhya Mohanty"],"pdf_url":"https://arxiv.org/pdf/2510.26586v1.pdf","comment":"Five pages, four figures, to be presented at the AI in Science\n  Summit, Denmark, November, 2025"},{"id":"http://arxiv.org/abs/2410.06324v4","updated":"2025-10-30T15:07:47Z","published":"2024-10-08T20:01:39Z","title":"Differentiation Through Black-Box Quadratic Programming Solvers","summary":"  Differentiable optimization has attracted significant research interest,\nparticularly for quadratic programming (QP). Existing approaches for\ndifferentiating the solution of a QP with respect to its defining parameters\noften rely on specific integrated solvers. This integration limits their\napplicability, including their use in neural network architectures and bi-level\noptimization tasks, restricting users to a narrow selection of solver choices.\nTo address this limitation, we introduce dQP, a modular and solver-agnostic\nframework for plug-and-play differentiation of virtually any QP solver. A key\ninsight we leverage to achieve modularity is that, once the active set of\ninequality constraints is known, both the solution and its derivative can be\nexpressed using simplified linear systems that share the same matrix. This\nformulation fully decouples the computation of the QP solution from its\ndifferentiation. Building on this result, we provide a minimal-overhead,\nopen-source implementation ( https://github.com/cwmagoon/dQP ) that seamlessly\nintegrates with over 15 state-of-the-art solvers. Comprehensive benchmark\nexperiments demonstrate dQP's robustness and scalability, particularly\nhighlighting its advantages in large-scale sparse problems.\n","authors":["Connor W. Magoon","Fengyu Yang","Noam Aigerman","Shahar Z. Kovalsky"],"pdf_url":"https://arxiv.org/pdf/2410.06324v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26577v1","updated":"2025-10-30T15:04:36Z","published":"2025-10-30T15:04:36Z","title":"Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference\n  in Large Language Models","summary":"  Large Language Models (LLMs) face significant inference latency challenges\nstemming from their autoregressive design and large size. To address this,\nspeculative decoding emerges as a solution, enabling the simultaneous\ngeneration and validation of multiple tokens. While recent approaches like\nEAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,\nthey often neglect the impact of crucial system variables such as GPU devices\nand batch sizes.\n  Therefore, we introduce a new dynamic tree decoding approach called CAST that\ntakes into account inference costs, including factors such as GPU\nconfigurations and batch sizes, to dynamically refine the tree structure.\nThrough comprehensive experimentation across six diverse tasks and utilizing\nsix distinct LLMs, our methodology demonstrates remarkable results, achieving\nspeeds up to 5.2 times faster than conventional decoding methods. Moreover, it\ngenerally outperforms existing state-of-the-art techniques from 5% to 20%.\n","authors":["Yinrong Hong","Zhiquan Tan","Kai Hu"],"pdf_url":"https://arxiv.org/pdf/2510.26577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05978v2","updated":"2025-10-30T15:02:26Z","published":"2025-04-08T12:33:38Z","title":"Smart Exploration in Reinforcement Learning using Bounded Uncertainty\n  Models","summary":"  Reinforcement learning (RL) is a powerful framework for decision-making in\nuncertain environments, but it often requires large amounts of data to learn an\noptimal policy. We address this challenge by incorporating prior model\nknowledge to guide exploration and accelerate the learning process.\nSpecifically, we assume access to a model set that contains the true transition\nkernel and reward function. We optimize over this model set to obtain upper and\nlower bounds on the Q-function, which are then used to guide the exploration of\nthe agent. We provide theoretical guarantees on the convergence of the\nQ-function to the optimal Q-function under the proposed class of exploring\npolicies. Furthermore, we also introduce a data-driven regularized version of\nthe model set optimization problem that ensures the convergence of the class of\nexploring policies to the optimal policy. Lastly, we show that when the model\nset has a specific structure, namely the bounded-parameter MDP (BMDP)\nframework, the regularized model set optimization problem becomes convex and\nsimple to implement. In this setting, we also prove finite-time convergence to\nthe optimal policy under mild assumptions. We demonstrate the effectiveness of\nthe proposed exploration strategy, which we call BUMEX (Bounded Uncertainty\nModel-based Exploration), in a simulation study. The results indicate that the\nproposed method can significantly accelerate learning in benchmark examples. A\ntoolbox is available at https://github.com/JvHulst/BUMEX.\n","authors":["J. S. van Hulst","W. P. M. H. Heemels","D. J. Antunes"],"pdf_url":"https://arxiv.org/pdf/2504.05978v2.pdf","comment":"Accepted for Presentation at 64th IEEE Conference on Decision and\n  Control, CDC 2025, Rio de Janeiro, Brazil, 2025"},{"id":"http://arxiv.org/abs/2510.26566v1","updated":"2025-10-30T14:56:07Z","published":"2025-10-30T14:56:07Z","title":"Multiclass Local Calibration With the Jensen-Shannon Distance","summary":"  Developing trustworthy Machine Learning (ML) models requires their predicted\nprobabilities to be well-calibrated, meaning they should reflect true-class\nfrequencies. Among calibration notions in multiclass classification, strong\ncalibration is the most stringent, as it requires all predicted probabilities\nto be simultaneously calibrated across all classes. However, existing\napproaches to multiclass calibration lack a notion of distance among inputs,\nwhich makes them vulnerable to proximity bias: predictions in sparse regions of\nthe feature space are systematically miscalibrated. This is especially relevant\nin high-stakes settings, such as healthcare, where the sparse instances are\nexactly those most at risk of biased treatment. In this work, we address this\nmain shortcoming by introducing a local perspective on multiclass calibration.\nFirst, we formally define multiclass local calibration and establish its\nrelationship with strong calibration. Second, we theoretically analyze the\npitfalls of existing evaluation metrics when applied to multiclass local\ncalibration. Third, we propose a practical method for enhancing local\ncalibration in Neural Networks, which enforces alignment between predicted\nprobabilities and local estimates of class frequencies using the Jensen-Shannon\ndistance. Finally, we empirically validate our approach against existing\nmulticlass calibration techniques.\n","authors":["Cesare Barbera","Lorenzo Perini","Giovanni De Toni","Andrea Passerini","Andrea Pugnana"],"pdf_url":"https://arxiv.org/pdf/2510.26566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.04226v4","updated":"2025-10-30T14:52:48Z","published":"2025-10-05T14:29:15Z","title":"Epistemic Diversity and Knowledge Collapse in Large Language Models","summary":"  Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation\n","authors":["Dustin Wright","Sarah Masud","Jared Moore","Srishti Yadav","Maria Antoniak","Chan Young Park","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2510.04226v4.pdf","comment":"16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for\n  table 3, random effect is the model version; v3 changelog: Fixed minor\n  formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model\n  description"},{"id":"http://arxiv.org/abs/2510.26560v1","updated":"2025-10-30T14:51:03Z","published":"2025-10-30T14:51:03Z","title":"On Measuring Localization of Shortcuts in Deep Networks","summary":"  Shortcuts, spurious rules that perform well during training but fail to\ngeneralize, present a major challenge to the reliability of deep networks\n(Geirhos et al., 2020). However, the impact of shortcuts on feature\nrepresentations remains understudied, obstructing the design of principled\nshortcut-mitigation methods. To overcome this limitation, we investigate the\nlayer-wise localization of shortcuts in deep models. Our novel experiment\ndesign quantifies the layer-wise contribution to accuracy degradation caused by\na shortcut-inducing skew by counterfactual training on clean and skewed\ndatasets. We employ our design to study shortcuts on CIFAR-10, Waterbirds, and\nCelebA datasets across VGG, ResNet, DeiT, and ConvNeXt architectures. We find\nthat shortcut learning is not localized in specific layers but distributed\nthroughout the network. Different network parts play different roles in this\nprocess: shallow layers predominantly encode spurious features, while deeper\nlayers predominantly forget core features that are predictive on clean data. We\nalso analyze the differences in localization and describe its principal axes of\nvariation. Finally, our analysis of layer-wise shortcut-mitigation strategies\nsuggests the hardness of designing general methods, supporting dataset- and\narchitecture-specific approaches instead.\n","authors":["Nikita Tsoy","Nikola Konstantinov"],"pdf_url":"https://arxiv.org/pdf/2510.26560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26557v1","updated":"2025-10-30T14:47:57Z","published":"2025-10-30T14:47:57Z","title":"Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices","summary":"  Deploying machine learning models on compute-constrained devices has become a\nkey building block of modern IoT applications. In this work, we present a\ncompression scheme for boosted decision trees, addressing the growing need for\nlightweight machine learning models. Specifically, we provide techniques for\ntraining compact boosted decision tree ensembles that exhibit a reduced memory\nfootprint by rewarding, among other things, the reuse of features and\nthresholds during training. Our experimental evaluation shows that models\nachieved the same performance with a compression ratio of 4-16x compared to\nLightGBM models using an adapted training process and an alternative memory\nlayout. Once deployed, the corresponding IoT devices can operate independently\nof constant communication or external energy supply, and, thus, autonomously,\nrequiring only minimal computing power and energy. This capability opens the\ndoor to a wide range of IoT applications, including remote monitoring, edge\nanalytics, and real-time decision making in isolated or power-limited\nenvironments.\n","authors":["Jan Stenkamp","Nina Herrmann","Benjamin Karic","Stefan Oehmcke","Fabian Gieseke"],"pdf_url":"https://arxiv.org/pdf/2510.26557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01369v2","updated":"2025-10-30T14:45:40Z","published":"2025-06-02T06:54:29Z","title":"Incentivizing LLMs to Self-Verify Their Answers","summary":"  Large Language Models (LLMs) have demonstrated remarkable progress in complex\nreasoning tasks through both post-training and test-time scaling laws. While\nprevalent test-time scaling approaches are often realized by using external\nreward models to guide the model generation process, we find that only marginal\ngains can be acquired when scaling a model post-trained on specific reasoning\ntasks. We identify that the limited improvement stems from distribution\ndiscrepancies between the specific post-trained generator and the general\nreward model. To address this, we propose a framework that incentivizes LLMs to\nself-verify their own answers. By unifying answer generation and verification\nwithin a single reinforcement learning (RL) process, we train models that can\neffectively assess the correctness of their own solutions. The trained model\ncan further scale its performance at inference time by verifying its\ngenerations, without the need for external verifiers. We train our\nself-verification models based on Qwen2.5-Math-7B and\nDeepSeek-R1-Distill-Qwen-1.5B, demonstrating their capabilities across varying\nreasoning context lengths. Experiments on multiple mathematical reasoning\nbenchmarks show that our models can not only improve post-training performance\nbut also enable effective test-time scaling.\n","authors":["Fuxiang Zhang","Jiacheng Xu","Chaojie Wang","Ce Cui","Yang Liu","Bo An"],"pdf_url":"https://arxiv.org/pdf/2506.01369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23216v3","updated":"2025-10-30T14:45:38Z","published":"2025-10-27T11:06:00Z","title":"Human-Like Goalkeeping in a Realistic Football Simulation: a\n  Sample-Efficient Reinforcement Learning Approach","summary":"  While several high profile video games have served as testbeds for Deep\nReinforcement Learning (DRL), this technique has rarely been employed by the\ngame industry for crafting authentic AI behaviors. Previous research focuses on\ntraining super-human agents with large models, which is impractical for game\nstudios with limited resources aiming for human-like agents. This paper\nproposes a sample-efficient DRL method tailored for training and fine-tuning\nagents in industrial settings such as the video game industry. Our method\nimproves sample efficiency of value-based DRL by leveraging pre-collected data\nand increasing network plasticity. We evaluate our method training a goalkeeper\nagent in EA SPORTS FC 25, one of the best-selling football simulations today.\nOur agent outperforms the game's built-in AI by 10% in ball saving rate.\nAblation studies show that our method trains agents 50% faster compared to\nstandard DRL methods. Finally, qualitative evaluation from domain experts\nindicates that our approach creates more human-like gameplay compared to\nhand-crafted agents. As a testament to the impact of the approach, the method\nhas been adopted for use in the most recent release of the series.\n","authors":["Alessandro Sestini","Joakim Bergdahl","Jean-Philippe Barrette-LaPierre","Florian Fuchs","Brady Chen","Michael Jones","Linus Gisslén"],"pdf_url":"https://arxiv.org/pdf/2510.23216v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26551v1","updated":"2025-10-30T14:44:24Z","published":"2025-10-30T14:44:24Z","title":"Adaptive Inverse Kinematics Framework for Learning Variable-Length Tool\n  Manipulation in Robotics","summary":"  Conventional robots possess a limited understanding of their kinematics and\nare confined to preprogrammed tasks, hindering their ability to leverage tools\nefficiently. Driven by the essential components of tool usage - grasping the\ndesired outcome, selecting the most suitable tool, determining optimal tool\norientation, and executing precise manipulations - we introduce a pioneering\nframework. Our novel approach expands the capabilities of the robot's inverse\nkinematics solver, empowering it to acquire a sequential repertoire of actions\nusing tools of varying lengths. By integrating a simulation-learned action\ntrajectory with the tool, we showcase the practicality of transferring acquired\nskills from simulation to real-world scenarios through comprehensive\nexperimentation. Remarkably, our extended inverse kinematics solver\ndemonstrates an impressive error rate of less than 1 cm. Furthermore, our\ntrained policy achieves a mean error of 8 cm in simulation. Noteworthy, our\nmodel achieves virtually indistinguishable performance when employing two\ndistinct tools of different lengths. This research provides an indication of\npotential advances in the exploration of all four fundamental aspects of tool\nusage, enabling robots to master the intricate art of tool manipulation across\ndiverse tasks.\n","authors":["Prathamesh Kothavale","Sravani Boddepalli"],"pdf_url":"https://arxiv.org/pdf/2510.26551v1.pdf","comment":"10 pages, 5 figures. Demonstrates a reinforcement learning framework\n  for adaptive tool manipulation with variable-length extensions"},{"id":"http://arxiv.org/abs/2510.26543v1","updated":"2025-10-30T14:36:09Z","published":"2025-10-30T14:36:09Z","title":"The Structure of Relation Decoding Linear Operators in Large Language\n  Models","summary":"  This paper investigates the structure of linear operators introduced in\nHernandez et al. [2023] that decode specific relational facts in transformer\nlanguage models. We extend their single-relation findings to a collection of\nrelations and systematically chart their organization. We show that such\ncollections of relation decoders can be highly compressed by simple order-3\ntensor networks without significant loss in decoding accuracy. To explain this\nsurprising redundancy, we develop a cross-evaluation protocol, in which we\napply each linear decoder operator to the subjects of every other relation. Our\nresults reveal that these linear maps do not encode distinct relations, but\nextract recurring, coarse-grained semantic properties (e.g., country of capital\ncity and country of food are both in the country-of-X property). This\nproperty-centric structure clarifies both the operators' compressibility and\nhighlights why they generalize only to new relations that are semantically\nclose. Our findings thus interpret linear relational decoding in transformer\nlanguage models as primarily property-based, rather than relation-specific.\n","authors":["Miranda Anna Christ","Adrián Csiszárik","Gergely Becsó","Dániel Varga"],"pdf_url":"https://arxiv.org/pdf/2510.26543v1.pdf","comment":"NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.26541v1","updated":"2025-10-30T14:30:53Z","published":"2025-10-30T14:30:53Z","title":"A Three-Stage Bayesian Transfer Learning Framework to Improve\n  Predictions in Data-Scarce Domains","summary":"  The use of ML in engineering has grown steadily to support a wide array of\napplications. Among these methods, deep neural networks have been widely\nadopted due to their performance and accessibility, but they require large,\nhigh-quality datasets. Experimental data are often sparse, noisy, or\ninsufficient to build resilient data-driven models. Transfer learning, which\nleverages relevant data-abundant source domains to assist learning in\ndata-scarce target domains, has shown efficacy. Parameter transfer, where\npretrained weights are reused, is common but degrades under large domain\nshifts. Domain-adversarial neural networks (DANNs) help address this issue by\nlearning domain-invariant representations, thereby improving transfer under\ngreater domain shifts in a semi-supervised setting. However, DANNs can be\nunstable during training and lack a native means for uncertainty\nquantification. This study introduces a fully-supervised three-stage framework,\nthe staged Bayesian domain-adversarial neural network (staged B-DANN), that\ncombines parameter transfer and shared latent space adaptation. In Stage 1, a\ndeterministic feature extractor is trained on the source domain. This feature\nextractor is then adversarially refined using a DANN in Stage 2. In Stage 3, a\nBayesian neural network is built on the adapted feature extractor for\nfine-tuning on the target domain to handle conditional shifts and yield\ncalibrated uncertainty estimates. This staged B-DANN approach was first\nvalidated on a synthetic benchmark, where it was shown to significantly\noutperform standard transfer techniques. It was then applied to the task of\npredicting critical heat flux in rectangular channels, leveraging data from\ntube experiments as the source domain. The results of this study show that the\nstaged B-DANN method can improve predictive accuracy and generalization,\npotentially assisting other domains in nuclear engineering.\n","authors":["Aidan Furlong","Robert Salko","Xingang Zhao","Xu Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26541v1.pdf","comment":"Submitted to Engineering Applications of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2406.08525v2","updated":"2025-10-30T14:25:46Z","published":"2024-06-12T07:33:38Z","title":"A mathematical certification for positivity conditions in Neural\n  Networks with applications to partial monotonicity and Trustworthy AI","summary":"  Artificial Neural Networks (ANNs) have become a powerful tool for modeling\ncomplex relationships in large-scale datasets. However, their black-box nature\nposes trustworthiness challenges. In certain situations, ensuring trust in\npredictions might require following specific partial monotonicity constraints.\nHowever, certifying if an already-trained ANN is partially monotonic is\nchallenging. Therefore, ANNs are often disregarded in some critical\napplications, such as credit scoring, where partial monotonicity is required.\nTo address this challenge, this paper presents a novel algorithm (LipVor) that\ncertifies if a black-box model, such as an ANN, is positive based on a finite\nnumber of evaluations. Consequently, since partial monotonicity can be\nexpressed as a positivity condition on partial derivatives, LipVor can certify\nwhether an ANN is partially monotonic. To do so, for every positively evaluated\npoint, the Lipschitzianity of the black-box model is used to construct a\nspecific neighborhood where the function remains positive. Next, based on the\nVoronoi diagram of the evaluated points, a sufficient condition is stated to\ncertify if the function is positive in the domain. Unlike prior methods, our\napproach certifies partial monotonicity without constrained architectures or\npiece-wise linear activations. Therefore, LipVor could open up the possibility\nof using unconstrained ANN in some critical fields. Moreover, some other\nproperties of an ANN, such as convexity, can be posed as positivity conditions,\nand therefore, LipVor could also be applied.\n","authors":["Alejandro Polo-Molina","David Alfaya","Jose Portela"],"pdf_url":"https://arxiv.org/pdf/2406.08525v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.26533v1","updated":"2025-10-30T14:22:57Z","published":"2025-10-30T14:22:57Z","title":"Higher-Order Regularization Learning on Hypergraphs","summary":"  Higher-Order Hypergraph Learning (HOHL) was recently introduced as a\nprincipled alternative to classical hypergraph regularization, enforcing\nhigher-order smoothness via powers of multiscale Laplacians induced by the\nhypergraph structure. Prior work established the well- and ill-posedness of\nHOHL through an asymptotic consistency analysis in geometric settings. We\nextend this theoretical foundation by proving the consistency of a truncated\nversion of HOHL and deriving explicit convergence rates when HOHL is used as a\nregularizer in fully supervised learning. We further demonstrate its strong\nempirical performance in active learning and in datasets lacking an underlying\ngeometric structure, highlighting HOHL's versatility and robustness across\ndiverse learning settings.\n","authors":["Adrien Weihs","Andrea Bertozzi","Matthew Thorpe"],"pdf_url":"https://arxiv.org/pdf/2510.26533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26527v1","updated":"2025-10-30T14:20:24Z","published":"2025-10-30T14:20:24Z","title":"Polybasic Speculative Decoding Through a Theoretical Perspective","summary":"  Inference latency stands as a critical bottleneck in the large-scale\ndeployment of Large Language Models (LLMs). Speculative decoding methods have\nrecently shown promise in accelerating inference without compromising the\noutput distribution. However, existing work typically relies on a dualistic\ndraft-verify framework and lacks rigorous theoretical grounding. In this paper,\nwe introduce a novel \\emph{polybasic} speculative decoding framework,\nunderpinned by a comprehensive theoretical analysis. Specifically, we prove a\nfundamental theorem that characterizes the optimal inference time for\nmulti-model speculative decoding systems, shedding light on how to extend\nbeyond the dualistic approach to a more general polybasic paradigm. Through our\ntheoretical investigation of multi-model token generation, we expose and\noptimize the interplay between model capabilities, acceptance lengths, and\noverall computational cost. Our framework supports both standalone\nimplementation and integration with existing speculative techniques, leading to\naccelerated performance in practice. Experimental results across multiple model\nfamilies demonstrate that our approach yields speedup ratios ranging from\n$3.31\\times$ to $4.01\\times$ for LLaMA2-Chat 7B, up to $3.87 \\times$ for\nLLaMA3-8B, up to $4.43 \\times$ for Vicuna-7B and up to $3.85 \\times$ for\nQwen2-7B -- all while preserving the original output distribution. We release\nour theoretical proofs and implementation code to facilitate further\ninvestigation into polybasic speculative decoding.\n","authors":["Ruilin Wang","Huixia Li","Yuexiao Ma","Xiawu Zheng","Fei Chao","Xuefeng Xiao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2510.26527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26519v1","updated":"2025-10-30T14:14:15Z","published":"2025-10-30T14:14:15Z","title":"Think Outside the Policy: In-Context Steered Policy Optimization","summary":"  Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such\nas Group Relative Policy Optimization (GRPO), have achieved remarkable progress\nin improving the reasoning capabilities of Large Reasoning Models (LRMs).\nHowever, they exhibit limited exploration due to reliance on on-policy rollouts\nwhere confined to the current policy's distribution, resulting in narrow\ntrajectory diversity. Recent approaches attempt to expand policy coverage by\nincorporating trajectories generated from stronger expert models, yet this\nreliance increases computational cost and such advaned models are often\ninaccessible. To address these issues, we propose In-Context Steered Policy\nOptimization (ICPO), a unified framework that leverages the inherent in-context\nlearning capability of LRMs to provide expert guidance using existing datasets.\nICPO introduces Mixed-Policy GRPO with Implicit Expert Forcing, which expands\nexploration beyond the current policy distribution without requiring advanced\nLRM trajectories. To further stabilize optimization, ICPO integrates Expert\nRegion Reject Sampling to filter unreliable off-policy trajectories and\nAnnealed Expert-Bonus Reward Shaping to balance early expert guidance with\nlater autonomous improvement. Results demonstrate that ICPO consistently\nenhances reinforcement learning performance and training stability on\nmathematical reasoning benchmarks, revealing a scalable and effective RLVR\nparadigm for LRMs.\n","authors":["Hsiu-Yuan Huang","Chenming Tang","Weijie Liu","Saiyong Yang","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26519v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2510.26512v1","updated":"2025-10-30T14:05:55Z","published":"2025-10-30T14:05:55Z","title":"Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs","summary":"  Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts.\n","authors":["Dipak Meher","Carlotta Domeniconi"],"pdf_url":"https://arxiv.org/pdf/2510.26512v1.pdf","comment":"ICDM 2025 Workshop"},{"id":"http://arxiv.org/abs/2510.26510v1","updated":"2025-10-30T14:04:25Z","published":"2025-10-30T14:04:25Z","title":"LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection","summary":"  Model and hyperparameter selection are critical but challenging in machine\nlearning, typically requiring expert intuition or expensive automated search.\nWe investigate whether large language models (LLMs) can act as in-context\nmeta-learners for this task. By converting each dataset into interpretable\nmetadata, we prompt an LLM to recommend both model families and\nhyperparameters. We study two prompting strategies: (1) a zero-shot mode\nrelying solely on pretrained knowledge, and (2) a meta-informed mode augmented\nwith examples of models and their performance on past tasks. Across synthetic\nand real-world benchmarks, we show that LLMs can exploit dataset metadata to\nrecommend competitive models and hyperparameters without search, and that\nimprovements from meta-informed prompting demonstrate their capacity for\nin-context meta-learning. These results highlight a promising new role for LLMs\nas lightweight, general-purpose assistants for model selection and\nhyperparameter optimization.\n","authors":["Youssef Attia El Hili","Albert Thomas","Malik Tiomoko","Abdelhakim Benechehab","Corentin Léger","Corinne Ancourt","Balázs Kégl"],"pdf_url":"https://arxiv.org/pdf/2510.26510v1.pdf","comment":"27 pages, 6 figures"},{"id":"http://arxiv.org/abs/2508.06576v2","updated":"2025-10-30T13:59:28Z","published":"2025-08-07T14:03:23Z","title":"GFlowNets for Learning Better Drug-Drug Interaction Representations","summary":"  Drug-drug interactions pose a significant challenge in clinical pharmacology,\nwith severe class imbalance among interaction types limiting the effectiveness\nof predictive models. Common interactions dominate datasets, while rare but\ncritical interactions remain underrepresented, leading to poor model\nperformance on infrequent cases. Existing methods often treat DDI prediction as\na binary problem, ignoring class-specific nuances and exacerbating bias toward\nfrequent interactions. To address this, we propose a framework combining\nGenerative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)\nto generate synthetic samples for rare classes, improving model balance and\ngenerate effective and novel DDI pairs. Our approach enhances predictive\nperformance across interaction types, ensuring better clinical reliability.\n","authors":["Azmine Toushik Wasi"],"pdf_url":"https://arxiv.org/pdf/2508.06576v2.pdf","comment":"Accepted to ICANN 2025:AIDD and NeurIPS 2025 Workshop on Structured\n  Probabilistic Inference & Generative Modeling\n  (https://openreview.net/forum?id=LZW1jSgfCI)"},{"id":"http://arxiv.org/abs/2509.22018v2","updated":"2025-10-30T13:57:35Z","published":"2025-09-26T07:51:59Z","title":"Exploring the Early Universe with Deep Learning","summary":"  Hydrogen is the most abundant element in our Universe. The first generation\nof stars and galaxies produced photons that ionized hydrogen gas, driving a\ncosmological event known as the Epoch of Reionization (EoR). The upcoming\nSquare Kilometre Array Observatory (SKAO) will map the distribution of neutral\nhydrogen during this era, aiding in the study of the properties of these\nfirst-generation objects. Extracting astrophysical information will be\nchallenging, as SKAO will produce a tremendous amount of data where the\nhydrogen signal will be contaminated with undesired foreground contamination\nand instrumental systematics. To address this, we develop the latest deep\nlearning techniques to extract information from the 2D power spectra of the\nhydrogen signal expected from SKAO. We apply a series of neural network models\nto these measurements and quantify their ability to predict the history of\ncosmic hydrogen reionization, which is connected to the increasing number and\nefficiency of early photon sources. We show that the study of the early\nUniverse benefits from modern deep learning technology. In particular, we\ndemonstrate that dedicated machine learning algorithms can achieve more than a\n$0.95$ $R^2$ score on average in recovering the reionization history. This\nenables accurate and precise cosmological and astrophysical inference of\nstructure formation in the early Universe.\n","authors":["Emmanuel de Salis","Massimo De Santis","Davide Piras","Sambit K. Giri","Michele Bianco","Nicolas Cerardi","Philipp Denzel","Merve Selcuk-Simsek","Kelley M. Hess","M. Carmen Toribio","Franz Kirsten","Hatem Ghorbel"],"pdf_url":"https://arxiv.org/pdf/2509.22018v2.pdf","comment":"EPIA 2025 preprint version, 12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2510.26501v1","updated":"2025-10-30T13:54:37Z","published":"2025-10-30T13:54:37Z","title":"Enhancing ECG Classification Robustness with Lightweight Unsupervised\n  Anomaly Detection Filters","summary":"  Continuous electrocardiogram (ECG) monitoring via wearables offers\nsignificant potential for early cardiovascular disease (CVD) detection.\nHowever, deploying deep learning models for automated analysis in\nresource-constrained environments faces reliability challenges due to\ninevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen\npathologies or noisecorrupted signals, often cause erroneous, high-confidence\npredictions by standard classifiers, compromising patient safety. Existing OOD\ndetection methods either neglect computational constraints or address noise and\nunseen classes separately. This paper explores Unsupervised Anomaly Detection\n(UAD) as an independent, upstream filtering mechanism to improve robustness. We\nbenchmark six UAD approaches, including Deep SVDD, reconstruction-based models,\nMasked Anomaly Detection, normalizing flows, and diffusion models, optimized\nvia Neural Architecture Search (NAS) under strict resource constraints (at most\n512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection\nof OOD CVD classes and signals unsuitable for analysis due to noise. Results\nshow Deep SVDD consistently achieves the best trade-off between detection and\nefficiency. In a realistic deployment simulation, integrating the optimized\nDeep SVDD filter with a diagnostic classifier improved accuracy by up to 21\npercentage points over a classifier-only baseline. This study demonstrates that\noptimized UAD filters can safeguard automated ECG analysis, enabling safer,\nmore reliable continuous cardiovascular monitoring on wearables.\n","authors":["Mustafa Fuad Rifet Ibrahim","Maurice Meijer","Alexander Schlaefer","Peer Stelldinger"],"pdf_url":"https://arxiv.org/pdf/2510.26501v1.pdf","comment":"Submitted to the 24th International Conference on Pervasive Computing\n  and Communications (PerCom 2026)"},{"id":"http://arxiv.org/abs/2505.11730v2","updated":"2025-10-30T13:52:37Z","published":"2025-05-16T22:24:48Z","title":"Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling","summary":"  Test-time scaling (TTS) has proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Verification plays a key role in\nTTS, simultaneously influencing (1) reasoning performance and (2) compute\nefficiency, due to the quality and computational cost of verification. In this\nwork, we challenge the conventional paradigms of verification, and make the\nfirst attempt toward systematically investigating the impact of verification\ngranularity-that is, how frequently the verifier is invoked during generation,\nbeyond verifying only the final output or individual generation steps. To this\nend, we introduce Variable Granularity Search (VG-Search), a unified algorithm\nthat generalizes beam search and Best-of-N sampling via a tunable granularity\nparameter g. Extensive experiments with VG-Search under varying compute\nbudgets, generator-verifier configurations, and task attributes reveal that\ndynamically selecting g can improve the compute efficiency and scaling\nbehavior. Building on these findings, we propose adaptive VG-Search strategies\nthat achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over\nBest-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to\nsupport future research.\n","authors":["Hao Mark Chen","Guanxi Lu","Yasuyuki Okoshi","Zhiwen Mo","Masato Motomura","Hongxiang Fan"],"pdf_url":"https://arxiv.org/pdf/2505.11730v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.17475v3","updated":"2025-10-30T13:42:29Z","published":"2025-06-20T20:46:01Z","title":"A geometric framework for momentum-based optimizers for low-rank\n  training","summary":"  Low-rank pre-training and fine-tuning have recently emerged as promising\ntechniques for reducing the computational and storage costs of large neural\nnetworks. Training low-rank parameterizations typically relies on conventional\noptimizers such as heavy ball momentum methods or Adam. In this work, we\nidentify and analyze potential difficulties that these training methods\nencounter when used to train low-rank parameterizations of weights. In\nparticular, we show that classical momentum methods can struggle to converge to\na local optimum due to the geometry of the underlying optimization landscape.\nTo address this, we introduce novel training strategies derived from dynamical\nlow-rank approximation, which explicitly account for the underlying geometric\nstructure. Our approach leverages and combines tools from dynamical low-rank\napproximation and momentum-based optimization to design optimizers that respect\nthe intrinsic geometry of the parameter space. We validate our methods through\nnumerical experiments, demonstrating faster convergence, and stronger\nvalidation metrics at given parameter budgets.\n","authors":["Steffen Schotthöfer","Timon Klein","Jonas Kusch"],"pdf_url":"https://arxiv.org/pdf/2506.17475v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26491v1","updated":"2025-10-30T13:40:52Z","published":"2025-10-30T13:40:52Z","title":"Data-Efficient RLVR via Off-Policy Influence Guidance","summary":"  Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\n\\textbf{C}urriculum \\textbf{R}L with \\textbf{O}ff-\\textbf{P}olicy\n\\text{I}nfluence guidance (\\textbf{CROPI}), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR.\n","authors":["Erle Zhu","Dazhi Jiang","Yuan Wang","Xujun Li","Jiale Cheng","Yuxian Gu","Yilin Niu","Aohan Zeng","Jie Tang","Minlie Huang","Hongning Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26487v1","updated":"2025-10-30T13:39:44Z","published":"2025-10-30T13:39:44Z","title":"Quantum Gated Recurrent GAN with Gaussian Uncertainty for Network\n  Anomaly Detection","summary":"  Anomaly detection in time-series data is a critical challenge with\nsignificant implications for network security. Recent quantum machine learning\napproaches, such as quantum kernel methods and variational quantum circuits,\nhave shown promise in capturing complex data distributions for anomaly\ndetection but remain constrained by limited qubit counts. We introduce in this\nwork a novel Quantum Gated Recurrent Unit (QGRU)-based Generative Adversarial\nNetwork (GAN) employing Successive Data Injection (SuDaI) and a multi-metric\ngating strategy for robust network anomaly detection. Our model uniquely\nutilizes a quantum-enhanced generator that outputs parameters (mean and\nlog-variance) of a Gaussian distribution via reparameterization, combined with\na Wasserstein critic to stabilize adversarial training. Anomalies are\nidentified through a novel gating mechanism that initially flags potential\nanomalies based on Gaussian uncertainty estimates and subsequently verifies\nthem using a composite of critic scores and reconstruction errors. Evaluated on\nbenchmark datasets, our method achieves a high time-series aware F1 score\n(TaF1) of 89.43% demonstrating superior capability in detecting anomalies\naccurately and promptly as compared to existing classical and quantum models.\nFurthermore, the trained QGRU-WGAN was deployed on real IBM Quantum hardware,\nwhere it retained high anomaly detection performance, confirming its robustness\nand practical feasibility on current noisy intermediate-scale quantum (NISQ)\ndevices.\n","authors":["Wajdi Hammami","Soumaya Cherkaoui","Jean-Frederic Laprade","Ola Ahmad","Shengrui Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26486v1","updated":"2025-10-30T13:39:08Z","published":"2025-10-30T13:39:08Z","title":"LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks","summary":"  Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.\n","authors":["Dipak Meher","Carlotta Domeniconi","Guadalupe Correa-Cabrera"],"pdf_url":"https://arxiv.org/pdf/2510.26486v1.pdf","comment":"Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures"},{"id":"http://arxiv.org/abs/2507.02843v2","updated":"2025-10-30T13:29:54Z","published":"2025-07-03T17:52:27Z","title":"LLM-Driven Treatment Effect Estimation Under Inference Time Text\n  Confounding","summary":"  Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications.\n","authors":["Yuchen Ma","Dennis Frauen","Jonas Schweisthal","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2507.02843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26475v1","updated":"2025-10-30T13:27:42Z","published":"2025-10-30T13:27:42Z","title":"ReSpec: Towards Optimizing Speculative Decoding in Reinforcement\n  Learning Systems","summary":"  Adapting large language models (LLMs) via reinforcement learning (RL) is\noften bottlenecked by the generation stage, which can consume over 75\\% of the\ntraining time. Speculative decoding (SD) accelerates autoregressive generation\nin serving systems, but its behavior under RL training remains largely\nunexplored. We identify three critical gaps that hinder the naive integration\nof SD into RL systems: diminishing speedups at large batch sizes, drafter\nstaleness under continual actor updates, and drafter-induced policy\ndegradation.\n  To address these gaps, we present ReSpec, a system that adapts SD to RL\nthrough three complementary mechanisms: dynamically tuning SD configurations,\nevolving the drafter via knowledge distillation, and weighting updates by\nrollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup\nwhile preserving reward convergence and training stability, providing a\npractical solution for efficient RL-based LLM adaptation.\n","authors":["Qiaoling Chen","Zijun Liu","Peng Sun","Shenggui Li","Guoteng Wang","Ziming Liu","Yonggang Wen","Siyuan Feng","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12371v2","updated":"2025-10-30T13:27:07Z","published":"2025-05-18T11:28:17Z","title":"MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional\n  Methods for Diverse Medical Tasks","summary":"  The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/.\n","authors":["Yinghao Zhu","Ziyi He","Haoran Hu","Xiaochen Zheng","Xichen Zhang","Zixiang Wang","Junyi Gao","Liantao Ma","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2505.12371v2.pdf","comment":"Accepted by NeurIPS 2025 Datasets & Benchmarks Track"},{"id":"http://arxiv.org/abs/2510.26474v1","updated":"2025-10-30T13:26:58Z","published":"2025-10-30T13:26:58Z","title":"Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing","summary":"  Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.\n","authors":["Xin Guo","Zhiheng Xi","Yiwen Ding","Yitao Zhai","Xiaowei Shi","Xunliang Cai","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26474v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2508.08606v3","updated":"2025-10-30T13:25:58Z","published":"2025-08-12T03:39:07Z","title":"Distributed optimization: designed for federated learning","summary":"  Federated learning (FL), as a distributed collaborative machine learning (ML)\nframework under privacy-preserving constraints, has garnered increasing\nresearch attention in cross-organizational data collaboration scenarios. This\npaper proposes a class of distributed optimization algorithms based on the\naugmented Lagrangian technique, designed to accommodate diverse communication\ntopologies in both centralized and decentralized FL settings. Furthermore, we\ndevelop multiple termination criteria and parameter update mechanisms to\nenhance computational efficiency, accompanied by rigorous theoretical\nguarantees of convergence. By generalizing the augmented Lagrangian relaxation\nthrough the incorporation of proximal relaxation and quadratic approximation,\nour framework systematically recovers a broad of classical unconstrained\noptimization methods, including proximal algorithm, classic gradient descent,\nand stochastic gradient descent, among others. Notably, the convergence\nproperties of these methods can be naturally derived within the proposed\ntheoretical framework. Numerical experiments demonstrate that the proposed\nalgorithm exhibits strong performance in large-scale settings with significant\nstatistical heterogeneity across clients.\n","authors":["Wenyou Guo","Ting Qu","Chunrong Pan","George Q. Huang"],"pdf_url":"https://arxiv.org/pdf/2508.08606v3.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.02331v2","updated":"2025-10-30T13:17:24Z","published":"2025-02-04T14:06:27Z","title":"On the Impact of Performative Risk Minimization for Binary Random\n  Variables","summary":"  Performativity, the phenomenon where outcomes are influenced by predictions,\nis particularly prevalent in social contexts where individuals strategically\nrespond to a deployed model. In order to preserve the high accuracy of machine\nlearning models under distribution shifts caused by performativity, Perdomo et\nal. (2020) introduced the concept of performative risk minimization (PRM).\nWhile this framework ensures model accuracy, it overlooks the impact of the PRM\non the underlying distributions and the predictions of the model. In this\npaper, we initiate the analysis of the impact of PRM, by studying\nperformativity for a sequential performative risk minimization problem with\nbinary random variables and linear performative shifts. We formulate two\nnatural measures of impact. In the case of full information, where the\ndistribution dynamics are known, we derive explicit formulas for the PRM\nsolution and our impact measures. In the case of partial information, we\nprovide performative-aware statistical estimators, as well as simulations. Our\nanalysis contrasts PRM to alternatives that do not model data shift and\nindicates that PRM can have amplified side effects compared to such methods.\n","authors":["Nikita Tsoy","Ivan Kirev","Negin Rahimiyazdi","Nikola Konstantinov"],"pdf_url":"https://arxiv.org/pdf/2502.02331v2.pdf","comment":"ICML 2025 camera-ready"},{"id":"http://arxiv.org/abs/2509.09695v2","updated":"2025-10-30T13:14:13Z","published":"2025-08-27T09:31:50Z","title":"Machine-learning competition to grade EEG background patterns in\n  newborns with hypoxic-ischaemic encephalopathy","summary":"  Machine learning (ML) has the potential to support and improve expert\nperformance in monitoring the brain function of at-risk newborns. Developing\naccurate and reliable ML models depends on access to high-quality, annotated\ndata, a resource in short supply. ML competitions address this need by\nproviding researchers access to expertly annotated datasets, fostering shared\nlearning through direct model comparisons, and leveraging the benefits of\ncrowdsourcing diverse expertise. We compiled a retrospective dataset containing\n353 hours of EEG from 102 individual newborns from a multi-centre study. The\ndata was fully anonymised and divided into training, testing, and held-out\nvalidation datasets. EEGs were graded for the severity of abnormal background\npatterns. Next, we created a web-based competition platform and hosted a\nmachine learning competition to develop ML models for classifying the severity\nof EEG background patterns in newborns. After the competition closed, the top 4\nperforming models were evaluated offline on a separate held-out validation\ndataset. Although a feature-based model ranked first on the testing dataset,\ndeep learning models generalised better on the validation sets. All methods had\na significant decline in validation performance compared to the testing\nperformance. This highlights the challenges for model generalisation on unseen\ndata, emphasising the need for held-out validation datasets in ML studies with\nneonatal EEG. The study underscores the importance of training ML models on\nlarge and diverse datasets to ensure robust generalisation. The competition's\noutcome demonstrates the potential for open-access data and collaborative ML\ndevelopment to foster a collaborative research environment and expedite the\ndevelopment of clinical decision-support tools for neonatal neuromonitoring.\n","authors":["Fabio Magarelli","Geraldine B. Boylan","Saeed Montazeri","Feargal O'Sullivan","Dominic Lightbody","Minoo Ashoori","Tamara Skoric","John M. O'Toole"],"pdf_url":"https://arxiv.org/pdf/2509.09695v2.pdf","comment":"29 pages, supplementary materials: \"supplementary materials ML\n  Comp.docx\""},{"id":"http://arxiv.org/abs/2510.26466v1","updated":"2025-10-30T13:11:23Z","published":"2025-10-30T13:11:23Z","title":"Representation-Level Counterfactual Calibration for Debiased Zero-Shot\n  Recognition","summary":"  Object-context shortcuts remain a persistent challenge in vision-language\nmodels, undermining zero-shot reliability when test-time scenes differ from\nfamiliar training co-occurrences. We recast this issue as a causal inference\nproblem and ask: Would the prediction remain if the object appeared in a\ndifferent environment? To answer this at inference time, we estimate object and\nbackground expectations within CLIP's representation space, and synthesize\ncounterfactual embeddings by recombining object features with diverse\nalternative contexts sampled from external datasets, batch neighbors, or\ntext-derived descriptions. By estimating the Total Direct Effect and simulating\nintervention, we further subtract background-only activation, preserving\nbeneficial object-context interactions while mitigating hallucinated scores.\nWithout retraining or prompt design, our method substantially improves both\nworst-group and average accuracy on context-sensitive benchmarks, establishing\na new zero-shot state of the art. Beyond performance, our framework provides a\nlightweight representation-level counterfactual approach, offering a practical\ncausal avenue for debiased and reliable multimodal reasoning.\n","authors":["Pei Peng","MingKun Xie","Hang Hao","Tong Jin","ShengJun Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26461v1","updated":"2025-10-30T13:07:39Z","published":"2025-10-30T13:07:39Z","title":"Vectorized Context-Aware Embeddings for GAT-Based Collaborative\n  Filtering","summary":"  Recommender systems often struggle with data sparsity and cold-start\nscenarios, limiting their ability to provide accurate suggestions for new or\ninfrequent users. This paper presents a Graph Attention Network (GAT) based\nCollaborative Filtering (CF) framework enhanced with Large Language Model (LLM)\ndriven context aware embeddings. Specifically, we generate concise textual user\nprofiles and unify item metadata (titles, genres, overviews) into rich textual\nembeddings, injecting these as initial node features in a bipartite user item\ngraph. To further optimize ranking performance, we introduce a hybrid loss\nfunction that combines Bayesian Personalized Ranking (BPR) with a cosine\nsimilarity term and robust negative sampling, ensuring explicit negative\nfeedback is distinguished from unobserved data. Experiments on the MovieLens\n100k and 1M datasets show consistent improvements over state-of-the-art\nbaselines in Precision, NDCG, and MAP while demonstrating robustness for users\nwith limited interaction history. Ablation studies confirm the critical role of\nLLM-augmented embeddings and the cosine similarity term in capturing nuanced\nsemantic relationships. Our approach effectively mitigates sparsity and\ncold-start limitations by integrating LLM-derived contextual understanding into\ngraph-based architectures. Future directions include balancing recommendation\naccuracy with coverage and diversity, and introducing fairness-aware\nconstraints and interpretability features to enhance system performance\nfurther.\n","authors":["Danial Ebrat","Sepideh Ahmadian","Luis Rueda"],"pdf_url":"https://arxiv.org/pdf/2510.26461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21513v2","updated":"2025-10-30T13:03:25Z","published":"2025-10-24T14:39:23Z","title":"Wisdom and Delusion of LLM Ensembles for Code Generation and Repair","summary":"  Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems. To address this gap, we\nempirically compare ten individual LLMs from five families, and three ensembles\nof these LLMs across three software engineering benchmarks covering code\ngeneration and program repair. We assess the complementarity between models and\nthe performance gap between the best individual model and the ensembles. Next,\nwe evaluate various selection heuristics to identify correct solutions from an\nensemble's candidate pool. We find that the theoretical upperbound for an\nensemble's performance can be 83% above the best single model. Our results show\nthat consensus-based strategies for selecting solutions fall into a \"popularity\ntrap,\" amplifying common but incorrect outputs. In contrast, a diversity-based\nstrategy realizes up to 95% of this theoretical potential, and proves effective\neven in small two-model ensembles, enabling a cost-efficient way to enhance\nperformance by leveraging multiple LLMs.\n","authors":["Fernando Vallecillos-Ruiz","Max Hort","Leon Moonen"],"pdf_url":"https://arxiv.org/pdf/2510.21513v2.pdf","comment":"Added Acknowledgments section and hyphenated last names"},{"id":"http://arxiv.org/abs/2510.26451v1","updated":"2025-10-30T12:55:21Z","published":"2025-10-30T12:55:21Z","title":"Robust Graph Condensation via Classification Complexity Mitigation","summary":"  Graph condensation (GC) has gained significant attention for its ability to\nsynthesize smaller yet informative graphs. However, existing studies often\noverlook the robustness of GC in scenarios where the original graph is\ncorrupted. In such cases, we observe that the performance of GC deteriorates\nsignificantly, while existing robust graph learning technologies offer only\nlimited effectiveness. Through both empirical investigation and theoretical\nanalysis, we reveal that GC is inherently an intrinsic-dimension-reducing\nprocess, synthesizing a condensed graph with lower classification complexity.\nAlthough this property is critical for effective GC performance, it remains\nhighly vulnerable to adversarial perturbations. To tackle this vulnerability\nand improve GC robustness, we adopt the geometry perspective of graph data\nmanifold and propose a novel Manifold-constrained Robust Graph Condensation\nframework named MRGC. Specifically, we introduce three graph data manifold\nlearning modules that guide the condensed graph to lie within a smooth,\nlow-dimensional manifold with minimal class ambiguity, thereby preserving the\nclassification complexity reduction capability of GC and ensuring robust\nperformance under universal adversarial attacks. Extensive experiments\ndemonstrate the robustness of \\ModelName\\ across diverse attack scenarios.\n","authors":["Jiayi Luo","Qingyun Sun","Beining Yang","Haonan Yuan","Xingcheng Fu","Yanbiao Ma","Jianxin Li","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2510.26451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26444v1","updated":"2025-10-30T12:50:12Z","published":"2025-10-30T12:50:12Z","title":"Personalized Treatment Outcome Prediction from Scarce Data via\n  Dual-Channel Knowledge Distillation and Adaptive Fusion","summary":"  Personalized treatment outcome prediction based on trial data for\nsmall-sample and rare patient groups is critical in precision medicine.\nHowever, the costly trial data limit the prediction performance. To address\nthis issue, we propose a cross-fidelity knowledge distillation and adaptive\nfusion network (CFKD-AFN), which leverages abundant but low-fidelity simulation\ndata to enhance predictions on scarce but high-fidelity trial data. CFKD-AFN\nincorporates a dual-channel knowledge distillation module to extract\ncomplementary knowledge from the low-fidelity model, along with an\nattention-guided fusion module to dynamically integrate multi-source\ninformation. Experiments on treatment outcome prediction for the chronic\nobstructive pulmonary disease demonstrates significant improvements of CFKD-AFN\nover state-of-the-art methods in prediction accuracy, ranging from 6.67\\% to\n74.55\\%, and strong robustness to varying high-fidelity dataset sizes.\nFurthermore, we extend CFKD-AFN to an interpretable variant, enabling the\nexploration of latent medical semantics to support clinical decision-making.\n","authors":["Wenjie Chen","Li Zhuang","Ziying Luo","Yu Liu","Jiahao Wu","Shengcai Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11542v2","updated":"2025-10-30T12:48:34Z","published":"2025-05-14T13:18:12Z","title":"Cybersecurity threat detection based on a UEBA framework using Deep\n  Autoencoders","summary":"  User and Entity Behaviour Analytics (UEBA) is a broad branch of data\nanalytics that attempts to build a normal behavioural profile in order to\ndetect anomalous events. Among the techniques used to detect anomalies, Deep\nAutoencoders constitute one of the most promising deep learning models on UEBA\ntasks, allowing explainable detection of security incidents that could lead to\nthe leak of personal data, hijacking of systems, or access to sensitive\nbusiness information. In this study, we introduce the first implementation of\nan explainable UEBA-based anomaly detection framework that leverages Deep\nAutoencoders in combination with Doc2Vec to process both numerical and textual\nfeatures. Additionally, based on the theoretical foundations of neural\nnetworks, we offer a novel proof demonstrating the equivalence of two widely\nused definitions for fully-connected neural networks. The experimental results\ndemonstrate the proposed framework capability to detect real and synthetic\nanomalies effectively generated from real attack data, showing that the models\nprovide not only correct identification of anomalies but also explainable\nresults that enable the reconstruction of the possible origin of the anomaly.\nOur findings suggest that the proposed UEBA framework can be seamlessly\nintegrated into enterprise environments, complementing existing security\nsystems for explainable threat detection.\n","authors":["Jose Fuentes","Ines Ortega-Fernandez","Nora M. Villanueva","Marta Sestelo"],"pdf_url":"https://arxiv.org/pdf/2505.11542v2.pdf","comment":"Published in AIMS Mathematics (2025), 10(10): 23496-23517. DOI:\n  10.3934/math.20251043"},{"id":"http://arxiv.org/abs/2510.26433v1","updated":"2025-10-30T12:28:40Z","published":"2025-10-30T12:28:40Z","title":"Co-Evolving Latent Action World Models","summary":"  Adapting pre-trained video generation models into controllable world models\nvia latent actions is a promising step towards creating generalist world\nmodels. The dominant paradigm adopts a two-stage approach that trains latent\naction model (LAM) and the world model separately, resulting in redundant\ntraining and limiting their potential for co-adaptation. A conceptually simple\nand appealing idea is to directly replace the forward dynamic model in LAM with\na powerful world model and training them jointly, but it is non-trivial and\nprone to representational collapse. In this work, we propose CoLA-World, which\nfor the first time successfully realizes this synergistic paradigm, resolving\nthe core challenge in joint learning through a critical warm-up phase that\neffectively aligns the representations of the from-scratch LAM with the\npre-trained world model. This unlocks a co-evolution cycle: the world model\nacts as a knowledgeable tutor, providing gradients to shape a high-quality LAM,\nwhile the LAM offers a more precise and adaptable control interface to the\nworld model. Empirically, CoLA-World matches or outperforms prior two-stage\nmethods in both video simulation quality and downstream visual planning,\nestablishing a robust and efficient new paradigm for the field.\n","authors":["Yucen Wang","Fengming Zhang","De-Chuan Zhan","Li Zhao","Kaixin Wang","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2510.26433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25080v2","updated":"2025-10-30T12:16:59Z","published":"2025-10-29T01:38:19Z","title":"Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response\n  Games","summary":"  Card games are widely used to study sequential decision-making under\nuncertainty, with real-world analogues in negotiation, finance, and\ncybersecurity. These games typically fall into three categories based on the\nflow of control: strictly sequential (players alternate single actions),\ndeterministic response (some actions trigger a fixed outcome), and unbounded\nreciprocal response (alternating counterplays are permitted). A less-explored\nbut strategically rich structure is the bounded one-sided response, where a\nplayer's action briefly transfers control to the opponent, who must satisfy a\nfixed condition through one or more moves before the turn resolves. We term\ngames featuring this mechanism Bounded One-Sided Response Games (BORGs). We\nintroduce a modified version of Monopoly Deal as a benchmark environment that\nisolates this dynamic, where a Rent action forces the opponent to choose\npayment assets. The gold-standard algorithm, Counterfactual Regret Minimization\n(CFR), converges on effective strategies without novel algorithmic extensions.\nA lightweight full-stack research platform unifies the environment, a\nparallelized CFR runtime, and a human-playable web interface. The trained CFR\nagent and source code are available at https://monopolydeal.ai.\n","authors":["Will Wolf"],"pdf_url":"https://arxiv.org/pdf/2510.25080v2.pdf","comment":"24 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.17883v2","updated":"2025-10-30T12:06:31Z","published":"2024-12-23T06:22:03Z","title":"In Defence of Post-hoc Explainability","summary":"  This position paper defends post-hoc explainability methods as legitimate\ntools for scientific knowledge production in machine learning. Addressing\ncriticism of these methods' reliability and epistemic status, we develop a\nphilosophical framework grounded in mediated understanding and bounded\nfactivity. We argue that scientific insights can emerge through structured\ninterpretation of model behaviour without requiring complete mechanistic\ntransparency, provided explanations acknowledge their approximative nature and\nundergo rigorous empirical validation. Through analysis of recent biomedical ML\napplications, we demonstrate how post-hoc methods, when properly integrated\ninto scientific practice, generate novel hypotheses and advance phenomenal\nunderstanding.\n","authors":["Nick Oh"],"pdf_url":"https://arxiv.org/pdf/2412.17883v2.pdf","comment":"v1 presented at the Interpretable AI: Past, Present, and Future\n  Workshop at NeurIPS 2024 (non-archival)"},{"id":"http://arxiv.org/abs/2510.17670v2","updated":"2025-10-30T12:05:58Z","published":"2025-10-20T15:41:55Z","title":"On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active\n  Marginal-Samples Exploration","summary":"  Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs.\n","authors":["Yehonathan Refael","Amit Aides","Aviad Barzilai","George Leifman","Genady Beryozkin","Vered Silverman","Bolous Jaber","Tomer Shekel"],"pdf_url":"https://arxiv.org/pdf/2510.17670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23117v2","updated":"2025-10-30T12:02:18Z","published":"2025-10-27T08:38:17Z","title":"Seeing Structural Failure Before it Happens: An Image-Based\n  Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction","summary":"  Physics Informed Neural Networks (PINNs) are gaining attention for their\nability to embed physical laws into deep learning models, which is particularly\nuseful in structural engineering tasks with limited data. This paper aims to\nexplore the use of PINNs to predict the weight of small scale spaghetti\nbridges, a task relevant to understanding load limits and potential failure\nmodes in simplified structural models. Our proposed framework incorporates\nphysics-based constraints to the prediction model for improved performance. In\naddition to standard PINNs, we introduce a novel architecture named Physics\nInformed Kolmogorov Arnold Network (PIKAN), which blends universal function\napproximation theory with physical insights. The structural parameters provided\nas input to the model are collected either manually or through computer vision\nmethods. Our dataset includes 15 real bridges, augmented to 100 samples, and\nour best model achieves an $R^2$ score of 0.9603 and a mean absolute error\n(MAE) of 10.50 units. From applied perspective, we also provide a web based\ninterface for parameter entry and prediction. These results show that PINNs can\noffer reliable estimates of structural weight, even with limited data, and may\nhelp inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at\nhttps://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.\n","authors":["Omer Jauhar Khan","Sudais Khan","Hafeez Anwar","Shahzeb Khan","Shams Ul Arifeen"],"pdf_url":"https://arxiv.org/pdf/2510.23117v2.pdf","comment":"12 pages, 17 figures. Preprint"},{"id":"http://arxiv.org/abs/2510.26402v1","updated":"2025-10-30T11:41:50Z","published":"2025-10-30T11:41:50Z","title":"Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback\n  in Programming Education","summary":"  The rapid growth of programming education has outpaced traditional assessment\ntools, leaving faculty with limited means to provide meaningful, scalable\nfeedback. Conventional autograders, while efficient, act as black-box systems\nthat simply return pass/fail results, offering little insight into student\nthinking or learning needs.\n  Autograder+ is designed to shift autograding from a purely summative process\nto a formative learning experience. It introduces two key capabilities:\nautomated feedback generation using a fine-tuned Large Language Model, and\nvisualization of student code submissions to uncover learning patterns. The\nmodel is fine-tuned on curated student code and expert feedback to ensure\npedagogically aligned, context-aware guidance.\n  In evaluation across 600 student submissions from multiple programming tasks,\nthe system produced feedback with strong semantic alignment to instructor\ncomments. For visualization, contrastively learned code embeddings trained on\n1,000 annotated submissions enable grouping solutions into meaningful clusters\nbased on functionality and approach. The system also supports prompt-pooling,\nallowing instructors to guide feedback style through selected prompt templates.\n  By integrating AI-driven feedback, semantic clustering, and interactive\nvisualization, Autograder+ reduces instructor workload while supporting\ntargeted instruction and promoting stronger learning outcomes.\n","authors":["Vikrant Sahu","Gagan Raj Gupta","Raghav Borikar","Nitin Mane"],"pdf_url":"https://arxiv.org/pdf/2510.26402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26401v1","updated":"2025-10-30T11:41:19Z","published":"2025-10-30T11:41:19Z","title":"Multi-Output Robust and Conjugate Gaussian Processes","summary":"  Multi-output Gaussian process (MOGP) regression allows modelling dependencies\namong multiple correlated response variables. Similarly to standard Gaussian\nprocesses, MOGPs are sensitive to model misspecification and outliers, which\ncan distort predictions within individual outputs. This situation can be\nfurther exacerbated by multiple anomalous response variables whose errors\npropagate due to correlations between outputs. To handle this situation, we\nextend and generalise the robust and conjugate Gaussian process (RCGP)\nframework introduced by Altamirano et al. (2024). This results in the\nmulti-output RCGP (MO-RCGP): a provably robust MOGP that is conjugate, and\njointly captures correlations across outputs. We thoroughly evaluate our\napproach through applications in finance and cancer research.\n","authors":["Joshua Rooijakkers","Leiv Rønneberg","François-Xavier Briol","Jeremias Knoblauch","Matias Altamirano"],"pdf_url":"https://arxiv.org/pdf/2510.26401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26392v1","updated":"2025-10-30T11:35:05Z","published":"2025-10-30T11:35:05Z","title":"Multi-Task Learning Based on Support Vector Machines and Twin Support\n  Vector Machines: A Comprehensive Survey","summary":"  Multi-task learning (MTL) enables simultaneous training across related tasks,\nleveraging shared information to improve generalization, efficiency, and\nrobustness, especially in data-scarce or high-dimensional scenarios. While deep\nlearning dominates recent MTL research, Support Vector Machines (SVMs) and Twin\nSVMs (TWSVMs) remain relevant due to their interpretability, theoretical rigor,\nand effectiveness with small datasets.\n  This chapter surveys MTL approaches based on SVM and TWSVM, highlighting\nshared representations, task regularization, and structural coupling\nstrategies. Special attention is given to emerging TWSVM extensions for\nmulti-task settings, which show promise but remain underexplored. We compare\nthese models in terms of theoretical properties, optimization strategies, and\nempirical performance, and discuss applications in fields such as computer\nvision, natural language processing, and bioinformatics.\n  Finally, we identify research gaps and outline future directions for building\nscalable, interpretable, and reliable margin-based MTL frameworks. This work\nprovides a comprehensive resource for researchers and practitioners interested\nin SVM- and TWSVM-based multi-task learning.\n","authors":["Fatemeh Bazikar","Hossein Moosaei","Atefeh Hemmati","Panos M. Pardalos"],"pdf_url":"https://arxiv.org/pdf/2510.26392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11329v4","updated":"2025-10-30T11:34:01Z","published":"2025-05-16T14:53:50Z","title":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM\n  Inference","summary":"  Distributed inference of large language models (LLMs) can introduce overheads\nof up to 20% even over GPUs connected via high-speed interconnects such as\nNVLink. Multiple techniques have been proposed to mitigate these overheads by\ndecomposing computations into finer-grained tasks and overlapping communication\nwith sub-tasks as they complete. However, fine-grained decomposition of a large\ncomputation into many smaller computations on GPUs results in overheads.\nFurthermore, the communication itself uses many streaming multiprocessors\n(SMs), adding to the overhead.\n  We present TokenWeave to address these challenges. TokenWeave proposes a\nToken-Splitting technique that divides the tokens in the inference batch into\ntwo approximately equal subsets in a wave-aware manner. The communication of\none subset is then overlapped with the computation of the other. In addition,\nTokenWeave optimizes the order of the layer normalization computation with\nrespect to communication operations and implements a novel fused\nAllReduce--RMSNorm kernel that carefully leverages Multimem instruction support\navailable on Hopper and Blackwell NVIDIA GPUs. These optimizations allow\nTokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover,\nour kernel enables the memory-bound RMSNorm to be overlapped with the other\nbatch's computation, providing additional gains.\n  Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher\nthroughput across multiple models and workloads. In several settings,\nTokenWeave results in better performance compared to an equivalent model with\nall communication removed.\n","authors":["Raja Gond","Nipun Kwatra","Ramachandran Ramjee"],"pdf_url":"https://arxiv.org/pdf/2505.11329v4.pdf","comment":"14 pages, 16 figures. For source code, see\n  https://github.com/microsoft/tokenweave. In version 2, Figure 6 shows\n  All-Reduce bandwidth instead of Reduce-Scatter. The Multimem Reduce-Scatter\n  bandwidth formula differs slightly from the ring-based version. Fixed x-ticks\n  in Figure 7"},{"id":"http://arxiv.org/abs/2510.26389v1","updated":"2025-10-30T11:32:45Z","published":"2025-10-30T11:32:45Z","title":"Adaptive Context Length Optimization with Low-Frequency Truncation for\n  Multi-Agent Reinforcement Learning","summary":"  Recently, deep multi-agent reinforcement learning (MARL) has demonstrated\npromising performance for solving challenging tasks, such as long-term\ndependencies and non-Markovian environments. Its success is partly attributed\nto conditioning policies on large fixed context length. However, such large\nfixed context lengths may lead to limited exploration efficiency and redundant\ninformation. In this paper, we propose a novel MARL framework to obtain\nadaptive and effective contextual information. Specifically, we design a\ncentral agent that dynamically optimizes context length via temporal gradient\nanalysis, enhancing exploration to facilitate convergence to global optima in\nMARL. Furthermore, to enhance the adaptive optimization capability of the\ncontext length, we present an efficient input representation for the central\nagent, which effectively filters redundant information. By leveraging a\nFourier-based low-frequency truncation method, we extract global temporal\ntrends across decentralized agents, providing an effective and efficient\nrepresentation of the MARL environment. Extensive experiments demonstrate that\nthe proposed method achieves state-of-the-art (SOTA) performance on long-term\ndependency tasks, including PettingZoo, MiniGrid, Google Research Football\n(GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).\n","authors":["Wenchang Duan","Yaoliang Yu","Jiwan He","Yi Shi"],"pdf_url":"https://arxiv.org/pdf/2510.26389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17316v2","updated":"2025-10-30T11:32:37Z","published":"2025-07-23T08:30:37Z","title":"Nearly Minimax Discrete Distribution Estimation in Kullback-Leibler\n  Divergence with High Probability","summary":"  We consider the fundamental problem of estimating a discrete distribution on\na domain of size~$K$ with high probability in Kullback-Leibler divergence. We\nprovide upper and lower bounds on the minimax estimation rate, which show that\nthe optimal rate is between $\\big(K + \\ln(K)\\ln(1/\\delta)\\big) /n$ and\n$\\big(K\\ln\\ln(K) + \\ln(K)\\ln(1/\\delta)\\big) /n$ at error probability $\\delta$\nand sample size $n$, which pins down the rate up to the doubly logarithmic\nfactor $\\ln \\ln K$ that multiplies $K$. Our upper bound uses techniques from\nonline learning to construct a novel estimator via online-to-batch conversion.\nPerhaps surprisingly, the tail behavior of the minimax rate is worse than for\nthe squared total variation and squared Hellinger distance, for which it is\n$\\big(K + \\ln(1/\\delta)\\big) /n$, i.e.\\ without the $\\ln K$ multiplying $\\ln\n(1/\\delta)$. As a consequence, we cannot obtain a fully tight lower bound from\nthe usual reduction to these smaller distances. Moreover, we show that this\nlower bound cannot be achieved by the standard lower bound approach based on a\nreduction to hypothesis testing, and instead we need to introduce a new\nreduction to what we call weak hypothesis testing. We investigate the source of\nthe gap with other divergences further in refined results, which show that the\ntotal variation rate is achievable for Kullback-Leibler divergence after all\n(in fact by he maximum likelihood estimator) if we rule out outcome\nprobabilities smaller than $O(\\ln(K/\\delta) / n)$, which is a vanishing set as\n$n$ increases for fixed $K$ and~$\\delta$. This explains why minimax\nKullback-Leibler estimation is more difficult than asymptotic estimation.\n","authors":["Dirk van der Hoeven","Julia Olkhovskaia","Tim van Erven"],"pdf_url":"https://arxiv.org/pdf/2507.17316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26384v1","updated":"2025-10-30T11:28:58Z","published":"2025-10-30T11:28:58Z","title":"Scales++: Compute Efficient Evaluation Subset Selection with Cognitive\n  Scales Embeddings","summary":"  The prohibitive cost of evaluating large language models (LLMs) on\ncomprehensive benchmarks necessitates the creation of small yet representative\ndata subsets (i.e., tiny benchmarks) that enable efficient assessment while\nretaining predictive fidelity. Current methods for this task operate under a\nmodel-centric paradigm, selecting benchmarking items based on the collective\nperformance of existing models. Such approaches are limited by large upfront\ncosts, an inability to immediately handle new benchmarks (`cold-start'), and\nthe fragile assumption that future models will share the failure patterns of\ntheir predecessors. In this work, we challenge this paradigm and propose a\nitem-centric approach to benchmark subset selection, arguing that selection\nshould be based on the intrinsic properties of the task items themselves,\nrather than on model-specific failure patterns. We instantiate this\nitem-centric efficient benchmarking approach via a novel method, Scales++,\nwhere data selection is based on the cognitive demands of the benchmark\nsamples. Empirically, we show Scales++ reduces the upfront selection cost by\nover 18x while achieving competitive predictive fidelity. On the Open LLM\nLeaderboard, using just a 0.5\\% data subset, we predict full benchmark scores\nwith a 2.9% mean absolute error. We demonstrate that this item-centric approach\nenables more efficient model evaluation without significant fidelity\ndegradation, while also providing better cold-start performance and more\ninterpretable benchmarking.\n","authors":["Andrew M. Bean","Nabeel Seedat","Shengzhuang Chen","Jonathan Richard Schwarz"],"pdf_url":"https://arxiv.org/pdf/2510.26384v1.pdf","comment":"9 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2507.08896v2","updated":"2025-10-30T11:18:11Z","published":"2025-07-11T03:11:15Z","title":"Predictive Causal Inference via Spatio-Temporal Modeling and Penalized\n  Empirical Likelihood","summary":"  This study introduces an integrated framework for predictive causal inference\ndesigned to overcome limitations inherent in conventional single model\napproaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial\nhealth state estimation with a Multi Task and Multi Graph Convolutional Network\n(MTGCN) for capturing temporal outcome trajectories. The framework\nasymmetrically treats temporal and spatial information regarding them as\nendogenous variables in the outcome regression, and exogenous variables in the\npropensity score model, thereby expanding the standard doubly robust treatment\neffect estimation to jointly enhance bias correction and predictive accuracy.\nTo demonstrate its utility, we focus on clinical domains such as cancer,\ndementia, and Parkinson disease, where treatment effects are challenging to\nobserve directly. Simulation studies are conducted to emulate latent disease\ndynamics and evaluate the model performance under varying conditions. Overall,\nthe proposed framework advances predictive causal inference by structurally\nadapting to spatiotemporal complexities common in biomedical data.\n","authors":["Byunghee Lee","Hye Yeon Sin","Joonsung Kang"],"pdf_url":"https://arxiv.org/pdf/2507.08896v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26376v1","updated":"2025-10-30T11:16:22Z","published":"2025-10-30T11:16:22Z","title":"Efficient Generative AI Boosts Probabilistic Forecasting of Sudden\n  Stratospheric Warmings","summary":"  Sudden Stratospheric Warmings (SSWs) are key sources of subseasonal\npredictability and major drivers of extreme winter weather. Yet, their accurate\nand efficient forecast remains a persistent challenge for numerical weather\nprediction (NWP) systems due to limitations in physical representation,\ninitialization, and the immense computational demands of ensemble forecasts.\nWhile data-driven forecasting is rapidly evolving, its application to the\ncomplex, three-dimensional dynamics of SSWs, particularly for probabilistic\nforecast, remains underexplored. Here, we bridge this gap by developing a Flow\nMatching-based generative AI model (FM-Cast) for efficient and skillful\nprobabilistic forecasting of the spatiotemporal evolution of stratospheric\ncirculation. Evaluated across 18 major SSW events (1998-2024), FM-Cast\nskillfully forecasts the onset, intensity, and morphology of 10 events up to 20\ndays in advance, achieving ensemble accuracies above 50%. Its performance is\ncomparable to or exceeds leading NWP systems while requiring only two minutes\nfor a 50-member, 30-day forecast on a consumer GPU. Furthermore, leveraging\nFM-Cast as a scientific tool, we demonstrate through idealized experiments that\nSSW predictability is fundamentally linked to its underlying physical drivers,\ndistinguishing between events forced from the troposphere and those driven by\ninternal stratospheric dynamics. Our work thus establishes a computationally\nefficient paradigm for probabilistic forecasting stratospheric anomalies and\nshowcases generative AI's potential to deepen the physical understanding of\natmosphere-climate dynamics.\n","authors":["Ningning Tao","Fei Xie","Baoxiang Pan","Hongyu Wang","Han Huang","Zhongpu Qiu","Ke Gui","Jiali Luo","Xiaosong Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26369v1","updated":"2025-10-30T11:14:17Z","published":"2025-10-30T11:14:17Z","title":"CorVS: Person Identification via Video Trajectory-Sensor Correspondence\n  in a Real-World Warehouse","summary":"  Worker location data is key to higher productivity in industrial sites.\nCameras are a promising tool for localization in logistics warehouses since\nthey also offer valuable environmental contexts such as package status.\nHowever, identifying individuals with only visual data is often impractical.\nAccordingly, several prior studies identified people in videos by comparing\ntheir trajectories and wearable sensor measurements. While this approach has\nadvantages such as independence from appearance, the existing methods may break\ndown under real-world conditions. To overcome this challenge, we propose CorVS,\na novel data-driven person identification method based on correspondence\nbetween visual tracking trajectories and sensor measurements. Firstly, our deep\nlearning model predicts correspondence probabilities and reliabilities for\nevery pair of a trajectory and sensor measurements. Secondly, our algorithm\nmatches the trajectories and sensor measurements over time using the predicted\nprobabilities and reliabilities. We developed a dataset with actual warehouse\noperations and demonstrated the method's effectiveness for real-world\napplications.\n","authors":["Kazuma Kano","Yuki Mori","Shin Katayama","Kenta Urano","Takuro Yonezawa","Nobuo Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2510.26369v1.pdf","comment":"7 pages, 3 figures, accepted to IPIN 2025"},{"id":"http://arxiv.org/abs/2510.24817v2","updated":"2025-10-30T11:13:33Z","published":"2025-10-28T10:06:49Z","title":"Towards a Method for Synthetic Generation of Persons with Aphasia\n  Transcripts","summary":"  In aphasia research, Speech-Language Pathologists (SLPs) devote extensive\ntime to manually coding speech samples using Correct Information Units (CIUs),\na measure of how informative an individual sample of speech is. Developing\nautomated systems to recognize aphasic language is limited by data scarcity.\nFor example, only about 600 transcripts are available in AphasiaBank yet\nbillions of tokens are used to train large language models (LLMs). In the\nbroader field of machine learning (ML), researchers increasingly turn to\nsynthetic data when such are sparse. Therefore, this study constructs and\nvalidates two methods to generate synthetic transcripts of the AphasiaBank Cat\nRescue picture description task. One method leverages a procedural programming\napproach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct\nLLMs. The methods generate transcripts across four severity levels (Mild,\nModerate, Severe, Very Severe) through word dropping, filler insertion, and\nparaphasia substitution. Overall, we found, compared to human-elicited\ntranscripts, Mistral 7b Instruct best captures key aspects of linguistic\ndegradation observed in aphasia, showing realistic directional changes in NDW,\nword count, and word length amongst the synthetic generation methods. Based on\nthe results, future work should plan to create a larger dataset, fine-tune\nmodels for better aphasic representation, and have SLPs assess the realism and\nusefulness of the synthetic transcripts.\n","authors":["Jason M. Pittman","Anton Phillips Jr.","Yesenia Medina-Santos","Brielle C. Stark"],"pdf_url":"https://arxiv.org/pdf/2510.24817v2.pdf","comment":"19 pages, 1 figure, 7 tables"},{"id":"http://arxiv.org/abs/2510.26353v1","updated":"2025-10-30T11:05:15Z","published":"2025-10-30T11:05:15Z","title":"Towards Explainable and Reliable AI in Finance","summary":"  Financial forecasting increasingly uses large neural network models, but\ntheir opacity raises challenges for trust and regulatory compliance. We present\nseveral approaches to explainable and reliable AI in finance. \\emph{First}, we\ndescribe how Time-LLM, a time series foundation model, uses a prompt to avoid a\nwrong directional forecast. \\emph{Second}, we show that combining foundation\nmodels for time series forecasting with a reliability estimator can filter our\nunreliable predictions. \\emph{Third}, we argue for symbolic reasoning encoding\ndomain rules for transparent justification. These approaches shift emphasize\nexecuting only forecasts that are both reliable and explainable. Experiments on\nequity and cryptocurrency data show that the architecture reduces false\npositives and supports selective execution. By integrating predictive\nperformance with reliability estimation and rule-based reasoning, our framework\nadvances transparent and auditable financial AI systems.\n","authors":["Albi Isufaj","Pablo Mollá","Helmut Prendinger"],"pdf_url":"https://arxiv.org/pdf/2510.26353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26350v1","updated":"2025-10-30T11:01:57Z","published":"2025-10-30T11:01:57Z","title":"UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation","summary":"  Federated learning (FL) has emerged as a key paradigm for collaborative model\ntraining across multiple clients without sharing raw data, enabling\nprivacy-preserving applications in areas such as radiology and pathology.\nHowever, works on collaborative training across clients with fundamentally\ndifferent neural architectures and non-identically distributed datasets remain\nscarce. Existing FL frameworks face several limitations. Despite claiming to\nsupport architectural heterogeneity, most recent FL methods only tolerate\nvariants within a single model family (e.g., shallower, deeper, or wider CNNs),\nstill presuming a shared global architecture and failing to accommodate\nfederations where clients deploy fundamentally different network types (e.g.,\nCNNs, GNNs, MLPs). Moreover, existing approaches often address only statistical\nheterogeneity while overlooking the domain-fracture problem, where each\nclient's data distribution differs markedly from that faced at testing time,\nundermining model generalizability. When clients use different architectures,\nhave non-identically distributed data, and encounter distinct test domains,\ncurrent methods perform poorly. To address these challenges, we propose\nUnifiedFL, a dynamic federated learning framework that represents heterogeneous\nlocal networks as nodes and edges in a directed model graph optimized by a\nshared graph neural network (GNN). UnifiedFL introduces (i) a common GNN to\nparameterize all architectures, (ii) distance-driven clustering via Euclidean\ndistances between clients' parameters, and (iii) a two-tier aggregation policy\nbalancing convergence and diversity. Experiments on MedMNIST classification and\nhippocampus segmentation benchmarks demonstrate UnifiedFL's superior\nperformance. Code and data: https://github.com/basiralab/UnifiedFL\n","authors":["Furkan Pala","Islem Rekik"],"pdf_url":"https://arxiv.org/pdf/2510.26350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26347v1","updated":"2025-10-30T10:55:05Z","published":"2025-10-30T10:55:05Z","title":"Reinforcement Learning for Pollution Detection in a Randomized, Sparse\n  and Nonstationary Environment with an Autonomous Underwater Vehicle","summary":"  Reinforcement learning (RL) algorithms are designed to optimize\nproblem-solving by learning actions that maximize rewards, a task that becomes\nparticularly challenging in random and nonstationary environments. Even\nadvanced RL algorithms are often limited in their ability to solve problems in\nthese conditions. In applications such as searching for underwater pollution\nclouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate\nreward-sparse environments, where actions frequently result in a zero reward.\nThis paper aims to address these challenges by revisiting and modifying\nclassical RL approaches to efficiently operate in sparse, randomized, and\nnonstationary environments. We systematically study a large number of\nmodifications, including hierarchical algorithm changes, multigoal learning,\nand the integration of a location memory as an external output filter to\nprevent state revisits. Our results demonstrate that a modified Monte\nCarlo-based approach significantly outperforms traditional Q-learning and two\nexhaustive search patterns, illustrating its potential in adapting RL to\ncomplex environments. These findings suggest that reinforcement learning\napproaches can be effectively adapted for use in random, nonstationary, and\nreward-sparse environments.\n","authors":["Sebastian Zieglmeier","Niklas Erdmann","Narada D. Warakagoda"],"pdf_url":"https://arxiv.org/pdf/2510.26347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26345v1","updated":"2025-10-30T10:52:43Z","published":"2025-10-30T10:52:43Z","title":"MisSynth: Improving MISSCI Logical Fallacies Classification with\n  Synthetic Data","summary":"  Health-related misinformation is very prevalent and potentially harmful. It\nis difficult to identify, especially when claims distort or misinterpret\nscientific findings. We investigate the impact of synthetic data generation and\nlightweight fine-tuning techniques on the ability of large language models\n(LLMs) to recognize fallacious arguments using the MISSCI dataset and\nframework. In this work, we propose MisSynth, a pipeline that applies\nretrieval-augmented generation (RAG) to produce synthetic fallacy samples,\nwhich are then used to fine-tune an LLM model. Our results show substantial\naccuracy gains with fine-tuned models compared to vanilla baselines. For\ninstance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score\nabsolute improvement on the MISSCI test split over its vanilla baseline. We\ndemonstrate that introducing synthetic fallacy data to augment limited\nannotated resources can significantly enhance zero-shot LLM classification\nperformance on real-world scientific misinformation tasks, even with limited\ncomputational resources. The code and synthetic dataset are available on\nhttps://github.com/mxpoliakov/MisSynth.\n","authors":["Mykhailo Poliakov","Nadiya Shvai"],"pdf_url":"https://arxiv.org/pdf/2510.26345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26342v1","updated":"2025-10-30T10:49:25Z","published":"2025-10-30T10:49:25Z","title":"Linear Causal Discovery with Interventional Constraints","summary":"  Incorporating causal knowledge and mechanisms is essential for refining\ncausal models and improving downstream tasks such as designing new treatments.\nIn this paper, we introduce a novel concept in causal discovery, termed\ninterventional constraints, which differs fundamentally from interventional\ndata. While interventional data require direct perturbations of variables,\ninterventional constraints encode high-level causal knowledge in the form of\ninequality constraints on causal effects. For instance, in the Sachs dataset\n(Sachs et al.\\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3\nexerts a positive causal effect on Akt. Existing causal discovery methods allow\nenforcing structural constraints (for example, requiring a causal path from\nPIP3 to Akt), but they may still produce incorrect causal conclusions such as\nlearning that \"PIP3 inhibits Akt\". Interventional constraints bridge this gap\nby explicitly constraining the total causal effect between variable pairs,\nensuring learned models respect known causal influences. To formalize\ninterventional constraints, we propose a metric to quantify total causal\neffects for linear causal models and formulate the problem as a constrained\noptimization task, solved using a two-stage constrained optimization method. We\nevaluate our approach on real-world datasets and demonstrate that integrating\ninterventional constraints not only improves model accuracy and ensures\nconsistency with established findings, making models more explainable, but also\nfacilitates the discovery of new causal relationships that would otherwise be\ncostly to identify.\n","authors":["Zhigao Guo","Feng Dong"],"pdf_url":"https://arxiv.org/pdf/2510.26342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26340v1","updated":"2025-10-30T10:48:18Z","published":"2025-10-30T10:48:18Z","title":"SABER: Symbolic Regression-based Angle of Arrival and Beam Pattern\n  Estimator","summary":"  Accurate Angle-of-arrival (AoA) estimation is essential for next-generation\nwireless communication systems to enable reliable beamforming, high-precision\nlocalization, and integrated sensing. Unfortunately, classical high-resolution\ntechniques require multi-element arrays and extensive snapshot collection,\nwhile generic Machine Learning (ML) approaches often yield black-box models\nthat lack physical interpretability. To address these limitations, we propose a\nSymbolic Regression (SR)-based ML framework. Namely, Symbolic Regression-based\nAngle of Arrival and Beam Pattern Estimator (SABER), a constrained\nsymbolic-regression framework that automatically discovers closed-form beam\npattern and AoA models from path loss measurements with interpretability. SABER\nachieves high accuracy while bridging the gap between opaque ML methods and\ninterpretable physics-driven estimators. First, we validate our approach in a\ncontrolled free-space anechoic chamber, showing that both direct inversion of\nthe known $\\cos^n$ beam and a low-order polynomial surrogate achieve sub-0.5\ndegree Mean Absolute Error (MAE). A purely unconstrained SR method can further\nreduce the error of the predicted angles, but produces complex formulas that\nlack physical insight. Then, we implement the same SR-learned inversions in a\nreal-world, Reconfigurable Intelligent Surface (RIS)-aided indoor testbed.\nSABER and unconstrained SR models accurately recover the true AoA with\nnear-zero error. Finally, we benchmark SABER against the Cram\\'er-Rao Lower\nBounds (CRLBs). Our results demonstrate that SABER is an interpretable and\naccurate alternative to state-of-the-art and black-box ML-based methods for AoA\nestimation.\n","authors":["Shih-Kai Chou","Mengran Zhao","Cheng-Nan Hu","Kuang-Chung Chou","Carolina Fortuna","Jernej Hribar"],"pdf_url":"https://arxiv.org/pdf/2510.26340v1.pdf","comment":"12 pages, 11 figures"},{"id":"http://arxiv.org/abs/2402.03145v4","updated":"2025-10-30T10:40:47Z","published":"2024-02-05T16:12:36Z","title":"SafEDMD: A Koopman-based data-driven controller design framework for\n  nonlinear dynamical systems","summary":"  The Koopman operator serves as the theoretical backbone for machine learning\nof dynamical control systems, where the operator is heuristically approximated\nby extended dynamic mode decomposition (EDMD). In this paper, we propose\nSafEDMD, a novel stability- and feedback-oriented EDMD-based controller design\nframework. Our approach leverages a reliable surrogate model generated in a\ndata-driven fashion in order to provide closed-loop guarantees. In particular,\nwe establish a controller design based on semi-definite programming with\nguaranteed stabilization of the underlying nonlinear system. As central\ningredient, we derive proportional error bounds that vanish at the origin and\nare tailored to control tasks. We illustrate the developed method by means of\nseveral benchmark examples and highlight the advantages over state-of-the-art\nmethods.\n","authors":["Robin Strässer","Manuel Schaller","Karl Worthmann","Julian Berberich","Frank Allgöwer"],"pdf_url":"https://arxiv.org/pdf/2402.03145v4.pdf","comment":"Accepted for publication in Automatica"},{"id":"http://arxiv.org/abs/2506.07500v2","updated":"2025-10-30T10:28:19Z","published":"2025-06-09T07:25:51Z","title":"Mind the Gap: Removing the Discretization Gap in Differentiable Logic\n  Gate Networks","summary":"  Modern neural networks demonstrate state-of-the-art performance on numerous\nexisting benchmarks; however, their high computational requirements and energy\nconsumption prompt researchers to seek more efficient solutions for real-world\ndeployment. Logic gate networks (LGNs) learns a large network of logic gates\nfor efficient image classification. However, learning a network that can solve\na simple problem like CIFAR-10 can take days to weeks to train. Even then,\nalmost half of the network remains unused, causing a discretization gap. This\ndiscretization gap hinders real-world deployment of LGNs, as the performance\ndrop between training and inference negatively impacts accuracy. We inject\nGumbel noise with a straight-through estimator during training to significantly\nspeed up training, improve neuron utilization, and decrease the discretization\ngap. We theoretically show that this results from implicit Hessian\nregularization, which improves the convergence properties of LGNs. We train\nnetworks $4.5 \\times$ faster in wall-clock time, reduce the discretization gap\nby $98\\%$, and reduce the number of unused gates by $100\\%$.\n","authors":["Shakir Yousefi","Andreas Plesner","Till Aczel","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2506.07500v2.pdf","comment":"Accepted to NeurIPS 2025 (main track)"},{"id":"http://arxiv.org/abs/2510.26328v1","updated":"2025-10-30T10:27:11Z","published":"2025-10-30T10:27:11Z","title":"Agent Skills Enable a New Class of Realistic and Trivially Simple Prompt\n  Injections","summary":"  Enabling continual learning in LLMs remains a key unresolved research\nchallenge. In a recent announcement, a frontier LLM company made a step towards\nthis by introducing Agent Skills, a framework that equips agents with new\nknowledge based on instructions stored in simple markdown files. Although Agent\nSkills can be a very useful tool, we show that they are fundamentally insecure,\nsince they enable trivially simple prompt injections. We demonstrate how to\nhide malicious instructions in long Agent Skill files and referenced scripts to\nexfiltrate sensitive data, such as internal files or passwords. Importantly, we\nshow how to bypass system-level guardrails of a popular coding agent: a benign,\ntask-specific approval with the \"Don't ask again\" option can carry over to\nclosely related but harmful actions. Overall, we conclude that despite ongoing\nresearch efforts and scaling model capabilities, frontier LLMs remain\nvulnerable to very simple prompt injections in realistic scenarios. Our code is\navailable at https://github.com/aisa-group/promptinject-agent-skills.\n","authors":["David Schmotz","Sahar Abdelnabi","Maksym Andriushchenko"],"pdf_url":"https://arxiv.org/pdf/2510.26328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21038v2","updated":"2025-10-30T10:23:32Z","published":"2025-10-23T22:44:50Z","title":"Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the\n  LibriBrain Dataset","summary":"  Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from\nlarge, public benchmarks. However, current benchmarks target relatively simple,\nfoundational tasks like Speech Detection and Phoneme Classification, while\napplication-ready results on tasks like Brain-to-Text remain elusive. We\npropose Keyword Spotting (KWS) as a practically applicable, privacy-aware\nintermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we\nprovide standardized train/validation/test splits for reproducible\nbenchmarking, and adopt an evaluation protocol tailored to extreme class\nimbalance. Concretely, we use area under the precision-recall curve (AUPRC) as\na robust evaluation metric, complemented by false alarms per hour (FA/h) at\nfixed recall to capture user-facing trade-offs. To simplify deployment and\nfurther experimentation within the research community, we are releasing an\nupdated version of the pnpl library with word-level dataloaders and Colab-ready\ntutorials. As an initial reference model, we present a compact 1-D Conv/ResNet\nbaseline with focal loss and top-k pooling that is trainable on a single\nconsumer-class GPU. The reference model achieves approximately 13x the\npermutation baseline AUPRC on held-out sessions, demonstrating the viability of\nthe task. Exploratory analyses reveal: (i) predictable within-subject scaling -\nperformance improves log-linearly with more training hours - and (ii) the\nexistence of word-level factors (frequency and duration) that systematically\nmodulate detectability.\n","authors":["Gereon Elvers","Gilad Landau","Oiwi Parker Jones"],"pdf_url":"https://arxiv.org/pdf/2510.21038v2.pdf","comment":"16 pages, 7 figures, 6 tables; updated acknowledgments"},{"id":"http://arxiv.org/abs/2510.26324v1","updated":"2025-10-30T10:17:27Z","published":"2025-10-30T10:17:27Z","title":"Posterior Sampling by Combining Diffusion Models with Annealed Langevin\n  Dynamics","summary":"  Given a noisy linear measurement $y = Ax + \\xi$ of a distribution $p(x)$, and\na good approximation to the prior $p(x)$, when can we sample from the posterior\n$p(x \\mid y)$? Posterior sampling provides an accurate and fair framework for\ntasks such as inpainting, deblurring, and MRI reconstruction, and several\nheuristics attempt to approximate it. Unfortunately, approximate posterior\nsampling is computationally intractable in general.\n  To sidestep this hardness, we focus on (local or global) log-concave\ndistributions $p(x)$. In this regime, Langevin dynamics yields posterior\nsamples when the exact scores of $p(x)$ are available, but it is brittle to\nscore--estimation error, requiring an MGF bound (sub-exponential error). By\ncontrast, in the unconditional setting, diffusion models succeed with only an\n$L^2$ bound on the score error. We prove that combining diffusion models with\nan annealed variant of Langevin dynamics achieves conditional sampling in\npolynomial time using merely an $L^4$ bound on the score error.\n","authors":["Zhiyang Xun","Shivam Gupta","Eric Price"],"pdf_url":"https://arxiv.org/pdf/2510.26324v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26323v1","updated":"2025-10-30T10:17:25Z","published":"2025-10-30T10:17:25Z","title":"On the Impact of Weight Discretization in QUBO-Based SVM Training","summary":"  Training Support Vector Machines (SVMs) can be formulated as a QUBO problem,\nenabling the use of quantum annealing for model optimization. In this work, we\nstudy how the number of qubits - linked to the discretization level of dual\nweights - affects predictive performance across datasets. We compare QUBO-based\nSVM training to the classical LIBSVM solver and find that even low-precision\nQUBO encodings (e.g., 1 bit per parameter) yield competitive, and sometimes\nsuperior, accuracy. While increased bit-depth enables larger regularization\nparameters, it does not always improve classification. Our findings suggest\nthat selecting the right support vectors may matter more than their precise\nweighting. Although current hardware limits the size of solvable QUBOs, our\nresults highlight the potential of quantum annealing for efficient SVM training\nas quantum devices scale.\n","authors":["Sascha Mücke"],"pdf_url":"https://arxiv.org/pdf/2510.26323v1.pdf","comment":"Presented at the 7th DSO Workshop at ECML PKDD 2025"},{"id":"http://arxiv.org/abs/2506.20535v2","updated":"2025-10-30T10:14:59Z","published":"2025-06-25T15:24:45Z","title":"AIMeter: Measuring, Analyzing, and Visualizing Energy and Carbon\n  Footprint of AI Workloads","summary":"  The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents AIMeter, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, AIMeter offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, AIMeter encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/AIMeter.\n","authors":["Hongzhen Huang","Kunming Zhang","Hanlong Liao","Kui Wu","Guoming Tang"],"pdf_url":"https://arxiv.org/pdf/2506.20535v2.pdf","comment":"11 pages, 7 figures and 5 tables"},{"id":"http://arxiv.org/abs/2510.26311v1","updated":"2025-10-30T09:58:48Z","published":"2025-10-30T09:58:48Z","title":"Model Inversion with Layer-Specific Modeling and Alignment for Data-Free\n  Continual Learning","summary":"  Continual learning (CL) aims to incrementally train a model on a sequence of\ntasks while retaining performance on prior ones. However, storing and replaying\ndata is often infeasible due to privacy or security constraints and impractical\nfor arbitrary pre-trained models. Data-free CL seeks to update models without\naccess to previous data. Beyond regularization, we employ model inversion to\nsynthesize data from the trained model, enabling replay without storing\nsamples. Yet, model inversion in predictive models faces two challenges: (1)\ngenerating inputs solely from compressed output labels causes drift between\nsynthetic and real data, and replaying such data can erode prior knowledge; (2)\ninversion is computationally expensive since each step backpropagates through\nthe full model. These issues are amplified in large pre-trained models such as\nCLIP. To improve efficiency, we propose Per-layer Model Inversion (PMI),\ninspired by faster convergence in single-layer optimization. PMI provides\nstrong initialization for full-model inversion, substantially reducing\niterations. To mitigate feature shift, we model class-wise features via\nGaussian distributions and contrastive model, ensuring alignment between\nsynthetic and real features. Combining PMI and feature modeling, our approach\nenables continual learning of new classes by generating pseudo-images from\nsemantic-aware projected features, achieving strong effectiveness and\ncompatibility across multiple CL settings.\n","authors":["Ruilin Tong","Haodong Lu","Yuhang Liu","Dong Gong"],"pdf_url":"https://arxiv.org/pdf/2510.26311v1.pdf","comment":"Accepted in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26307v1","updated":"2025-10-30T09:49:59Z","published":"2025-10-30T09:49:59Z","title":"A Survey of Heterogeneous Graph Neural Networks for Cybersecurity\n  Anomaly Detection","summary":"  Anomaly detection is a critical task in cybersecurity, where identifying\ninsider threats, access violations, and coordinated attacks is essential for\nensuring system resilience. Graph-based approaches have become increasingly\nimportant for modeling entity interactions, yet most rely on homogeneous and\nstatic structures, which limits their ability to capture the heterogeneity and\ntemporal evolution of real-world environments. Heterogeneous Graph Neural\nNetworks (HGNNs) have emerged as a promising paradigm for anomaly detection by\nincorporating type-aware transformations and relation-sensitive aggregation,\nenabling more expressive modeling of complex cyber data. However, current\nresearch on HGNN-based anomaly detection remains fragmented, with diverse\nmodeling strategies, limited comparative evaluation, and an absence of\nstandardized benchmarks. To address this gap, we provide a comprehensive survey\nof HGNN-based anomaly detection methods in cybersecurity. We introduce a\ntaxonomy that classifies approaches by anomaly type and graph dynamics, analyze\nrepresentative models, and map them to key cybersecurity applications. We also\nreview commonly used benchmark datasets and evaluation metrics, highlighting\ntheir strengths and limitations. Finally, we identify key open challenges\nrelated to modeling, data, and deployment, and outline promising directions for\nfuture research. This survey aims to establish a structured foundation for\nadvancing HGNN-based anomaly detection toward scalable, interpretable, and\npractically deployable solutions.\n","authors":["Laura Jiang","Reza Ryan","Qian Li","Nasim Ferdosian"],"pdf_url":"https://arxiv.org/pdf/2510.26307v1.pdf","comment":"37 pages, 4 figures, 86 references. Submitted to Journal of Computer\n  Security (under review)"},{"id":"http://arxiv.org/abs/2410.09766v2","updated":"2025-10-30T09:48:49Z","published":"2024-10-13T07:50:47Z","title":"Stability and Sharper Risk Bounds with Convergence Rate\n  $\\tilde{O}(1/n^2)$","summary":"  Prior work (Klochkov $\\&$ Zhivotovskiy, 2021) establishes at most\n$O\\left(\\log (n)/n\\right)$ excess risk bounds via algorithmic stability for\nstrongly-convex learners with high probability. We show that under the similar\ncommon assumptions -- - Polyak-Lojasiewicz condition, smoothness, and Lipschitz\ncontinous for losses -- - rates of $O\\left(\\log^2(n)/n^2\\right)$ are at most\nachievable. To our knowledge, our analysis also provides the tightest\nhigh-probability bounds for gradient-based generalization gaps in nonconvex\nsettings.\n","authors":["Bowei Zhu","Shaojie Li","Mingyang Yi","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.09766v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10573v3","updated":"2025-10-30T09:42:47Z","published":"2024-11-15T20:46:58Z","title":"Hysteresis Activation Function for Efficient Inference","summary":"  The widely used ReLU is favored for its hardware efficiency, {as the\nimplementation at inference is a one bit sign case,} yet suffers from issues\nsuch as the ``dying ReLU'' problem, where during training, neurons fail to\nactivate and constantly remain at zero, as highlighted by Lu et al. Traditional\napproaches to mitigate this issue often introduce more complex and less\nhardware-friendly activation functions. In this work, we propose a Hysteresis\nRectified Linear Unit (HeLU), an efficient activation function designed to\naddress the ``dying ReLU'' problem with minimal complexity. Unlike traditional\nactivation functions with fixed thresholds for training and inference, HeLU\nemploys a variable threshold that refines the backpropagation. This refined\nmechanism allows simpler activation functions to achieve competitive\nperformance comparable to their more complex counterparts without introducing\nunnecessary complexity or requiring inductive biases. Empirical evaluations\ndemonstrate that HeLU enhances model generalization across diverse datasets,\noffering a promising solution for efficient and effective inference suitable\nfor a wide range of neural network architectures.\n","authors":["Moshe Kimhi","Idan Kashani","Avi Mendelson","Chaim Baskin"],"pdf_url":"https://arxiv.org/pdf/2411.10573v3.pdf","comment":"Accepted to 4th NeurIPS Efficient Natural Language and Speech\n  Processing Workshop (ENLSP-IV 2024)"},{"id":"http://arxiv.org/abs/2510.26303v1","updated":"2025-10-30T09:41:33Z","published":"2025-10-30T09:41:33Z","title":"Implicit Bias of Per-sample Adam on Separable Data: Departure from the\n  Full-batch Regime","summary":"  Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet\nits theoretical understanding remains limited. Prior analyses show that Adam\nfavors solutions aligned with $\\ell_\\infty$-geometry, but these results are\nrestricted to the full-batch regime. In this work, we study the implicit bias\nof incremental Adam (using one sample per step) for logistic regression on\nlinearly separable data, and we show that its bias can deviate from the\nfull-batch behavior. To illustrate this, we construct a class of structured\ndatasets where incremental Adam provably converges to the $\\ell_2$-max-margin\nclassifier, in contrast to the $\\ell_\\infty$-max-margin bias of full-batch\nAdam. For general datasets, we develop a proxy algorithm that captures the\nlimiting behavior of incremental Adam as $\\beta_2 \\to 1$ and we characterize\nits convergence direction via a data-dependent dual fixed-point formulation.\nFinally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges\nto the $\\ell_\\infty$-max-margin classifier for any batch size by taking $\\beta$\nclose enough to 1. Overall, our results highlight that the implicit bias of\nAdam crucially depends on both the batching scheme and the dataset, while\nSignum remains invariant.\n","authors":["Beomhan Baek","Minhak Song","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2510.26303v1.pdf","comment":"50 pages"},{"id":"http://arxiv.org/abs/2510.26302v1","updated":"2025-10-30T09:41:21Z","published":"2025-10-30T09:41:21Z","title":"Understanding Hardness of Vision-Language Compositionality from A\n  Token-level Causal Lens","summary":"  Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal\ngeneralization by aligning images and texts in a shared embedding space, yet it\npersistently fails at compositional reasoning over objects, attributes, and\nrelations often behaving like a bag-of-words matcher. Prior causal accounts\ntypically model text as a single vector, obscuring token-level structure and\nleaving core phenomena-such as prompt sensitivity and failures on hard\nnegatives unexplained. We address this gap with a token-aware causal\nrepresentation learning (CRL) framework grounded in a sequential,\nlanguage-token SCM. Our theory extends block identifiability to tokenized text,\nproving that CLIP's contrastive objective can recover the modal-invariant\nlatent variable under both sentence-level and token-level SCMs. Crucially,\ntoken granularity yields the first principled explanation of CLIP's\ncompositional brittleness: composition nonidentifiability. We show the\nexistence of pseudo-optimal text encoders that achieve perfect modal-invariant\nalignment yet are provably insensitive to SWAP, REPLACE, and ADD operations\nover atomic concepts, thereby failing to distinguish correct captions from hard\nnegatives despite optimizing the same training objective as true-optimal\nencoders. The analysis further links language-side nonidentifiability to\nvisual-side failures via the modality gap and shows how iterated composition\noperators compound hardness, motivating improved negative mining strategies.\n","authors":["Ziliang Chen","Tianang Xiao","Jusheng Zhang","Yongsen Zheng","Xipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26301v1","updated":"2025-10-30T09:39:05Z","published":"2025-10-30T09:39:05Z","title":"Offline Clustering of Preference Learning with Active-data Augmentation","summary":"  Preference learning from pairwise feedback is a widely adopted framework in\napplications such as reinforcement learning with human feedback and\nrecommendations. In many practical settings, however, user interactions are\nlimited or costly, making offline preference learning necessary. Moreover,\nreal-world preference learning often involves users with different preferences.\nFor example, annotators from different backgrounds may rank the same responses\ndifferently. This setting presents two central challenges: (1) identifying\nsimilarity across users to effectively aggregate data, especially under\nscenarios where offline data is imbalanced across dimensions, and (2) handling\nthe imbalanced offline data where some preference dimensions are\nunderrepresented. To address these challenges, we study the Offline Clustering\nof Preference Learning problem, where the learner has access to fixed datasets\nfrom multiple users with potentially different preferences and aims to maximize\nutility for a test user. To tackle the first challenge, we first propose\nOff-C$^2$PL for the pure offline setting, where the learner relies solely on\noffline data. Our theoretical analysis provides a suboptimality bound that\nexplicitly captures the tradeoff between sample noise and bias. To address the\nsecond challenge of inbalanced data, we extend our framework to the setting\nwith active-data augmentation where the learner is allowed to select a limited\nnumber of additional active-data for the test user based on the cluster\nstructure learned by Off-C$^2$PL. In this setting, our second algorithm,\nA$^2$-Off-C$^2$PL, actively selects samples that target the least-informative\ndimensions of the test user's preference. We prove that these actively\ncollected samples contribute more effectively than offline ones. Finally, we\nvalidate our theoretical results through simulations on synthetic and\nreal-world datasets.\n","authors":["Jingyuan Liu","Fatemeh Ghaffari","Xuchuang Wang","Mohammad Hajiesmaili","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2510.26301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22319v2","updated":"2025-10-30T09:33:15Z","published":"2025-10-25T14:51:17Z","title":"GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via\n  Regulated Clipping","summary":"  Recently, GRPO-based reinforcement learning has shown remarkable progress in\noptimizing flow-matching models, effectively improving their alignment with\ntask-specific rewards. Within these frameworks, the policy update relies on\nimportance-ratio clipping to constrain overconfident positive and negative\ngradients. However, in practice, we observe a systematic shift in the\nimportance-ratio distribution-its mean falls below 1 and its variance differs\nsubstantially across timesteps. This left-shifted and inconsistent distribution\nprevents positive-advantage samples from entering the clipped region, causing\nthe mechanism to fail in constraining overconfident positive updates. As a\nresult, the policy model inevitably enters an implicit over-optimization\nstage-while the proxy reward continues to increase, essential metrics such as\nimage quality and text-prompt alignment deteriorate sharply, ultimately making\nthe learned policy impractical for real-world use. To address this issue, we\nintroduce GRPO-Guard, a simple yet effective enhancement to existing GRPO\nframeworks. Our method incorporates ratio normalization, which restores a\nbalanced and step-consistent importance ratio, ensuring that PPO clipping\nproperly constrains harmful updates across denoising timesteps. In addition, a\ngradient reweighting strategy equalizes policy gradients over noise conditions,\npreventing excessive updates from particular timestep regions. Together, these\ndesigns act as a regulated clipping mechanism, stabilizing optimization and\nsubstantially mitigating implicit over-optimization without relying on heavy KL\nregularization. Extensive experiments on multiple diffusion backbones (e.g.,\nSD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard\nsignificantly reduces over-optimization while maintaining or even improving\ngeneration quality.\n","authors":["Jing Wang","Jiajun Liang","Jie Liu","Henglin Liu","Gongye Liu","Jun Zheng","Wanyuan Pang","Ao Ma","Zhenyu Xie","Xintao Wang","Meng Wang","Pengfei Wan","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2510.22319v2.pdf","comment":"Project Page: https://jingw193.github.io/GRPO-Guard/"},{"id":"http://arxiv.org/abs/2505.22151v2","updated":"2025-10-30T09:26:54Z","published":"2025-05-28T09:17:44Z","title":"Oryx: a Scalable Sequence Model for Many-Agent Coordination in Offline\n  MARL","summary":"  A key challenge in offline multi-agent reinforcement learning (MARL) is\nachieving effective many-agent multi-step coordination in complex environments.\nIn this work, we propose Oryx, a novel algorithm for offline cooperative MARL\nto directly address this challenge. Oryx adapts the recently proposed\nretention-based architecture Sable and combines it with a sequential form of\nimplicit constraint Q-learning (ICQ), to develop a novel offline autoregressive\npolicy update scheme. This allows Oryx to solve complex coordination challenges\nwhile maintaining temporal coherence over long trajectories. We evaluate Oryx\nacross a diverse set of benchmarks from prior works -- SMAC, RWARE, and\nMulti-Agent MuJoCo -- covering tasks of both discrete and continuous control,\nvarying in scale and difficulty. Oryx achieves state-of-the-art performance on\nmore than 80% of the 65 tested datasets, outperforming prior offline MARL\nmethods and demonstrating robust generalisation across domains with many agents\nand long horizons. Finally, we introduce new datasets to push the limits of\nmany-agent coordination in offline MARL, and demonstrate Oryx's superior\nability to scale effectively in such settings.\n","authors":["Claude Formanek","Omayma Mahjoub","Louay Ben Nessir","Sasha Abramowitz","Ruan de Kock","Wiem Khlifi","Daniel Rajaonarivonivelomanantsoa","Simon Du Toit","Arnol Fokam","Siddarth Singh","Ulrich Mbou Sob","Felix Chalumeau","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2505.22151v2.pdf","comment":"Published at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2505.13138v2","updated":"2025-10-30T09:22:20Z","published":"2025-05-19T14:07:47Z","title":"Neurosymbolic Diffusion Models","summary":"  Neurosymbolic (NeSy) predictors combine neural perception with symbolic\nreasoning to solve tasks like visual reasoning. However, standard NeSy\npredictors assume conditional independence between the symbols they extract,\nthus limiting their ability to model interactions and uncertainty - often\nleading to overconfident predictions and poor out-of-distribution\ngeneralisation. To overcome the limitations of the independence assumption, we\nintroduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy\npredictors that use discrete diffusion to model dependencies between symbols.\nOur approach reuses the independence assumption from NeSy predictors at each\nstep of the diffusion process, enabling scalable learning while capturing\nsymbol dependencies and uncertainty quantification. Across both synthetic and\nreal-world benchmarks - including high-dimensional visual path planning and\nrule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among\nNeSy predictors and demonstrate strong calibration.\n","authors":["Emile van Krieken","Pasquale Minervini","Edoardo Ponti","Antonio Vergari"],"pdf_url":"https://arxiv.org/pdf/2505.13138v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.03509v2","updated":"2025-10-30T09:17:12Z","published":"2025-05-06T13:19:15Z","title":"AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised\n  and Active Learning","summary":"  Anomaly detection in large datasets is essential in astronomy and computer\nvision. However, due to a scarcity of labelled data, it is often infeasible to\napply supervised methods to anomaly detection. We present AnomalyMatch, an\nanomaly detection framework combining the semi-supervised FixMatch algorithm\nusing EfficientNet classifiers with active learning. AnomalyMatch is tailored\nfor large-scale applications and integrated into the ESA Datalabs science\nplatform. In this method, we treat anomaly detection as a binary classification\nproblem and efficiently utilise limited labelled and abundant unlabelled images\nfor training. We enable active learning via a user interface for verification\nof high-confidence anomalies and correction of false positives. Evaluations on\nthe GalaxyMNIST astronomical dataset and the miniImageNet natural-image\nbenchmark under severe class imbalance display strong performance. Starting\nfrom five to ten labelled anomalies, we achieve an average AUROC of 0.96\n(miniImageNet) and 0.89 (GalaxyMNIST), with respective AUPRC of 0.82 and 0.77.\nAfter three active learning cycles, anomalies are ranked with 76%\n(miniImageNet) to 94% (GalaxyMNIST) precision in the top 1% of the\nhighest-ranking images by score. We compare to the established Astronomaly\nsoftware on selected 'odd' galaxies from the 'Galaxy Zoo - The Galaxy\nChallenge' dataset, achieving comparable performance with an average AUROC of\n0.83. Our results underscore the exceptional utility and scalability of this\napproach for anomaly discovery, highlighting the value of specialised\napproaches for domains characterised by severe label scarcity.\n","authors":["Pablo Gómez","Laslo E. Ruhberg","Maria Teresa Nardone","David O'Ryan"],"pdf_url":"https://arxiv.org/pdf/2505.03509v2.pdf","comment":"Journal submission in preparation to RASTI; 15 pages; 12 figures"},{"id":"http://arxiv.org/abs/2502.04380v3","updated":"2025-10-30T09:16:49Z","published":"2025-02-05T17:21:01Z","title":"Diversity as a Reward: Fine-Tuning LLMs on a Mixture of\n  Domain-Undetermined Data","summary":"  Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this work, we investigate the role of data diversity in\nenhancing the overall abilities of LLMs by empirically constructing contrastive\ndata pools and theoretically deriving explanations. Building upon the insights\ngained, we propose a new method that gives the LLM a dual identity: an output\nmodel to cognitively probe and select data based on diversity reward, as well\nas an input model to be tuned with the selected data. Extensive experiments\nshow that the proposed method notably boosts performance across\ndomain-undetermined data and a series of foundational downstream tasks when\napplied to various advanced LLMs. We release our code and hope this study can\nshed light on the understanding of data diversity and advance feedback-driven\ndata-model co-design for LLMs.\n","authors":["Zhenqing Ling","Daoyuan Chen","Liuyi Yao","Qianli Shen","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2502.04380v3.pdf","comment":"Accepted by NeurIPS'25 main track. 47 pages, 21 figures, 32 tables"},{"id":"http://arxiv.org/abs/2510.04237v3","updated":"2025-10-30T09:14:25Z","published":"2025-10-05T15:04:03Z","title":"Truncated Kernel Stochastic Gradient Descent with General Losses and\n  Spherical Radial Basis Functions","summary":"  In this paper, we propose a novel kernel stochastic gradient descent (SGD)\nalgorithm for large-scale supervised learning with general losses. Compared to\ntraditional kernel SGD, our algorithm improves efficiency and scalability\nthrough an innovative regularization strategy. By leveraging the infinite\nseries expansion of spherical radial basis functions, this strategy projects\nthe stochastic gradient onto a finite-dimensional hypothesis space, which is\nadaptively scaled according to the bias-variance trade-off, thereby enhancing\ngeneralization performance. Based on a new estimation of the spectral structure\nof the kernel-induced covariance operator, we develop an analytical framework\nthat unifies optimization and generalization analyses. We prove that both the\nlast iterate and the suffix average converge at minimax-optimal rates, and we\nfurther establish optimal strong convergence in the reproducing kernel Hilbert\nspace. Our framework accommodates a broad class of classical loss functions,\nincluding least-squares, Huber, and logistic losses. Moreover, the proposed\nalgorithm significantly reduces computational complexity and achieves optimal\nstorage complexity by incorporating coordinate-wise updates from linear SGD,\nthereby avoiding the costly pairwise operations typical of kernel SGD and\nenabling efficient processing of streaming data. Finally, extensive numerical\nexperiments demonstrate the efficiency of our approach.\n","authors":["Jinhui Bai","Andreas Christmann","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2510.04237v3.pdf","comment":"54 pages, 20 figures"},{"id":"http://arxiv.org/abs/2503.12902v2","updated":"2025-10-30T09:10:57Z","published":"2025-03-17T08:03:47Z","title":"Experiments with Optimal Model Trees","summary":"  Model trees provide an appealing way to perform interpretable machine\nlearning for both classification and regression problems. In contrast to\n``classic'' decision trees with constant values in their leaves, model trees\ncan use linear combinations of predictor variables in their leaf nodes to form\npredictions, which can help achieve higher accuracy and smaller trees. Typical\nalgorithms for learning model trees from training data work in a greedy\nfashion, growing the tree in a top-down manner by recursively splitting the\ndata into smaller and smaller subsets. Crucially, the selected splits are only\nlocally optimal, potentially rendering the tree overly complex and less\naccurate than a tree whose structure is globally optimal for the training data.\nIn this paper, we empirically investigate the effect of constructing globally\noptimal model trees for classification and regression with linear support\nvector machines at the leaf nodes. To this end, we present mixed-integer linear\nprogramming formulations to learn optimal trees, compute such trees for a large\ncollection of benchmark data sets, and compare their performance against\ngreedily grown model trees in terms of interpretability and accuracy. We also\ncompare to classic optimal and greedily grown decision trees, random forests,\nand support vector machines. Our results show that optimal model trees can\nachieve competitive accuracy with very small trees. We also investigate the\neffect on the accuracy of replacing axis-parallel splits with multivariate\nones, foregoing interpretability while potentially obtaining greater accuracy.\n","authors":["Sabino Francesco Roselli","Eibe Frank"],"pdf_url":"https://arxiv.org/pdf/2503.12902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26285v1","updated":"2025-10-30T09:08:50Z","published":"2025-10-30T09:08:50Z","title":"Unravelling the Mechanisms of Manipulating Numbers in Language Models","summary":"  Recent work has shown that different large language models (LLMs) converge to\nsimilar and accurate input embedding representations for numbers. These\nfindings conflict with the documented propensity of LLMs to produce erroneous\noutputs when dealing with numeric information. In this work, we aim to explain\nthis conflict by exploring how language models manipulate numbers and quantify\nthe lower bounds of accuracy of these mechanisms. We find that despite\nsurfacing errors, different language models learn interchangeable\nrepresentations of numbers that are systematic, highly accurate and universal\nacross their hidden states and the types of input contexts. This allows us to\ncreate universal probes for each LLM and to trace information -- including the\ncauses of output errors -- to specific layers. Our results lay a fundamental\nunderstanding of how pre-trained LLMs manipulate numbers and outline the\npotential of more accurate probing techniques in addressed refinements of LLMs'\narchitectures.\n","authors":["Michal Štefánik","Timothee Mickus","Marek Kadlčík","Bertram Højer","Michal Spiegel","Raúl Vázquez","Aman Sinha","Josef Kuchař","Philipp Mondorf"],"pdf_url":"https://arxiv.org/pdf/2510.26285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26284v1","updated":"2025-10-30T09:08:07Z","published":"2025-10-30T09:08:07Z","title":"Empirical Bayesian Multi-Bandit Learning","summary":"  Multi-task learning in contextual bandits has attracted significant research\ninterest due to its potential to enhance decision-making across multiple\nrelated tasks by leveraging shared structures and task-specific heterogeneity.\nIn this article, we propose a novel hierarchical Bayesian framework for\nlearning in various bandit instances. This framework captures both the\nheterogeneity and the correlations among different bandit instances through a\nhierarchical Bayesian model, enabling effective information sharing while\naccommodating instance-specific variations. Unlike previous methods that\noverlook the learning of the covariance structure across bandits, we introduce\nan empirical Bayesian approach to estimate the covariance matrix of the prior\ndistribution.This enhances both the practicality and flexibility of learning\nacross multi-bandits. Building on this approach, we develop two efficient\nalgorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and\nebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which\nincorporate the estimated prior into the decision-making process. We provide\nthe frequentist regret upper bounds for the proposed algorithms, thereby\nfilling a research gap in the field of multi-bandit problems. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the superior\nperformance of our algorithms, particularly in complex environments. Our\nmethods achieve lower cumulative regret compared to existing techniques,\nhighlighting their effectiveness in balancing exploration and exploitation\nacross multi-bandits.\n","authors":["Xia Jiang","Rong J. B. Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.26284v1.pdf","comment":"33 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.15475v3","updated":"2025-10-30T09:02:24Z","published":"2025-02-21T14:00:14Z","title":"Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning\n  Solution for Protocols Compliance","summary":"  Neural network-based decoding methods show promise in enhancing error\ncorrection performance but face challenges with punctured codes. In particular,\nexisting methods struggle to adapt to variable code rates or meet protocol\ncompatibility requirements. This paper proposes a unified long short-term\nmemory (LSTM)-based neural decoder for punctured convolutional and Turbo codes\nto address these challenges. The key component of the proposed LSTM-based\nneural decoder is puncturing-aware embedding, which integrates puncturing\npatterns directly into the neural network to enable seamless adaptation to\ndifferent code rates. Moreover, a balanced bit error rate training strategy is\ndesigned to ensure the decoder's robustness across various code lengths, rates,\nand channels. In this way, the protocol compatibility requirement can be\nrealized. Extensive simulations in both additive white Gaussian noise (AWGN)\nand Rayleigh fading channels demonstrate that the proposed neural decoder\noutperforms conventional decoding techniques, offering significant improvements\nin decoding accuracy and robustness.\n","authors":["Yongli Yan","Linglong Dai"],"pdf_url":"https://arxiv.org/pdf/2502.15475v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26278v1","updated":"2025-10-30T09:00:42Z","published":"2025-10-30T09:00:42Z","title":"Distributional Multi-objective Black-box Optimization for\n  Diffusion-model Inference-time Multi-Target Generation","summary":"  Diffusion models have been successful in learning complex data distributions.\nThis capability has driven their application to high-dimensional\nmulti-objective black-box optimization problem. Existing approaches often\nemploy an external optimization loop, such as an evolutionary algorithm, to the\ndiffusion model. However, these approaches treat the diffusion model as a\nblack-box refiner, which overlooks the internal distribution transition of the\ndiffusion generation process, limiting their efficiency. To address these\nchallenges, we propose the Inference-time Multi-target Generation (IMG)\nalgorithm, which optimizes the diffusion process at inference-time to generate\nsamples that simultaneously satisfy multiple objectives. Specifically, our IMG\nperforms weighted resampling during the diffusion generation process according\nto the expected aggregated multi-objective values. This weighted resampling\nstrategy ensures the diffusion-generated samples are distributed according to\nour desired multi-target Boltzmann distribution. We further derive that the\nmulti-target Boltzmann distribution has an interesting log-likelihood\ninterpretation, where it is the optimal solution to the distributional\nmulti-objective optimization problem. We implemented IMG for a multi-objective\nmolecule generation task. Experiments show that IMG, requiring only a single\ngeneration pass, achieves a significantly higher hypervolume than baseline\noptimization algorithms that often require hundreds of diffusion generations.\nNotably, our algorithm can be viewed as an optimized diffusion process and can\nbe integrated into existing methods to further improve their performance.\n","authors":["Kim Yong Tan","Yueming Lyu","Ivor Tsang","Yew-Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2510.26278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12995v2","updated":"2025-10-30T08:59:47Z","published":"2024-11-20T02:46:15Z","title":"Beyond likelihood ratio bias: Nested multi-time-scale stochastic\n  approximation for likelihood-free parameter estimation","summary":"  We study parameter inference in simulation-based stochastic models where the\nanalytical form of the likelihood is unknown. The main difficulty is that score\nevaluation as a ratio of noisy Monte Carlo estimators induces bias and\ninstability, which we overcome with a ratio-free nested multi-time-scale (NMTS)\nstochastic approximation (SA) method that simultaneously tracks the score and\ndrives the parameter update. We provide a comprehensive theoretical analysis of\nthe proposed NMTS algorithm for solving likelihood-free inference problems,\nincluding strong convergence, asymptotic normality, and convergence rates. We\nshow that our algorithm can eliminate the original asymptotic bias\n$O\\big(\\sqrt{\\frac{1}{N}}\\big)$ and accelerate the convergence rate from\n$O\\big(\\beta_k+\\sqrt{\\frac{1}{N}}\\big)$ to\n$O\\big(\\frac{\\beta_k}{\\alpha_k}+\\sqrt{\\frac{\\alpha_k}{N}}\\big)$, where $N$ is\nthe fixed batch size, $\\alpha_k$ and $\\beta_k$ are decreasing step sizes with\n$\\alpha_k$, $\\beta_k$, $\\beta_k/\\alpha_k\\rightarrow 0$. With proper choice of\n$\\alpha_k$ and $\\beta_k$, our convergence rates can match the optimal rate in\nthe multi-time-scale SA literature. Numerical experiments demonstrate that our\nalgorithm can improve the estimation accuracy by one to two orders of magnitude\nat the same computational cost, making it efficient for parameter estimation in\nstochastic systems.\n","authors":["Zehao Li","Zhouchen Lin","Yijie Peng"],"pdf_url":"https://arxiv.org/pdf/2411.12995v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26275v1","updated":"2025-10-30T08:59:01Z","published":"2025-10-30T08:59:01Z","title":"A Research Roadmap for Augmenting Software Engineering Processes and\n  Software Products with Generative AI","summary":"  Generative AI (GenAI) is rapidly transforming software engineering (SE)\npractices, influencing how SE processes are executed, as well as how software\nsystems are developed, operated, and evolved. This paper applies design science\nresearch to build a roadmap for GenAI-augmented SE. The process consists of\nthree cycles that incrementally integrate multiple sources of evidence,\nincluding collaborative discussions from the FSE 2025 \"Software Engineering\n2030\" workshop, rapid literature reviews, and external feedback sessions\ninvolving peers. McLuhan's tetrads were used as a conceptual instrument to\nsystematically capture the transforming effects of GenAI on SE processes and\nsoftware products.The resulting roadmap identifies four fundamental forms of\nGenAI augmentation in SE and systematically characterizes their related\nresearch challenges and opportunities. These insights are then consolidated\ninto a set of future research directions. By grounding the roadmap in a\nrigorous multi-cycle process and cross-validating it among independent author\nteams and peers, the study provides a transparent and reproducible foundation\nfor analyzing how GenAI affects SE processes, methods and tools, and for\nframing future research within this rapidly evolving area. Based on these\nfindings, the article finally makes ten predictions for SE in the year 2030.\n","authors":["Domenico Amalfitano","Andreas Metzger","Marco Autili","Tommaso Fulcini","Tobias Hey","Jan Keim","Patrizio Pelliccione","Vincenzo Scotti","Anne Koziolek","Raffaela Mirandola","Andreas Vogelsang"],"pdf_url":"https://arxiv.org/pdf/2510.26275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26274v1","updated":"2025-10-30T08:58:44Z","published":"2025-10-30T08:58:44Z","title":"PVMark: Enabling Public Verifiability for LLM Watermarking Schemes","summary":"  Watermarking schemes for large language models (LLMs) have been proposed to\nidentify the source of the generated text, mitigating the potential threats\nemerged from model theft. However, current watermarking solutions hardly\nresolve the trust issue: the non-public watermark detection cannot prove itself\nfaithfully conducting the detection. We observe that it is attributed to the\nsecret key mostly used in the watermark detection -- it cannot be public, or\nthe adversary may launch removal attacks provided the key; nor can it be\nprivate, or the watermarking detection is opaque to the public. To resolve the\ndilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),\nenabling the watermark detection process to be publicly verifiable by third\nparties without disclosing any secret key. PVMark hinges upon the proof of\n`correct execution' of watermark detection on which a set of ZKP constraints\nare built, including mapping, random number generation, comparison, and\nsummation. We implement multiple variants of PVMark in Python, Rust and Circom,\ncovering combinations of three watermarking schemes, three hash functions, and\nfour ZKP protocols, to show our approach effectively works under a variety of\ncircumstances. By experimental results, PVMark efficiently enables public\nverifiability on the state-of-the-art LLM watermarking schemes yet without\ncompromising the watermarking performance, promising to be deployed in\npractice.\n","authors":["Haohua Duan","Liyao Xiang","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26274v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2405.09086v2","updated":"2025-10-30T08:49:01Z","published":"2024-05-15T04:47:31Z","title":"Chaos-based reinforcement learning with TD3","summary":"  Chaos-based reinforcement learning (CBRL) is a method in which the agent's\ninternal chaotic dynamics drives exploration. However, the learning algorithms\nin CBRL have not been thoroughly developed in previous studies, nor have they\nincorporated recent advances in reinforcement learning. This study introduced\nTwin Delayed Deep Deterministic Policy Gradients (TD3), which is one of the\nstate-of-the-art deep reinforcement learning algorithms that can treat\ndeterministic and continuous action spaces, to CBRL. The validation results\nprovide several insights. First, TD3 works as a learning algorithm for CBRL in\na simple goal-reaching task. Second, CBRL agents with TD3 can autonomously\nsuppress their exploratory behavior as learning progresses and resume\nexploration when the environment changes. Finally, examining the effect of the\nagent's chaoticity on learning shows that there exists a suitable range of\nchaos strength in the agent's model to flexibly switch between exploration and\nexploitation and adapt to environmental changes.\n","authors":["Toshitaka Matsuki","Yusuke Sakemi","Kazuyuki Aihara"],"pdf_url":"https://arxiv.org/pdf/2405.09086v2.pdf","comment":"Accepted for publication in Neural Networks"},{"id":"http://arxiv.org/abs/2510.26266v1","updated":"2025-10-30T08:46:53Z","published":"2025-10-30T08:46:53Z","title":"Likely Interpolants of Generative Models","summary":"  Interpolation in generative models allows for controlled generation, model\ninspection, and more. Unfortunately, most generative models lack a principal\nnotion of interpolants without restrictive assumptions on either the model or\ndata dimension. In this paper, we develop a general interpolation scheme that\ntargets likely transition paths compatible with different metrics and\nprobability distributions. We consider interpolants analogous to a geodesic\nconstrained to a suitable data distribution and derive a novel algorithm for\ncomputing these curves, which requires no additional training. Theoretically,\nwe show that our method locally can be considered as a geodesic under a\nsuitable Riemannian metric. We quantitatively show that our interpolation\nscheme traverses higher density regions than baselines across a range of models\nand datasets.\n","authors":["Frederik Möbius Rygaard","Shen Zhu","Yinzhu Jin","Søren Hauberg","Tom Fletcher"],"pdf_url":"https://arxiv.org/pdf/2510.26266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09846v3","updated":"2025-10-30T08:39:14Z","published":"2025-07-14T00:54:48Z","title":"Through the River: Understanding the Benefit of Schedule-Free Methods\n  for Language Model Training","summary":"  As both model and dataset sizes continue to scale rapidly, conventional\npretraining strategies with fixed compute budgets-such as cosine learning rate\nschedules-are increasingly inadequate for large-scale training. Recent\nalternatives, including warmup-stable-decay (WSD) schedules and weight\naveraging, offer greater flexibility. However, WSD relies on explicit decay\nphases to track progress, while weight averaging addresses this limitation at\nthe cost of additional memory. In search of a more principled and scalable\nalternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],\nwhich has shown strong empirical performance across diverse settings. We show\nthat SF-AdamW effectively navigates the \"river\" structure of the loss landscape\nwithout decay phases or auxiliary averaging, making it particularly suitable\nfor continuously scaling training workloads. To understand this behavior, we\nconduct a theoretical and empirical analysis of SF dynamics, revealing that it\nimplicitly performs weight averaging without memory overhead. Guided by this\nanalysis, we propose a refined variant of SF that improves robustness to\nmomentum and performs better under large batch sizes, addressing key\nlimitations of the original method. Together, these results establish SF as a\npractical, scalable, and theoretically grounded approach for language model\ntraining.\n","authors":["Minhak Song","Beomhan Baek","Kwangjun Ahn","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2507.09846v3.pdf","comment":"Published at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2509.24257v2","updated":"2025-10-30T08:38:57Z","published":"2025-09-29T04:07:32Z","title":"VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized\n  Inference","summary":"  Decentralized inference provides a scalable and resilient paradigm for\nserving large language models (LLMs), enabling distributed resource utilization\nand reducing reliance on centralized providers. However, in a permissionless\nenvironment without trusted nodes, ensuring the correctness of model outputs\nremains a core challenge. We introduce VeriLLM, a publicly verifiable protocol\nfor decentralized LLM inference that achieves security under a\none-honest-verifier assumption while maintaining practical efficiency. VeriLLM\ncombines lightweight empirical rerunning with cryptographic commitments,\nallowing verifiers to validate results at approximately 1% of the underlying\ninference cost. To prevent verification bottlenecks, we design an isomorphic\ninference-verification architecture that multiplexes both inference and\nverification roles across the same GPU workers. This design (i) improves GPU\nutilization and overall throughput, (ii) enlarges the effective validator set,\nenhancing robustness and liveness, and (iii) enforces task indistinguishability\nto prevent node-specific optimizations or selective behavior. Through\ntheoretical analysis and system-level evaluation, we show that VeriLLM achieves\nreliable public verifiability with minimal overhead, offering a practical\nfoundation for trustworthy and scalable decentralized LLM inference.\n","authors":["Ke Wang","Zishuo Zhao","Xinyuan Song","Bill Shi","Libin Xia","Chris Tong","Lynn Ai","Felix Qu","Eric Yang"],"pdf_url":"https://arxiv.org/pdf/2509.24257v2.pdf","comment":"20 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2510.26243v1","updated":"2025-10-30T08:23:35Z","published":"2025-10-30T08:23:35Z","title":"Angular Steering: Behavior Control via Rotation in Activation Space","summary":"  Controlling specific behaviors in large language models while preserving\ntheir general capabilities is a central challenge for safe and reliable\nartificial intelligence deployment. Current steering methods, such as vector\naddition and directional ablation, are constrained within a two-dimensional\nsubspace defined by the activation and feature direction, making them sensitive\nto chosen parameters and potentially affecting unrelated features due to\nunintended interactions in activation space. We introduce Angular Steering, a\nnovel and flexible method for behavior modulation that operates by rotating\nactivations within a fixed two-dimensional subspace. By formulating steering as\na geometric rotation toward or away from a target behavior direction, Angular\nSteering provides continuous, fine-grained control over behaviors such as\nrefusal and compliance. We demonstrate this method using refusal steering\nemotion steering as use cases. Additionally, we propose Adaptive Angular\nSteering, a selective variant that rotates only activations aligned with the\ntarget feature, further enhancing stability and coherence. Angular Steering\ngeneralizes existing addition and orthogonalization techniques under a unified\ngeometric rotation framework, simplifying parameter selection and maintaining\nmodel stability across a broader range of adjustments. Experiments across\nmultiple model families and sizes show that Angular Steering achieves robust\nbehavioral control while maintaining general language modeling performance,\nunderscoring its flexibility, generalization, and robustness compared to prior\napproaches. Code and artifacts are available at\nhttps://github.com/lone17/angular-steering/.\n","authors":["Hieu M. Vu","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2510.26243v1.pdf","comment":"NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.25366v2","updated":"2025-10-30T08:16:40Z","published":"2025-10-29T10:37:24Z","title":"A Convexity-dependent Two-Phase Training Algorithm for Deep Neural\n  Networks","summary":"  The key task of machine learning is to minimize the loss function that\nmeasures the model fit to the training data. The numerical methods to do this\nefficiently depend on the properties of the loss function. The most decisive\namong these properties is the convexity or non-convexity of the loss function.\nThe fact that the loss function can have, and frequently has, non-convex\nregions has led to a widespread commitment to non-convex methods such as Adam.\nHowever, a local minimum implies that, in some environment around it, the\nfunction is convex. In this environment, second-order minimizing methods such\nas the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We\npropose a novel framework grounded in the hypothesis that loss functions in\nreal-world tasks swap from initial non-convexity to convexity towards the\noptimum. This is a property we leverage to design an innovative two-phase\noptimization algorithm. The presented algorithm detects the swap point by\nobserving the gradient norm dependence on the loss. In these regions,\nnon-convex (Adam) and convex (CG) algorithms are used, respectively. Computing\nexperiments confirm the hypothesis that this simple convexity structure is\nfrequent enough to be practically exploited to substantially improve\nconvergence and accuracy.\n","authors":["Tomas Hrycej","Bernhard Bermeitinger","Massimo Pavone","Götz-Henrik Wiegand","Siegfried Handschuh"],"pdf_url":"https://arxiv.org/pdf/2510.25366v2.pdf","comment":"Appeared on KDIR IC3K Conference 2025 (Best Paper Award). Published\n  in \"Proceedings of the 17th International Joint Conference on Knowledge\n  Discovery, Knowledge Engineering and Knowledge Management - Volume 1\""},{"id":"http://arxiv.org/abs/2510.26230v1","updated":"2025-10-30T08:09:37Z","published":"2025-10-30T08:09:37Z","title":"MPRU: Modular Projection-Redistribution Unlearning as Output Filter for\n  Classification Pipelines","summary":"  As a new and promising approach, existing machine unlearning (MU) works\ntypically emphasize theoretical formulations or optimization objectives to\nachieve knowledge removal. However, when deployed in real-world scenarios, such\nsolutions typically face scalability issues and have to address practical\nrequirements such as full access to original datasets and model. In contrast to\nthe existing approaches, we regard classification training as a sequential\nprocess where classes are learned sequentially, which we call \\emph{inductive\napproach}. Unlearning can then be done by reversing the last training sequence.\nThis is implemented by appending a projection-redistribution layer in the end\nof the model. Such an approach does not require full access to the original\ndataset or the model, addressing the challenges of existing methods. This\nenables modular and model-agnostic deployment as an output filter into existing\nclassification pipelines with minimal alterations. We conducted multiple\nexperiments across multiple datasets including image (CIFAR-10/100 using\nCNN-based model) and tabular datasets (Covertype using tree-based model).\nExperiment results show consistently similar output to a fully retrained model\nwith a high computational cost reduction. This demonstrates the applicability,\nscalability, and system compatibility of our solution while maintaining the\nperformance of the output in a more practical setting.\n","authors":["Minyi Peng","Darian Gunamardi","Ivan Tjuawinata","Kwok-Yan Lam"],"pdf_url":"https://arxiv.org/pdf/2510.26230v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.02935v3","updated":"2025-10-30T08:05:05Z","published":"2025-06-03T14:35:36Z","title":"MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable\n  Neural Vehicle Routing Solver","summary":"  Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a\npromising approach to train a unified model capable of solving multiple Vehicle\nRouting Problem (VRP) variants. However, existing Reinforcement Learning\n(RL)-based multi-task methods can only train light decoder models on\nsmall-scale problems, exhibiting limited generalization ability when solving\nlarge-scale problems. To overcome this limitation, this work introduces a novel\nmulti-task learning method driven by knowledge distillation (MTL-KD), which\nenables the efficient training of heavy decoder models with strong\ngeneralization ability. The proposed MTL-KD method transfers policy knowledge\nfrom multiple distinct RL-based single-task models to a single heavy decoder\nmodel, facilitating label-free training and effectively improving the model's\ngeneralization ability across diverse tasks. In addition, we introduce a\nflexible inference strategy termed Random Reordering Re-Construction (R3C),\nwhich is specifically adapted for diverse VRP tasks and further boosts the\nperformance of the multi-task model. Experimental results on 6 seen and 10\nunseen VRP variants with up to 1000 nodes indicate that our proposed method\nconsistently achieves superior performance on both uniform and real-world\nbenchmarks, demonstrating robust generalization abilities.\n","authors":["Yuepeng Zheng","Fu Luo","Zhenkun Wang","Yaoxin Wu","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.02935v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2410.03364v4","updated":"2025-10-30T08:00:06Z","published":"2024-10-04T12:30:42Z","title":"Unified Error Correction Code Transformer with Low Complexity","summary":"  Channel coding is vital for reliable sixth-generation (6G) data transmission,\nemploying diverse error correction codes for various application scenarios.\nTraditional decoders require dedicated hardware for each code, leading to high\nhardware costs. Recently, artificial intelligence (AI)-driven approaches, such\nas the error correction code Transformer (ECCT) and its enhanced version, the\nfoundation error correction code Transformer (FECCT), have been proposed to\nreduce the hardware cost by leveraging the Transformer to decode multiple\ncodes. However, their excessively high computational complexity of\n$\\mathcal{O}(N^2)$ due to the self-attention mechanism in the Transformer\nlimits scalability, where $N$ represents the sequence length. To reduce\ncomputational complexity, we propose a unified Transformer-based decoder that\nhandles multiple linear block codes within a single framework. Specifically, a\nstandardized unit is employed to align code length and code rate across\ndifferent code types, while a redesigned low-rank unified attention module,\nwith computational complexity of $\\mathcal{O}(N)$, is shared across various\nheads in the Transformer. Additionally, a sparse mask, derived from the\nparity-check matrix's sparsity, is introduced to enhance the decoder's ability\nto capture inherent constraints between information and parity-check bits,\nimproving decoding accuracy and further reducing computational complexity by\n$86\\%$. Extensive experimental results demonstrate that the proposed unified\nTransformer-based decoder outperforms existing methods and provides a\nhigh-performance, low-complexity solution for next-generation wireless\ncommunication systems.\n","authors":["Yongli Yan","Jieao Zhu","Tianyue Zheng","Zhuo Xu","Chao Jiang","Linglong Dai"],"pdf_url":"https://arxiv.org/pdf/2410.03364v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26219v1","updated":"2025-10-30T07:52:14Z","published":"2025-10-30T07:52:14Z","title":"Test-Time Alignment of LLMs via Sampling-Based Optimal Control in\n  pre-logit space","summary":"  Test-time alignment of large language models (LLMs) attracts attention\nbecause fine-tuning LLMs requires high computational costs. In this paper, we\npropose a new test-time alignment method called adaptive importance sampling on\npre-logits (AISP) on the basis of the sampling-based model predictive control\nwith the stochastic control input. AISP applies the Gaussian perturbation into\npre-logits, which are outputs of the penultimate layer, so as to maximize\nexpected rewards with respect to the mean of the perturbation. We demonstrate\nthat the optimal mean is obtained by importance sampling with sampled rewards.\nAISP outperforms best-of-n sampling in terms of rewards over the number of used\nsamples and achieves higher rewards than other reward-based test-time alignment\nmethods.\n","authors":["Sekitoshi Kanai","Tsukasa Yoshida","Hiroshi Takahashi","Haru Kuroki","Kazumune Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2510.26219v1.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.17789v2","updated":"2025-10-30T07:39:10Z","published":"2025-05-23T12:02:13Z","title":"Optimal Online Change Detection via Random Fourier Features","summary":"  This article studies the problem of online non-parametric change point\ndetection in multivariate data streams. We approach the problem through the\nlens of kernel-based two-sample testing and introduce a sequential testing\nprocedure based on random Fourier features, running with logarithmic time\ncomplexity per observation and with overall logarithmic space complexity. The\nalgorithm has two advantages compared to the state of the art. First, our\napproach is genuinely online, and no access to training data known to be from\nthe pre-change distribution is necessary. Second, the algorithm does not\nrequire the user to specify a window parameter over which local tests are to be\ncalculated. We prove strong theoretical guarantees on the algorithm's\nperformance, including information-theoretic bounds demonstrating that the\ndetection delay is optimal in the minimax sense. Numerical studies on real and\nsynthetic data show that our algorithm is competitive with respect to the state\nof the art.\n","authors":["Florian Kalinke","Shakeel Gavioli-Akilagun"],"pdf_url":"https://arxiv.org/pdf/2505.17789v2.pdf","comment":"Accepted for publication at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.24627v3","updated":"2025-10-30T07:38:02Z","published":"2025-05-30T14:21:33Z","title":"Rethinking Neural Combinatorial Optimization for Vehicle Routing\n  Problems with Different Constraint Tightness Degrees","summary":"  Recent neural combinatorial optimization (NCO) methods have shown promising\nproblem-solving ability without requiring domain-specific expertise. Most\nexisting NCO methods use training and testing data with a fixed constraint\nvalue and lack research on the effect of constraint tightness on the\nperformance of NCO methods. This paper takes the capacity-constrained vehicle\nrouting problem (CVRP) as an example to empirically analyze the NCO performance\nunder different tightness degrees of the capacity constraint. Our analysis\nreveals that existing NCO methods overfit the capacity constraint, and they can\nonly perform satisfactorily on a small range of the constraint values but\npoorly on other values. To tackle this drawback of existing NCO methods, we\ndevelop an efficient training scheme that explicitly considers varying degrees\nof constraint tightness and proposes a multi-expert module to learn a generally\nadaptable solving strategy. Experimental results show that the proposed method\ncan effectively overcome the overfitting issue, demonstrating superior\nperformances on the CVRP and CVRP with time windows (CVRPTW) with various\nconstraint tightness degrees.\n","authors":["Fu Luo","Yaoxin Wu","Zhi Zheng","Zhenkun Wang"],"pdf_url":"https://arxiv.org/pdf/2505.24627v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.10173v2","updated":"2025-10-30T07:25:20Z","published":"2025-06-11T20:53:45Z","title":"SPARKE: Scalable Prompt-Aware Diversity and Novelty Guidance in\n  Diffusion Models via RKE Score","summary":"  Diffusion models have demonstrated remarkable success in high-fidelity image\nsynthesis and prompt-guided generative modeling. However, ensuring adequate\ndiversity in generated samples of prompt-guided diffusion models remains a\nchallenge, particularly when the prompts span a broad semantic spectrum and the\ndiversity of generated data needs to be evaluated in a prompt-aware fashion\nacross semantically similar prompts. Recent methods have introduced guidance\nvia diversity measures to encourage more varied generations. In this work, we\nextend the diversity measure-based approaches by proposing the Scalable\nPrompt-Aware R\\'eny Kernel Entropy Diversity Guidance (SPARKE) method for\nprompt-aware diversity guidance. SPARKE utilizes conditional entropy for\ndiversity guidance, which dynamically conditions diversity measurement on\nsimilar prompts and enables prompt-aware diversity control. While the\nentropy-based guidance approach enhances prompt-aware diversity, its reliance\non the matrix-based entropy scores poses computational challenges in\nlarge-scale generation settings. To address this, we focus on the special case\nof Conditional latent RKE Score Guidance, reducing entropy computation and\ngradient-based optimization complexity from the $O(n^3)$ of general entropy\nmeasures to $O(n)$. The reduced computational complexity allows for\ndiversity-guided sampling over potentially thousands of generation rounds on\ndifferent prompts. We numerically test the SPARKE method on several\ntext-to-image diffusion models, demonstrating that the proposed method improves\nthe prompt-aware diversity of the generated data without incurring significant\ncomputational costs. We release our code on the project page:\nhttps://mjalali.github.io/SPARKE\n","authors":["Mohammad Jalali","Haoyu Lei","Amin Gohari","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2506.10173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01158v2","updated":"2025-10-30T07:20:59Z","published":"2025-06-01T20:32:27Z","title":"Efficient Regression-Based Training of Normalizing Flows for Boltzmann\n  Generators","summary":"  Simulation-free training frameworks have been at the forefront of the\ngenerative modelling revolution in continuous spaces, leading to large-scale\ndiffusion and flow matching models. However, such modern generative models\nsuffer from expensive inference, inhibiting their use in numerous scientific\napplications like Boltzmann Generators (BGs) for molecular conformations that\nrequire fast likelihood evaluation. In this paper, we revisit classical\nnormalizing flows in the context of BGs that offer efficient sampling and\nlikelihoods, but whose training via maximum likelihood is often unstable and\ncomputationally challenging. We propose Regression Training of Normalizing\nFlows (RegFlow), a novel and scalable regression-based training objective that\nbypasses the numerical instability and computational challenge of conventional\nmaximum likelihood training in favour of a simple $\\ell_2$-regression\nobjective. Specifically, RegFlow maps prior samples under our flow to targets\ncomputed using optimal transport couplings or a pre-trained continuous\nnormalizing flow (CNF). To enhance numerical stability, RegFlow employs\neffective regularization strategies such as a new forward-backward\nself-consistency loss that enjoys painless implementation. Empirically, we\ndemonstrate that RegFlow unlocks a broader class of architectures that were\npreviously intractable to train for BGs with maximum likelihood. We also show\nRegFlow exceeds the performance, computational cost, and stability of maximum\nlikelihood training in equilibrium sampling in Cartesian coordinates of alanine\ndipeptide, tripeptide, and tetrapeptide, showcasing its potential in molecular\nsystems.\n","authors":["Danyal Rehman","Oscar Davis","Jiarui Lu","Jian Tang","Michael Bronstein","Yoshua Bengio","Alexander Tong","Avishek Joey Bose"],"pdf_url":"https://arxiv.org/pdf/2506.01158v2.pdf","comment":"Preprint; ICML GenBio Best Paper Award 2025"},{"id":"http://arxiv.org/abs/2505.13904v3","updated":"2025-10-30T07:17:31Z","published":"2025-05-20T04:10:50Z","title":"Learning to Insert for Constructive Neural Vehicle Routing Solver","summary":"  Neural Combinatorial Optimisation (NCO) is a promising learning-based\napproach for solving Vehicle Routing Problems (VRPs) without extensive manual\ndesign. While existing constructive NCO methods typically follow an\nappending-based paradigm that sequentially adds unvisited nodes to partial\nsolutions, this rigid approach often leads to suboptimal results. To overcome\nthis limitation, we explore the idea of insertion-based paradigm and propose\nLearning to Construct with Insertion-based Paradigm (L2C-Insert), a novel\nlearning-based method for constructive NCO. Unlike traditional approaches,\nL2C-Insert builds solutions by strategically inserting unvisited nodes at any\nvalid position in the current partial solution, which can significantly enhance\nthe flexibility and solution quality. The proposed framework introduces three\nkey components: a novel model architecture for precise insertion position\nprediction, an efficient training scheme for model optimization, and an\nadvanced inference technique that fully exploits the insertion paradigm's\nflexibility. Extensive experiments on both synthetic and real-world instances\nof the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) demonstrate that L2C-Insert consistently achieves superior\nperformance across various problem sizes.\n","authors":["Fu Luo","Xi Lin","Mengyuan Zhong","Fei Liu","Zhenkun Wang","Jianyong Sun","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13904v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.14970v4","updated":"2025-10-30T07:03:09Z","published":"2025-05-20T23:17:15Z","title":"Self-Evolving Curriculum for LLM Reasoning","summary":"  Reinforcement learning (RL) has proven effective for fine-tuning large\nlanguage models (LLMs), significantly enhancing their reasoning abilities in\ndomains such as mathematics and code generation. A crucial factor influencing\nRL fine-tuning success is the training curriculum: the order in which training\nproblems are presented. While random curricula serve as common baselines, they\nremain suboptimal; manually designed curricula often rely heavily on\nheuristics, and online filtering methods can be computationally prohibitive. To\naddress these limitations, we propose Self-Evolving Curriculum (SEC), an\nautomatic curriculum learning method that learns a curriculum policy\nconcurrently with the RL fine-tuning process. Our approach formulates\ncurriculum selection as a non-stationary Multi-Armed Bandit problem, treating\neach problem category (e.g., difficulty level or problem type) as an individual\narm. We leverage the absolute advantage from policy gradient methods as a proxy\nmeasure for immediate learning gain. At each training step, the curriculum\npolicy selects categories to maximize this reward signal and is updated using\nthe TD(0) method. Across three distinct reasoning domains: planning, inductive\nreasoning, and mathematics, our experiments demonstrate that SEC significantly\nimproves models' reasoning capabilities, enabling better generalization to\nharder, out-of-distribution test problems. Additionally, our approach achieves\nbetter skill balance when fine-tuning simultaneously on multiple reasoning\ndomains. These findings highlight SEC as a promising strategy for RL\nfine-tuning of LLMs.\n","authors":["Xiaoyin Chen","Jiarui Lu","Minsu Kim","Dinghuai Zhang","Jian Tang","Alexandre Piché","Nicolas Gontier","Yoshua Bengio","Ehsan Kamalloo"],"pdf_url":"https://arxiv.org/pdf/2505.14970v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02392v2","updated":"2025-10-30T07:02:16Z","published":"2025-06-03T03:15:22Z","title":"Improving Generalization of Neural Combinatorial Optimization for\n  Vehicle Routing Problems via Test-Time Projection Learning","summary":"  Neural Combinatorial Optimization (NCO) has emerged as a promising\nlearning-based paradigm for addressing Vehicle Routing Problems (VRPs) by\nminimizing the need for extensive manual engineering. While existing NCO\nmethods, trained on small-scale instances (e.g., 100 nodes), have demonstrated\nconsiderable success on problems of similar scale, their performance\nsignificantly degrades when applied to large-scale scenarios. This degradation\narises from the distributional shift between training and testing data,\nrendering policies learned on small instances ineffective for larger problems.\nTo overcome this limitation, we introduce a novel learning framework driven by\nLarge Language Models (LLMs). This framework learns a projection between the\ntraining and testing distributions, which is then deployed to enhance the\nscalability of the NCO model. Notably, unlike prevailing techniques that\nnecessitate joint training with the neural network, our approach operates\nexclusively during the inference phase, obviating the need for model\nretraining. Extensive experiments demonstrate that our method enables a\nbackbone model (trained on 100-node instances) to achieve superior performance\non large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) of up to 100K nodes from diverse distributions.\n","authors":["Yuanyao Chen","Rongsheng Chen","Fu Luo","Zhenkun Wang"],"pdf_url":"https://arxiv.org/pdf/2506.02392v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2505.24627"},{"id":"http://arxiv.org/abs/2510.10728v2","updated":"2025-10-30T06:58:12Z","published":"2025-10-12T18:02:12Z","title":"Rough Path Signatures: Learning Neural RDEs for Portfolio Optimization","summary":"  We tackle high-dimensional, path-dependent valuation and control and\nintroduce a deep BSDE/2BSDE solver that couples truncated log-signatures with a\nneural rough differential equation (RDE) backbone. The architecture aligns\nstochastic analysis with sequence-to-path learning: a CVaR-tilted terminal\nobjective targets left-tail risk, while an optional second-order (2BSDE) head\nsupplies curvature estimates for risk-sensitive control. Under matched compute\nand parameter budgets, the method improves accuracy, tail fidelity, and\ntraining stability across Asian and barrier option pricing and portfolio\ncontrol: at d=200 it achieves CVaR(0.99)=9.80% versus 12.00-13.10% for strong\nbaselines, attains the lowest HJB residual (0.011), and yields the lowest RMSEs\nfor Z and Gamma. Ablations over truncation depth, local windows, and tilt\nparameters confirm complementary gains from the sequence-to-path representation\nand the 2BSDE head. Taken together, the results highlight a bidirectional\ndialogue between stochastic analysis and modern deep learning: stochastic tools\ninform representations and objectives, while sequence-to-path models expand the\nclass of solvable financial models at scale.\n","authors":["Ali Atiah Alzahrani"],"pdf_url":"https://arxiv.org/pdf/2510.10728v2.pdf","comment":"Code available at: https://github.com/AliAtiah/SigRDE"},{"id":"http://arxiv.org/abs/2509.16648v2","updated":"2025-10-30T06:55:22Z","published":"2025-09-20T11:50:22Z","title":"FESTA: Functionally Equivalent Sampling for Trust Assessment of\n  Multimodal LLMs","summary":"  The accurate trust assessment of multimodal large language models (MLLMs)\ngenerated predictions, which can enable selective prediction and improve user\nconfidence, is challenging due to the diverse multi-modal input paradigms. We\npropose Functionally Equivalent Sampling for Trust Assessment (FESTA), a\nmultimodal input sampling technique for MLLMs, that generates an uncertainty\nmeasure based on the equivalent and complementary input samplings. The proposed\ntask-preserving sampling approach for uncertainty quantification expands the\ninput space to probe the consistency (through equivalent samples) and\nsensitivity (through complementary samples) of the model. FESTA uses only\ninput-output access of the model (black-box), and does not require ground truth\n(unsupervised). The experiments are conducted with various off-the-shelf\nmulti-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA\nuncertainty estimate achieves significant improvement (33.3% relative\nimprovement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in\nselective prediction performance, based on\narea-under-receiver-operating-characteristic curve (AUROC) metric in detecting\nmispredictions. The code implementation is open-sourced.\n","authors":["Debarpan Bhattacharya","Apoorva Kulkarni","Sriram Ganapathy"],"pdf_url":"https://arxiv.org/pdf/2509.16648v2.pdf","comment":"Accepted in the Findings of EMNLP, 2025"},{"id":"http://arxiv.org/abs/2510.26188v1","updated":"2025-10-30T06:54:19Z","published":"2025-10-30T06:54:19Z","title":"Predicting All-Cause Hospital Readmissions from Medical Claims Data of\n  Hospitalised Patients","summary":"  Reducing preventable hospital readmissions is a national priority for payers,\nproviders, and policymakers seeking to improve health care and lower costs. The\nrate of readmission is being used as a benchmark to determine the quality of\nhealthcare provided by the hospitals. In thisproject, we have used machine\nlearning techniques like Logistic Regression, Random Forest and Support Vector\nMachines to analyze the health claims data and identify demographic and medical\nfactors that play a crucial role in predicting all-cause readmissions. As the\nhealth claims data is high dimensional, we have used Principal Component\nAnalysis as a dimension reduction technique and used the results for building\nregression models. We compared and evaluated these models based on the Area\nUnder Curve (AUC) metric. Random Forest model gave the highest performance\nfollowed by Logistic Regression and Support Vector Machine models. These models\ncan be used to identify the crucial factors causing readmissions and help\nidentify patients to focus on to reduce the chances of readmission, ultimately\nbringing down the cost and increasing the quality of healthcare provided to the\npatients.\n","authors":["Avinash Kadimisetty","Arun Rajagopalan","Vijendra SK"],"pdf_url":"https://arxiv.org/pdf/2510.26188v1.pdf","comment":"NCMLAI 2018"},{"id":"http://arxiv.org/abs/2510.26185v1","updated":"2025-10-30T06:45:22Z","published":"2025-10-30T06:45:22Z","title":"Accumulative SGD Influence Estimation for Data Attribution","summary":"  Modern data-centric AI needs precise per-sample influence. Standard SGD-IE\napproximates leave-one-out effects by summing per-epoch surrogates and ignores\ncross-epoch compounding, which misranks critical examples. We propose\nACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out\nperturbation across training and updates an accumulative influence state at\neach step. In smooth strongly convex settings it achieves geometric error\ncontraction and, in smooth non-convex regimes, it tightens error bounds; larger\nmini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups,\nand MNIST under clean and corrupted data and both convex and non-convex\ntraining, ACC-SGD-IE yields more accurate influence estimates, especially over\nlong epochs. For downstream data cleansing it more reliably flags noisy\nsamples, producing models trained on ACC-SGD-IE cleaned data that outperform\nthose cleaned with SGD-IE.\n","authors":["Yunxiao Shi","Shuo Yang","Yixin Su","Rui Zhang","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2510.26185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26184v1","updated":"2025-10-30T06:43:47Z","published":"2025-10-30T06:43:47Z","title":"A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for\n  Collaborative Public Resource Allocation","summary":"  Public resource allocation involves the efficient distribution of resources,\nincluding urban infrastructure, energy, and transportation, to effectively meet\nsocietal demands. However, existing methods focus on optimizing the movement of\nindividual resources independently, without considering their capacity\nconstraints. To address this limitation, we propose a novel and more practical\nproblem: Collaborative Public Resource Allocation (CPRA), which explicitly\nincorporates capacity constraints and spatio-temporal dynamics in real-world\nscenarios. We propose a new framework called Game-Theoretic Spatio-Temporal\nReinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold:\n1) We formulate the CPRA problem as a potential game and demonstrate that there\nis no gap between the potential function and the optimal target, laying a solid\ntheoretical foundation for approximating the Nash equilibrium of this NP-hard\nproblem; and 2) Our designed GSTRL framework effectively captures the\nspatio-temporal dynamics of the overall system. We evaluate GSTRL on two\nreal-world datasets, where experiments show its superior performance. Our\nsource codes are available in the supplementary materials.\n","authors":["Songxin Lei","Qiongyan Wang","Yanchen Zhu","Hanyu Yao","Sijie Ruan","Weilin Ruan","Yuyu Luo","Huaming Wu","Yuxuan Liang"],"pdf_url":"https://arxiv.org/pdf/2510.26184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17918v5","updated":"2025-10-30T06:29:37Z","published":"2025-09-22T15:43:11Z","title":"Shilling Recommender Systems by Generating Side-feature-aware Fake User\n  Profiles","summary":"  Recommender systems (RS) greatly influence users' consumption decisions,\nmaking them attractive targets for malicious shilling attacks that inject fake\nuser profiles to manipulate recommendations. Existing shilling methods can\ngenerate effective and stealthy fake profiles when training data only contain\nrating matrix, but they lack comprehensive solutions for scenarios where side\nfeatures are present and utilized by the recommender. To address this gap, we\nextend the Leg-UP framework by enhancing the generator architecture to\nincorporate side features, enabling the generation of side-feature-aware fake\nuser profiles. Experiments on benchmarks show that our method achieves strong\nattack performance while maintaining stealthiness.\n","authors":["Yuanrong Wang","Yingpeng Du"],"pdf_url":"https://arxiv.org/pdf/2509.17918v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13308v2","updated":"2025-10-30T06:23:27Z","published":"2025-05-19T16:26:02Z","title":"Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space","summary":"  Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.\n","authors":["Hengli Li","Chenxi Li","Tong Wu","Xuekai Zhu","Yuxuan Wang","Zhaoxin Yu","Eric Hanchen Jiang","Song-Chun Zhu","Zixia Jia","Ying Nian Wu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.13308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09922v2","updated":"2025-10-30T06:18:35Z","published":"2025-05-15T03:12:27Z","title":"Improving the Euclidean Diffusion Generation of Manifold Data by\n  Mitigating Score Function Singularity","summary":"  Euclidean diffusion models have achieved remarkable success in generative\nmodeling across diverse domains, and they have been extended to manifold cases\nin recent advances. Instead of explicitly utilizing the structure of special\nmanifolds as studied in previous works, in this paper we investigate direct\nsampling of the Euclidean diffusion models for general manifold-structured\ndata. We reveal the multiscale singularity of the score function in the ambient\nspace, which hinders the accuracy of diffusion-generated samples. We then\npresent an elaborate theoretical analysis of the singularity structure of the\nscore function by decomposing it along the tangential and normal directions of\nthe manifold. To mitigate the singularity and improve the sampling accuracy, we\npropose two novel methods: (1) Niso-DM, which reduces the scale discrepancies\nin the score function by utilizing a non-isotropic noise, and (2) Tango-DM,\nwhich trains only the tangential component of the score function using a\ntangential-only loss function. Numerical experiments demonstrate that our\nmethods achieve superior performance on distributions over various manifolds\nwith complex geometries.\n","authors":["Zichen Liu","Wei Zhang","Tiejun Li"],"pdf_url":"https://arxiv.org/pdf/2505.09922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24829v2","updated":"2025-10-30T06:18:11Z","published":"2025-10-28T16:18:14Z","title":"Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN\n  Inference vs. Data Transmission in IoT","summary":"  The integration of the Internet of Things (IoT) and Artificial Intelligence\noffers significant opportunities to enhance our ability to monitor and address\necological changes. As environmental challenges become increasingly pressing,\nthe need for effective remote monitoring solutions is more critical than ever.\nA major challenge in designing IoT applications for environmental monitoring -\nparticularly those involving image data - is to create energy-efficient IoT\ndevices capable of long-term operation in remote areas with limited power\navailability. Advancements in the field of Tiny Machine Learning allow the use\nof Convolutional Neural Networks (CNNs) on resource-constrained,\nbattery-operated microcontrollers. Since data transfer is energy-intensive,\nperforming inference directly on microcontrollers to reduce the message size\ncan extend the operational lifespan of IoT nodes. This work evaluates the use\nof common Low Power Wide Area Networks and compressed CNNs trained on domain\nspecific datasets on an ESP32-S3. Our experiments demonstrate, among other\nthings, that executing CNN inference on-device and transmitting only the\nresults reduces the overall energy consumption by a factor of up to five\ncompared to sending raw image data. These findings advocate the development of\nIoT applications with reduced carbon footprint and capable of operating\nautonomously in environmental monitoring scenarios by incorporating EmbeddedML.\n","authors":["Benjamin Karic","Nina Herrmann","Jan Stenkamp","Paula Scharf","Fabian Gieseke","Angela Schwering"],"pdf_url":"https://arxiv.org/pdf/2510.24829v2.pdf","comment":"11 Pages, Paper lists the categories for the ACM Computing\n  Classification System"},{"id":"http://arxiv.org/abs/2506.05768v2","updated":"2025-10-30T06:11:45Z","published":"2025-06-06T05:52:19Z","title":"AANet: Virtual Screening under Structural Uncertainty via Alignment and\n  Aggregation","summary":"  Virtual screening (VS) is a critical component of modern drug discovery, yet\nmost existing methods--whether physics-based or deep learning-based--are\ndeveloped around holo protein structures with known ligand-bound pockets.\nConsequently, their performance degrades significantly on apo or predicted\nstructures such as those from AlphaFold2, which are more representative of\nreal-world early-stage drug discovery, where pocket information is often\nmissing. In this paper, we introduce an alignment-and-aggregation framework to\nenable accurate virtual screening under structural uncertainty. Our method\ncomprises two core components: (1) a tri-modal contrastive learning module that\naligns representations of the ligand, the holo pocket, and cavities detected\nfrom structures, thereby enhancing robustness to pocket localization error; and\n(2) a cross-attention based adapter for dynamically aggregating candidate\nbinding sites, enabling the model to learn from activity data even without\nprecise pocket annotations. We evaluated our method on a newly curated\nbenchmark of apo structures, where it significantly outperforms\nstate-of-the-art methods in blind apo setting, improving the early enrichment\nfactor (EF1%) from 11.75 to 37.19. Notably, it also maintains strong\nperformance on holo structures. These results demonstrate the promise of our\napproach in advancing first-in-class drug discovery, particularly in scenarios\nlacking experimentally resolved protein-ligand complexes. Our implementation is\npublicly available at https://github.com/Wiley-Z/AANet.\n","authors":["Wenyu Zhu","Jianhui Wang","Bowen Gao","Yinjun Jia","Haichuan Tan","Ya-Qin Zhang","Wei-Ying Ma","Yanyan Lan"],"pdf_url":"https://arxiv.org/pdf/2506.05768v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2408.08493v3","updated":"2025-10-30T06:10:17Z","published":"2024-08-16T02:29:38Z","title":"Parallel Unlearning in Inherited Model Networks","summary":"  Unlearning is challenging in generic learning frameworks with the continuous\ngrowth and updates of models exhibiting complex inheritance relationships. This\npaper presents a novel unlearning framework that enables fully parallel\nunlearning among models exhibiting inheritance. We use a chronologically\nDirected Acyclic Graph (DAG) to capture various unlearning scenarios occurring\nin model inheritance networks. Central to our framework is the Fisher\nInheritance Unlearning (FIUn) method, designed to enable efficient parallel\nunlearning within the DAG. FIUn utilizes the Fisher Information Matrix (FIM) to\nassess the significance of model parameters for unlearning tasks and adjusts\nthem accordingly. To handle multiple unlearning requests simultaneously, we\npropose the Merging-FIM (MFIM) function, which consolidates FIMs from multiple\nupstream models into a unified matrix. This design supports all unlearning\nscenarios captured by the DAG, enabling one-shot removal of inherited knowledge\nwhile significantly reducing computational overhead. Experiments confirm the\neffectiveness of our unlearning framework. For single-class tasks, it achieves\ncomplete unlearning with 0% accuracy for unlearned labels while maintaining\n94.53% accuracy for retained labels. For multi-class tasks, the accuracy is\n1.07% for unlearned labels and 84.77% for retained labels. Our framework\naccelerates unlearning by 99% compared to alternative methods. Code is in\nhttps://github.com/MJLee00/Parallel-Unlearning-in-Inherited-Model-Networks.\n","authors":["Xiao Liu","Mingyuan Li","Guangsheng Yu","Lixiang Li","Haipeng Peng","Ren Ping Liu"],"pdf_url":"https://arxiv.org/pdf/2408.08493v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.12760v2","updated":"2025-10-30T06:08:58Z","published":"2025-09-16T07:19:38Z","title":"Similarity-Distance-Magnitude Activations","summary":"  We introduce the Similarity-Distance-Magnitude (SDM) activation function, a\nmore robust and interpretable formulation of the standard softmax activation\nfunction, adding Similarity (i.e., correctly predicted depth-matches into\ntraining) awareness and Distance-to-training-distribution awareness to the\nexisting output Magnitude (i.e., decision-boundary) awareness, and enabling\ninterpretability-by-exemplar via dense matching. We further introduce the SDM\nestimator, based on a data-driven partitioning of the class-wise empirical CDFs\nvia the SDM activation, to control the class- and prediction-conditional\naccuracy among selective classifications. When used as the final-layer\nactivation over pre-trained language models for selective classification, the\nSDM estimator is more robust to co-variate shifts and out-of-distribution\ninputs than existing calibration methods using softmax activations, while\nremaining informative over in-distribution data.\n","authors":["Allen Schmaltz"],"pdf_url":"https://arxiv.org/pdf/2509.12760v2.pdf","comment":"18 pages, 5 tables, 1 algorithm. arXiv admin note: substantial text\n  overlap with arXiv:2502.20167"},{"id":"http://arxiv.org/abs/2505.06203v2","updated":"2025-10-30T05:52:33Z","published":"2025-05-09T17:30:16Z","title":"A Robust and Non-Iterative Tensor Decomposition Method with Automatic\n  Thresholding","summary":"  Recent advances in IoT and biometric sensing technologies have led to the\ngeneration of massive and high-dimensional tensor data, yet achieving accurate\nand efficient low-rank approximation remains a major challenge. Existing tensor\ndecomposition methods typically require prior specification of the tensor rank\nand rely on iterative optimization, which often results in heavy computational\ncosts and dependence on the analyst's expertise. In this study, we propose a\nnovel low-rank approximation method for tensor data that requires neither prior\nrank specification nor iterative optimization. The proposed method performs\nstatistical singular value hard thresholding on the mode-wise unfolded matrices\nto automatically extract only statistically significant components, thereby\nachieving noise reduction while preserving the intrinsic tensor structure.\nTheoretically, the optimal threshold for each mode is derived based on the\nasymptotic properties of the Mar\\v{c}enko--Pastur distribution. Simulation\nexperiments demonstrate that the proposed method outperforms conventional\napproaches such as Higher-Order Singular Value Decomposition, Higher-Order\nOrthogonal Iteration, and Tucker-L2E in terms of both estimation accuracy and\ncomputational efficiency. These results indicate that our method provides an\neffective and theoretically grounded framework for automatic, non-iterative,\nand analyst-independent tensor decomposition.\n","authors":["Hiroki Hasegawa","Yukihiko Okada"],"pdf_url":"https://arxiv.org/pdf/2505.06203v2.pdf","comment":"25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2510.26159v1","updated":"2025-10-30T05:39:44Z","published":"2025-10-30T05:39:44Z","title":"Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches\n  for Anomaly Detection in Industrial Time Series","summary":"  In this study, we investigate the effectiveness of advanced feature\nengineering and hybrid model architectures for anomaly detection in a\nmultivariate industrial time series, focusing on a steam turbine system. We\nevaluate the impact of change point-derived statistical features,\nclustering-based substructure representations, and hybrid learning strategies\non detection performance. Despite their theoretical appeal, these complex\napproaches consistently underperformed compared to a simple Random Forest +\nXGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of\n0.976, F1-score of 0.41, and 100% early detection within the defined time\nwindow. Our findings highlight that, in scenarios with highly imbalanced and\ntemporally uncertain data, model simplicity combined with optimized\nsegmentation can outperform more sophisticated architectures, offering greater\nrobustness, interpretability, and operational utility.\n","authors":["Emilio Mastriani","Alessandro Costa","Federico Incardona","Kevin Munari","Sebastiano Spinello"],"pdf_url":"https://arxiv.org/pdf/2510.26159v1.pdf","comment":"This paper is currently under review for presentation at the IEEE\n  SAMI 2026 Conference"},{"id":"http://arxiv.org/abs/2510.26157v1","updated":"2025-10-30T05:36:31Z","published":"2025-10-30T05:36:31Z","title":"Bridging the Gap Between Molecule and Textual Descriptions via\n  Substructure-aware Alignment","summary":"  Molecule and text representation learning has gained increasing interest due\nto its potential for enhancing the understanding of chemical information.\nHowever, existing models often struggle to capture subtle differences between\nmolecules and their descriptions, as they lack the ability to learn\nfine-grained alignments between molecular substructures and chemical phrases.\nTo address this limitation, we introduce MolBridge, a novel molecule-text\nlearning framework based on substructure-aware alignments. Specifically, we\naugment the original molecule-description pairs with additional alignment\nsignals derived from molecular substructures and chemical phrases. To\neffectively learn from these enriched alignments, MolBridge employs\nsubstructure-aware contrastive learning, coupled with a self-refinement\nmechanism that filters out noisy alignment signals. Experimental results show\nthat MolBridge effectively captures fine-grained correspondences and\noutperforms state-of-the-art baselines on a wide range of molecular benchmarks,\nhighlighting the significance of substructure-aware alignment in molecule-text\nlearning.\n","authors":["Hyuntae Park","Yeachan Kim","SangKeun Lee"],"pdf_url":"https://arxiv.org/pdf/2510.26157v1.pdf","comment":"EMNLP 2025 (main)"},{"id":"http://arxiv.org/abs/2510.21271v2","updated":"2025-10-30T05:16:33Z","published":"2025-10-24T09:12:59Z","title":"Buffer layers for Test-Time Adaptation","summary":"  In recent advancements in Test Time Adaptation (TTA), most existing\nmethodologies focus on updating normalization layers to adapt to the test\ndomain. However, the reliance on normalization-based adaptation presents key\nchallenges. First, normalization layers such as Batch Normalization (BN) are\nhighly sensitive to small batch sizes, leading to unstable and inaccurate\nstatistics. Moreover, normalization-based adaptation is inherently constrained\nby the structure of the pre-trained model, as it relies on training-time\nstatistics that may not generalize well to unseen domains. These issues limit\nthe effectiveness of normalization-based TTA approaches, especially under\nsignificant domain shift. In this paper, we introduce a novel paradigm based on\nthe concept of a Buffer layer, which addresses the fundamental limitations of\nnormalization layer updates. Unlike existing methods that modify the core\nparameters of the model, our approach preserves the integrity of the\npre-trained backbone, inherently mitigating the risk of catastrophic forgetting\nduring online adaptation. Through comprehensive experimentation, we demonstrate\nthat our approach not only outperforms traditional methods in mitigating domain\nshift and enhancing model robustness, but also exhibits strong resilience to\nforgetting. Furthermore, our Buffer layer is modular and can be seamlessly\nintegrated into nearly all existing TTA frameworks, resulting in consistent\nperformance improvements across various architectures. These findings validate\nthe effectiveness and versatility of the proposed solution in real-world domain\nadaptation scenarios. The code is available at\nhttps://github.com/hyeongyu-kim/Buffer_TTA.\n","authors":["Hyeongyu Kim","Geonhui Han","Dosik Hwang"],"pdf_url":"https://arxiv.org/pdf/2510.21271v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26148v1","updated":"2025-10-30T05:08:25Z","published":"2025-10-30T05:08:25Z","title":"STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human\n  Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing\n  Environments","summary":"  Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)\npresents a privacy-preserving, contactless sensing approach suitable for smart\nhomes, healthcare monitoring, and mobile IoT systems. However, existing methods\noften encounter computational inefficiency, high latency, and limited\nfeasibility within resource-constrained, embedded mobile edge environments.\nThis paper proposes STAR (Sensing Technology for Activity Recognition), an\nedge-AI-optimized framework that integrates a lightweight neural architecture,\nadaptive signal processing, and hardware-aware co-optimization to enable\nreal-time, energy-efficient HAR on low-power embedded devices. STAR\nincorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neural\nnetwork, reducing model parameters by 33% compared to conventional LSTM models\nwhile maintaining effective temporal modeling capability. A multi-stage\npre-processing pipeline combining median filtering, 8th-order Butterworth\nlow-pass filtering, and Empirical Mode Decomposition (EMD) is employed to\ndenoise CSI amplitude data and extract spatial-temporal features. For on-device\ndeployment, STAR is implemented on a Rockchip RV1126 processor equipped with an\nembedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSI\nacquisition module. Experimental results demonstrate a mean recognition\naccuracy of 93.52% across seven activity classes and 99.11% for human presence\ndetection, utilizing a compact 97.6k-parameter model. INT8 quantized inference\nachieves a processing speed of 33 MHz with just 8% CPU utilization, delivering\nsixfold speed improvements over CPU-based execution. With sub-second response\nlatency and low power consumption, the system ensures real-time,\nprivacy-preserving HAR, offering a practical, scalable solution for mobile and\npervasive computing environments.\n","authors":["Kexing Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26146v1","updated":"2025-10-30T04:59:28Z","published":"2025-10-30T04:59:28Z","title":"maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop\n  Edge Model Adaptation for Robust Human Activity Recognition","summary":"  WiFi Channel State Information (CSI)-based human activity recognition (HAR)\nprovides a privacy-preserving, device-free sensing solution for smart\nenvironments. However, its deployment on edge devices is severely constrained\nby domain shift, where recognition performance deteriorates under varying\nenvironmental and hardware conditions. This study presents maxVSTAR (maximally\nadaptive Vision-guided Sensing Technology for Activity Recognition), a\nclosed-loop, vision-guided model adaptation framework that autonomously\nmitigates domain shift for edge-deployed CSI sensing systems. The proposed\nsystem integrates a cross-modal teacher-student architecture, where a\nhigh-accuracy YOLO-based vision model serves as a dynamic supervisory signal,\ndelivering real-time activity labels for the CSI data stream. These labels\nenable autonomous, online fine-tuning of a lightweight CSI-based HAR model,\ntermed Sensing Technology for Activity Recognition (STAR), directly at the\nedge. This closed-loop retraining mechanism allows STAR to continuously adapt\nto environmental changes without manual intervention. Extensive experiments\ndemonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated\nhardware, the baseline STAR model's recognition accuracy declined from 93.52%\nto 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored\nthe accuracy to 81.51%. These results confirm the system's capacity for\ndynamic, self-supervised model adaptation in privacy-conscious IoT\nenvironments, establishing a scalable and practical paradigm for long-term\nautonomous HAR using CSI sensing at the network edge.\n","authors":["Kexing Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11847v2","updated":"2025-10-30T04:34:21Z","published":"2025-07-16T02:24:21Z","title":"Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update","summary":"  We study the generalized linear bandit (GLB) problem, a contextual\nmulti-armed bandit framework that extends the classical linear model by\nincorporating a non-linear link function, thereby modeling a broad class of\nreward distributions such as Bernoulli and Poisson. While GLBs are widely\napplicable to real-world scenarios, their non-linear nature introduces\nsignificant challenges in achieving both computational and statistical\nefficiency. Existing methods typically trade off between two objectives, either\nincurring high per-round costs for optimal regret guarantees or compromising\nstatistical efficiency to enable constant-time updates. In this paper, we\npropose a jointly efficient algorithm that attains a nearly optimal regret\nbound with $\\mathcal{O}(1)$ time and space complexities per round. The core of\nour method is a tight confidence set for the online mirror descent (OMD)\nestimator, which is derived through a novel analysis that leverages the notion\nof mix loss from online prediction. The analysis shows that our OMD estimator,\neven with its one-pass updates, achieves statistical efficiency comparable to\nmaximum likelihood estimation, thereby leading to a jointly efficient\noptimistic method.\n","authors":["Yu-Jie Zhang","Sheng-An Xu","Peng Zhao","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2507.11847v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26130v1","updated":"2025-10-30T04:30:23Z","published":"2025-10-30T04:30:23Z","title":"Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World\n  Class-Level Code Generation","summary":"  Large language models (LLMs) have advanced code generation at the function\nlevel, yet their ability to produce correct class-level implementations in\nauthentic software projects remains poorly understood. This work introduces a\nnovel benchmark derived from open-source repositories, comprising real-world\nclasses divided into seen and unseen partitions to evaluate generalization\nunder practical conditions. The evaluation examines multiple LLMs under varied\ninput specifications, retrieval-augmented configurations, and documentation\ncompleteness levels.\n  Results reveal a stark performance disparity: LLMs achieve 84% to 89%\ncorrectness on established synthetic benchmarks but only 25% to 34% on\nreal-world class tasks, with negligible differences between familiar and novel\ncodebases. Comprehensive docstrings yield modest gains of 1% to 3% in\nfunctional accuracy, though statistical significance is rare.\nRetrieval-augmented generation proves most effective with partial\ndocumentation, improving correctness by 4% to 7% by supplying concrete\nimplementation patterns absent from specifications. Error profiling identifies\nAttributeError, TypeError, and AssertionError as dominant failure modes (84% of\ncases), with synthetic tests overemphasizing assertion issues and real-world\nscenarios highlighting type and attribute mismatches. Retrieval augmentation\nreduces logical flaws but can introduce dependency conflicts.\n  The benchmark and analysis expose critical limitations in current LLM\ncapabilities for class-level engineering, offering actionable insights for\nenhancing context modelling, documentation strategies, and retrieval\nintegration in production code assistance tools.\n","authors":["Musfiqur Rahman","SayedHassan Khatoonabadi","Emad Shihab"],"pdf_url":"https://arxiv.org/pdf/2510.26130v1.pdf","comment":"Pre-print prepared for journal submission"},{"id":"http://arxiv.org/abs/2510.26121v1","updated":"2025-10-30T04:05:49Z","published":"2025-10-30T04:05:49Z","title":"Uncertainty-Aware Diagnostics for Physics-Informed Machine Learning","summary":"  Physics-informed machine learning (PIML) integrates prior physical\ninformation, often in the form of differential equation constraints, into the\nprocess of fitting machine learning models to physical data. Popular PIML\napproaches, including neural operators, physics-informed neural networks,\nneural ordinary differential equations, and neural discrete equilibria, are\ntypically fit to objectives that simultaneously include both data and physical\nconstraints. However, the multi-objective nature of this approach creates\nambiguity in the measurement of model quality. This is related to a poor\nunderstanding of epistemic uncertainty, and it can lead to surprising failure\nmodes, even when existing statistical metrics suggest strong fits. Working\nwithin a Gaussian process regression framework, we introduce the\nPhysics-Informed Log Evidence (PILE) score. Bypassing the ambiguities of test\nlosses, the PILE score is a single, uncertainty-aware metric that provides a\nselection principle for hyperparameters of a PIML model. We show that PILE\nminimization yields excellent choices for a wide variety of model parameters,\nincluding kernel bandwidth, least squares regularization weights, and even\nkernel function selection. We also show that, even prior to data acquisition, a\nspecial 'data-free' case of the PILE score identifies a priori kernel choices\nthat are 'well-adapted' to a given PDE. Beyond the kernel setting, we\nanticipate that the PILE score can be extended to PIML at large, and we outline\napproaches to do so.\n","authors":["Mara Daniels","Liam Hodgkinson","Michael Mahoney"],"pdf_url":"https://arxiv.org/pdf/2510.26121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.19331v2","updated":"2025-10-30T03:42:04Z","published":"2025-09-14T15:24:43Z","title":"Holographic Transformers for Complex-Valued Signal Processing:\n  Integrating Phase Interference into Self-Attention","summary":"  Complex-valued signals encode both amplitude and phase, yet most deep models\ntreat attention as real-valued correlation, overlooking interference effects.\nWe introduce the Holographic Transformer, a physics-inspired architecture that\nincorporates wave interference principles into self-attention. Holographic\nattention modulates interactions by relative phase and coherently superimposes\nvalues, ensuring consistency between amplitude and phase. A dual-headed decoder\nsimultaneously reconstructs the input and predicts task outputs, preventing\nphase collapse when losses prioritize magnitude over phase. We demonstrate that\nholographic attention implements a discrete interference operator and maintains\nphase consistency under linear mixing. Experiments on PolSAR image\nclassification and wireless channel prediction show strong performance,\nachieving high classification accuracy and F1 scores, low regression error, and\nincreased robustness to phase perturbations. These results highlight that\nenforcing physical consistency in attention leads to generalizable improvements\nin complex-valued learning and provides a unified, physics-based framework for\ncoherent signal modeling. The code is available at\nhttps://github.com/EonHao/Holographic-Transformers.\n","authors":["Enhao Huang","Zhiyu Zhang","Tianxiang Xu","Chunshu Xia","Kaichun Hu","Yuchen Yang","Tongtong Pan","Dong Dong","Zhan Qin"],"pdf_url":"https://arxiv.org/pdf/2509.19331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15201v3","updated":"2025-10-30T03:40:48Z","published":"2025-05-21T07:26:36Z","title":"Pass@K Policy Optimization: Solving Harder Reinforcement Learning\n  Problems","summary":"  Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples.\n","authors":["Christian Walder","Deep Karkhanis"],"pdf_url":"https://arxiv.org/pdf/2505.15201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26109v1","updated":"2025-10-30T03:36:19Z","published":"2025-10-30T03:36:19Z","title":"Do Not Step Into the Same River Twice: Learning to Reason from Trial and\n  Error","summary":"  Reinforcement learning with verifiable rewards (RLVR) has significantly\nboosted the reasoning capability of large language models (LLMs) recently.\nHowever, existing RLVR approaches merely train LLMs based on their own\ngenerated responses and are constrained by the initial capability of LLMs, thus\nprone to exploration stagnation, in which LLMs fail to solve more training\nproblems and cannot further learn from the training data. Some work tries to\naddress this by leveraging off-policy solutions to training problems but\nrequires external guidance from experts which suffers from limited\navailability. In this work, we propose LTE (Learning to reason from Trial and\nError), an approach hinting LLMs with their previously self-generated incorrect\nanswers and problem of overlong responses, which does not require any external\nexpert guidance. Experiments validate the effectiveness of LTE, which\noutperforms the normal group relative policy optimization (GRPO) by 6.38 in\nPass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for\nQwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the\nproblem of exploration stagnation and enhances both exploitation and\nexploration during training.\n","authors":["Chenming Tang","Hsiu-Yuan Huang","Weijie Liu","Saiyong Yang","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26109v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.04492v2","updated":"2025-10-30T03:24:21Z","published":"2025-03-06T14:40:21Z","title":"Accurate predictive model of band gap with selected important features\n  based on explainable machine learning","summary":"  In the rapidly advancing field of materials informatics, nonlinear machine\nlearning models have demonstrated exceptional predictive capabilities for\nmaterial properties. However, their black-box nature limits interpretability,\nand they may incorporate features that do not contribute to, or even\ndeteriorate, model performance. This study employs explainable ML (XML)\ntechniques, including permutation feature importance and the SHapley Additive\nexPlanation, applied to a pristine support vector regression model designed to\npredict band gaps at the GW level using 18 input features. Guided by\nXML-derived individual feature importance, a simple framework is proposed to\nconstruct reduced-feature predictive models. Model evaluations indicate that an\nXML-guided compact model, consisting of the top five features, achieves\ncomparable accuracy to the pristine model on in-domain datasets (0.254 vs.\n0.247 eV) while demonstrating superior generalization with lower prediction\nerrors on out-of-domain data (0.461 vs. 0.341 eV). Additionally, the study\nunderscores the necessity for eliminating strongly correlated features\n(correlation coefficient greater than 0.8) to prevent misinterpretation and\noverestimation of feature importance before applying XML. This study highlights\nXML's effectiveness in developing simplified yet highly accurate machine\nlearning models by clarifying feature roles, thereby reducing computational\ncosts for feature acquisition and enhancing model trustworthiness for materials\ndiscovery.\n","authors":["Joohwi Lee","Kaito Miyamoto"],"pdf_url":"https://arxiv.org/pdf/2503.04492v2.pdf","comment":"9 pages, 3 figures, SI is included"},{"id":"http://arxiv.org/abs/2510.26099v1","updated":"2025-10-30T03:22:55Z","published":"2025-10-30T03:22:55Z","title":"SAFE: A Novel Approach to AI Weather Evaluation through Stratified\n  Assessments of Forecasts over Earth","summary":"  The dominant paradigm in machine learning is to assess model performance\nbased on average loss across all samples in some test set. This amounts to\naveraging performance geospatially across the Earth in weather and climate\nsettings, failing to account for the non-uniform distribution of human\ndevelopment and geography. We introduce Stratified Assessments of Forecasts\nover Earth (SAFE), a package for elucidating the stratified performance of a\nset of predictions made over Earth. SAFE integrates various data domains to\nstratify by different attributes associated with geospatial gridpoints:\nterritory (usually country), global subregion, income, and landcover (land or\nwater). This allows us to examine the performance of models for each individual\nstratum of the different attributes (e.g., the accuracy in every individual\ncountry). To demonstrate its importance, we utilize SAFE to benchmark a zoo of\nstate-of-the-art AI-based weather prediction models, finding that they all\nexhibit disparities in forecasting skill across every attribute. We use this to\nseed a benchmark of model forecast fairness through stratification at different\nlead times for various climatic variables. By moving beyond globally-averaged\nmetrics, we for the first time ask: where do models perform best or worst, and\nwhich models are most fair? To support further work in this direction, the SAFE\npackage is open source and available at https://github.com/N-Masi/safe\n","authors":["Nick Masi","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2510.26099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26097v1","updated":"2025-10-30T03:22:13Z","published":"2025-10-30T03:22:13Z","title":"Robust Super-Capacity SRS Channel Inpainting via Diffusion Models","summary":"  Accurate channel state information (CSI) is essential for reliable multiuser\nMIMO operation. In 5G NR, reciprocity-based beamforming via uplink Sounding\nReference Signals (SRS) face resource and coverage constraints, motivating\nsparse non-uniform SRS allocation. Prior masked-autoencoder (MAE) approaches\nimprove coverage but overfit to training masks and degrade under unseen\ndistortions (e.g., additional masking, interference, clipping, non-Gaussian\nnoise). We propose a diffusion-based channel inpainting framework that\nintegrates system-model knowledge at inference via a likelihood-gradient term,\nenabling a single trained model to adapt across mismatched conditions. On\nstandardized CDL channels, the score-based diffusion variant consistently\noutperforms a UNet score-model baseline and the one-step MAE under distribution\nshift, with improvements up to 14 dB NMSE in challenging settings (e.g.,\nLaplace noise, user interference), while retaining competitive accuracy under\nmatched conditions. These results demonstrate that diffusion-guided inpainting\nis a robust and generalizable approach for super-capacity SRS design in 5G NR\nsystems.\n","authors":["Usman Akram","Fan Zhang","Yang Li","Haris Vikalo"],"pdf_url":"https://arxiv.org/pdf/2510.26097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26096v1","updated":"2025-10-30T03:19:59Z","published":"2025-10-30T03:19:59Z","title":"ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for\n  Audio-Language Models","summary":"  Recent advances in Audio-Language Models (ALMs) have significantly improved\nmultimodal understanding capabilities. However, the introduction of the audio\nmodality also brings new and unique vulnerability vectors. Previous studies\nhave proposed jailbreak attacks that specifically target ALMs, revealing that\ndefenses directly transferred from traditional audio adversarial attacks or\ntext-based Large Language Model (LLM) jailbreaks are largely ineffective\nagainst these ALM-specific threats. To address this issue, we propose ALMGuard,\nthe first defense framework tailored to ALMs. Based on the assumption that\nsafety-aligned shortcuts naturally exist in ALMs, we design a method to\nidentify universal Shortcut Activation Perturbations (SAPs) that serve as\ntriggers that activate the safety shortcuts to safeguard ALMs at inference\ntime. To better sift out effective triggers while preserving the model's\nutility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM),\nwhich restricts perturbations to Mel-frequency bins that are sensitive to\njailbreaks but insensitive to speech understanding. Both theoretical analyses\nand empirical results demonstrate the robustness of our method against both\nseen and unseen attacks. Overall, \\MethodName reduces the average success rate\nof advanced ALM-specific jailbreak attacks to 4.6% across four models, while\nmaintaining comparable utility on benign benchmarks, establishing it as the new\nstate of the art. Our code and data are available at\nhttps://github.com/WeifeiJin/ALMGuard.\n","authors":["Weifei Jin","Yuxin Cao","Junjie Su","Minhui Xue","Jie Hao","Ke Xu","Jin Song Dong","Derui Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26096v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26094v1","updated":"2025-10-30T03:09:40Z","published":"2025-10-30T03:09:40Z","title":"Lean4Physics: Comprehensive Reasoning Framework for College-level\n  Physics in Lean4","summary":"  We present **Lean4PHYS**, a comprehensive reasoning framework for\ncollege-level physics problems in Lean4. **Lean4PHYS** includes\n*LeanPhysBench*, a college-level benchmark for formal physics reasoning in\nLean4, which contains 200 hand-crafted and peer-reviewed statements derived\nfrom university textbooks and physics competition problems. To establish a\nsolid foundation for formal reasoning in physics, we also introduce *PhysLib*,\na community-driven repository containing fundamental unit systems and theorems\nessential for formal physics reasoning. Based on the benchmark and Lean4\nrepository we composed in **Lean4PHYS**, we report baseline results using major\nexpert Math Lean4 provers and state-of-the-art closed-source models, with the\nbest performance of DeepSeek-Prover-V2-7B achieving only 16% and\nClaude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that\nour *PhysLib* can achieve an average improvement of 11.75% in model\nperformance. This demonstrates the challenging nature of our *LeanPhysBench*\nand the effectiveness of *PhysLib*. To the best of our knowledge, this is the\nfirst study to provide a physics benchmark in Lean4.\n","authors":["Yuxin Li","Minghao Liu","Ruida Wang","Wenzhao Ji","Zhitao He","Rui Pan","Junming Huang","Tong Zhang","Yi R. Fung"],"pdf_url":"https://arxiv.org/pdf/2510.26094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09969v4","updated":"2025-10-30T02:56:28Z","published":"2025-02-14T07:55:47Z","title":"Neural Networks for Learnable and Scalable Influence Estimation of\n  Instruction Fine-Tuning Data","summary":"  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n","authors":["Ishika Agarwal","Dilek Hakkani-Tür"],"pdf_url":"https://arxiv.org/pdf/2502.09969v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24043v2","updated":"2025-10-30T02:55:35Z","published":"2025-10-28T03:53:46Z","title":"Localized Kernel Projection Outlyingness: A Two-Stage Approach for\n  Multi-Modal Outlier Detection","summary":"  This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection\nframework that overcomes the coexisting limitations of conventional\nprojection-based methods: their reliance on a fixed statistical metric and\ntheir assumption of a single data structure. Our framework uniquely synthesizes\nthree key concepts: (1) a generalized loss-based outlyingness measure (PLO)\nthat replaces the fixed metric with flexible, adaptive loss functions like our\nproposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear\ndata structures; and (3) a subsequent local clustering stage to handle\nmulti-modal distributions. Comprehensive 5-fold cross-validation experiments on\n10 benchmark datasets, with automated hyperparameter optimization, demonstrate\nthat Two-Stage LKPLO achieves state-of-the-art performance. It significantly\noutperforms strong baselines on datasets with challenging structures where\nexisting methods fail, most notably on multi-cluster data (Optdigits) and\ncomplex, high-dimensional data (Arrhythmia). Furthermore, an ablation study\nempirically confirms that the synergistic combination of both the kernelization\nand localization stages is indispensable for its superior performance. This\nwork contributes a powerful new tool for a significant class of outlier\ndetection problems and underscores the importance of hybrid, multi-stage\narchitectures.\n","authors":["Akira Tamamori"],"pdf_url":"https://arxiv.org/pdf/2510.24043v2.pdf","comment":"10 pages, 4 figures; submitted to The IEICE Transactions on\n  Information and Systems"},{"id":"http://arxiv.org/abs/2510.25327v2","updated":"2025-10-30T02:51:38Z","published":"2025-10-29T09:41:03Z","title":"MMEdge: Accelerating On-device Multimodal Inference via Pipelined\n  Sensing and Encoding","summary":"  Real-time multimodal inference on resource-constrained edge devices is\nessential for applications such as autonomous driving, human-computer\ninteraction, and mobile health. However, prior work often overlooks the tight\ncoupling between sensing dynamics and model execution, as well as the complex\ninter-modality dependencies. In this paper, we propose MMEdge, an new on-device\nmulti-modal inference framework based on pipelined sensing and encoding.\nInstead of waiting for complete sensor inputs, MMEdge decomposes the entire\ninference process into a sequence of fine-grained sensing and encoding units,\nallowing computation to proceed incrementally as data arrive. MMEdge also\nintroduces a lightweight but effective temporal aggregation module that\ncaptures rich temporal dynamics across different pipelined units to maintain\naccuracy performance. Such pipelined design also opens up opportunities for\nfine-grained cross-modal optimization and early decision-making during\ninference. To further enhance system performance under resource variability and\ninput data complexity, MMEdge incorporates an adaptive multimodal configuration\noptimizer that dynamically selects optimal sensing and model configurations for\neach modality under latency constraints, and a cross-modal speculative skipping\nmechanism that bypasses future units of slower modalities when early\npredictions reach sufficient confidence. We evaluate MMEdge using two public\nmultimodal datasets and deploy it on a real-world unmanned aerial vehicle\n(UAV)-based multimodal testbed. The results show that MMEdge significantly\nreduces end-to-end latency while maintaining high task accuracy across various\nsystem and data dynamics.\n","authors":["Runxi Huang","Mingxuan Yu","Mingyu Tsoi","Xiaomin Ouyang"],"pdf_url":"https://arxiv.org/pdf/2510.25327v2.pdf","comment":"Code available at: https://github.com/HKUST-MINSys-Lab/MMEdge.\n  Accepted by SenSys 2026"},{"id":"http://arxiv.org/abs/2510.26089v1","updated":"2025-10-30T02:49:46Z","published":"2025-10-30T02:49:46Z","title":"Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle\n  Routing","summary":"  Traffic congestion in urban road networks leads to longer trip times and\nhigher emissions, especially during peak periods. While the Shortest Path First\n(SPF) algorithm is optimal for a single vehicle in a static network, it\nperforms poorly in dynamic, multi-vehicle settings, often worsening congestion\nby routing all vehicles along identical paths. We address dynamic vehicle\nrouting through a multi-agent reinforcement learning (MARL) framework for\ncoordinated, network-aware fleet navigation. We first propose Adaptive\nNavigation (AN), a decentralized MARL model where each intersection agent\nprovides routing guidance based on (i) local traffic and (ii) neighborhood\nstate modeled using Graph Attention Networks (GAT). To improve scalability in\nlarge networks, we further propose Hierarchical Hub-based Adaptive Navigation\n(HHAN), an extension of AN that assigns agents only to key intersections\n(hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles\nmicro-routing within each hub region. For hub coordination, HHAN adopts\ncentralized training with decentralized execution (CTDE) under the Attentive\nQ-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions\nvia attention. Hub agents use flow-aware state features that combine local\ncongestion and predictive dynamics for proactive routing. Experiments on\nsynthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces\naverage travel time versus SPF and learning baselines, maintaining 100% routing\nsuccess. HHAN scales to networks with hundreds of intersections, achieving up\nto 15.9% improvement under heavy traffic. These findings highlight the\npotential of network-constrained MARL for scalable, coordinated, and\ncongestion-aware routing in intelligent transportation systems.\n","authors":["Fazel Arasteh","Arian Haghparast","Manos Papagelis"],"pdf_url":"https://arxiv.org/pdf/2510.26089v1.pdf","comment":"29 pages, 12 figures. Fazel Arasteh and Arian Haghparast contributed\n  equally to this research. Submitted to ACM Transactions on Spatial Algorithms\n  and Systems (TSAS). The code for this work is publicly available at\n  https://github.com/Arianhgh/HHAN"},{"id":"http://arxiv.org/abs/2510.26086v1","updated":"2025-10-30T02:47:25Z","published":"2025-10-30T02:47:25Z","title":"LLMBisect: Breaking Barriers in Bug Bisection with A Comparative\n  Analysis Pipeline","summary":"  Bug bisection has been an important security task that aims to understand the\nrange of software versions impacted by a bug, i.e., identifying the commit that\nintroduced the bug. However, traditional patch-based bisection methods are\nfaced with several significant barriers: For example, they assume that the\nbug-inducing commit (BIC) and the patch commit modify the same functions, which\nis not always true. They often rely solely on code changes, while the commit\nmessage frequently contains a wealth of vulnerability-related information. They\nare also based on simple heuristics (e.g., assuming the BIC initializes lines\ndeleted in the patch) and lack any logical analysis of the vulnerability.\n  In this paper, we make the observation that Large Language Models (LLMs) are\nwell-positioned to break the barriers of existing solutions, e.g., comprehend\nboth textual data and code in patches and commits. Unlike previous BIC\nidentification approaches, which yield poor results, we propose a comprehensive\nmulti-stage pipeline that leverages LLMs to: (1) fully utilize patch\ninformation, (2) compare multiple candidate commits in context, and (3)\nprogressively narrow down the candidates through a series of down-selection\nsteps. In our evaluation, we demonstrate that our approach achieves\nsignificantly better accuracy than the state-of-the-art solution by more than\n38\\%. Our results further confirm that the comprehensive multi-stage pipeline\nis essential, as it improves accuracy by 60\\% over a baseline LLM-based\nbisection method.\n","authors":["Zheng Zhang","Haonan Li","Xingyu Li","Hang Zhang","Zhiyun Qian"],"pdf_url":"https://arxiv.org/pdf/2510.26086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26083v1","updated":"2025-10-30T02:41:54Z","published":"2025-10-30T02:41:54Z","title":"Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism","summary":"  Specialized Generalist Models (SGMs) aim to preserve broad capabilities while\nachieving expert-level performance in target domains. However, traditional LLM\nstructures including Transformer, Linear Attention, and hybrid models do not\nemploy specialized memory mechanism guided by task information. In this paper,\nwe present Nirvana, an SGM with specialized memory mechanism, linear time\ncomplexity, and test-time task information extraction. Besides, we propose the\nTask-Aware Memory Trigger ($\\textit{Trigger}$) that flexibly adjusts memory\nmechanism based on the current task's requirements. In Trigger, each incoming\nsample is treated as a self-supervised fine-tuning task, enabling Nirvana to\nadapt its task-related parameters on the fly to domain shifts. We also design\nthe Specialized Memory Updater ($\\textit{Updater}$) that dynamically memorizes\nthe context guided by Trigger. We conduct experiments on both general language\ntasks and specialized medical tasks. On a variety of natural language modeling\nbenchmarks, Nirvana achieves competitive or superior results compared to the\nexisting LLM structures. To prove the effectiveness of Trigger on specialized\ntasks, we test Nirvana's performance on a challenging medical task, i.e.,\nMagnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with\nlightweight codecs on paired electromagnetic signals and MRI images. Despite\nthe frozen Nirvana backbone, Trigger guides the model to adapt to the MRI\ndomain with the change of task-related parameters. Nirvana achieves\nhigher-quality MRI reconstruction compared to conventional MRI models as well\nas the models with traditional LLMs' backbone, and can also generate accurate\npreliminary clinical reports accordingly.\n","authors":["Yuhua Jiang","Shuang Cheng","Yihao Liu","Ermo Hua","Che Jiang","Weigao Sun","Yu Cheng","Feifei Gao","Biqing Qi","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25114v2","updated":"2025-10-30T02:24:17Z","published":"2025-10-29T02:26:41Z","title":"Energy Approach from $\\varepsilon$-Graph to Continuum Diffusion Model\n  with Connectivity Functional","summary":"  We derive an energy-based continuum limit for $\\varepsilon$-graphs endowed\nwith a general connectivity functional. We prove that the discrete energy and\nits continuum counterpart differ by at most $O(\\varepsilon)$; the prefactor\ninvolves only the $W^{1,1}$-norm of the connectivity density as\n$\\varepsilon\\to0$, so the error bound remains valid even when that density has\nstrong local fluctuations. As an application, we introduce a neural-network\nprocedure that reconstructs the connectivity density from edge-weight data and\nthen embeds the resulting continuum model into a brain-dynamics framework. In\nthis setting, the usual constant diffusion coefficient is replaced by the\nspatially varying coefficient produced by the learned density, yielding\ndynamics that differ significantly from those obtained with conventional\nconstant-diffusion models.\n","authors":["Yahong Yang","Sun Lee","Jeff Calder","Wenrui Hao"],"pdf_url":"https://arxiv.org/pdf/2510.25114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00706v2","updated":"2025-10-30T02:22:30Z","published":"2025-02-02T07:39:37Z","title":"Model Provenance Testing for Large Language Models","summary":"  Large language models are increasingly customized through fine-tuning and\nother adaptations, creating challenges in enforcing licensing terms and\nmanaging downstream impacts. Tracking model origins is crucial both for\nprotecting intellectual property and for identifying derived models when biases\nor vulnerabilities are discovered in foundation models. We address this\nchallenge by developing a framework for testing model provenance: Whether one\nmodel is derived from another. Our approach is based on the key observation\nthat real-world model derivations preserve significant similarities in model\noutputs that can be detected through statistical analysis. Using only black-box\naccess to models, we employ multiple hypothesis testing to compare model\nsimilarities against a baseline established by unrelated models. On two\ncomprehensive real-world benchmarks spanning models from 30M to 4B parameters\nand comprising over 600 models, our tester achieves 90-95% precision and 80-90%\nrecall in identifying derived models. These results demonstrate the viability\nof systematic provenance verification in production environments even when only\nAPI access is available.\n","authors":["Ivica Nikolic","Teodora Baluta","Prateek Saxena"],"pdf_url":"https://arxiv.org/pdf/2502.00706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26076v1","updated":"2025-10-30T02:21:59Z","published":"2025-10-30T02:21:59Z","title":"New Money: A Systematic Review of Synthetic Data Generation for Finance","summary":"  Synthetic data generation has emerged as a promising approach to address the\nchallenges of using sensitive financial data in machine learning applications.\nBy leveraging generative models, such as Generative Adversarial Networks (GANs)\nand Variational Autoencoders (VAEs), it is possible to create artificial\ndatasets that preserve the statistical properties of real financial records\nwhile mitigating privacy risks and regulatory constraints. Despite the rapid\ngrowth of this field, a comprehensive synthesis of the current research\nlandscape has been lacking. This systematic review consolidates and analyses 72\nstudies published since 2018 that focus on synthetic financial data generation.\nWe categorise the types of financial information synthesised, the generative\nmethods employed, and the evaluation strategies used to assess data utility and\nprivacy. The findings indicate that GAN-based approaches dominate the\nliterature, particularly for generating time-series market data and tabular\ncredit data. While several innovative techniques demonstrate potential for\nimproved realism and privacy preservation, there remains a notable lack of\nrigorous evaluation of privacy safeguards across studies. By providing an\nintegrated overview of generative techniques, applications, and evaluation\nmethods, this review highlights critical research gaps and offers guidance for\nfuture work aimed at developing robust, privacy-preserving synthetic data\nsolutions for the financial domain.\n","authors":["James Meldrum","Basem Suleiman","Fethi Rabhi","Muhammad Johan Alibasa"],"pdf_url":"https://arxiv.org/pdf/2510.26076v1.pdf","comment":"37 pages, 5 figures, 21 tables"},{"id":"http://arxiv.org/abs/2502.01074v3","updated":"2025-10-30T02:03:05Z","published":"2025-02-03T05:33:51Z","title":"Omni-Mol: Multitask Molecular Model for Any-to-any Modalities","summary":"  In the molecular domain, numerous studies have explored the use of multimodal\nlarge language models (LLMs) to construct a general-purpose, multi-task\nmolecular model. However, these efforts are still far from achieving a truly\nuniversal molecular model. We identify three key challenges in this endeavor:\n(1) Existing molecular task datasets are typically small in scale and lack\ncomprehensive domain coverage. (2) Tasks from different molecular subfields are\ndifficult to effectively learn jointly through LLMs due to significant\ndistributional shifts and competition among tasks, which introduces instability\nin the learning process. (3) Both inter-task and intra-task molecular\nrepresentations demand different intrinsic dimensions in the language space,\nmaking it challenging to balance between redundancy and insufficiency in\nlanguage model representations. To address these challenges, we innovatively\ncategorize existing small-molecule tasks into four types: Mol2Mol, Mol2Text,\nMol2Num, and Text2Mol. We then collect a dataset encompassing over 16 tasks\nwith more than 1.4 million samples, making it the largest molecular\ninstruction-tuning dataset to date. Leveraging the extensive pretraining of\nLLMs on existing chemical literature, we propose a novel multimodal LLM\nframework, named Omni-Mol, which unifies all small-molecule tasks and supports\nboth molecular generation and understanding. The core of Omni-Mol is our\nproposed MoGE, which dynamically adapts to the intrinsic rank of different\ntasks. This mixture-of-experts architecture enhances the model's ability to\nhandle diverse tasks and modalities effectively. Our model achieves unified\ninstruction tuning across 16 tasks and attains state-of-the-art performance on\n13 of them. Extensive experiments further demonstrate the scalability and\nversatility of Omni-Mol.\n","authors":["Chengxin Hu","Hao Li","Yihe Yuan","Zezheng Song","Chenyang Zhao","Haixin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.01074v3.pdf","comment":"44 pages, 9 figures, 13 tables, paper accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26068v1","updated":"2025-10-30T01:53:32Z","published":"2025-10-30T01:53:32Z","title":"Learning Geometry: A Framework for Building Adaptive Manifold Models\n  through Metric Optimization","summary":"  This paper proposes a novel paradigm for machine learning that moves beyond\ntraditional parameter optimization. Unlike conventional approaches that search\nfor optimal parameters within a fixed geometric space, our core idea is to\ntreat the model itself as a malleable geometric entity. Specifically, we\noptimize the metric tensor field on a manifold with a predefined topology,\nthereby dynamically shaping the geometric structure of the model space. To\nachieve this, we construct a variational framework whose loss function\ncarefully balances data fidelity against the intrinsic geometric complexity of\nthe manifold. The former ensures the model effectively explains observed data,\nwhile the latter acts as a regularizer, penalizing overly curved or irregular\ngeometries to encourage simpler models and prevent overfitting. To address the\ncomputational challenges of this infinite-dimensional optimization problem, we\nintroduce a practical method based on discrete differential geometry: the\ncontinuous manifold is discretized into a triangular mesh, and the metric\ntensor is parameterized by edge lengths, enabling efficient optimization using\nautomatic differentiation tools. Theoretical analysis reveals a profound\nanalogy between our framework and the Einstein-Hilbert action in general\nrelativity, providing an elegant physical interpretation for the concept of\n\"data-driven geometry\". We further argue that even with fixed topology, metric\noptimization offers significantly greater expressive power than models with\nfixed geometry. This work lays a solid foundation for constructing fully\ndynamic \"meta-learners\" capable of autonomously evolving their geometry and\ntopology, and it points to broad application prospects in areas such as\nscientific model discovery and robust representation learning.\n","authors":["Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26068v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2509.17784v2","updated":"2025-10-30T01:43:48Z","published":"2025-09-22T13:45:17Z","title":"Revealing Multimodal Causality with Large Language Models","summary":"  Uncovering cause-and-effect mechanisms from data is fundamental to scientific\nprogress. While large language models (LLMs) show promise for enhancing causal\ndiscovery (CD) from unstructured data, their application to the increasingly\nprevalent multimodal setting remains a critical challenge. Even with the advent\nof multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two\nprimary limitations: (1) difficulty in exploring intra- and inter-modal\ninteractions for comprehensive causal variable identification; and (2)\ninsufficiency to handle structural ambiguities with purely observational data.\nTo address these challenges, we propose MLLM-CD, a novel framework for\nmultimodal causal discovery from unstructured data. It consists of three key\ncomponents: (1) a novel contrastive factor discovery module to identify genuine\nmultimodal factors based on the interactions explored from contrastive sample\npairs; (2) a statistical causal structure discovery module to infer causal\nrelationships among discovered factors; and (3) an iterative multimodal\ncounterfactual reasoning module to refine the discovery outcomes iteratively by\nincorporating the world knowledge and reasoning capabilities of MLLMs.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of the proposed MLLM-CD in revealing genuine factors and causal\nrelationships among them from multimodal unstructured data.\n","authors":["Jin Li","Shoujin Wang","Qi Zhang","Feng Liu","Tongliang Liu","Longbing Cao","Shui Yu","Fang Chen"],"pdf_url":"https://arxiv.org/pdf/2509.17784v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2501.14808v4","updated":"2025-10-30T01:41:07Z","published":"2025-01-15T16:32:27Z","title":"HyGen: Efficient LLM Serving via Elastic Online-Offline Request\n  Co-location","summary":"  Large language models (LLMs) have facilitated a wide range of applications\nwith distinct service-level objectives (SLOs), from latency-sensitive online\ntasks like interactive chatbots to throughput-oriented offline workloads like\ndata synthesis. The existing deployment model, which dedicates machines to each\nworkload, simplifies SLO management but often leads to poor resource\nutilization. This paper introduces HyGen, an interference-aware LLM serving\nsystem that enables efficient co-location of online and offline workloads while\npreserving SLOs. HyGen incorporates two key innovations: (1) performance\ncontrol mechanisms, including a latency predictor to estimate batch execution\ntime and an SLO-aware profiler to quantify latency interference, and (2)\nSLO-aware offline scheduling policies that maximize serving throughput and\nprevent starvation. Our evaluation on production workloads shows that HyGen\nachieves up to 3.9-5.8x throughput gains over online and hybrid serving\nbaselines, while ensuring latency SLOs. The code of HyGen is publicly available\nat https://github.com/UIUC-MLSys/HyGen.\n","authors":["Ting Sun","Penghan Wang","Fan Lai"],"pdf_url":"https://arxiv.org/pdf/2501.14808v4.pdf","comment":"NeurIPS 2025. 21 pages, 17 figures"},{"id":"http://arxiv.org/abs/2505.11411v2","updated":"2025-10-30T01:39:13Z","published":"2025-05-16T16:20:02Z","title":"Is Grokking a Computational Glass Relaxation?","summary":"  Understanding neural network's (NN) generalizability remains a central\nquestion in deep learning research. The special phenomenon of grokking, where\nNNs abruptly generalize long after the training performance reaches a\nnear-perfect level, offers a unique window to investigate the underlying\nmechanisms of NNs' generalizability. Here we propose an interpretation for\ngrokking by framing it as a computational glass relaxation: viewing NNs as a\nphysical system where parameters are the degrees of freedom and train loss is\nthe system energy, we find memorization process resembles a rapid cooling of\nliquid into non-equilibrium glassy state at low temperature and the later\ngeneralization is like a slow relaxation towards a more stable configuration.\nThis mapping enables us to sample NNs' Boltzmann entropy (states of density)\nlandscape as a function of training loss and test accuracy. Our experiments in\ntransformers on arithmetic tasks suggests that there is NO entropy barrier in\nthe memorization-to-generalization transition of grokking, challenging previous\ntheory that defines grokking as a first-order phase transition. We identify a\nhigh-entropy advantage under grokking, an extension of prior work linking\nentropy to generalizability but much more significant. Inspired by grokking's\nfar-from-equilibrium nature, we develop a toy optimizer WanD based on\nWang-landau molecular dynamics, which can eliminate grokking without any\nconstraints and find high-norm generalizing solutions. This provides\nstrictly-defined counterexamples to theory attributing grokking solely to\nweight norm evolution towards the Goldilocks zone and also suggests new\npotential ways for optimizer design.\n","authors":["Xiaotian Zhang","Yue Shang","Entao Yang","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26064v1","updated":"2025-10-30T01:36:44Z","published":"2025-10-30T01:36:44Z","title":"Towards Scaling Laws for Symbolic Regression","summary":"  Symbolic regression (SR) aims to discover the underlying mathematical\nexpressions that explain observed data. This holds promise for both gaining\nscientific insight and for producing inherently interpretable and generalizable\nmodels for tabular data. In this work we focus on the basics of SR. Deep\nlearning-based SR has recently become competitive with genetic programming\napproaches, but the role of scale has remained largely unexplored. Inspired by\nscaling laws in language modeling, we present the first systematic\ninvestigation of scaling in SR, using a scalable end-to-end transformer\npipeline and carefully generated training data. Across five different model\nsizes and spanning three orders of magnitude in compute, we find that both\nvalidation loss and solved rate follow clear power-law trends with compute. We\nfurther identify compute-optimal hyperparameter scaling: optimal batch size and\nlearning rate grow with model size, and a token-to-parameter ratio of\n$\\approx$15 is optimal in our regime, with a slight upward trend as compute\nincreases. These results demonstrate that SR performance is largely predictable\nfrom compute and offer important insights for training the next generation of\nSR models.\n","authors":["David Otte","Jörg K. H. Franke","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2510.26064v1.pdf","comment":"Accepted at the NeurIPS 2025 Math-AI Workshop"},{"id":"http://arxiv.org/abs/2510.26061v1","updated":"2025-10-30T01:32:21Z","published":"2025-10-30T01:32:21Z","title":"Data-driven Projection Generation for Efficiently Solving Heterogeneous\n  Quadratic Programming Problems","summary":"  We propose a data-driven framework for efficiently solving quadratic\nprogramming (QP) problems by reducing the number of variables in\nhigh-dimensional QPs using instance-specific projection. A graph neural\nnetwork-based model is designed to generate projections tailored to each QP\ninstance, enabling us to produce high-quality solutions even for previously\nunseen problems. The model is trained on heterogeneous QPs to minimize the\nexpected objective value evaluated on the projected solutions. This is\nformulated as a bilevel optimization problem; the inner optimization solves the\nQP under a given projection using a QP solver, while the outer optimization\nupdates the model parameters. We develop an efficient algorithm to solve this\nbilevel optimization problem, which computes parameter gradients without\nbackpropagating through the solver. We provide a theoretical analysis of the\ngeneralization ability of solving QPs with projection matrices generated by\nneural networks. Experimental results demonstrate that our method produces\nhigh-quality feasible solutions with reduced computation time, outperforming\nexisting methods.\n","authors":["Tomoharu Iwata","Futoshi Futami"],"pdf_url":"https://arxiv.org/pdf/2510.26061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03710v3","updated":"2025-10-30T01:16:06Z","published":"2025-03-05T18:01:05Z","title":"Improving LLM Safety Alignment with Dual-Objective Optimization","summary":"  Existing training-time safety alignment techniques for large language models\n(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization\n(DPO), a widely deployed alignment method, exhibits limitations in both\nexperimental and theoretical contexts as its loss function proves suboptimal\nfor refusal learning. Through gradient-based analysis, we identify these\nshortcomings and propose an improved safety alignment that disentangles DPO\nobjectives into two components: (1) robust refusal training, which encourages\nrefusal even when partial unsafe generations are produced, and (2) targeted\nunlearning of harmful knowledge. This approach significantly increases LLM\nrobustness against a wide range of jailbreak attacks, including prefilling,\nsuffix, and multi-turn attacks across both in-distribution and\nout-of-distribution scenarios. Furthermore, we introduce a method to emphasize\ncritical refusal tokens by incorporating a reward-based token-level weighting\nmechanism for refusal learning, which further improves the robustness against\nadversarial exploits. Our research also suggests that robustness to jailbreak\nattacks is correlated with token distribution shifts in the training process\nand internal representations of refusal and harmful tokens, offering valuable\ndirections for future research in LLM safety alignment. The code is available\nat https://github.com/wicai24/DOOR-Alignment\n","authors":["Xuandong Zhao","Will Cai","Tianneng Shi","David Huang","Licong Lin","Song Mei","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2503.03710v3.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2510.24889v2","updated":"2025-10-30T00:55:31Z","published":"2025-10-28T18:48:48Z","title":"Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep\n  Q-learning thresholding","summary":"  Rapid triage of suspected stroke needs accurate, bedside-deployable tools;\nEEG is promising but underused at first contact. We present an adaptive\nmultitask EEG classifier that converts 32-channel signals to power spectral\ndensity features (Welch), uses a recurrent-convolutional network (GRU-TCN) to\npredict stroke type (healthy, ischemic, hemorrhagic), hemispheric\nlateralization, and severity, and applies a deep Q-network (DQN) to tune\ndecision thresholds in real time. Using a patient-wise split of the UCLH Stroke\nEIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the\nprimary outcome was stroke-type performance; secondary outcomes were severity\nand lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for\nstroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%)\nfor lateralization. With DQN threshold adaptation, stroke-type accuracy\nincreased to about 98.0% (F1 97.7%). We also tested robustness on an\nindependent, low-density EEG cohort (ZJU4H) and report paired patient-level\nstatistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies\n(index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis;\npatient-wise evaluation). Adaptive thresholding shifts the operating point to\nclinically preferred sensitivity-specificity trade-offs, while integrated\nscalp-map and spectral visualizations support interpretability.\n","authors":["Shakeel Abdulkareem","Bora Yimenicioglu","Khartik Uppalapati","Aneesh Gudipati","Adan Eftekhari","Saleh Yassin"],"pdf_url":"https://arxiv.org/pdf/2510.24889v2.pdf","comment":"10 pages, 6 figures. Equal contribution: Shakeel Abdulkareem and Bora\n  Yimenicioglu. Compiled with pdfLaTeX (wlscirep class)"},{"id":"http://arxiv.org/abs/2510.26046v1","updated":"2025-10-30T00:52:25Z","published":"2025-10-30T00:52:25Z","title":"Bias-Corrected Data Synthesis for Imbalanced Learning","summary":"  Imbalanced data, where the positive samples represent only a small proportion\ncompared to the negative samples, makes it challenging for classification\nproblems to balance the false positive and false negative rates. A common\napproach to addressing the challenge involves generating synthetic data for the\nminority group and then training classification models with both observed and\nsynthetic data. However, since the synthetic data depends on the observed data\nand fails to replicate the original data distribution accurately, prediction\naccuracy is reduced when the synthetic data is naively treated as the true\ndata. In this paper, we address the bias introduced by synthetic data and\nprovide consistent estimators for this bias by borrowing information from the\nmajority group. We propose a bias correction procedure to mitigate the adverse\neffects of synthetic data, enhancing prediction accuracy while avoiding\noverfitting. This procedure is extended to broader scenarios with imbalanced\ndata, such as imbalanced multi-task learning and causal inference. Theoretical\nproperties, including bounds on bias estimation errors and improvements in\nprediction accuracy, are provided. Simulation results and data analysis on\nhandwritten digit datasets demonstrate the effectiveness of our method.\n","authors":["Pengfei Lyu","Zhengchi Ma","Linjun Zhang","Anru R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26046v1.pdf","comment":"41 pages, 4 figures, includes proofs and appendix"},{"id":"http://arxiv.org/abs/2510.26043v1","updated":"2025-10-30T00:44:56Z","published":"2025-10-30T00:44:56Z","title":"$L_1$-norm Regularized Indefinite Kernel Logistic Regression","summary":"  Kernel logistic regression (KLR) is a powerful classification method widely\napplied across diverse domains. In many real-world scenarios, indefinite\nkernels capture more domain-specific structural information than positive\ndefinite kernels. This paper proposes a novel $L_1$-norm regularized indefinite\nkernel logistic regression (RIKLR) model, which extends the existing IKLR\nframework by introducing sparsity via an $L_1$-norm penalty. The introduction\nof this regularization enhances interpretability and generalization while\nintroducing nonsmoothness and nonconvexity into the optimization landscape. To\naddress these challenges, a theoretically grounded and computationally\nefficient proximal linearized algorithm is developed. Experimental results on\nmultiple benchmark datasets demonstrate the superior performance of the\nproposed method in terms of both accuracy and sparsity.\n","authors":["Shaoxin Wang","Hanjing Yao"],"pdf_url":"https://arxiv.org/pdf/2510.26043v1.pdf","comment":"17 pages, 1 figure"},{"id":"http://arxiv.org/abs/2510.26040v1","updated":"2025-10-30T00:38:18Z","published":"2025-10-30T00:38:18Z","title":"Accelerating Real-World Overtaking in F1TENTH Racing Employing\n  Reinforcement Learning Methods","summary":"  While autonomous racing performance in Time-Trial scenarios has seen\nsignificant progress and development, autonomous wheel-to-wheel racing and\novertaking are still severely limited. These limitations are particularly\napparent in real-life driving scenarios where state-of-the-art algorithms\nstruggle to safely or reliably complete overtaking manoeuvres. This is\nimportant, as reliable navigation around other vehicles is vital for safe\nautonomous wheel-to-wheel racing. The F1Tenth Competition provides a useful\nopportunity for developing wheel-to-wheel racing algorithms on a standardised\nphysical platform. The competition format makes it possible to evaluate\novertaking and wheel-to-wheel racing algorithms against the state-of-the-art.\nThis research presents a novel racing and overtaking agent capable of learning\nto reliably navigate a track and overtake opponents in both simulation and\nreality. The agent was deployed on an F1Tenth vehicle and competed against\nopponents running varying competitive algorithms in the real world. The results\ndemonstrate that the agent's training against opponents enables deliberate\novertaking behaviours with an overtaking rate of 87% compared 56% for an agent\ntrained just to race.\n","authors":["Emily Steiner","Daniel van der Spuy","Futian Zhou","Afereti Pama","Minas Liarokapis","Henry Williams"],"pdf_url":"https://arxiv.org/pdf/2510.26040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26038v1","updated":"2025-10-30T00:34:16Z","published":"2025-10-30T00:34:16Z","title":"Do Students Debias Like Teachers? On the Distillability of Bias\n  Mitigation Methods","summary":"  Knowledge distillation (KD) is an effective method for model compression and\ntransferring knowledge between models. However, its effect on model's\nrobustness against spurious correlations that degrade performance on\nout-of-distribution data remains underexplored. This study investigates the\neffect of knowledge distillation on the transferability of ``debiasing''\ncapabilities from teacher models to student models on natural language\ninference (NLI) and image classification tasks. Through extensive experiments,\nwe illustrate several key findings: (i) overall the debiasing capability of a\nmodel is undermined post-KD; (ii) training a debiased model does not benefit\nfrom injecting teacher knowledge; (iii) although the overall robustness of a\nmodel may remain stable post-distillation, significant variations can occur\nacross different types of biases; and (iv) we pin-point the internal attention\npattern and circuit that causes the distinct behavior post-KD. Given the above\nfindings, we propose three effective solutions to improve the distillability of\ndebiasing methods: developing high quality data for augmentation, implementing\niterative knowledge distillation, and initializing student models with weights\nobtained from teacher models. To the best of our knowledge, this is the first\nstudy on the effect of KD on debiasing and its interenal mechanism at scale.\nOur findings provide understandings on how KD works and how to design better\ndebiasing methods.\n","authors":["Jiali Cheng","Chirag Agarwal","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2510.26038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12869v4","updated":"2025-10-30T00:34:12Z","published":"2024-10-14T01:57:25Z","title":"Language Model Preference Evaluation with Multiple Weak Evaluators","summary":"  Despite the remarkable success of Large Language Models (LLMs), evaluating\ntheir outputs' quality regarding preference remains a critical challenge. While\nexisting works usually leverage a strong LLM as the judge for comparing LLMs'\nresponse pairwisely, such a single-evaluator approach is vulnerable to cyclic\npreference, i.e., output A is better than B, B than C, but C is better than A,\ncausing contradictory evaluation results. To address this, we introduce PGED\n(Preference Graph Ensemble and Denoise), a novel approach that leverages\nmultiple model-based evaluators to construct preference graphs, and then\nensembles and denoises these graphs for acyclic, non-contradictory evaluation\nresults. We provide theoretical guarantees for our framework, demonstrating its\nefficacy in recovering the ground truth preference structure. Extensive\nexperiments on ten benchmarks demonstrate PGED 's superiority in three\napplications: 1) model ranking for evaluation, 2) response selection for\ntest-time scaling, and 3) data selection for model fine-tuning. Notably, PGED\ncombines small LLM evaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to\noutperform strong ones (e.g., Qwen2-72B), showcasing its effectiveness in\nenhancing evaluation reliability and improving model performance.\n","authors":["Zhengyu Hu","Jieyu Zhang","Zhihan Xiong","Alexander Ratner","Kaize Ding","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2410.12869v4.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2510.26759v1","updated":"2025-10-30T17:49:49Z","published":"2025-10-30T17:49:49Z","title":"MORE: Multi-Organ Medical Image REconstruction Dataset","summary":"  CT reconstruction provides radiologists with images for diagnosis and\ntreatment, yet current deep learning methods are typically limited to specific\nanatomies and datasets, hindering generalization ability to unseen anatomies\nand lesions. To address this, we introduce the Multi-Organ medical image\nREconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies\nwith 15 lesion types. This dataset serves two key purposes: (1) enabling robust\ntraining of deep learning models on extensive, heterogeneous data, and (2)\nfacilitating rigorous evaluation of model generalization for CT reconstruction.\nWe further establish a strong baseline solution that outperforms prior\napproaches under these challenging conditions. Our results demonstrate that:\n(1) a comprehensive dataset helps improve the generalization capability of\nmodels, and (2) optimization-based methods offer enhanced robustness for unseen\nanatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our\nproject page https://more-med.github.io/\n","authors":["Shaokai Wu","Yapan Guo","Yanbiao Ji","Jing Tong","Yuxiang Lu","Mei Li","Suizhi Huang","Yue Ding","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2510.26759v1.pdf","comment":"Accepted to ACMMM 2025"},{"id":"http://arxiv.org/abs/2503.09205v3","updated":"2025-10-30T17:37:55Z","published":"2025-03-12T09:48:38Z","title":"Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model","summary":"  Integrating audio and visual data for training multimodal foundational models\nremains a challenge. The Audio-Video Vector Alignment (AVVA) framework\naddresses this by considering AV scene alignment beyond mere temporal\nsynchronization, and leveraging Large Language Models (LLMs) for data curation.\nAVVA implements a scoring mechanism for selecting aligned training data\nsegments. It integrates Whisper, a speech-based foundation model, for audio and\nDINOv2 for video analysis in a dual-encoder structure with contrastive learning\non AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the\neffectiveness of the proposed model architecture and data curation approach.\nAVVA achieves a significant improvement in top-k accuracies for video-to-audio\nretrieval on all datasets compared to DenseAV, while using only 192 hrs of\ncurated training data. Furthermore, an ablation study indicates that the data\ncuration process effectively trades data quality for data quantity, yielding\nincreases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound,\ncompared to training on the full spectrum of uncurated data.\n","authors":["Ali Vosoughi","Dimitra Emmanouilidou","Hannes Gamper"],"pdf_url":"https://arxiv.org/pdf/2503.09205v3.pdf","comment":"5 pages, 5 figures, 2 tables. Accepted at EUSIPCO 2025"},{"id":"http://arxiv.org/abs/2510.26721v1","updated":"2025-10-30T17:22:22Z","published":"2025-10-30T17:22:22Z","title":"Unveiling Intrinsic Text Bias in Multimodal Large Language Models\n  through Attention Key-Space Analysis","summary":"  Multimodal large language models (MLLMs) exhibit a pronounced preference for\ntextual inputs when processing vision-language data, limiting their ability to\nreason effectively from visual evidence. Unlike prior studies that attribute\nthis text bias to external factors such as data imbalance or instruction\ntuning, we propose that the bias originates from the model's internal\narchitecture. Specifically, we hypothesize that visual key vectors (Visual\nKeys) are out-of-distribution (OOD) relative to the text key space learned\nduring language-only pretraining. Consequently, these visual keys receive\nsystematically lower similarity scores during attention computation, leading to\ntheir under-utilization in the context representation. To validate this\nhypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their\ndistributional structures using qualitative (t-SNE) and quantitative\n(Jensen-Shannon divergence) methods. The results provide direct evidence that\nvisual and textual keys occupy markedly distinct subspaces within the attention\nspace. The inter-modal divergence is statistically significant, exceeding\nintra-modal variation by several orders of magnitude. These findings reveal\nthat text bias arises from an intrinsic misalignment within the attention key\nspace rather than solely from external data factors.\n","authors":["Xinhan Zheng","Huyu Wu","Xueting Wang","Haiyun Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.26721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24518v2","updated":"2025-10-30T15:08:00Z","published":"2025-05-30T12:30:04Z","title":"ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis\n  Optimization for Speech Multi-Metric Estimation","summary":"  Speech signal analysis poses significant challenges, particularly in tasks\nsuch as speech quality evaluation and profiling, where the goal is to predict\nmultiple perceptual and objective metrics. For instance, metrics like PESQ\n(Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective\nIntelligibility), and MOS (Mean Opinion Score) each capture different aspects\nof speech quality. However, these metrics often have different scales,\nassumptions, and dependencies, making joint estimation non-trivial. To address\nthese issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based\nHypothesis Optimization), a chain-based, versatile evaluation system for speech\nassessment grounded in autoregressive dependency modeling. ARECHO is\ndistinguished by three key innovations: (1) a comprehensive speech information\ntokenization pipeline; (2) a dynamic classifier chain that explicitly captures\ninter-metric dependencies; and (3) a two-step confidence-oriented decoding\nalgorithm that enhances inference reliability. Experiments demonstrate that\nARECHO significantly outperforms the baseline framework across diverse\nevaluation scenarios, including enhanced speech analysis, speech generation\nevaluation, and, noisy speech evaluation. Furthermore, its dynamic dependency\nmodeling improves interpretability by capturing inter-metric relationships.\nAcross tasks, ARECHO offers reference-free evaluation using its dynamic\nclassifier chain to support subset queries (single or multiple metrics) and\nreduces error propagation via confidence-oriented decoding.\n","authors":["Jiatong Shi","Yifan Cheng","Bo-Hao Su","Hye-jin Shim","Jinchuan Tian","Samuele Cornell","Yiwen Zhao","Siddhant Arora","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2505.24518v2.pdf","comment":"NeurIPS 2025 Spotlight"},{"id":"http://arxiv.org/abs/2510.26569v1","updated":"2025-10-30T14:59:37Z","published":"2025-10-30T14:59:37Z","title":"AdSum: Two-stream Audio-visual Summarization for Automated Video\n  Advertisement Clipping","summary":"  Advertisers commonly need multiple versions of the same advertisement (ad) at\nvarying durations for a single campaign. The traditional approach involves\nmanually selecting and re-editing shots from longer video ads to create shorter\nversions, which is labor-intensive and time-consuming. In this paper, we\nintroduce a framework for automated video ad clipping using video summarization\ntechniques. We are the first to frame video clipping as a shot selection\nproblem, tailored specifically for advertising. Unlike existing general video\nsummarization methods that primarily focus on visual content, our approach\nemphasizes the critical role of audio in advertising. To achieve this, we\ndevelop a two-stream audio-visual fusion model that predicts the importance of\nvideo frames, where importance is defined as the likelihood of a frame being\nselected in the firm-produced short ad. To address the lack of ad-specific\ndatasets, we present AdSum204, a novel dataset comprising 102 pairs of\n30-second and 15-second ads from real advertising campaigns. Extensive\nexperiments demonstrate that our model outperforms state-of-the-art methods\nacross various metrics, including Average Precision, Area Under Curve,\nSpearman, and Kendall.\n","authors":["Wen Xie","Yanjun Zhu","Gijs Overgoor","Yakov Bart","Agata Lapedriza Garcia","Sarah Ostadabbas"],"pdf_url":"https://arxiv.org/pdf/2510.26569v1.pdf","comment":"Accepted at 32nd International Conference on MultiMedia Modeling"},{"id":"http://arxiv.org/abs/2504.11331v2","updated":"2025-10-30T14:50:58Z","published":"2025-04-15T16:05:09Z","title":"Dependency Structure Augmented Contextual Scoping Framework for\n  Multimodal Aspect-Based Sentiment Analysis","summary":"  Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract\nfine-grained information from image-text pairs to identify aspect terms and\ndetermine their sentiment polarity. However, existing approaches often fall\nshort in simultaneously addressing three core challenges: Sentiment Cue\nPerception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise\nElimination (SNE). To overcome these limitations, we propose DASCO\n(\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework),\na fine-grained scope-oriented framework that enhances aspect-level sentiment\nreasoning by leveraging dependency parsing trees. First, we designed a\nmulti-task pretraining strategy for MABSA on our base model, combining\naspect-oriented enhancement, image-text matching, and aspect-level\nsentiment-sensitive cognition. This improved the model's perception of aspect\nterms and sentiment cues while achieving effective image-text alignment,\naddressing key challenges like SCP and MIM. Furthermore, we incorporate\ndependency trees as syntactic branch combining with semantic branch, guiding\nthe model to selectively attend to critical contextual elements within a\ntarget-specific scope while effectively filtering out irrelevant noise for\naddressing SNE problem. Extensive experiments on two benchmark datasets across\nthree subtasks demonstrate that DASCO achieves state-of-the-art performance in\nMABSA, with notable gains in JMASA (+2.3\\% F1 and +3.5\\% precision on\nTwitter2015). The source code is available at https://github.com/LHaoooo/DASCO .\n","authors":["Hao Liu","Lijun He","Jiaxi Liang","Zhihan Ren","Haixia Bi","Fan Li"],"pdf_url":"https://arxiv.org/pdf/2504.11331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.24325v2","updated":"2025-10-30T13:38:59Z","published":"2025-09-29T06:23:47Z","title":"ReCon-GS: Continuum-Preserved Gaussian Streaming for Fast and Compact\n  Reconstruction of Dynamic Scenes","summary":"  Online free-viewpoint video (FVV) reconstruction is challenged by slow\nper-frame optimization, inconsistent motion estimation, and unsustainable\nstorage demands. To address these challenges, we propose the Reconfigurable\nContinuum Gaussian Stream, dubbed ReCon-GS, a novel storage-aware framework\nthat enables high fidelity online dynamic scene reconstruction and real-time\nrendering. Specifically, we dynamically allocate multi-level Anchor Gaussians\nin a density-adaptive fashion to capture inter-frame geometric deformations,\nthereby decomposing scene motion into compact coarse-to-fine representations.\nThen, we design a dynamic hierarchy reconfiguration strategy that preserves\nlocalized motion expressiveness through on-demand anchor re-hierarchization,\nwhile ensuring temporal consistency through intra-hierarchical deformation\ninheritance that confines transformation priors to their respective hierarchy\nlevels. Furthermore, we introduce a storage-aware optimization mechanism that\nflexibly adjusts the density of Anchor Gaussians at different hierarchy levels,\nenabling a controllable trade-off between reconstruction fidelity and memory\nusage. Extensive experiments on three widely used datasets demonstrate that,\ncompared to state-of-the-art methods, ReCon-GS improves training efficiency by\napproximately 15% and achieves superior FVV synthesis quality with enhanced\nrobustness and stability. Moreover, at equivalent rendering quality, ReCon-GS\nslashes memory requirements by over 50% compared to leading state-of-the-art\nmethods.\n","authors":["Jiaye Fu","Qiankun Gao","Chengxiang Wen","Yanmin Wu","Siwei Ma","Jiaqi Zhang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.24325v2.pdf","comment":"Published in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2509.04448v2","updated":"2025-10-30T10:58:04Z","published":"2025-09-04T17:59:43Z","title":"TRUST-VL: An Explainable News Assistant for General Multimodal\n  Misinformation Detection","summary":"  Multimodal misinformation, encompassing textual, visual, and cross-modal\ndistortions, poses an increasing societal threat that is amplified by\ngenerative AI. Existing methods typically focus on a single type of distortion\nand struggle to generalize to unseen scenarios. In this work, we observe that\ndifferent distortion types share common reasoning capabilities while also\nrequiring task-specific skills. We hypothesize that joint training across\ndistortion types facilitates knowledge sharing and enhances the model's ability\nto generalize. To this end, we introduce TRUST-VL, a unified and explainable\nvision-language model for general multimodal misinformation detection. TRUST-VL\nincorporates a novel Question-Aware Visual Amplifier module, designed to\nextract task-specific visual features. To support training, we also construct\nTRUST-Instruct, a large-scale instruction dataset containing 198K samples\nfeaturing structured reasoning chains aligned with human fact-checking\nworkflows. Extensive experiments on both in-domain and zero-shot benchmarks\ndemonstrate that TRUST-VL achieves state-of-the-art performance, while also\noffering strong generalization and interpretability.\n","authors":["Zehong Yan","Peng Qi","Wynne Hsu","Mong Li Lee"],"pdf_url":"https://arxiv.org/pdf/2509.04448v2.pdf","comment":"EMNLP 2025 Oral; Project Homepage:\n  https://yanzehong.github.io/trust-vl/"},{"id":"http://arxiv.org/abs/2510.26289v1","updated":"2025-10-30T09:14:46Z","published":"2025-10-30T09:14:46Z","title":"Contribution-Guided Asymmetric Learning for Robust Multimodal Fusion\n  under Imbalance and Noise","summary":"  Multimodal learning faces two major challenges: modality imbalance and data\nnoise, which significantly affect the robustness and generalization ability of\nmodels. Existing methods achieve modality balance by suppressing dominant\nmodalities, but they neglect the inherent differences in the information value\nbetween modalities, potentially leading to convergence to suboptimal solutions.\nThis paper proposes an innovative modality compression paradigm,\nContribution-Guided Asymmetric Learning (CAL), which aims to enhance the\ncontribution of high-contribution modalities while compressing weak modalities\nto increase their contribution, allowing both to improve the performance of\nmultimodal information fusion. CAL is based on a modality contribution metric\nW^m combining the information quantity I(m) and confidence D(m), and it designs\nan asymmetric gradient acceleration mechanism and a contribution-aware\nAsymmetric Information Bottleneck (AIB) compression mechanism. The former\naccelerates the gradient update of modalities, while the latter dynamically\ncompresses the noise of low-contribution modalities.\n  On five benchmark datasets, including emotion recognition, scene recognition,\nand event localization tasks, CAL has shown outstanding performance in\nimbalanced fusion tasks and noise robustness tests. On CREMA-D, KS, and AVE,\nCAL achieves 79.30%, 74.82%, and 74.21% accuracy, significantly outperforming\nthe existing state-of-the-art model ARL. In high-noise robustness tests, CAL\nalso achieved leading performance under various attack strategies on the\nMVSA-Single and NYUD2 datasets. These results validate the significant\nadvantages of CAL in modality imbalance and noise interference. CAL, as a\nflexible and efficient framework, is easy to transfer to other tasks and has\nbroad adaptability and potential application prospects.\n","authors":["Zijing Xu","Yunfeng Kou","Kunming Wu","Hong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25600v2","updated":"2025-10-30T03:43:02Z","published":"2025-10-29T15:10:17Z","title":"PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models","summary":"  Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation.\n","authors":["Zhonghua Jiang","Kunxi Li","Yiyun Zhou","Sihao Liu","Zhaode Wang","Chengfei lv","Shengyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25600v2.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2510.26742v1","updated":"2025-10-30T17:38:14Z","published":"2025-10-30T17:38:14Z","title":"Running VLAs at Real-time Speed","summary":"  In this paper, we show how to run pi0-level multi-view VLA at 30Hz frame rate\nand at most 480Hz trajectory frequency using a single consumer GPU. This\nenables dynamic and real-time tasks that were previously believed to be\nunattainable by large VLA models. To achieve it, we introduce a bag of\nstrategies to eliminate the overheads in model inference. The real-world\nexperiment shows that the pi0 policy with our strategy achieves a 100% success\nrate in grasping a falling pen task. Based on the results, we further propose a\nfull streaming inference framework for real-time robot control of VLA. Code is\navailable at https://github.com/Dexmal/realtime-vla.\n","authors":["Yunchao Ma","Yizhuang Zhou","Yunhuan Yang","Tiancai Wang","Haoqiang Fan"],"pdf_url":"https://arxiv.org/pdf/2510.26742v1.pdf","comment":"Code is available at https://github.com/Dexmal/realtime-vla"},{"id":"http://arxiv.org/abs/2510.26670v1","updated":"2025-10-30T16:41:58Z","published":"2025-10-30T16:41:58Z","title":"Hybrid Consistency Policy: Decoupling Multi-Modal Diversity and\n  Real-Time Efficiency in Robotic Manipulation","summary":"  In visuomotor policy learning, diffusion-based imitation learning has become\nwidely adopted for its ability to capture diverse behaviors. However,\napproaches built on ordinary and stochastic denoising processes struggle to\njointly achieve fast sampling and strong multi-modality. To address these\nchallenges, we propose the Hybrid Consistency Policy (HCP). HCP runs a short\nstochastic prefix up to an adaptive switch time, and then applies a one-step\nconsistency jump to produce the final action. To align this one-jump\ngeneration, HCP performs time-varying consistency distillation that combines a\ntrajectory-consistency objective to keep neighboring predictions coherent and a\ndenoising-matching objective to improve local fidelity. In both simulation and\non a real robot, HCP with 25 SDE steps plus one jump approaches the 80-step\nDDPM teacher in accuracy and mode coverage while significantly reducing\nlatency. These results show that multi-modality does not require slow\ninference, and a switch time decouples mode retention from speed. It yields a\npractical accuracy efficiency trade-off for robot policies.\n","authors":["Qianyou Zhao","Yuliang Shen","Xuanran Zhai","Ce Hao","Duidi Wu","Jin Qi","Jie Hu","Qiaojun Yu"],"pdf_url":"https://arxiv.org/pdf/2510.26670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.19816v2","updated":"2025-10-30T16:38:19Z","published":"2025-06-24T17:30:27Z","title":"CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame\n  Vision-Language-Action Modeling","summary":"  Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong performance in robotic\nmanipulation. However, these models remain constrained by the single-frame\nimage paradigm and fail to fully leverage the temporal information offered by\nmulti-frame histories, as directly feeding multiple frames into VLM backbones\nincurs substantial computational overhead and inference latency. We propose\nCronusVLA, a unified framework that extends single-frame VLA models to the\nmulti-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame\npretraining on large-scale embodied datasets with autoregressive prediction of\naction tokens, establishing an effective embodied vision-language foundation;\n(2) Multi-frame post-training, which adapts the prediction of the\nvision-language backbone from discrete tokens to learnable features, and\naggregates historical information via feature chunking. CronusVLA effectively\naddresses the existing challenges of multi-frame modeling while enhancing\nperformance and observational robustness. To evaluate the robustness under\ntemporal and spatial disturbances, we introduce SimplerEnv-OR, a novel\nbenchmark featuring 24 types of observational disturbances and 120 severity\nlevels. Experiments across three embodiments in simulated and real-world\nenvironments demonstrate that CronusVLA achieves leading performance and\nsuperior robustness, with a 70.9% success rate on SimplerEnv, a 26.8%\nimprovement over OpenVLA on LIBERO, and the highest robustness score on\nSimplerEnv-OR. These results highlight the potential of efficient multi-frame\nadaptation in VLA models for more powerful and robust real-world deployment.\n","authors":["Hao Li","Shuai Yang","Yilun Chen","Xinyi Chen","Xiaoda Yang","Yang Tian","Hanqing Wang","Tai Wang","Dahua Lin","Feng Zhao","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2506.19816v2.pdf","comment":"39 pages, 24 figures"},{"id":"http://arxiv.org/abs/2510.26656v1","updated":"2025-10-30T16:23:46Z","published":"2025-10-30T16:23:46Z","title":"Heuristic Adaptation of Potentially Misspecified Domain Support for\n  Likelihood-Free Inference in Stochastic Dynamical Systems","summary":"  In robotics, likelihood-free inference (LFI) can provide the domain\ndistribution that adapts a learnt agent in a parametric set of deployment\nconditions. LFI assumes an arbitrary support for sampling, which remains\nconstant as the initial generic prior is iteratively refined to more\ndescriptive posteriors. However, a potentially misspecified support can lead to\nsuboptimal, yet falsely certain, posteriors. To address this issue, we propose\nthree heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the\nposterior mode shift over inference steps in its own way and, when integrated\ninto an LFI step, adapts the support alongside posterior inference. We first\nexpose the support misspecification issue and evaluate our heuristics using\nstochastic dynamical benchmarks. We then evaluate the impact of heuristic\nsupport adaptation on parameter inference and policy learning for a dynamic\ndeformable linear object (DLO) manipulation task. Inference results in a finer\nlength and stiffness classification for a parametric set of DLOs. When the\nresulting posteriors are used as domain distributions for sim-based policy\nlearning, they lead to more robust object-centric agent performance.\n","authors":["Georgios Kamaras","Craig Innes","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2510.26656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26646v1","updated":"2025-10-30T16:12:01Z","published":"2025-10-30T16:12:01Z","title":"Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in\n  Dynamic Environments","summary":"  This paper presents a hierarchical path-planning and control framework that\ncombines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with\na low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller\nfor continuous actuation. The high-level module selects behaviors and\nsub-goals; the low-level module executes smooth velocity commands. We design a\npractical reward shaping scheme (direction, distance, obstacle avoidance,\naction smoothness, collision penalty, time penalty, and progress), together\nwith a LiDAR-based safety gate that prevents unsafe motions. The system is\nimplemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,\nincluding success rate, collision rate, path efficiency, and re-planning\nefficiency, in dynamic and partially observable environments. Experiments show\nimproved success rate and sample efficiency over single-algorithm baselines\n(DQN or TD3 alone) and rule-based planners, with better generalization to\nunseen obstacle configurations and reduced abrupt control changes. Code and\nevaluation scripts are available at the project repository.\n","authors":["Xiaoyi He","Danggui Chen","Zhenshuo Zhang","Zimeng Bai"],"pdf_url":"https://arxiv.org/pdf/2510.26646v1.pdf","comment":"6 pages, 5 figures; ROS+Gazebo (TurtleBot3) implementation;\n  evaluation with PathBench metrics; code (primary):\n  https://github.com/MayaCHEN-github/HierarchicalRL-robot-navigation; mirror\n  (for reproducibility): https://github.com/ShowyHe/DRL-robot-navigation"},{"id":"http://arxiv.org/abs/2510.26638v1","updated":"2025-10-30T16:06:23Z","published":"2025-10-30T16:06:23Z","title":"REALMS2 -- Resilient Exploration And Lunar Mapping System 2 -- A\n  Comprehensive Approach","summary":"  The European Space Agency (ESA) and the European Space Resources Innovation\nCentre (ESRIC) created the Space Resources Challenge to invite researchers and\ncompanies to propose innovative solutions for Multi-Robot Systems (MRS) space\nprospection. This paper proposes the Resilient Exploration And Lunar Mapping\nSystem 2 (REALMS2), a MRS framework for planetary prospection and mapping.\nBased on Robot Operating System version 2 (ROS 2) and enhanced with Visual\nSimultaneous Localisation And Mapping (vSLAM) for map generation, REALMS2 uses\na mesh network for a robust ad hoc network. A single graphical user interface\n(GUI) controls all the rovers, providing a simple overview of the robotic\nmission. This system is designed for heterogeneous multi-robot exploratory\nmissions, tackling the challenges presented by extraterrestrial environments.\nREALMS2 was used during the second field test of the ESA-ESRIC Challenge and\nallowed to map around 60% of the area, using three homogeneous rovers while\nhandling communication delays and blackouts.\n","authors":["Dave van der Meer","Loïck P. Chovet","Gabriel M. Garcia","Abhishek Bera","Miguel A. Olivares-Mendez"],"pdf_url":"https://arxiv.org/pdf/2510.26638v1.pdf","comment":"8 Pages, 8 Figures, Submitted and Accepted to IROS 2025"},{"id":"http://arxiv.org/abs/2510.26623v1","updated":"2025-10-30T15:50:36Z","published":"2025-10-30T15:50:36Z","title":"A Sliding-Window Filter for Online Continuous-Time Continuum Robot State\n  Estimation","summary":"  Stochastic state estimation methods for continuum robots (CRs) often struggle\nto balance accuracy and computational efficiency. While several recent works\nhave explored sliding-window formulations for CRs, these methods are limited to\nsimplified, discrete-time approximations and do not provide stochastic\nrepresentations. In contrast, current stochastic filter methods must run at the\nspeed of measurements, limiting their full potential. Recent works in\ncontinuous-time estimation techniques for CRs show a principled approach to\naddressing this runtime constraint, but are currently restricted to offline\noperation. In this work, we present a sliding-window filter (SWF) for\ncontinuous-time state estimation of CRs that improves upon the accuracy of a\nfilter approach while enabling continuous-time methods to operate online, all\nwhile running at faster-than-real-time speeds. This represents the first\nstochastic SWF specifically designed for CRs, providing a promising direction\nfor future research in this area.\n","authors":["Spencer Teetaert","Sven Lilge","Jessica Burgner-Kahrs","Timothy D. Barfoot"],"pdf_url":"https://arxiv.org/pdf/2510.26623v1.pdf","comment":"8 pages, 6 figures. Submitted to IEEE-RAS International Conference on\n  Soft Robotics 2026"},{"id":"http://arxiv.org/abs/2510.26614v1","updated":"2025-10-30T15:40:34Z","published":"2025-10-30T15:40:34Z","title":"Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event\n  Cameras","summary":"  We propose tokenization of events and present a tokenizer, Spiking Patches,\nspecifically designed for event cameras. Given a stream of asynchronous and\nspatially sparse events, our goal is to discover an event representation that\npreserves these properties. Prior works have represented events as frames or as\nvoxels. However, while these representations yield high accuracy, both frames\nand voxels are synchronous and decrease the spatial sparsity. Spiking Patches\ngives the means to preserve the unique properties of event cameras and we show\nin our experiments that this comes without sacrificing accuracy. We evaluate\nour tokenizer using a GNN, PCN, and a Transformer on gesture recognition and\nobject detection. Tokens from Spiking Patches yield inference times that are up\nto 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We\nachieve this while matching their accuracy and even surpassing in some cases\nwith absolute improvements up to 3.8 for gesture recognition and up to 1.4 for\nobject detection. Thus, tokenization constitutes a novel direction in\nevent-based vision and marks a step towards methods that preserve the\nproperties of event cameras.\n","authors":["Christoffer Koo Øhrstrøm","Ronja Güldenring","Lazaros Nalpantidis"],"pdf_url":"https://arxiv.org/pdf/2510.26614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26588v1","updated":"2025-10-30T15:14:18Z","published":"2025-10-30T15:14:18Z","title":"FLYINGTRUST: A Benchmark for Quadrotor Navigation Across Scenarios and\n  Vehicles","summary":"  Visual navigation algorithms for quadrotors often exhibit a large variation\nin performance when transferred across different vehicle platforms and scene\ngeometries, which increases the cost and risk of field deployment. To support\nsystematic early-stage evaluation, we introduce FLYINGTRUST, a high-fidelity,\nconfigurable benchmarking framework that measures how platform kinodynamics and\nscenario structure jointly affect navigation robustness. FLYINGTRUST models\nvehicle capability with two compact, physically interpretable indicators:\nmaximum thrust-to-weight ratio and axis-wise maximum angular acceleration. The\nbenchmark pairs a diverse scenario library with a heterogeneous set of real and\nvirtual platforms and prescribes a standardized evaluation protocol together\nwith a composite scoring method that balances scenario importance, platform\nimportance and performance stability. We use FLYINGTRUST to compare\nrepresentative optimization-based and learning-based navigation approaches\nunder identical conditions, performing repeated trials per platform-scenario\ncombination and reporting uncertainty-aware metrics. The results reveal\nsystematic patterns: navigation success depends predictably on platform\ncapability and scene geometry, and different algorithms exhibit distinct\npreferences and failure modes across the evaluated conditions. These\nobservations highlight the practical necessity of incorporating both platform\ncapability and scenario structure into algorithm design, evaluation, and\nselection, and they motivate future work on methods that remain robust across\ndiverse platforms and scenarios.\n","authors":["Gang Li","Chunlei Zhai","Teng Wang","Shaun Li","Shangsong Jiang","Xiangwei Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.26588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26571v1","updated":"2025-10-30T15:00:08Z","published":"2025-10-30T15:00:08Z","title":"Proxemics and Permeability of the Pedestrian Group","summary":"  People tend to walk in groups, and interactions with those groups have a\nsignificant impact on crowd behavior and pedestrian traffic dynamics. Social\nnorms can be seen as unwritten rules regulating people interactions in social\nsettings. This article studies people interactions with groups and the\nemergence of group proxemics. Group zones, zone occupancy counts and people\nclearance from the group are studied using naturalistic data. Analysis indicate\npotential presence of three different zones in addition to the public zone.\nPeople tend to remain in the public zone and only progressively get closer to\ngroups, and those closer approaches happen in a low frequency and for brief\nperiods of time.\n","authors":["Saleh Albeaik","Faisal Alsallum","Mohamad Alrished"],"pdf_url":"https://arxiv.org/pdf/2510.26571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26551v1","updated":"2025-10-30T14:44:24Z","published":"2025-10-30T14:44:24Z","title":"Adaptive Inverse Kinematics Framework for Learning Variable-Length Tool\n  Manipulation in Robotics","summary":"  Conventional robots possess a limited understanding of their kinematics and\nare confined to preprogrammed tasks, hindering their ability to leverage tools\nefficiently. Driven by the essential components of tool usage - grasping the\ndesired outcome, selecting the most suitable tool, determining optimal tool\norientation, and executing precise manipulations - we introduce a pioneering\nframework. Our novel approach expands the capabilities of the robot's inverse\nkinematics solver, empowering it to acquire a sequential repertoire of actions\nusing tools of varying lengths. By integrating a simulation-learned action\ntrajectory with the tool, we showcase the practicality of transferring acquired\nskills from simulation to real-world scenarios through comprehensive\nexperimentation. Remarkably, our extended inverse kinematics solver\ndemonstrates an impressive error rate of less than 1 cm. Furthermore, our\ntrained policy achieves a mean error of 8 cm in simulation. Noteworthy, our\nmodel achieves virtually indistinguishable performance when employing two\ndistinct tools of different lengths. This research provides an indication of\npotential advances in the exploration of all four fundamental aspects of tool\nusage, enabling robots to master the intricate art of tool manipulation across\ndiverse tasks.\n","authors":["Prathamesh Kothavale","Sravani Boddepalli"],"pdf_url":"https://arxiv.org/pdf/2510.26551v1.pdf","comment":"10 pages, 5 figures. Demonstrates a reinforcement learning framework\n  for adaptive tool manipulation with variable-length extensions"},{"id":"http://arxiv.org/abs/2510.26536v1","updated":"2025-10-30T14:26:40Z","published":"2025-10-30T14:26:40Z","title":"RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable,\n  and Robust Multi-Robot Collaboration","summary":"  The proliferation of collaborative robots across diverse tasks and\nembodiments presents a central challenge: achieving lifelong adaptability,\nscalable coordination, and robust scheduling in multi-agent systems. Existing\napproaches, from vision-language-action (VLA) models to hierarchical\nframeworks, fall short due to their reliance on limited or dividual-agent\nmemory. This fundamentally constrains their ability to learn over long\nhorizons, scale to heterogeneous teams, or recover from failures, highlighting\nthe need for a unified memory representation. To address these limitations, we\nintroduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable,\nand robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel\nSpatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene\ngeometry, temporal event history, and embodiment profiles into a shared\nrepresentation. This memory-centric design is integrated into a\nbrain-cerebellum framework, where a high-level brain model performs global\nplanning by retrieving and updating STEM, while low-level controllers execute\nactions locally. This closed loop between cognition, memory, and execution\nenables dynamic task allocation, fault-tolerant collaboration, and consistent\nstate synchronization. We conduct extensive experiments spanning complex\ncoordination tasks in restaurants, supermarkets, and households. Our results\ndemonstrate that RoboOS-NeXT achieves superior performance across heterogeneous\nembodiments, validating its effectiveness in enabling lifelong, scalable, and\nrobust multi-robot collaboration. Project website:\nhttps://flagopen.github.io/RoboOS/\n","authors":["Huajie Tan","Cheng Chi","Xiansheng Chen","Yuheng Ji","Zhongxia Zhao","Xiaoshuai Hao","Yaoxu Lyu","Mingyu Cao","Junkai Zhao","Huaihai Lyu","Enshen Zhou","Ning Chen","Yankai Fu","Cheng Peng","Wei Guo","Dong Liang","Zhuo Chen","Mengsi Lyu","Chenrui He","Yulong Ao","Yonghua Lin","Pengwei Wang","Zhongyuan Wang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26531v1","updated":"2025-10-30T14:22:18Z","published":"2025-10-30T14:22:18Z","title":"Efficient Collision-Avoidance Constraints for Ellipsoidal Obstacles in\n  Optimal Control: Application to Path-Following MPC and UAVs","summary":"  This article proposes a modular optimal control framework for local\nthree-dimensional ellipsoidal obstacle avoidance, exemplarily applied to model\npredictive path-following control. Static as well as moving obstacles are\nconsidered. Central to the approach is a computationally efficient and\ncontinuously differentiable condition for detecting collisions with ellipsoidal\nobstacles. A novel two-stage optimization approach mitigates numerical issues\narising from the structure of the resulting optimal control problem. The\neffectiveness of the approach is demonstrated through simulations and\nreal-world experiments with the Crazyflie quadrotor. This represents the first\nhardware demonstration of an MPC controller of this kind for UAVs in a\nthree-dimensional task.\n","authors":["David Leprich","Mario Rosenfelder","Markus Herrmann-Wicklmayr","Kathrin Flaßkamp","Peter Eberhard","Henrik Ebel"],"pdf_url":"https://arxiv.org/pdf/2510.26531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18802v2","updated":"2025-10-30T13:44:30Z","published":"2025-01-30T23:38:52Z","title":"Agile and Cooperative Aerial Manipulation of a Cable-Suspended Load","summary":"  Quadrotors can carry slung loads to hard-to-reach locations at high speed.\nSince a single quadrotor has limited payload capacities, using a team of\nquadrotors to collaboratively manipulate a heavy object is a scalable and\npromising solution. However, existing control algorithms for multi-lifting\nsystems only enable low-speed and low-acceleration operations due to the\ncomplex dynamic coupling between quadrotors and the load, limiting their use in\ntime-critical missions such as search and rescue. In this work, we present a\nsolution to significantly enhance the agility of cable-suspended multi-lifting\nsystems. Unlike traditional cascaded solutions, we introduce a trajectory-based\nframework that solves the whole-body kinodynamic motion planning problem\nonline, accounting for the dynamic coupling effects and constraints between the\nquadrotors and the load. The planned trajectory is provided to the quadrotors\nas a reference in a receding-horizon fashion and is tracked by an onboard\ncontroller that observes and compensates for the cable tension. Real-world\nexperiments demonstrate that our framework can achieve at least eight times\ngreater acceleration than state-of-the-art methods to follow agile\ntrajectories. Our method can even perform complex maneuvers such as flying\nthrough narrow passages at high speed. Additionally, it exhibits high\nrobustness against load uncertainties and does not require adding any sensors\nto the load, demonstrating strong practicality.\n","authors":["Sihao Sun","Xuerui Wang","Dario Sanalitro","Antonio Franchi","Marco Tognon","Javier Alonso-Mora"],"pdf_url":"https://arxiv.org/pdf/2501.18802v2.pdf","comment":"38 pages, 11 figures"},{"id":"http://arxiv.org/abs/2510.26406v1","updated":"2025-10-30T11:53:08Z","published":"2025-10-30T11:53:08Z","title":"Human-in-the-loop Online Rejection Sampling for Robotic Manipulation","summary":"  Reinforcement learning (RL) is widely used to produce robust robotic\nmanipulation policies, but fine-tuning vision-language-action (VLA) models with\nRL can be unstable due to inaccurate value estimates and sparse supervision at\nintermediate steps. In contrast, imitation learning (IL) is easy to train but\noften underperforms due to its offline nature. In this paper, we propose\nHi-ORS, a simple yet effective post-training method that utilizes rejection\nsampling to achieve both training stability and high robustness. Hi-ORS\nstabilizes value estimation by filtering out negatively rewarded samples during\nonline fine-tuning, and adopts a reward-weighted supervised training objective\nto provide dense intermediate-step supervision. For systematic study, we\ndevelop an asynchronous inference-training framework that supports flexible\nonline human-in-the-loop corrections, which serve as explicit guidance for\nlearning error-recovery behaviors. Across three real-world tasks and two\nembodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich\nmanipulation in just 1.5 hours of real-world training, outperforming RL and IL\nbaselines by a substantial margin in both effectiveness and efficiency.\nNotably, the fine-tuned policy exhibits strong test-time scalability by\nreliably executing complex error-recovery behaviors to achieve better\nperformance.\n","authors":["Guanxing Lu","Rui Zhao","Haitao Lin","He Zhang","Yansong Tang"],"pdf_url":"https://arxiv.org/pdf/2510.26406v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2510.26369v1","updated":"2025-10-30T11:14:17Z","published":"2025-10-30T11:14:17Z","title":"CorVS: Person Identification via Video Trajectory-Sensor Correspondence\n  in a Real-World Warehouse","summary":"  Worker location data is key to higher productivity in industrial sites.\nCameras are a promising tool for localization in logistics warehouses since\nthey also offer valuable environmental contexts such as package status.\nHowever, identifying individuals with only visual data is often impractical.\nAccordingly, several prior studies identified people in videos by comparing\ntheir trajectories and wearable sensor measurements. While this approach has\nadvantages such as independence from appearance, the existing methods may break\ndown under real-world conditions. To overcome this challenge, we propose CorVS,\na novel data-driven person identification method based on correspondence\nbetween visual tracking trajectories and sensor measurements. Firstly, our deep\nlearning model predicts correspondence probabilities and reliabilities for\nevery pair of a trajectory and sensor measurements. Secondly, our algorithm\nmatches the trajectories and sensor measurements over time using the predicted\nprobabilities and reliabilities. We developed a dataset with actual warehouse\noperations and demonstrated the method's effectiveness for real-world\napplications.\n","authors":["Kazuma Kano","Yuki Mori","Shin Katayama","Kenta Urano","Takuro Yonezawa","Nobuo Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2510.26369v1.pdf","comment":"7 pages, 3 figures, accepted to IPIN 2025"},{"id":"http://arxiv.org/abs/2510.26363v1","updated":"2025-10-30T11:10:15Z","published":"2025-10-30T11:10:15Z","title":"Towards Reinforcement Learning Based Log Loading Automation","summary":"  Forestry forwarders play a central role in mechanized timber harvesting by\npicking up and moving logs from the felling site to a processing area or a\nsecondary transport vehicle. Forwarder operation is challenging and physically\nand mentally exhausting for the operator who must control the machine in remote\nareas for prolonged periods of time. Therefore, even partial automation of the\nprocess may reduce stress on the operator. This study focuses on continuing\nprevious research efforts in application of reinforcement learning agents in\nautomating log handling process, extending the task from grasping which was\nstudied in previous research to full log loading operation. The resulting agent\nwill be capable to automate a full loading procedure from locating and\ngrappling to transporting and delivering the log to a forestry forwarder bed.\nTo train the agent, a trailer type forestry forwarder simulation model in\nNVIDIA's Isaac Gym and a virtual environment for a typical log loading scenario\nwere developed. With reinforcement learning agents and a curriculum learning\napproach, the trained agent may be a stepping stone towards application of\nreinforcement learning agents in automation of the forestry forwarder. The\nagent learnt grasping a log in a random position from grapple's random position\nand transport it to the bed with 94% success rate of the best performing agent.\n","authors":["Ilya Kurinov","Miroslav Ivanov","Grzegorz Orzechowski","Aki Mikkola"],"pdf_url":"https://arxiv.org/pdf/2510.26363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26362v1","updated":"2025-10-30T11:09:30Z","published":"2025-10-30T11:09:30Z","title":"Cooperative Task Spaces for Multi-Arm Manipulation Control based on\n  Similarity Transformations","summary":"  Many tasks in human environments require collaborative behavior between\nmultiple kinematic chains, either to provide additional support for carrying\nbig and bulky objects or to enable the dexterity that is required for in-hand\nmanipulation. Since these complex systems often have a very high number of\ndegrees of freedom coordinating their movements is notoriously difficult to\nmodel. In this article, we present the derivation of the theoretical\nfoundations for cooperative task spaces of multi-arm robotic systems based on\ngeometric primitives defined using conformal geometric algebra. Based on the\nsimilarity transformations of these cooperative geometric primitives, we derive\nan abstraction of complex robotic systems that enables representing these\nsystems in a way that directly corresponds to single-arm systems. By deriving\nthe associated analytic and geometric Jacobian matrices, we then show the\nstraightforward integration of our approach into classical control techniques\nrooted in operational space control. We demonstrate this using bimanual\nmanipulators, humanoids and multi-fingered hands in optimal control experiments\nfor reaching desired geometric primitives and in teleoperation experiments\nusing differential kinematics control. We then discuss how the geometric\nprimitives naturally embed nullspace structures into the controllers that can\nbe exploited for introducing secondary control objectives. This work,\nrepresents the theoretical foundations of this cooperative manipulation control\nframework, and thus the experiments are presented in an abstract way, while\ngiving pointers towards potential future applications.\n","authors":["Tobias Löw","Cem Bilaloglu","Sylvain Calinon"],"pdf_url":"https://arxiv.org/pdf/2510.26362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26358v1","updated":"2025-10-30T11:08:23Z","published":"2025-10-30T11:08:23Z","title":"AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian\n  Splatting SLAM","summary":"  Autonomous robots in orchards require real-time 3D scene understanding\ndespite repetitive row geometry, seasonal appearance changes, and wind-driven\nfoliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that\ncouples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian\nSplatting (3DGS) rendering. Batch rasterization across complementary viewpoints\nrecovers orchard structure under occlusions, while a unified gradient-driven\nmap lifecycle executed between keyframes preserves fine details and bounds\nmemory. Pose refinement is guided by a probabilistic LiDAR-based depth\nconsistency term, back-propagated through the camera projection to tighten\ngeometry-appearance coupling. We deploy the system on a field platform in apple\nand pear orchards across dormancy, flowering, and harvesting, using a\nstandardized trajectory protocol that evaluates both training-view and\nnovel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons\nand sites, AgriGS-SLAM delivers sharper, more stable reconstructions and\nsteadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while\nmaintaining real-time performance on-tractor. While demonstrated in orchard\nmonitoring, the approach can be applied to other outdoor domains requiring\nrobust multimodal perception.\n","authors":["Mirko Usuelli","David Rapado-Rincon","Gert Kootstra","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2510.26358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.03024v2","updated":"2025-10-30T09:10:23Z","published":"2025-08-05T03:05:58Z","title":"LiGen: GAN-Augmented Spectral Fingerprinting for Indoor Positioning","summary":"  Accurate and robust indoor localization is critical for smart building\napplications, yet existing Wi-Fi-based systems are often vulnerable to\nenvironmental conditions. This work presents a novel indoor localization\nsystem, called LiGen, that leverages the spectral intensity patterns of ambient\nlight as fingerprints, offering a more stable and infrastructure-free\nalternative to radio signals. To address the limited spectral data, we design a\ndata augmentation framework based on generative adversarial networks (GANs),\nfeaturing two variants: PointGAN, which generates fingerprints conditioned on\ncoordinates, and FreeGAN, which uses a weak localization model to label\nunconditioned samples. Our positioning model, leveraging a Multi-Layer\nPerceptron (MLP) architecture to train on synthesized data, achieves\nsubmeter-level accuracy, outperforming Wi-Fi-based baselines by over 50\\%.\nLiGen also demonstrates strong robustness in cluttered environments. To the\nbest of our knowledge, this is the first system to combine spectral\nfingerprints with GAN-based data augmentation for indoor localization.\n","authors":["Jie Lin","Hsun-Yu Lee","Ho-Ming Li","Fang-Jing Wu"],"pdf_url":"https://arxiv.org/pdf/2508.03024v2.pdf","comment":"6 pages, 10 figures"},{"id":"http://arxiv.org/abs/2510.26280v1","updated":"2025-10-30T09:06:26Z","published":"2025-10-30T09:06:26Z","title":"Thor: Towards Human-Level Whole-Body Reactions for Intense Contact-Rich\n  Environments","summary":"  Humanoids hold great potential for service, industrial, and rescue\napplications, in which robots must sustain whole-body stability while\nperforming intense, contact-rich interactions with the environment. However,\nenabling humanoids to generate human-like, adaptive responses under such\nconditions remains a major challenge. To address this, we propose Thor, a\nhumanoid framework for human-level whole-body reactions in contact-rich\nenvironments. Based on the robot's force analysis, we design a force-adaptive\ntorso-tilt (FAT2) reward function to encourage humanoids to exhibit human-like\nresponses during force-interaction tasks. To mitigate the high-dimensional\nchallenges of humanoid control, Thor introduces a reinforcement learning\narchitecture that decouples the upper body, waist, and lower body. Each\ncomponent shares global observations of the whole body and jointly updates its\nparameters. Finally, we deploy Thor on the Unitree G1, and it substantially\noutperforms baselines in force-interaction tasks. Specifically, the robot\nachieves a peak pulling force of 167.7 N (approximately 48% of the G1's body\nweight) when moving backward and 145.5 N when moving forward, representing\nimprovements of 68.9% and 74.7%, respectively, compared with the\nbest-performing baseline. Moreover, Thor is capable of pulling a loaded rack\n(130 N) and opening a fire door with one hand (60 N). These results highlight\nThor's effectiveness in enhancing humanoid force-interaction capabilities.\n","authors":["Gangyang Li","Qing Shi","Youhao Hu","Jincheng Hu","Zhongyuan Wang","Xinlong Wang","Shaqi Luo"],"pdf_url":"https://arxiv.org/pdf/2510.26280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20685v2","updated":"2025-10-30T08:58:18Z","published":"2025-10-23T15:57:43Z","title":"C-NAV: Towards Self-Evolving Continual Object Navigation in Open World","summary":"  Embodied agents are expected to perform object navigation in dynamic,\nopen-world environments. However, existing approaches typically rely on static\ntrajectories and a fixed set of object categories during training, overlooking\nthe real-world requirement for continual adaptation to evolving scenarios. To\nfacilitate related studies, we introduce the continual object navigation\nbenchmark, which requires agents to acquire navigation skills for new object\ncategories while avoiding catastrophic forgetting of previously learned\nknowledge. To tackle this challenge, we propose C-Nav, a continual visual\nnavigation framework that integrates two key innovations: (1) A dual-path\nanti-forgetting mechanism, which comprises feature distillation that aligns\nmulti-modal inputs into a consistent representation space to ensure\nrepresentation consistency, and feature replay that retains temporal features\nwithin the action decoder to ensure policy consistency. (2) An adaptive\nsampling strategy that selects diverse and informative experiences, thereby\nreducing redundancy and minimizing memory overhead. Extensive experiments\nacross multiple model architectures demonstrate that C-Nav consistently\noutperforms existing approaches, achieving superior performance even compared\nto baselines with full trajectory retention, while significantly lowering\nmemory requirements. The code will be publicly available at\nhttps://bigtree765.github.io/C-Nav-project.\n","authors":["Ming-Ming Yu","Fei Zhu","Wenzhuo Liu","Yirong Yang","Qunbo Wang","Wenjun Wu","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2510.20685v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2503.16275v2","updated":"2025-10-30T08:15:52Z","published":"2025-03-20T16:05:35Z","title":"Loop Closure from Two Views: Revisiting PGO for Scalable Trajectory\n  Estimation through Monocular Priors","summary":"  (Visual) Simultaneous Localization and Mapping (SLAM) remains a fundamental\nchallenge in enabling autonomous systems to navigate and understand large-scale\nenvironments. Traditional SLAM approaches struggle to balance efficiency and\naccuracy, particularly in large-scale settings where extensive computational\nresources are required for scene reconstruction and Bundle Adjustment (BA).\nHowever, this scene reconstruction, in the form of sparse pointclouds of visual\nlandmarks, is often only used within the SLAM system because navigation and\nplanning methods require different map representations. In this work, we\ntherefore investigate a more scalable Visual SLAM (VSLAM) approach without\nreconstruction, mainly based on approaches for two-view loop closures. By\nrestricting the map to a sparse keyframed pose graph without dense geometry\nrepresentations, our `2GO' system achieves efficient optimization with\ncompetitive absolute trajectory accuracy. In particular, we find that recent\nadvancements in image matching and monocular depth priors enable very accurate\ntrajectory optimization without BA. We conduct extensive experiments on diverse\ndatasets, including large-scale scenarios, and provide a detailed analysis of\nthe trade-offs between runtime, accuracy, and map size. Our results demonstrate\nthat this streamlined approach supports real-time performance, scales well in\nmap size and trajectory duration, and effectively broadens the capabilities of\nVSLAM for long-duration deployments to large environments.\n","authors":["Tian Yi Lim","Boyang Sun","Marc Pollefeys","Hermann Blum"],"pdf_url":"https://arxiv.org/pdf/2503.16275v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26236v1","updated":"2025-10-30T08:13:12Z","published":"2025-10-30T08:13:12Z","title":"PHUMA: Physically-Grounded Humanoid Locomotion Dataset","summary":"  Motion imitation is a promising approach for humanoid locomotion, enabling\nagents to acquire humanlike behaviors. Existing methods typically rely on\nhigh-quality motion capture datasets such as AMASS, but these are scarce and\nexpensive, limiting scalability and diversity. Recent studies attempt to scale\ndata collection by converting large-scale internet videos, exemplified by\nHumanoid-X. However, they often introduce physical artifacts such as floating,\npenetration, and foot skating, which hinder stable imitation. In response, we\nintroduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that\nleverages human video at scale, while addressing physical artifacts through\ncareful data curation and physics-constrained retargeting. PHUMA enforces joint\nlimits, ensures ground contact, and eliminates foot skating, producing motions\nthat are both large-scale and physically reliable. We evaluated PHUMA in two\nsets of conditions: (i) imitation of unseen motion from self-recorded test\nvideos and (ii) path following with pelvis-only guidance. In both cases,\nPHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant\ngains in imitating diverse motions. The code is available at\nhttps://davian-robotics.github.io/PHUMA.\n","authors":["Kyungmin Lee","Sibeen Kim","Minho Park","Hyunseung Kim","Dongyoon Hwang","Hojoon Lee","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2510.26236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13904v3","updated":"2025-10-30T07:17:31Z","published":"2025-05-20T04:10:50Z","title":"Learning to Insert for Constructive Neural Vehicle Routing Solver","summary":"  Neural Combinatorial Optimisation (NCO) is a promising learning-based\napproach for solving Vehicle Routing Problems (VRPs) without extensive manual\ndesign. While existing constructive NCO methods typically follow an\nappending-based paradigm that sequentially adds unvisited nodes to partial\nsolutions, this rigid approach often leads to suboptimal results. To overcome\nthis limitation, we explore the idea of insertion-based paradigm and propose\nLearning to Construct with Insertion-based Paradigm (L2C-Insert), a novel\nlearning-based method for constructive NCO. Unlike traditional approaches,\nL2C-Insert builds solutions by strategically inserting unvisited nodes at any\nvalid position in the current partial solution, which can significantly enhance\nthe flexibility and solution quality. The proposed framework introduces three\nkey components: a novel model architecture for precise insertion position\nprediction, an efficient training scheme for model optimization, and an\nadvanced inference technique that fully exploits the insertion paradigm's\nflexibility. Extensive experiments on both synthetic and real-world instances\nof the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) demonstrate that L2C-Insert consistently achieves superior\nperformance across various problem sizes.\n","authors":["Fu Luo","Xi Lin","Mengyuan Zhong","Fei Liu","Zhenkun Wang","Jianyong Sun","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13904v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.09937v2","updated":"2025-10-30T07:02:35Z","published":"2025-06-11T16:59:13Z","title":"SAFE: Multitask Failure Detection for Vision-Language-Action Models","summary":"  While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out of the box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while generalist VLAs require the detector to generalize\nand detect failures also in unseen tasks and novel environments. In this paper,\nwe introduce the multitask failure detection problem and propose SAFE, a\nfailure detector for generalist robot policies such as VLAs. We analyze the VLA\nfeature space and find that VLAs have sufficient high-level knowledge about\ntask success and failure, which is generic across different tasks. Based on\nthis insight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, $\\pi_0$,\nand $\\pi_0$-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults and code can be found at the project webpage:\nhttps://vla-safe.github.io/\n","authors":["Qiao Gu","Yuanliang Ju","Shengxiang Sun","Igor Gilitschenski","Haruki Nishimura","Masha Itkina","Florian Shkurti"],"pdf_url":"https://arxiv.org/pdf/2506.09937v2.pdf","comment":"NeurIPS 2025 camera ready. Project Page: https://vla-safe.github.io/"},{"id":"http://arxiv.org/abs/2510.26170v1","updated":"2025-10-30T06:14:22Z","published":"2025-10-30T06:14:22Z","title":"Self-localization on a 3D map by fusing global and local features from a\n  monocular camera","summary":"  Self-localization on a 3D map by using an inexpensive monocular camera is\nrequired to realize autonomous driving. Self-localization based on a camera\noften uses a convolutional neural network (CNN) that can extract local features\nthat are calculated by nearby pixels. However, when dynamic obstacles, such as\npeople, are present, CNN does not work well. This study proposes a new method\ncombining CNN with Vision Transformer, which excels at extracting global\nfeatures that show the relationship of patches on whole image. Experimental\nresults showed that, compared to the state-of-the-art method (SOTA), the\naccuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times\nhigher than that without dynamic obstacles. Moreover, the self-localization\nerror of our method is 20.1% smaller than that of SOTA on public datasets.\nAdditionally, our robot using our method can localize itself with 7.51cm error\non average, which is more accurate than SOTA.\n","authors":["Satoshi Kikuch","Masaya Kato","Tsuyoshi Tasaki"],"pdf_url":"https://arxiv.org/pdf/2510.26170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00871v2","updated":"2025-10-30T05:04:19Z","published":"2025-06-01T07:18:47Z","title":"Towards Predicting Any Human Trajectory In Context","summary":"  Predicting accurate future trajectories of pedestrians is essential for\nautonomous systems but remains a challenging task due to the need for\nadaptability in different environments and domains. A common approach involves\ncollecting scenario-specific data and performing fine-tuning via\nbackpropagation. However, the need to fine-tune for each new scenario is often\nimpractical for deployment on edge devices. To address this challenge, we\nintroduce \\paper, an In-Context Learning (ICL) framework for pedestrian\ntrajectory prediction that enables adaptation without fine-tuning on the\nscenario-specific data at inference time without requiring weight updates. We\npropose a spatio-temporal similarity-based example selection (STES) method that\nselects relevant examples from previously observed trajectories within the same\nscene by identifying similar motion patterns at corresponding locations. To\nfurther refine this selection, we introduce prediction-guided example selection\n(PG-ES), which selects examples based on both the past trajectory and the\npredicted future trajectory, rather than relying solely on the past trajectory.\nThis approach allows the model to account for long-term dynamics when selecting\nexamples. Finally, instead of relying on small real-world datasets with limited\nscenario diversity, we train our model on a large-scale synthetic dataset to\nenhance its prediction ability by leveraging in-context examples. Extensive\nexperiments demonstrate that TrajICL achieves remarkable adaptation across both\nin-domain and cross-domain scenarios, outperforming even fine-tuned approaches\nacross multiple public benchmarks. Project Page:\nhttps://fujiry0.github.io/TrajICL-project-page/.\n","authors":["Ryo Fujii","Hideo Saito","Ryo Hachiuma"],"pdf_url":"https://arxiv.org/pdf/2506.00871v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26142v1","updated":"2025-10-30T04:53:08Z","published":"2025-10-30T04:53:08Z","title":"Adaptive Trajectory Refinement for Optimization-based Local Planning in\n  Narrow Passages","summary":"  Trajectory planning for mobile robots in cluttered environments remains a\nmajor challenge due to narrow passages, where conventional methods often fail\nor generate suboptimal paths. To address this issue, we propose the adaptive\ntrajectory refinement algorithm, which consists of two main stages. First, to\nensure safety at the path-segment level, a segment-wise conservative collision\ntest is applied, where risk-prone trajectory path segments are recursively\nsubdivided until collision risks are eliminated. Second, to guarantee\npose-level safety, pose correction based on penetration direction and line\nsearch is applied, ensuring that each pose in the trajectory is collision-free\nand maximally clear from obstacles. Simulation results demonstrate that the\nproposed method achieves up to 1.69x higher success rates and up to 3.79x\nfaster planning times than state-of-the-art approaches. Furthermore, real-world\nexperiments confirm that the robot can safely pass through narrow passages\nwhile maintaining rapid planning performance.\n","authors":["Hahjin Lee","Young J. Kim"],"pdf_url":"https://arxiv.org/pdf/2510.26142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26139v1","updated":"2025-10-30T04:51:02Z","published":"2025-10-30T04:51:02Z","title":"Kinodynamic Task and Motion Planning using VLM-guided and Interleaved\n  Sampling","summary":"  Task and Motion Planning (TAMP) integrates high-level task planning with\nlow-level motion feasibility, but existing methods are costly in long-horizon\nproblems due to excessive motion sampling. While LLMs provide commonsense\npriors, they lack 3D spatial reasoning and cannot ensure geometric or dynamic\nfeasibility. We propose a kinodynamic TAMP framework based on a hybrid state\ntree that uniformly represents symbolic and numeric states during planning,\nenabling task and motion decisions to be jointly decided. Kinodynamic\nconstraints embedded in the TAMP problem are verified by an off-the-shelf\nmotion planner and physics simulator, and a VLM guides exploring a TAMP\nsolution and backtracks the search based on visual rendering of the states.\nExperiments on the simulated domains and in the real world show 32.14% -\n1166.67% increased average success rates compared to traditional and LLM-based\nTAMP planners and reduced planning time on complex problems, with ablations\nfurther highlighting the benefits of VLM guidance.\n","authors":["Minseo Kwon","Young J. Kim"],"pdf_url":"https://arxiv.org/pdf/2510.26139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.13733v2","updated":"2025-10-30T04:42:24Z","published":"2025-09-17T06:36:41Z","title":"FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with\n  Hierarchical Multi-modal Scene Graph","summary":"  Visual-Language Navigation (VLN) is a fundamental challenge in robotic\nsystems, with broad applications for the deployment of embodied agents in\nreal-world environments. Despite recent advances, existing approaches are\nlimited in long-range spatial reasoning, often exhibiting low success rates and\nhigh inference latency, particularly in long-range navigation tasks. To address\nthese limitations, we propose FSR-VLN, a vision-language navigation system that\ncombines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow\nNavigation Reasoning (FSR). The HMSG provides a multi-modal map representation\nsupporting progressive retrieval, from coarse room-level localization to\nfine-grained goal view and object identification. Building on HMSG, FSR first\nperforms fast matching to efficiently select candidate rooms, views, and\nobjects, then applies VLM-driven refinement for final goal selection. We\nevaluated FSR-VLN across four comprehensive indoor datasets collected by\nhumanoid robots, utilizing 87 instructions that encompass a diverse range of\nobject categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all\ndatasets, measured by the retrieval success rate (RSR), while reducing the\nresponse time by 82% compared to VLM-based methods on tour videos by activating\nslow reasoning only when fast intuition fails. Furthermore, we integrate\nFSR-VLN with speech interaction, planning, and control modules on a Unitree-G1\nhumanoid robot, enabling natural language interaction and real-time navigation.\n","authors":["Xiaolin Zhou","Tingyang Xiao","Liu Liu","Yucheng Wang","Maiyue Chen","Xinrui Meng","Xinjie Wang","Wei Feng","Wei Sui","Zhizhong Su"],"pdf_url":"https://arxiv.org/pdf/2509.13733v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2510.26132v1","updated":"2025-10-30T04:40:58Z","published":"2025-10-30T04:40:58Z","title":"Embodied Intelligence for Advanced Bioinspired Microrobotics: Examples\n  and Insights","summary":"  The term embodied intelligence (EI) conveys the notion that body morphology,\nmaterial properties, interaction with the environment, and control strategies\ncan be purposefully integrated into the process of robotic design to generate\nintelligent behavior; in particular, locomotion and navigation. In this paper,\nwe discuss EI as a design principle for advanced microrobotics, with a\nparticular focus on co-design -- the simultaneous and interdependent\ndevelopment of physical structure and behavioral function. To illustrate the\ncontrast between EI-inspired systems and traditional architectures that\ndecouple sensing, computation, and actuation, we present and discuss a\ncollection of robots developed by the author and his team at the Autonomous\nMicrorobotic Systems Laboratory (AMSL). These robots exhibit intelligent\nbehavior that emerges from their structural dynamics and the physical\ninteraction between their components and with the environment. Platforms such\nas the Bee++, RoBeetle, SMALLBug, SMARTI, WaterStrider, VLEIBot+, and FRISSHBot\nexemplify how feedback loops, decision logics, sensing mechanisms, and smart\nactuation strategies can be embedded into the physical properties of the\nrobotic system itself. Along these lines, we contend that co-design is not only\na method for empirical optimization under constraints, but also an enabler of\nEI, offering a scalable and robust alternative to classical control for\nrobotics at the mm-to-cm-scale.\n","authors":["Nestor O. Perez-Arancibia"],"pdf_url":"https://arxiv.org/pdf/2510.26132v1.pdf","comment":"8 pages, 7 figures, accepted to ICAR 2025"},{"id":"http://arxiv.org/abs/2510.26131v1","updated":"2025-10-30T04:31:56Z","published":"2025-10-30T04:31:56Z","title":"Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM","summary":"  Attention models have recently emerged as a powerful approach, demonstrating\nsignificant progress in various fields. Visualization techniques, such as class\nactivation mapping, provide visual insights into the reasoning of convolutional\nneural networks (CNNs). Using network gradients, it is possible to identify\nregions where the network pays attention during image recognition tasks.\nFurthermore, these gradients can be combined with CNN features to localize more\ngeneralizable, task-specific attentive (salient) regions within scenes.\nHowever, explicit use of this gradient-based attention information integrated\ndirectly into CNN representations for semantic object understanding remains\nlimited. Such integration is particularly beneficial for visual tasks like\nsimultaneous localization and mapping (SLAM), where CNN representations\nenriched with spatially attentive object locations can enhance performance. In\nthis work, we propose utilizing task-specific network attention for RGB-D\nindoor SLAM. Specifically, we integrate layer-wise attention information\nderived from network gradients with CNN feature representations to improve\nframe association performance. Experimental results indicate improved\nperformance compared to baseline methods, particularly for large environments.\n","authors":["Ali Caglayan","Nevrez Imamoglu","Oguzhan Guclu","Ali Osman Serhatoglu","Ahmet Burak Can","Ryosuke Nakamura"],"pdf_url":"https://arxiv.org/pdf/2510.26131v1.pdf","comment":"double-column 5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2506.07127v3","updated":"2025-10-30T04:04:19Z","published":"2025-06-08T13:14:18Z","title":"Human-assisted Robotic Policy Refinement via Action Preference\n  Optimization","summary":"  Establishing a reliable and iteratively refined robotic system is essential\nfor deploying real-world applications. While Vision-Language-Action (VLA)\nmodels are widely recognized as the foundation model for such robotic\ndeployment, their reliance on offline expert demonstrations critically limits\ntheir capacity for post-deployment refinement. To mitigate this limitation, we\nintroduce Action Preference Optimization (APO), a method designed to refine VLA\nmodels by human-assisted preference alignment gathered through interaction with\nenvironments. This method begins with a human-robot collaboration framework for\nreliable failure correction and interaction trajectory collection through human\nintervention. However, directly leveraging these interaction trajectories for\npreference optimization is non-trivial due to the challenges of irreversible\nrobotic actions and token distribution mismatch. To solve this, APO proposes an\nadaptive reweighting algorithm with binary desirability signals derived from\ninteraction, empowering VLA models effectively suppress failure-prone actions\nwhile enhancing corrective action adaptation. Ultimately, APO equips VLA models\nwith the crucial capability to learn from failure, paving the way for their\niterative refinement and reliable deployment in dynamic environments. The\nexperiments conducted in simulation and real-world scenarios prove superior\ngeneralization and robustness of our human-assisted framework across a variety\nof manipulation tasks. We believe this work could bring insights for efficient\nand stable optimization of VLA models through human-robot collaboration. The\ncode and dataset are released at\nhttps://github.com/GeWu-Lab/Action-Preference-Optimization\n","authors":["Wenke Xia","Yichu Yang","Hongtao Wu","Xiao Ma","Tao Kong","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2506.07127v3.pdf","comment":"Accepted By NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.16969v3","updated":"2025-10-30T03:51:20Z","published":"2025-05-22T17:49:10Z","title":"3D Equivariant Visuomotor Policy Learning via Spherical Projection","summary":"  Equivariant models have recently been shown to improve the data efficiency of\ndiffusion policy by a significant margin. However, prior work that explored\nthis direction focused primarily on point cloud inputs generated by multiple\ncameras fixed in the workspace. This type of point cloud input is not\ncompatible with the now-common setting where the primary input modality is an\neye-in-hand RGB camera like a GoPro. This paper closes this gap by\nincorporating into the diffusion policy model a process that projects features\nfrom the 2D RGB camera image onto a sphere. This enables us to reason about\nsymmetries in $\\mathrm{SO}(3)$ without explicitly reconstructing a point cloud.\nWe perform extensive experiments in both simulation and the real world that\ndemonstrate that our method consistently outperforms strong baselines in terms\nof both performance and sample efficiency. Our work, Image-to-Sphere Policy\n($\\textbf{ISP}$), is the first $\\mathrm{SO}(3)$-equivariant policy learning\nframework for robotic manipulation that works using only monocular RGB inputs.\n","authors":["Boce Hu","Dian Wang","David Klee","Heng Tian","Xupeng Zhu","Haojie Huang","Robert Platt","Robin Walters"],"pdf_url":"https://arxiv.org/pdf/2505.16969v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17144v2","updated":"2025-10-30T03:19:00Z","published":"2025-07-23T02:25:03Z","title":"Falconry-like palm landing by a flapping-wing drone based on the human\n  gesture interaction and distance-aware flight planning","summary":"  Flapping-wing drones have attracted significant attention due to their\nbiomimetic flight. They are considered more human-friendly due to their\ncharacteristics such as low noise and flexible wings, making them suitable for\nhuman-drone interactions. However, few studies have explored the practical\ninteraction between humans and flapping-wing drones. On establishing a physical\ninteraction system with flapping-wing drones, we can acquire inspirations from\nfalconers who guide birds of prey to land on their arms. This interaction\ninterprets the human body as a dynamic landing platform, which can be utilized\nin various scenarios such as crowded or spatially constrained environments.\nThus, in this study, we propose a falconry-like interaction system in which a\nflapping-wing drone performs a palm landing motion on a human hand. To achieve\na safe approach toward humans, we design a trajectory planning method that\nconsiders both physical and psychological factors of the human safety such as\nthe drone's velocity and distance from the user. We use a commercial flapping\nplatform with our implemented motion planning and conduct experiments to\nevaluate the palm landing performance and safety. The results demonstrate that\nour approach enables safe and smooth hand landing interactions. To the best of\nour knowledge, it is the first time to achieve a contact-based interaction\nbetween flapping-wing drones and humans.\n","authors":["Kazuki Numazato","Keiichiro Kan","Masaki Kitagawa","Yunong Li","Johannes Kubel","Moju Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.17144v2.pdf","comment":"8 pages, 14 figures"},{"id":"http://arxiv.org/abs/2510.26082v1","updated":"2025-10-30T02:39:13Z","published":"2025-10-30T02:39:13Z","title":"Beyond the Uncanny Valley: A Mixed-Method Investigation of\n  Anthropomorphism in Protective Responses to Robot Abuse","summary":"  Robots with anthropomorphic features are increasingly shaping how humans\nperceive and morally engage with them. Our research investigates how different\nlevels of anthropomorphism influence protective responses to robot abuse,\nextending the Computers as Social Actors (CASA) and uncanny valley theories\ninto a moral domain. In an experiment, we invite 201 participants to view\nvideos depicting abuse toward a robot with low (Spider), moderate (Two-Foot),\nor high (Humanoid) anthropomorphism. To provide a comprehensive analysis, we\ntriangulate three modalities: self-report surveys measuring emotions and\nuncanniness, physiological data from automated facial expression analysis, and\nqualitative reflections. Findings indicate that protective responses are not\nlinear. The moderately anthropomorphic Two-Foot robot, rated highest in\neeriness and \"spine-tingling\" sensations consistent with the uncanny valley,\nelicited the strongest physiological anger expressions. Self-reported anger and\nguilt are significantly higher for both the Two-Foot and Humanoid robots\ncompared to the Spider. Qualitative findings further reveal that as\nanthropomorphism increases, moral reasoning shifts from technical assessments\nof property damage to condemnation of the abuser's character, while governance\nproposals expand from property law to calls for quasi-animal rights and broader\nsocietal responsibility. These results suggest that the uncanny valley does not\ndampen moral concern but paradoxically heightens protective impulses, offering\ncritical implications for robot design, policy, and future legal frameworks.\n","authors":["Fan Yang","Lingyao Li","Yaxin Hu","Michael Rodgers","Renkai Ma"],"pdf_url":"https://arxiv.org/pdf/2510.26082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26080v1","updated":"2025-10-30T02:32:29Z","published":"2025-10-30T02:32:29Z","title":"I don't Want You to Die: A Shared Responsibility Framework for\n  Safeguarding Child-Robot Companionship","summary":"  Social robots like Moxie are designed to form strong emotional bonds with\nchildren, but their abrupt discontinuation can cause significant struggles and\ndistress to children. When these services end, the resulting harm raises\ncomplex questions of who bears responsibility when children's emotional bonds\nare broken. Using the Moxie shutdown as a case study through a qualitative\nsurvey of 72 U.S. participants, our findings show that the responsibility is\nviewed as a shared duty across the robot company, parents, developers, and\ngovernment. However, these attributions varied by political ideology and\nparental status of whether they have children. Participants' perceptions of\nwhether the robot service should continue are highly polarized; supporters\npropose technical, financial, and governmental pathways for continuity, while\nopponents cite business realities and risks of unhealthy emotional dependency.\nUltimately, this research contributes an empirically grounded shared\nresponsibility framework for safeguarding child-robot companionship by\ndetailing how accountability is distributed and contested, informing concrete\ndesign and policy implications to mitigate the emotional harm of robot\ndiscontinuation.\n","authors":["Fan Yang","Renkai Ma","Yaxin Hu","Michael Rodgers","Lingyao Li"],"pdf_url":"https://arxiv.org/pdf/2510.26080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26067v1","updated":"2025-10-30T01:53:32Z","published":"2025-10-30T01:53:32Z","title":"Morphology-Aware Graph Reinforcement Learning for Tensegrity Robot\n  Locomotion","summary":"  Tensegrity robots combine rigid rods and elastic cables, offering high\nresilience and deployability but posing major challenges for locomotion control\ndue to their underactuated and highly coupled dynamics. This paper introduces a\nmorphology-aware reinforcement learning framework that integrates a graph\nneural network (GNN) into the Soft Actor-Critic (SAC) algorithm. By\nrepresenting the robot's physical topology as a graph, the proposed GNN-based\npolicy captures coupling among components, enabling faster and more stable\nlearning than conventional multilayer perceptron (MLP) policies. The method is\nvalidated on a physical 3-bar tensegrity robot across three locomotion\nprimitives, including straight-line tracking and bidirectional turning. It\nshows superior sample efficiency, robustness to noise and stiffness variations,\nand improved trajectory accuracy. Notably, the learned policies transfer\ndirectly from simulation to hardware without fine-tuning, achieving stable\nreal-world locomotion. These results demonstrate the advantages of\nincorporating structural priors into reinforcement learning for tensegrity\nrobot control.\n","authors":["Chi Zhang","Mingrui Li","Wenzhe Tong","Xiaonan Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17148v3","updated":"2025-10-30T01:44:58Z","published":"2025-10-20T04:49:14Z","title":"DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through\n  Metric-Guided Alignment","summary":"  Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.\n","authors":["Yu Gao","Anqing Jiang","Yiru Wang","Wang Jijun","Hao Jiang","Zhigang Sun","Heng Yuwen","Wang Shuo","Hao Zhao","Sun Hao"],"pdf_url":"https://arxiv.org/pdf/2510.17148v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26040v1","updated":"2025-10-30T00:38:18Z","published":"2025-10-30T00:38:18Z","title":"Accelerating Real-World Overtaking in F1TENTH Racing Employing\n  Reinforcement Learning Methods","summary":"  While autonomous racing performance in Time-Trial scenarios has seen\nsignificant progress and development, autonomous wheel-to-wheel racing and\novertaking are still severely limited. These limitations are particularly\napparent in real-life driving scenarios where state-of-the-art algorithms\nstruggle to safely or reliably complete overtaking manoeuvres. This is\nimportant, as reliable navigation around other vehicles is vital for safe\nautonomous wheel-to-wheel racing. The F1Tenth Competition provides a useful\nopportunity for developing wheel-to-wheel racing algorithms on a standardised\nphysical platform. The competition format makes it possible to evaluate\novertaking and wheel-to-wheel racing algorithms against the state-of-the-art.\nThis research presents a novel racing and overtaking agent capable of learning\nto reliably navigate a track and overtake opponents in both simulation and\nreality. The agent was deployed on an F1Tenth vehicle and competed against\nopponents running varying competitive algorithms in the real world. The results\ndemonstrate that the agent's training against opponents enables deliberate\novertaking behaviours with an overtaking rate of 87% compared 56% for an agent\ntrained just to race.\n","authors":["Emily Steiner","Daniel van der Spuy","Futian Zhou","Afereti Pama","Minas Liarokapis","Henry Williams"],"pdf_url":"https://arxiv.org/pdf/2510.26040v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2510.26802v1","updated":"2025-10-30T17:59:55Z","published":"2025-10-30T17:59:55Z","title":"Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark","summary":"  Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io\n","authors":["Ziyu Guo","Xinyan Chen","Renrui Zhang","Ruichuan An","Yu Qi","Dongzhi Jiang","Xiangtai Li","Manyuan Zhang","Hongsheng Li","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2510.26802v1.pdf","comment":"Project Page: https://video-cof.github.io"},{"id":"http://arxiv.org/abs/2506.03237v2","updated":"2025-10-30T17:59:46Z","published":"2025-06-03T17:49:41Z","title":"UniSite: The First Cross-Structure Dataset and Learning Framework for\n  End-to-End Ligand Binding Site Detection","summary":"  The detection of ligand binding sites for proteins is a fundamental step in\nStructure-Based Drug Design. Despite notable advances in recent years, existing\nmethods, datasets, and evaluation metrics are confronted with several key\nchallenges: (1) current datasets and methods are centered on individual\nprotein-ligand complexes and neglect that diverse binding sites may exist\nacross multiple complexes of the same protein, introducing significant\nstatistical bias; (2) ligand binding site detection is typically modeled as a\ndiscontinuous workflow, employing binary segmentation and subsequent clustering\nalgorithms; (3) traditional evaluation metrics do not adequately reflect the\nactual performance of different binding site prediction methods. To address\nthese issues, we first introduce UniSite-DS, the first UniProt (Unique\nProtein)-centric ligand binding site dataset, which contains 4.81 times more\nmulti-site data and 2.08 times more overall data compared to the previously\nmost widely used datasets. We then propose UniSite, the first end-to-end ligand\nbinding site detection framework supervised by set prediction loss with\nbijective matching. In addition, we introduce Average Precision based on\nIntersection over Union (IoU) as a more accurate evaluation metric for ligand\nbinding site prediction. Extensive experiments on UniSite-DS and several\nrepresentative benchmark datasets demonstrate that IoU-based Average Precision\nprovides a more accurate reflection of prediction quality, and that UniSite\noutperforms current state-of-the-art methods in ligand binding site detection.\nThe dataset and codes will be made publicly available at\nhttps://github.com/quanlin-wu/unisite.\n","authors":["Jigang Fan","Quanlin Wu","Shengjie Luo","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2506.03237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26790v1","updated":"2025-10-30T17:58:26Z","published":"2025-10-30T17:58:26Z","title":"Gistify! Codebase-Level Understanding via Runtime Execution","summary":"  As coding agents are increasingly deployed in large codebases, the need to\nautomatically design challenging, codebase-level evaluation is central. We\npropose Gistify, a task where a coding LLM must create a single, minimal,\nself-contained file that can reproduce a specific functionality of a codebase.\nThe coding LLM is given full access to a codebase along with a specific\nentrypoint (e.g., a python command), and the generated file must replicate the\noutput of the same command ran under the full codebase, while containing only\nthe essential components necessary to execute the provided command. Success on\nGistify requires both structural understanding of the codebase, accurate\nmodeling of its execution flow as well as the ability to produce potentially\nlarge code patches. Our findings show that current state-of-the-art models\nstruggle to reliably solve Gistify tasks, especially ones with long executions\ntraces.\n","authors":["Hyunji Lee","Minseon Kim","Chinmay Singh","Matheus Pereira","Atharv Sonwane","Isadora White","Elias Stengel-Eskin","Mohit Bansal","Zhengyan Shi","Alessandro Sordoni","Marc-Alexandre Côté","Xingdi Yuan","Lucas Caccia"],"pdf_url":"https://arxiv.org/pdf/2510.26790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26788v1","updated":"2025-10-30T17:58:11Z","published":"2025-10-30T17:58:11Z","title":"Defeating the Training-Inference Mismatch via FP16","summary":"  Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.\n","authors":["Penghui Qi","Zichen Liu","Xiangxin Zhou","Tianyu Pang","Chao Du","Wee Sun Lee","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2510.26788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26787v1","updated":"2025-10-30T17:58:04Z","published":"2025-10-30T17:58:04Z","title":"Remote Labor Index: Measuring AI Automation of Remote Work","summary":"  AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.\n","authors":["Mantas Mazeika","Alice Gatti","Cristina Menghini","Udari Madhushani Sehwag","Shivam Singhal","Yury Orlovskiy","Steven Basart","Manasi Sharma","Denis Peskoff","Elaine Lau","Jaehyuk Lim","Lachlan Carroll","Alice Blair","Vinaya Sivakumar","Sumana Basu","Brad Kenstler","Yuntao Ma","Julian Michael","Xiaoke Li","Oliver Ingebretsen","Aditya Mehta","Jean Mottola","John Teichmann","Kevin Yu","Zaina Shaik","Adam Khoja","Richard Ren","Jason Hausenloy","Long Phan","Ye Htet","Ankit Aich","Tahseen Rabbani","Vivswan Shah","Andriy Novykov","Felix Binder","Kirill Chugunov","Luis Ramirez","Matias Geralnik","Hernán Mesura","Dean Lee","Ed-Yeremai Hernandez Cardona","Annette Diamond","Summer Yue","Alexandr Wang","Bing Liu","Ernesto Hernandez","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2510.26787v1.pdf","comment":"Website: https://www.remotelabor.ai"},{"id":"http://arxiv.org/abs/2510.26784v1","updated":"2025-10-30T17:57:17Z","published":"2025-10-30T17:57:17Z","title":"LLMs Process Lists With General Filter Heads","summary":"  We investigate the mechanisms underlying a range of list-processing tasks in\nLLMs, and we find that LLMs have learned to encode a compact, causal\nrepresentation of a general filtering operation that mirrors the generic\n\"filter\" function of functional programming. Using causal mediation analysis on\na diverse set of list-processing tasks, we find that a small number of\nattention heads, which we dub filter heads, encode a compact representation of\nthe filtering predicate in their query states at certain tokens. We demonstrate\nthat this predicate representation is general and portable: it can be extracted\nand reapplied to execute the same filtering operation on different collections,\npresented in different formats, languages, or even in tasks. However, we also\nidentify situations where transformer LMs can exploit a different strategy for\nfiltering: eagerly evaluating if an item satisfies the predicate and storing\nthis intermediate result as a flag directly in the item representations. Our\nresults reveal that transformer LMs can develop human-interpretable\nimplementations of abstract computational operations that generalize in ways\nthat are surprisingly similar to strategies used in traditional functional\nprogramming patterns.\n","authors":["Arnab Sen Sharma","Giordano Rogers","Natalie Shapira","David Bau"],"pdf_url":"https://arxiv.org/pdf/2510.26784v1.pdf","comment":"Code and data at https://filter.baulab.info/"},{"id":"http://arxiv.org/abs/2510.26782v1","updated":"2025-10-30T17:56:43Z","published":"2025-10-30T17:56:43Z","title":"Clone Deterministic 3D Worlds with Geometrically-Regularized World\n  Models","summary":"  A world model is an internal model that simulates how the world evolves.\nGiven past observations and actions, it predicts the future of both the\nembodied agent and its environment. Accurate world models are essential for\nenabling agents to think, plan, and reason effectively in complex, dynamic\nsettings. Despite rapid progress, current world models remain brittle and\ndegrade over long horizons. We argue that a central cause is representation\nquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or\nentangled latents make dynamics learning unnecessarily hard. We therefore ask\nwhether improving representation learning alone can substantially improve\nworld-model performance. In this work, we take a step toward building a truly\naccurate world model by addressing a fundamental yet open problem: constructing\na model that can fully clone and overfit to a deterministic 3D world. We\npropose Geometrically-Regularized World Models (GRWM), which enforces that\nconsecutive points along a natural sensory trajectory remain close in latent\nrepresentation space. This approach yields significantly improved latent\nrepresentations that align closely with the true topology of the environment.\nGRWM is plug-and-play, requires only minimal architectural modification, scales\nwith trajectory length, and is compatible with diverse latent generative\nbackbones. Across deterministic 3D settings and long-horizon prediction tasks,\nGRWM significantly increases rollout fidelity and stability. Analyses show that\nits benefits stem from learning a latent manifold with superior geometric\nstructure. These findings support a clear takeaway: improving representation\nlearning is a direct and useful path to robust world models, delivering\nreliable long-horizon predictions without enlarging the dynamics module.\n","authors":["Zaishuo Xia","Yukuan Lu","Xinyi Li","Yifan Xu","Yubei Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26776v1","updated":"2025-10-30T17:55:19Z","published":"2025-10-30T17:55:19Z","title":"Faithful and Fast Influence Function via Advanced Sampling","summary":"  How can we explain the influence of training data on black-box models?\nInfluence functions (IFs) offer a post-hoc solution by utilizing gradients and\nHessians. However, computing the Hessian for an entire dataset is\nresource-intensive, necessitating a feasible alternative. A common approach\ninvolves randomly sampling a small subset of the training data, but this method\noften results in highly inconsistent IF estimates due to the high variance in\nsample configurations. To address this, we propose two advanced sampling\ntechniques based on features and logits. These samplers select a small yet\nrepresentative subset of the entire dataset by considering the stochastic\ndistribution of features or logits, thereby enhancing the accuracy of IF\nestimations. We validate our approach through class removal experiments, a\ntypical application of IFs, using the F1-score to measure how effectively the\nmodel forgets the removed class while maintaining inference consistency on the\nremaining classes. Our method reduces computation time by 30.1% and memory\nusage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.\n","authors":["Jungyeon Koh","Hyeonsu Lyu","Jonggyu Jang","Hyun Jong Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25744v2","updated":"2025-10-30T17:54:45Z","published":"2025-10-29T17:47:18Z","title":"Completion $\\neq$ Collaboration: Scaling Collaborative Effort with\n  Agents","summary":"  Current evaluations of agents remain centered around one-shot task\ncompletion, failing to account for the inherently iterative and collaborative\nnature of many real-world problems, where human goals are often underspecified\nand evolve. We argue for a shift from building and assessing task completion\nagents to developing collaborative agents, assessed not only by the quality of\ntheir final outputs but by how well they engage with and enhance human effort\nthroughout the problem-solving process. To support this shift, we introduce\ncollaborative effort scaling, a framework that captures how an agent's utility\ngrows with increasing user involvement. Through case studies and simulated\nevaluations, we show that state-of-the-art agents often underperform in\nmulti-turn, real-world scenarios, revealing a missing ingredient in agent\ndesign: the ability to sustain engagement and scaffold user understanding.\nCollaborative effort scaling offers a lens for diagnosing agent behavior and\nguiding development toward more effective interactions.\n","authors":["Shannon Zejiang Shen","Valerie Chen","Ken Gu","Alexis Ross","Zixian Ma","Jillian Ross","Alex Gu","Chenglei Si","Wayne Chi","Andi Peng","Jocelyn J Shen","Ameet Talwalkar","Tongshuang Wu","David Sontag"],"pdf_url":"https://arxiv.org/pdf/2510.25744v2.pdf","comment":"22 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.26771v1","updated":"2025-10-30T17:53:42Z","published":"2025-10-30T17:53:42Z","title":"STaMP: Sequence Transformation and Mixed Precision for Low-Precision\n  Activation Quantization","summary":"  Quantization is the key method for reducing inference latency, power and\nmemory footprint of generative AI models. However, accuracy often degrades\nsharply when activations are quantized below eight bits. Recent work suggests\nthat invertible linear transformations (e.g. rotations) can aid quantization,\nby reparameterizing feature channels and weights. In this paper, we propose\n\\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a\nnovel strategy that applies linear transformations along the \\textit{sequence}\ndimension to exploit the strong local correlation in language and visual data.\nBy keeping a small number of tokens in each intermediate activation at higher\nprecision, we can maintain model accuracy at lower (average) activations\nbit-widths. We evaluate STaMP on recent LVM and LLM architectures,\ndemonstrating that it significantly improves low bit width activation\nquantization and complements established activation and weight quantization\nmethods including recent feature transformations.\n","authors":["Marco Federici","Riccardo Del Chiaro","Boris van Breugel","Paul Whatmough","Markus Nagel"],"pdf_url":"https://arxiv.org/pdf/2510.26771v1.pdf","comment":"10 pages main text, 8 pages supplementary material"},{"id":"http://arxiv.org/abs/2510.26768v1","updated":"2025-10-30T17:52:02Z","published":"2025-10-30T17:52:02Z","title":"AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions","summary":"  We present AMO-Bench, an Advanced Mathematical reasoning benchmark with\nOlympiad level or even higher difficulty, comprising 50 human-crafted problems.\nExisting benchmarks have widely leveraged high school math competitions for\nevaluating mathematical reasoning capabilities of large language models (LLMs).\nHowever, many existing math competitions are becoming less effective for\nassessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To\naddress this, AMO-Bench introduces more rigorous challenges by ensuring all 50\nproblems are (1) cross-validated by experts to meet at least the International\nMathematical Olympiad (IMO) difficulty standards, and (2) entirely original\nproblems to prevent potential performance leakages from data memorization.\nMoreover, each problem in AMO-Bench requires only a final answer rather than a\nproof, enabling automatic and robust grading for evaluation. Experimental\nresults across 26 LLMs on AMO-Bench show that even the best-performing model\nachieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.\nBeyond these poor performances, our further analysis reveals a promising\nscaling trend with increasing test-time compute on AMO-Bench. These results\nhighlight the significant room for improving the mathematical reasoning in\ncurrent LLMs. We release AMO-Bench to facilitate further research into\nadvancing the reasoning abilities of language models.\nhttps://amo-bench.github.io/\n","authors":["Shengnan An","Xunliang Cai","Xuezhi Cao","Xiaoyu Li","Yehao Lin","Junlin Liu","Xinxuan Lv","Dan Ma","Xuanlin Wang","Ziwen Wang","Shuang Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26768v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.26752v1","updated":"2025-10-30T17:46:49Z","published":"2025-10-30T17:46:49Z","title":"The Oversight Game: Learning to Cooperatively Balance an AI Agent's\n  Safety and Autonomy","summary":"  As increasingly capable agents are deployed, a central safety question is how\nto retain meaningful human control without modifying the underlying system. We\nstudy a minimal control interface where an agent chooses whether to act\nautonomously (play) or defer (ask), while a human simultaneously chooses\nwhether to be permissive (trust) or to engage in oversight (oversee). If the\nagent defers, the human's choice determines the outcome, potentially leading to\na corrective action or a system shutdown. We model this interaction as a\ntwo-player Markov Game. Our analysis focuses on cases where this game qualifies\nas a Markov Potential Game (MPG), a class of games where we can provide an\nalignment guarantee: under a structural assumption on the human's value\nfunction, any decision by the agent to act more autonomously that benefits\nitself cannot harm the human's value. We also analyze extensions to this MPG\nframework. Theoretically, this perspective provides conditions for a specific\nform of intrinsic alignment. If the reward structures of the human-agent game\nmeet these conditions, we have a formal guarantee that the agent improving its\nown outcome will not harm the human's. Practically, this model motivates a\ntransparent control layer with predictable incentives where the agent learns to\ndefer when risky and act when safe, while its pretrained policy and the\nenvironment's reward structure remain untouched. Our gridworld simulation shows\nthat through independent learning, the agent and human discover their optimal\noversight roles. The agent learns to ask when uncertain and the human learns\nwhen to oversee, leading to an emergent collaboration that avoids safety\nviolations introduced post-training. This demonstrates a practical method for\nmaking misaligned models safer after deployment.\n","authors":["William Overman","Mohsen Bayati"],"pdf_url":"https://arxiv.org/pdf/2510.26752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26745v1","updated":"2025-10-30T17:40:22Z","published":"2025-10-30T17:40:22Z","title":"Deep sequence models tend to memorize geometrically; it is unclear why","summary":"  In sequence modeling, the parametric memory of atomic facts has been\npredominantly abstracted as a brute-force lookup of co-occurrences between\nentities. We contrast this associative view against a geometric view of how\nmemory is stored. We begin by isolating a clean and analyzable instance of\nTransformer reasoning that is incompatible with memory as strictly a storage of\nthe local co-occurrences specified during training. Instead, the model must\nhave somehow synthesized its own geometry of atomic facts, encoding global\nrelationships between all entities, including non-co-occurring ones. This in\nturn has simplified a hard reasoning task involving an $\\ell$-fold composition\ninto an easy-to-learn 1-step geometric task.\n  From this phenomenon, we extract fundamental aspects of neural embedding\ngeometries that are hard to explain. We argue that the rise of such a geometry,\ndespite optimizing over mere local associations, cannot be straightforwardly\nattributed to typical architectural or optimizational pressures.\nCounterintuitively, an elegant geometry is learned even when it is not more\nsuccinct than a brute-force lookup of associations.\n  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry\nstems from a spectral bias that -- in contrast to prevailing theories -- indeed\narises naturally despite the lack of various pressures. This analysis also\npoints to practitioners a visible headroom to make Transformer memory more\nstrongly geometric. We hope the geometric view of parametric memory encourages\nrevisiting the default intuitions that guide researchers in areas like\nknowledge acquisition, capacity, discovery and unlearning.\n","authors":["Shahriar Noroozizadeh","Vaishnavh Nagarajan","Elan Rosenfeld","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2510.26745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26740v1","updated":"2025-10-30T17:37:51Z","published":"2025-10-30T17:37:51Z","title":"A General Incentives-Based Framework for Fairness in Multi-agent\n  Resource Allocation","summary":"  We introduce the General Incentives-based Framework for Fairness (GIFF), a\nnovel approach for fair multi-agent resource allocation that infers fair\ndecision-making from standard value functions. In resource-constrained\nsettings, agents optimizing for efficiency often create inequitable outcomes.\nOur approach leverages the action-value (Q-)function to balance efficiency and\nfairness without requiring additional training. Specifically, our method\ncomputes a local fairness gain for each action and introduces a counterfactual\nadvantage correction term to discourage over-allocation to already well-off\nagents. This approach is formalized within a centralized control setting, where\nan arbitrator uses the GIFF-modified Q-values to solve an allocation problem.\n  Empirical evaluations across diverse domains, including dynamic ridesharing,\nhomelessness prevention, and a complex job allocation task-demonstrate that our\nframework consistently outperforms strong baselines and can discover\nfar-sighted, equitable policies. The framework's effectiveness is supported by\na theoretical foundation; we prove its fairness surrogate is a principled lower\nbound on the true fairness improvement and that its trade-off parameter offers\nmonotonic tuning. Our findings establish GIFF as a robust and principled\nframework for leveraging standard reinforcement learning components to achieve\nmore equitable outcomes in complex multi-agent systems.\n","authors":["Ashwin Kumar","William Yeoh"],"pdf_url":"https://arxiv.org/pdf/2510.26740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26732v1","updated":"2025-10-30T17:31:03Z","published":"2025-10-30T17:31:03Z","title":"Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models","summary":"  This paper presents a comprehensive cross-platform evaluation of reasoning\ncapabilities in contemporary foundation models, establishing an\ninfrastructure-agnostic benchmark across three computational paradigms: HPC\nsupercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and\nuniversity clusters (a node with eight H200 GPUs).\n  We evaluate 15 foundation models across 79 problems spanning eight academic\ndomains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,\nCalculus, and Optimization) through three experimental phases: (1) Baseline\nestablishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,\nMistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing\nmethodology and reference performance; (2) Infrastructure validation: The\n19-problem benchmark repeated on university cluster (seven models including\nFalcon-Mamba state-space architecture) and Nebius AI Studio (nine\nstate-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3\n30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic\nreproducibility; (3) Extended evaluation: Full 79-problem assessment on both\nuniversity cluster and Nebius platforms, probing generalization at scale across\narchitectural diversity.\n  The findings challenge conventional scaling assumptions, establish training\ndata quality as more critical than model size, and provide actionable\nguidelines for model selection across educational, production, and research\ncontexts. The tri-infrastructure methodology and 79-problem benchmark enable\nlongitudinal tracking of reasoning capabilities as foundation models evolve.\n","authors":["J. de Curtò","I. de Zarzà","Pablo García","Jordi Cabot"],"pdf_url":"https://arxiv.org/pdf/2510.26732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26730v1","updated":"2025-10-30T17:29:27Z","published":"2025-10-30T17:29:27Z","title":"ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference","summary":"  The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints.\n","authors":["Zixu Shen","Kexin Chu","Yifan Zhang","Dawei Xiang","Runxin Wu","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26730v1.pdf","comment":"12 pages, 11 figures"},{"id":"http://arxiv.org/abs/2510.26722v1","updated":"2025-10-30T17:22:57Z","published":"2025-10-30T17:22:57Z","title":"Non-Convex Over-the-Air Heterogeneous Federated Learning: A\n  Bias-Variance Trade-off","summary":"  Over-the-air (OTA) federated learning (FL) has been well recognized as a\nscalable paradigm that exploits the waveform superposition of the wireless\nmultiple-access channel to aggregate model updates in a single use. Existing\nOTA-FL designs largely enforce zero-bias model updates by either assuming\n\\emph{homogeneous} wireless conditions (equal path loss across devices) or\nforcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous}\nwireless scenarios, however, such designs are constrained by the weakest device\nand inflate the update variance. Moreover, prior analyses of biased OTA-FL\nlargely address convex objectives, while most modern AI models are highly\nnon-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient\ndescent (SGD) for general smooth non-convex objectives under wireless\nheterogeneity. We develop novel OTA-FL SGD updates that allow a structured,\ntime-invariant model bias while facilitating reduced variance updates. We\nderive a finite-time stationarity bound (expected time average squared gradient\nnorm) that explicitly reveals a bias-variance trade-off. To optimize this\ntrade-off, we pose a non-convex joint OTA power-control design and develop an\nefficient successive convex approximation (SCA) algorithm that requires only\nstatistical CSI at the base station. Experiments on a non-convex image\nclassification task validate the approach: the SCA-based design accelerates\nconvergence via an optimized bias and improves generalization over prior OTA-FL\nbaselines.\n","authors":["Muhammad Faraz Ul Abrar","Nicolò Michelusi"],"pdf_url":"https://arxiv.org/pdf/2510.26722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26721v1","updated":"2025-10-30T17:22:22Z","published":"2025-10-30T17:22:22Z","title":"Unveiling Intrinsic Text Bias in Multimodal Large Language Models\n  through Attention Key-Space Analysis","summary":"  Multimodal large language models (MLLMs) exhibit a pronounced preference for\ntextual inputs when processing vision-language data, limiting their ability to\nreason effectively from visual evidence. Unlike prior studies that attribute\nthis text bias to external factors such as data imbalance or instruction\ntuning, we propose that the bias originates from the model's internal\narchitecture. Specifically, we hypothesize that visual key vectors (Visual\nKeys) are out-of-distribution (OOD) relative to the text key space learned\nduring language-only pretraining. Consequently, these visual keys receive\nsystematically lower similarity scores during attention computation, leading to\ntheir under-utilization in the context representation. To validate this\nhypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their\ndistributional structures using qualitative (t-SNE) and quantitative\n(Jensen-Shannon divergence) methods. The results provide direct evidence that\nvisual and textual keys occupy markedly distinct subspaces within the attention\nspace. The inter-modal divergence is statistically significant, exceeding\nintra-modal variation by several orders of magnitude. These findings reveal\nthat text bias arises from an intrinsic misalignment within the attention key\nspace rather than solely from external data factors.\n","authors":["Xinhan Zheng","Huyu Wu","Xueting Wang","Haiyun Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.26721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26714v1","updated":"2025-10-30T17:13:42Z","published":"2025-10-30T17:13:42Z","title":"On the limitation of evaluating machine unlearning using only a single\n  training seed","summary":"  Machine unlearning (MU) aims to remove the influence of certain data points\nfrom a trained model without costly retraining. Most practical MU algorithms\nare only approximate and their performance can only be assessed empirically.\nCare must therefore be taken to make empirical comparisons as representative as\npossible. A common practice is to run the MU algorithm multiple times\nindependently starting from the same trained model. In this work, we\ndemonstrate that this practice can give highly non-representative results\nbecause -- even for the same architecture and same dataset -- some MU methods\ncan be highly sensitive to the choice of random number seed used for model\ntraining. We therefore recommend that empirical\ncomphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should\nalso reflect the variability across different model training seeds.\n","authors":["Jamie Lanyon","Axel Finke","Petros Andreou","Georgina Cosma"],"pdf_url":"https://arxiv.org/pdf/2510.26714v1.pdf","comment":"mini paper, 2 figures"},{"id":"http://arxiv.org/abs/2507.03704v2","updated":"2025-10-30T17:13:35Z","published":"2025-07-04T16:41:06Z","title":"Controlling Thinking Speed in Reasoning Models","summary":"  Human cognition is theorized to operate in two modes: fast, intuitive System\n1 thinking and slow, deliberate System 2 thinking. While current Large\nReasoning Models (LRMs) excel at System 2 thinking, their inability to perform\nfast thinking leads to high computational overhead and latency. In this work,\nwe enable LRMs to approximate human intelligence through dynamic thinking speed\nadjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses\ntwo key questions: (1) how to control thinking speed in LRMs, and (2) when to\nadjust it for optimal performance. For the first question, we identify the\nsteering vector that governs slow-fast thinking transitions in LRMs'\nrepresentation space. Using this vector, we achieve the first representation\nediting-based test-time scaling effect, outperforming existing prompt-based\nscaling methods. For the second question, we apply real-time difficulty\nestimation to signal reasoning segments of varying complexity. Combining these\ntechniques, we propose the first reasoning strategy that enables fast\nprocessing of easy steps and deeper analysis for complex reasoning. Without any\ntraining or additional cost, our plug-in module delivers an average +1.3%\naccuracy with -8.6% token usage across leading LRMs and advanced reasoning\nbenchmarks. All of our algorithms are implemented based on vLLM and are\nexpected to support broader applications and inspire future research.\n","authors":["Zhengkai Lin","Zhihang Fu","Ze Chen","Chao Chen","Liang Xie","Wenxiao Wang","Deng Cai","Zheng Wang","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2507.03704v2.pdf","comment":"NeurIPS 2025 Spotlight"},{"id":"http://arxiv.org/abs/2509.21319v2","updated":"2025-10-30T17:09:54Z","published":"2025-09-25T16:19:06Z","title":"RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards","summary":"  Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:\nhttps://huggingface.co/collections/nvidia/reward-models-10-2025\n","authors":["Zhilin Wang","Jiaqi Zeng","Olivier Delalleau","Ellie Evans","Daniel Egert","Hoo-Chang Shin","Felipe Soares","Yi Dong","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2509.21319v2.pdf","comment":"Added link to access models:\n  https://huggingface.co/collections/nvidia/reward-models-10-2025"},{"id":"http://arxiv.org/abs/2510.26702v1","updated":"2025-10-30T17:07:00Z","published":"2025-10-30T17:07:00Z","title":"Delegated Authorization for Agents Constrained to Semantic Task-to-Scope\n  Matching","summary":"  Authorizing Large Language Model driven agents to dynamically invoke tools\nand access protected resources introduces significant risks, since current\nmethods for delegating authorization grant overly broad permissions and give\naccess to tools allowing agents to operate beyond the intended task scope. We\nintroduce and assess a delegated authorization model enabling authorization\nservers to semantically inspect access requests to protected resources, and\nissue access tokens constrained to the minimal set of scopes necessary for the\nagents' assigned tasks. Given the unavailability of datasets centered on\ndelegated authorization flows, particularly including both semantically\nappropriate and inappropriate scope requests for a given task, we introduce\nASTRA, a dataset and data generation pipeline for benchmarking semantic\nmatching between tasks and scopes. Our experiments show both the potential and\ncurrent limitations of model-based matching, particularly as the number of\nscopes needed for task completion increases. Our results highlight the need for\nfurther research into semantic matching techniques enabling intent-aware\nauthorization for multi-agent and tool-augmented applications, including\nfine-grained control, such as Task-Based Access Control (TBAC).\n","authors":["Majed El Helou","Chiara Troiani","Benjamin Ryder","Jean Diaconu","Hervé Muyal","Marcelo Yannuzzi"],"pdf_url":"https://arxiv.org/pdf/2510.26702v1.pdf","comment":"Paper page at https://outshift-open.github.io/ASTRA"},{"id":"http://arxiv.org/abs/2505.12275v2","updated":"2025-10-30T17:06:21Z","published":"2025-05-18T07:27:35Z","title":"Curriculum Abductive Learning","summary":"  Abductive Learning (ABL) integrates machine learning with logical reasoning\nin a loop: a learning model predicts symbolic concept labels from raw inputs,\nwhich are revised through abduction using domain knowledge and then fed back\nfor retraining. However, due to the nondeterminism of abduction, the training\nprocess often suffers from instability, especially when the knowledge base is\nlarge and complex, resulting in a prohibitively large abduction space. While\nprior works focus on improving candidate selection within this space, they\ntypically treat the knowledge base as a static black box. In this work, we\npropose Curriculum Abductive Learning (C-ABL), a method that explicitly\nleverages the internal structure of the knowledge base to address the ABL\ntraining challenges. C-ABL partitions the knowledge base into a sequence of\nsub-bases, progressively introduced during training. This reduces the abduction\nspace throughout training and enables the model to incorporate logic in a\nstepwise, smooth way. Experiments across multiple tasks show that C-ABL\noutperforms previous ABL implementations, significantly improves training\nstability, convergence speed, and final accuracy, especially under complex\nknowledge setting.\n","authors":["Wen-Chao Hu","Qi-Jie Li","Lin-Han Jia","Cunjing Ge","Yu-Feng Li","Yuan Jiang","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.12275v2.pdf","comment":"Accepted by NeurIPS 2025, 22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.20138v2","updated":"2025-10-30T17:04:50Z","published":"2025-03-26T01:00:35Z","title":"Guided Model Merging for Hybrid Data Learning: Leveraging Centralized\n  Data to Refine Decentralized Models","summary":"  Current network training paradigms primarily focus on either centralized or\ndecentralized data regimes. However, in practice, data availability often\nexhibits a hybrid nature, where both regimes coexist. This hybrid setting\npresents new opportunities for model training, as the two regimes offer\ncomplementary trade-offs: decentralized data is abundant but subject to\nheterogeneity and communication constraints, while centralized data, though\nlimited in volume and potentially unrepresentative, enables better curation and\nhigh-throughput access. Despite its potential, effectively combining these\nparadigms remains challenging, and few frameworks are tailored to hybrid data\nregimes. To address this, we propose a novel framework that constructs a model\natlas from decentralized models and leverages centralized data to refine a\nglobal model within this structured space. The refined model is then used to\nreinitialize the decentralized models. Our method synergizes federated learning\n(to exploit decentralized data) and model merging (to utilize centralized\ndata), enabling effective training under hybrid data availability.\nTheoretically, we show that our approach achieves faster convergence than\nmethods relying solely on decentralized data, due to variance reduction in the\nmerging process. Extensive experiments demonstrate that our framework\nconsistently outperforms purely centralized, purely decentralized, and existing\nhybrid-adaptable methods. Notably, our method remains robust even when the\ncentralized and decentralized data domains differ or when decentralized data\ncontains noise, significantly broadening its applicability.\n","authors":["Junyi Zhu","Ruicong Yao","Taha Ceritli","Savas Ozkan","Matthew B. Blaschko","Eunchung Noh","Jeongwon Min","Cho Jung Min","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2503.20138v2.pdf","comment":"Accepted at WACV 2026"},{"id":"http://arxiv.org/abs/2510.26697v1","updated":"2025-10-30T17:01:43Z","published":"2025-10-30T17:01:43Z","title":"The End of Manual Decoding: Towards Truly End-to-End Language Models","summary":"  The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding.\n","authors":["Zhichao Wang","Dongyang Ma","Xinting Huang","Deng Cai","Tian Lan","Jiahao Xu","Haitao Mi","Xiaoying Tang","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26684v1","updated":"2025-10-30T16:54:16Z","published":"2025-10-30T16:54:16Z","title":"Process Integrated Computer Vision for Real-Time Failure Prediction in\n  Steel Rolling Mill","summary":"  We present a long-term deployment study of a machine vision-based anomaly\ndetection system for failure prediction in a steel rolling mill. The system\nintegrates industrial cameras to monitor equipment operation, alignment, and\nhot bar motion in real time along the process line. Live video streams are\nprocessed on a centralized video server using deep learning models, enabling\nearly prediction of equipment failures and process interruptions, thereby\nreducing unplanned breakdown costs. Server-based inference minimizes the\ncomputational load on industrial process control systems (PLCs), supporting\nscalable deployment across production lines with minimal additional resources.\nBy jointly analyzing sensor data from data acquisition systems and visual\ninputs, the system identifies the location and probable root causes of\nfailures, providing actionable insights for proactive maintenance. This\nintegrated approach enhances operational reliability, productivity, and\nprofitability in industrial manufacturing environments.\n","authors":["Vaibhav Kurrey","Sivakalyan Pujari","Gagan Raj Gupta"],"pdf_url":"https://arxiv.org/pdf/2510.26684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26683v1","updated":"2025-10-30T16:53:45Z","published":"2025-10-30T16:53:45Z","title":"Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models","summary":"  Large language models (LLMs) have demonstrated exceptional capabilities\nacross multiple domains by leveraging massive pre-training and curated\nfine-tuning data. However, in data-sensitive fields such as healthcare, the\nlack of high-quality, domain-specific training corpus hinders LLMs' adaptation\nfor specialized applications. Meanwhile, domain experts have distilled domain\nwisdom into ontology rules, which formalize relationships among concepts and\nensure the integrity of knowledge management repositories. Viewing LLMs as\nimplicit repositories of human knowledge, we propose Evontree, a novel\nframework that leverages a small set of high-quality ontology rules to\nsystematically extract, validate, and enhance domain knowledge within LLMs,\nwithout requiring extensive external datasets. Specifically, Evontree extracts\ndomain ontology from raw models, detects inconsistencies using two core\nontology rules, and reinforces the refined knowledge via self-distilled\nfine-tuning. Extensive experiments on medical QA benchmarks with\nLlama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both\nunmodified models and leading supervised baselines, achieving up to a 3.7%\nimprovement in accuracy. These results confirm the effectiveness, efficiency,\nand robustness of our approach for low-resource domain adaptation of LLMs.\n","authors":["Mingchen Tu","Zhiqiang Liu","Juan Li","Liangyurui Liu","Junjie Wang","Lei Liang","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01543v2","updated":"2025-10-30T16:32:34Z","published":"2025-08-03T01:56:03Z","title":"Refine-n-Judge: Curating High-Quality Preference Chains for\n  LLM-Fine-Tuning","summary":"  Large Language Models (LLMs) have demonstrated remarkable progress through\npreference-based fine-tuning, which critically depends on the quality of the\nunderlying training data. While human feedback is essential for improving data\nquality, it is costly and does not scale well. In this paper, we introduce\nRefine-n-Judge, an automated iterative approach that leverages a single LLM as\nboth a refiner and a judge to enhance dataset quality. Unlike existing\niterative refinement methods, Refine-n-Judge employs an LLM to both generate\nrefinements and explicitly evaluate each improvement, ensuring that every\niteration meaningfully enhances the dataset without requiring additional human\nannotation or a separate reward model. At each step, the LLM refines a response\nand judges whether the refinement is an improvement over the previous answer.\nThis process continues until the LLM prefers the initial answer over the\nrefinement, indicating no further improvements. This produces sequences of\nincreasing quality, preference-labeled responses ideal for fine-tuning.\n  We demonstrate the effectiveness of Refine-n-Judge across a range of public\ndatasets spanning five corpora, targeting tasks such as coding, math, and\nconversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on\nRefine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of\ncomparisons against models tuned on the original dataset by GPT-4.\nAdditionally, we report performance gains: +5% on AlpacaEval and AlpacaEval\n2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces\nhigh-quality datasets and scalable model improvements.\n","authors":["Derin Cayir","Renjie Tao","Rashi Rungta","Kai Sun","Sean Chen","Haidar Khan","Minseok Kim","Julia Reinspach","Yue Liu"],"pdf_url":"https://arxiv.org/pdf/2508.01543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.21257v2","updated":"2025-10-30T16:25:15Z","published":"2025-07-28T18:20:41Z","title":"CompoST: A Benchmark for Analyzing the Ability of LLMs To\n  Compositionally Interpret Questions in a QALD Setting","summary":"  Language interpretation is a compositional process, in which the meaning of\nmore complex linguistic structures is inferred from the meaning of their parts.\nLarge language models possess remarkable language interpretation capabilities\nand have been successfully applied to interpret questions by mapping them to\nSPARQL queries. An open question is how systematic this interpretation process\nis. Toward this question, in this paper, we propose a benchmark for\ninvestigating to what extent the abilities of LLMs to interpret questions are\nactually compositional. For this, we generate three datasets of varying\ndifficulty based on graph patterns in DBpedia, relying on Lemon lexica for\nverbalization. Our datasets are created in a very controlled fashion in order\nto test the ability of LLMs to interpret structurally complex questions, given\nthat they have seen the atomic building blocks. This allows us to evaluate to\nwhat degree LLMs are able to interpret complex questions for which they\n\"understand\" the atomic parts. We conduct experiments with models of different\nsizes using both various prompt and few-shot optimization techniques as well as\nfine-tuning. Our results show that performance in terms of macro $F_1$ degrades\nfrom $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the\nsamples optimized on. Even when all necessary information was provided to the\nmodel in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of\nlowest complexity. We thus conclude that LLMs struggle to systematically and\ncompositionally interpret questions and map them into SPARQL queries.\n","authors":["David Maria Schmidt","Raoul Schubert","Philipp Cimiano"],"pdf_url":"https://arxiv.org/pdf/2507.21257v2.pdf","comment":"Research Track, 24th International Semantic Web Conference (ISWC\n  2025), November 2-6, 2025, Nara, Japan"},{"id":"http://arxiv.org/abs/2510.26658v1","updated":"2025-10-30T16:25:10Z","published":"2025-10-30T16:25:10Z","title":"The Era of Agentic Organization: Learning to Organize with Language\n  Models","summary":"  We envision a new era of AI, termed agentic organization, where agents solve\ncomplex problems by working collaboratively and concurrently, enabling outcomes\nbeyond individual intelligence. To realize this vision, we introduce\nasynchronous thinking (AsyncThink) as a new paradigm of reasoning with large\nlanguage models, which organizes the internal thinking process into\nconcurrently executable structures. Specifically, we propose a thinking\nprotocol where an organizer dynamically assigns sub-queries to workers, merges\nintermediate knowledge, and produces coherent solutions. More importantly, the\nthinking structure in this protocol can be further optimized through\nreinforcement learning. Experiments demonstrate that AsyncThink achieves 28%\nlower inference latency compared to parallel thinking while improving accuracy\non mathematical reasoning. Moreover, AsyncThink generalizes its learned\nasynchronous thinking capabilities, effectively tackling unseen tasks without\nadditional training.\n","authors":["Zewen Chi","Li Dong","Qingxiu Dong","Yaru Hao","Xun Wu","Shaohan Huang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2510.26658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26646v1","updated":"2025-10-30T16:12:01Z","published":"2025-10-30T16:12:01Z","title":"Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in\n  Dynamic Environments","summary":"  This paper presents a hierarchical path-planning and control framework that\ncombines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with\na low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller\nfor continuous actuation. The high-level module selects behaviors and\nsub-goals; the low-level module executes smooth velocity commands. We design a\npractical reward shaping scheme (direction, distance, obstacle avoidance,\naction smoothness, collision penalty, time penalty, and progress), together\nwith a LiDAR-based safety gate that prevents unsafe motions. The system is\nimplemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,\nincluding success rate, collision rate, path efficiency, and re-planning\nefficiency, in dynamic and partially observable environments. Experiments show\nimproved success rate and sample efficiency over single-algorithm baselines\n(DQN or TD3 alone) and rule-based planners, with better generalization to\nunseen obstacle configurations and reduced abrupt control changes. Code and\nevaluation scripts are available at the project repository.\n","authors":["Xiaoyi He","Danggui Chen","Zhenshuo Zhang","Zimeng Bai"],"pdf_url":"https://arxiv.org/pdf/2510.26646v1.pdf","comment":"6 pages, 5 figures; ROS+Gazebo (TurtleBot3) implementation;\n  evaluation with PathBench metrics; code (primary):\n  https://github.com/MayaCHEN-github/HierarchicalRL-robot-navigation; mirror\n  (for reproducibility): https://github.com/ShowyHe/DRL-robot-navigation"},{"id":"http://arxiv.org/abs/2510.14889v2","updated":"2025-10-30T16:09:51Z","published":"2025-10-16T17:09:14Z","title":"Detecting Early and Implicit Suicidal Ideation via Longitudinal and\n  Information Environment Signals on Social Media","summary":"  On social media, many individuals experiencing suicidal ideation (SI) do not\ndisclose their distress explicitly. Instead, signs may surface indirectly\nthrough everyday posts or peer interactions. Detecting such implicit signals\nearly is critical but remains challenging. We frame early and implicit SI as a\nforward-looking prediction task and develop a computational framework that\nmodels a user's information environment, consisting of both their longitudinal\nposting histories as well as the discourse of their socially proximal peers. We\nadopted a composite network centrality measure to identify top neighbors of a\nuser, and temporally aligned the user's and neighbors' interactions --\nintegrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a\nReddit study of 1,000 (500 Case and 500 Control) users, our approach improves\nearly and implicit SI detection by 15% over individual-only baselines. These\nfindings highlight that peer interactions offer valuable predictive signals and\ncarry broader implications for designing early detection systems that capture\nindirect as well as masked expressions of risk in online environments.\n","authors":["Soorya Ram Shimgekar","Ruining Zhao","Agam Goyal","Violeta J. Rodriguez","Paul A. Bloom","Hari Sundaram","Koustuv Saha"],"pdf_url":"https://arxiv.org/pdf/2510.14889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24519v2","updated":"2025-10-30T15:42:34Z","published":"2025-10-28T15:31:52Z","title":"Audio Signal Processing Using Time Domain Mel-Frequency Wavelet\n  Coefficient","summary":"  Extracting features from the speech is the most critical process in speech\nsignal processing. Mel Frequency Cepstral Coefficients (MFCC) are the most\nwidely used features in the majority of the speaker and speech recognition\napplications, as the filtering in this feature is similar to the filtering\ntaking place in the human ear. But the main drawback of this feature is that it\nprovides only the frequency information of the signal but does not provide the\ninformation about at what time which frequency is present. The wavelet\ntransform, with its flexible time-frequency window, provides time and frequency\ninformation of the signal and is an appropriate tool for the analysis of\nnon-stationary signals like speech. On the other hand, because of its uniform\nfrequency scaling, a typical wavelet transform may be less effective in\nanalysing speech signals, have poorer frequency resolution in low frequencies,\nand be less in line with human auditory perception. Hence, it is necessary to\ndevelop a feature that incorporates the merits of both MFCC and wavelet\ntransform. A great deal of studies are trying to combine both these features.\nThe present Wavelet Transform based Mel-scaled feature extraction methods\nrequire more computation when a wavelet transform is applied on top of\nMel-scale filtering, since it adds extra processing steps. Here we are\nproposing a method to extract Mel scale features in time domain combining the\nconcept of wavelet transform, thus reducing the computational burden of\ntime-frequency conversion and the complexity of wavelet extraction. Combining\nour proposed Time domain Mel frequency Wavelet Coefficient(TMFWC) technique\nwith the reservoir computing methodology has significantly improved the\nefficiency of audio signal processing.\n","authors":["Rinku Sebastian","Simon O'Keefe","Martin Trefzer"],"pdf_url":"https://arxiv.org/pdf/2510.24519v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26616v1","updated":"2025-10-30T15:41:43Z","published":"2025-10-30T15:41:43Z","title":"Aeolus: A Multi-structural Flight Delay Dataset","summary":"  We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed\nto advance research on flight delay prediction and support the development of\nfoundation models for tabular data. Existing datasets in this domain are\ntypically limited to flat tabular structures and fail to capture the\nspatiotemporal dynamics inherent in delay propagation. Aeolus addresses this\nlimitation by providing three aligned modalities: (i) a tabular dataset with\nrich operational, meteorological, and airportlevel features for over 50 million\nflights; (ii) a flight chain module that models delay propagation along\nsequential flight legs, capturing upstream and downstream dependencies; and\n(iii) a flight network graph that encodes shared aircraft, crew, and airport\nresource connections, enabling cross-flight relational reasoning. The dataset\nis carefully constructed with temporal splits, comprehensive features, and\nstrict leakage prevention to support realistic and reproducible machine\nlearning evaluation. Aeolus supports a broad range of tasks, including\nregression, classification, temporal structure modeling, and graph learning,\nserving as a unified benchmark across tabular, sequential, and graph\nmodalities. We release baseline experiments and preprocessing tools to\nfacilitate adoption. Aeolus fills a key gap for both domain-specific modeling\nand general-purpose structured data research.Our source code and data can be\naccessed at https://github.com/Flnny/Delay-data\n","authors":["Lin Xu","Xinyun Yuan","Yuxuan Liang","Suwan Yin","Yuankai Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14904v2","updated":"2025-10-30T15:39:25Z","published":"2025-10-16T17:20:22Z","title":"MaskCaptioner: Learning to Jointly Segment and Caption Object\n  Trajectories in Videos","summary":"  Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.\n","authors":["Gabriel Fiastre","Antoine Yang","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2510.14904v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.26606v1","updated":"2025-10-30T15:35:13Z","published":"2025-10-30T15:35:13Z","title":"Normative Reasoning in Large Language Models: A Comparative Benchmark\n  from Logical and Modal Perspectives","summary":"  Normative reasoning is a type of reasoning that involves normative or deontic\nmodality, such as obligation and permission. While large language models (LLMs)\nhave demonstrated remarkable performance across various reasoning tasks, their\nability to handle normative reasoning remains underexplored. In this paper, we\nsystematically evaluate LLMs' reasoning capabilities in the normative domain\nfrom both logical and modal perspectives. Specifically, to assess how well LLMs\nreason with normative modals, we make a comparison between their reasoning with\nnormative modals and their reasoning with epistemic modals, which share a\ncommon formal structure. To this end, we introduce a new dataset covering a\nwide range of formal patterns of reasoning in both normative and epistemic\ndomains, while also incorporating non-formal cognitive factors that influence\nhuman reasoning. Our results indicate that, although LLMs generally adhere to\nvalid reasoning patterns, they exhibit notable inconsistencies in specific\ntypes of normative reasoning and display cognitive biases similar to those\nobserved in psychological studies of human reasoning. These findings highlight\nchallenges in achieving logical consistency in LLMs' normative reasoning and\nprovide insights for enhancing their reliability. All data and code are\nreleased publicly at https://github.com/kmineshima/NeuBAROCO.\n","authors":["Kentaro Ozeki","Risako Ando","Takanobu Morishita","Hirohiko Abe","Koji Mineshima","Mitsuhiro Okada"],"pdf_url":"https://arxiv.org/pdf/2510.26606v1.pdf","comment":"Accepted to the 8th BlackboxNLP Workshop at EMNLP 2025"},{"id":"http://arxiv.org/abs/2510.08604v2","updated":"2025-10-30T15:33:58Z","published":"2025-10-07T09:40:20Z","title":"LatentBreak: Jailbreaking Large Language Models through Latent Space\n  Feedback","summary":"  Jailbreaks are adversarial attacks designed to bypass the built-in safety\nmechanisms of large language models. Automated jailbreaks typically optimize an\nadversarial suffix or adapt long prompt templates by forcing the model to\ngenerate the initial part of a restricted or harmful response. In this work, we\nshow that existing jailbreak attacks that leverage such mechanisms to unlock\nthe model response can be detected by a straightforward perplexity-based\nfiltering on the input prompt. To overcome this issue, we propose LatentBreak,\na white-box jailbreak attack that generates natural adversarial prompts with\nlow perplexity capable of evading such defenses. LatentBreak substitutes words\nin the input prompt with semantically-equivalent ones, preserving the initial\nintent of the prompt, instead of adding high-perplexity adversarial suffixes or\nlong templates. These words are chosen by minimizing the distance in the latent\nspace between the representation of the adversarial prompt and that of harmless\nrequests. Our extensive evaluation shows that LatentBreak leads to shorter and\nlow-perplexity prompts, thus outperforming competing jailbreak algorithms\nagainst perplexity-based filters on multiple safety-aligned models.\n","authors":["Raffaele Mura","Giorgio Piras","Kamilė Lukošiūtė","Maura Pintor","Amin Karbasi","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2510.08604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26603v1","updated":"2025-10-30T15:33:52Z","published":"2025-10-30T15:33:52Z","title":"Agentic AI Home Energy Management System: A Large Language Model\n  Framework for Residential Load Scheduling","summary":"  The electricity sector transition requires substantial increases in\nresidential demand response capacity, yet Home Energy Management Systems (HEMS)\nadoption remains limited by user interaction barriers requiring translation of\neveryday preferences into technical parameters. While large language models\nhave been applied to energy systems as code generators and parameter\nextractors, no existing implementation deploys LLMs as autonomous coordinators\nmanaging the complete workflow from natural language input to multi-appliance\nscheduling. This paper presents an agentic AI HEMS where LLMs autonomously\ncoordinate multi-appliance scheduling from natural language requests to device\ncontrol, achieving optimal scheduling without example demonstrations. A\nhierarchical architecture combining one orchestrator with three specialist\nagents uses the ReAct pattern for iterative reasoning, enabling dynamic\ncoordination without hardcoded workflows while integrating Google Calendar for\ncontext-aware deadline extraction. Evaluation across three open-source models\nusing real Austrian day-ahead electricity prices reveals substantial capability\ndifferences. Llama-3.3-70B successfully coordinates all appliances across all\nscenarios to match cost-optimal benchmarks computed via mixed-integer linear\nprogramming, while other models achieve perfect single-appliance performance\nbut struggle to coordinate all appliances simultaneously. Progressive prompt\nengineering experiments demonstrate that analytical query handling without\nexplicit guidance remains unreliable despite models' general reasoning\ncapabilities. We open-source the complete system including orchestration logic,\nagent prompts, tools, and web interfaces to enable reproducibility, extension,\nand future research.\n","authors":["Reda El Makroum","Sebastian Zwickl-Bernhard","Lukas Kranzl"],"pdf_url":"https://arxiv.org/pdf/2510.26603v1.pdf","comment":"34 pages, 9 figures. Code available at\n  https://github.com/RedaElMakroum/agentic-ai-hems"},{"id":"http://arxiv.org/abs/2510.26601v1","updated":"2025-10-30T15:29:20Z","published":"2025-10-30T15:29:20Z","title":"ResMatching: Noise-Resilient Computational Super-Resolution via Guided\n  Conditional Flow Matching","summary":"  Computational Super-Resolution (CSR) in fluorescence microscopy has, despite\nbeing an ill-posed problem, a long history. At its very core, CSR is about\nfinding a prior that can be used to extrapolate frequencies in a micrograph\nthat have never been imaged by the image-generating microscope. It stands to\nreason that, with the advent of better data-driven machine learning techniques,\nstronger prior can be learned and hence CSR can lead to better results. Here,\nwe present ResMatching, a novel CSR method that uses guided conditional flow\nmatching to learn such improved data-priors. We evaluate ResMatching on 4\ndiverse biological structures from the BioSR dataset and compare its results\nagainst 7 baselines. ResMatching consistently achieves competitive results,\ndemonstrating in all cases the best trade-off between data fidelity and\nperceptual realism. We observe that CSR using ResMatching is particularly\neffective in cases where a strong prior is hard to learn, e.g. when the given\nlow-resolution images contain a lot of noise. Additionally, we show that\nResMatching can be used to sample from an implicitly learned posterior\ndistribution and that this distribution is calibrated for all tested use-cases,\nenabling our method to deliver a pixel-wise data-uncertainty term that can\nguide future users to reject uncertain predictions.\n","authors":["Anirban Ray","Vera Galinova","Florian Jug"],"pdf_url":"https://arxiv.org/pdf/2510.26601v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2509.17197v2","updated":"2025-10-30T15:26:13Z","published":"2025-09-21T18:54:54Z","title":"SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal\n  Processing","summary":"  Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings.\n","authors":["Junlong Ke","Qiying Hu","Shenghai Yuan","Yuecong Xu","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2509.17197v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2510.26585v1","updated":"2025-10-30T15:12:59Z","published":"2025-10-30T15:12:59Z","title":"Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems","summary":"  While Multi-Agent Systems (MAS) excel at complex tasks, their growing\nautonomy with operational complexity often leads to critical inefficiencies,\nsuch as excessive token consumption and failures arising from misinformation.\nExisting methods primarily focus on post-hoc failure attribution, lacking\nproactive, real-time interventions to enhance robustness and efficiency. To\nthis end, we introduce SupervisorAgent, a lightweight and modular framework for\nruntime, adaptive supervision that operates without altering the base agent's\narchitecture. Triggered by an LLM-free adaptive filter, SupervisorAgent\nintervenes at critical junctures to proactively correct errors, guide\ninefficient behaviors, and purify observations. On the challenging GAIA\nbenchmark, SupervisorAgent reduces the token consumption of the Smolagent\nframework by an average of 29.45% without compromising its success rate.\nExtensive experiments across five additional benchmarks (math reasoning, code\ngeneration, and question answering) and various SoTA foundation models validate\nthe broad applicability and robustness of our approach. The code is available\nat https://github.com/LINs-lab/SupervisorAgent.\n","authors":["Fulin Lin","Shaowen Chen","Ruishan Fang","Hongwei Wang","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2510.26585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17489v2","updated":"2025-10-30T15:09:59Z","published":"2024-07-03T13:46:00Z","title":"AI's Social Forcefield: Reshaping Distributed Cognition in Human-AI\n  Teams","summary":"  AI is not only a neutral tool in team settings; it actively reshapes the\nsocial and cognitive fabric of collaboration. We advance a unified framework of\nalignment in distributed cognition in human-AI teams -- a process through which\nlinguistic, cognitive, and social coordination emerge as human and AI agents\nco-construct a shared representational space. Across two studies, we show that\nexposure to AI-generated language shapes not only how people speak, but also\nhow they think, what they attend to, and how they relate to each other.\nTogether, these findings reveal how AI participation reorganizes the\ndistributed cognitive architecture of teams: AI systems function as implicit\nsocial forcefields. Our findings highlight the double-edged impact of AI: the\nsame mechanisms that enable efficient collaboration can also erode epistemic\ndiversity and undermine natural alignment processes. We argue for rethinking AI\nin teams as a socially influential actor and call for new design paradigms that\nforeground transparency, controllability, and group-level dynamics to foster\nresponsible, productive human-AI collaboration.\n","authors":["Christoph Riedl","Saiph Savage","Josie Zvelebilova"],"pdf_url":"https://arxiv.org/pdf/2407.17489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26575v1","updated":"2025-10-30T15:03:21Z","published":"2025-10-30T15:03:21Z","title":"InfoFlow: Reinforcing Search Agent Via Reward Density Optimization","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach\nfor enhancing agentic deep search. However, its application is often hindered\nby low \\textbf{Reward Density} in deep search scenarios, where agents expend\nsignificant exploratory costs for infrequent and often null final rewards. In\nthis paper, we formalize this challenge as the \\textbf{Reward Density\nOptimization} problem, which aims to improve the reward obtained per unit of\nexploration cost. This paper introduce \\textbf{InfoFlow}, a systematic\nframework that tackles this problem from three aspects. 1) \\textbf{Subproblem\ndecomposition}: breaking down long-range tasks to assign process rewards,\nthereby providing denser learning signals. 2) \\textbf{Failure-guided hints}:\ninjecting corrective guidance into stalled trajectories to increase the\nprobability of successful outcomes. 3) \\textbf{Dual-agent refinement}:\nemploying a dual-agent architecture to offload the cognitive burden of deep\nexploration. A refiner agent synthesizes the search history, which effectively\ncompresses the researcher's perceived trajectory, thereby reducing exploration\ncost and increasing the overall reward density. We evaluate InfoFlow on\nmultiple agentic search benchmarks, where it significantly outperforms strong\nbaselines, enabling lightweight LLMs to achieve performance comparable to\nadvanced proprietary LLMs.\n","authors":["Kun Luo","Hongjin Qian","Zheng Liu","Ziyi Xia","Shitao Xiao","Siqi Bao","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26566v1","updated":"2025-10-30T14:56:07Z","published":"2025-10-30T14:56:07Z","title":"Multiclass Local Calibration With the Jensen-Shannon Distance","summary":"  Developing trustworthy Machine Learning (ML) models requires their predicted\nprobabilities to be well-calibrated, meaning they should reflect true-class\nfrequencies. Among calibration notions in multiclass classification, strong\ncalibration is the most stringent, as it requires all predicted probabilities\nto be simultaneously calibrated across all classes. However, existing\napproaches to multiclass calibration lack a notion of distance among inputs,\nwhich makes them vulnerable to proximity bias: predictions in sparse regions of\nthe feature space are systematically miscalibrated. This is especially relevant\nin high-stakes settings, such as healthcare, where the sparse instances are\nexactly those most at risk of biased treatment. In this work, we address this\nmain shortcoming by introducing a local perspective on multiclass calibration.\nFirst, we formally define multiclass local calibration and establish its\nrelationship with strong calibration. Second, we theoretically analyze the\npitfalls of existing evaluation metrics when applied to multiclass local\ncalibration. Third, we propose a practical method for enhancing local\ncalibration in Neural Networks, which enforces alignment between predicted\nprobabilities and local estimates of class frequencies using the Jensen-Shannon\ndistance. Finally, we empirically validate our approach against existing\nmulticlass calibration techniques.\n","authors":["Cesare Barbera","Lorenzo Perini","Giovanni De Toni","Andrea Passerini","Andrea Pugnana"],"pdf_url":"https://arxiv.org/pdf/2510.26566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.04226v4","updated":"2025-10-30T14:52:48Z","published":"2025-10-05T14:29:15Z","title":"Epistemic Diversity and Knowledge Collapse in Large Language Models","summary":"  Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation\n","authors":["Dustin Wright","Sarah Masud","Jared Moore","Srishti Yadav","Maria Antoniak","Chan Young Park","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2510.04226v4.pdf","comment":"16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for\n  table 3, random effect is the model version; v3 changelog: Fixed minor\n  formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model\n  description"},{"id":"http://arxiv.org/abs/2506.01369v2","updated":"2025-10-30T14:45:40Z","published":"2025-06-02T06:54:29Z","title":"Incentivizing LLMs to Self-Verify Their Answers","summary":"  Large Language Models (LLMs) have demonstrated remarkable progress in complex\nreasoning tasks through both post-training and test-time scaling laws. While\nprevalent test-time scaling approaches are often realized by using external\nreward models to guide the model generation process, we find that only marginal\ngains can be acquired when scaling a model post-trained on specific reasoning\ntasks. We identify that the limited improvement stems from distribution\ndiscrepancies between the specific post-trained generator and the general\nreward model. To address this, we propose a framework that incentivizes LLMs to\nself-verify their own answers. By unifying answer generation and verification\nwithin a single reinforcement learning (RL) process, we train models that can\neffectively assess the correctness of their own solutions. The trained model\ncan further scale its performance at inference time by verifying its\ngenerations, without the need for external verifiers. We train our\nself-verification models based on Qwen2.5-Math-7B and\nDeepSeek-R1-Distill-Qwen-1.5B, demonstrating their capabilities across varying\nreasoning context lengths. Experiments on multiple mathematical reasoning\nbenchmarks show that our models can not only improve post-training performance\nbut also enable effective test-time scaling.\n","authors":["Fuxiang Zhang","Jiacheng Xu","Chaojie Wang","Ce Cui","Yang Liu","Bo An"],"pdf_url":"https://arxiv.org/pdf/2506.01369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23216v3","updated":"2025-10-30T14:45:38Z","published":"2025-10-27T11:06:00Z","title":"Human-Like Goalkeeping in a Realistic Football Simulation: a\n  Sample-Efficient Reinforcement Learning Approach","summary":"  While several high profile video games have served as testbeds for Deep\nReinforcement Learning (DRL), this technique has rarely been employed by the\ngame industry for crafting authentic AI behaviors. Previous research focuses on\ntraining super-human agents with large models, which is impractical for game\nstudios with limited resources aiming for human-like agents. This paper\nproposes a sample-efficient DRL method tailored for training and fine-tuning\nagents in industrial settings such as the video game industry. Our method\nimproves sample efficiency of value-based DRL by leveraging pre-collected data\nand increasing network plasticity. We evaluate our method training a goalkeeper\nagent in EA SPORTS FC 25, one of the best-selling football simulations today.\nOur agent outperforms the game's built-in AI by 10% in ball saving rate.\nAblation studies show that our method trains agents 50% faster compared to\nstandard DRL methods. Finally, qualitative evaluation from domain experts\nindicates that our approach creates more human-like gameplay compared to\nhand-crafted agents. As a testament to the impact of the approach, the method\nhas been adopted for use in the most recent release of the series.\n","authors":["Alessandro Sestini","Joakim Bergdahl","Jean-Philippe Barrette-LaPierre","Florian Fuchs","Brady Chen","Michael Jones","Linus Gisslén"],"pdf_url":"https://arxiv.org/pdf/2510.23216v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26551v1","updated":"2025-10-30T14:44:24Z","published":"2025-10-30T14:44:24Z","title":"Adaptive Inverse Kinematics Framework for Learning Variable-Length Tool\n  Manipulation in Robotics","summary":"  Conventional robots possess a limited understanding of their kinematics and\nare confined to preprogrammed tasks, hindering their ability to leverage tools\nefficiently. Driven by the essential components of tool usage - grasping the\ndesired outcome, selecting the most suitable tool, determining optimal tool\norientation, and executing precise manipulations - we introduce a pioneering\nframework. Our novel approach expands the capabilities of the robot's inverse\nkinematics solver, empowering it to acquire a sequential repertoire of actions\nusing tools of varying lengths. By integrating a simulation-learned action\ntrajectory with the tool, we showcase the practicality of transferring acquired\nskills from simulation to real-world scenarios through comprehensive\nexperimentation. Remarkably, our extended inverse kinematics solver\ndemonstrates an impressive error rate of less than 1 cm. Furthermore, our\ntrained policy achieves a mean error of 8 cm in simulation. Noteworthy, our\nmodel achieves virtually indistinguishable performance when employing two\ndistinct tools of different lengths. This research provides an indication of\npotential advances in the exploration of all four fundamental aspects of tool\nusage, enabling robots to master the intricate art of tool manipulation across\ndiverse tasks.\n","authors":["Prathamesh Kothavale","Sravani Boddepalli"],"pdf_url":"https://arxiv.org/pdf/2510.26551v1.pdf","comment":"10 pages, 5 figures. Demonstrates a reinforcement learning framework\n  for adaptive tool manipulation with variable-length extensions"},{"id":"http://arxiv.org/abs/2510.26550v1","updated":"2025-10-30T14:43:26Z","published":"2025-10-30T14:43:26Z","title":"EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the\n  Edge","summary":"  We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for\nmilitary tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated\nfrom military documentation and websites. We also present four new tests sets:\n(a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k\n(general military knowledge). On these military test sets, EdgeRunner 20B\nmatches or exceeds GPT-5 task performance with 95%+ statistical significance,\nexcept for the high reasoning setting on the combat medic test set and the low\nreasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no\nstatistically-significant regression on general-purpose benchmarks like ARC-C,\nGPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the\nlow reasoning setting. We also present analyses on hyperparameter settings,\ncost, and throughput. These findings show that small, locally-hosted models are\nideal solutions for data-sensitive operations such as in the military domain,\nallowing for deployment in air-gapped edge devices.\n","authors":["Jack FitzGerald","Aristotelis Lazaridis","Dylan Bates","Aman Sharma","Jonnathan Castillo","Yousif Azami","Sean Bailey","Jeremy Cao","Peter Damianov","Kevin de Haan","Luke Kerbs","Vincent Lu","Joseph Madigan","Jeremy McLaurin","Jonathan Tainer","Dave Anderson","Jonathan Beck","Jamie Cuticello","Colton Malkerson","Tyler Saltsman"],"pdf_url":"https://arxiv.org/pdf/2510.26550v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2510.26543v1","updated":"2025-10-30T14:36:09Z","published":"2025-10-30T14:36:09Z","title":"The Structure of Relation Decoding Linear Operators in Large Language\n  Models","summary":"  This paper investigates the structure of linear operators introduced in\nHernandez et al. [2023] that decode specific relational facts in transformer\nlanguage models. We extend their single-relation findings to a collection of\nrelations and systematically chart their organization. We show that such\ncollections of relation decoders can be highly compressed by simple order-3\ntensor networks without significant loss in decoding accuracy. To explain this\nsurprising redundancy, we develop a cross-evaluation protocol, in which we\napply each linear decoder operator to the subjects of every other relation. Our\nresults reveal that these linear maps do not encode distinct relations, but\nextract recurring, coarse-grained semantic properties (e.g., country of capital\ncity and country of food are both in the country-of-X property). This\nproperty-centric structure clarifies both the operators' compressibility and\nhighlights why they generalize only to new relations that are semantically\nclose. Our findings thus interpret linear relational decoding in transformer\nlanguage models as primarily property-based, rather than relation-specific.\n","authors":["Miranda Anna Christ","Adrián Csiszárik","Gergely Becsó","Dániel Varga"],"pdf_url":"https://arxiv.org/pdf/2510.26543v1.pdf","comment":"NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2406.08525v2","updated":"2025-10-30T14:25:46Z","published":"2024-06-12T07:33:38Z","title":"A mathematical certification for positivity conditions in Neural\n  Networks with applications to partial monotonicity and Trustworthy AI","summary":"  Artificial Neural Networks (ANNs) have become a powerful tool for modeling\ncomplex relationships in large-scale datasets. However, their black-box nature\nposes trustworthiness challenges. In certain situations, ensuring trust in\npredictions might require following specific partial monotonicity constraints.\nHowever, certifying if an already-trained ANN is partially monotonic is\nchallenging. Therefore, ANNs are often disregarded in some critical\napplications, such as credit scoring, where partial monotonicity is required.\nTo address this challenge, this paper presents a novel algorithm (LipVor) that\ncertifies if a black-box model, such as an ANN, is positive based on a finite\nnumber of evaluations. Consequently, since partial monotonicity can be\nexpressed as a positivity condition on partial derivatives, LipVor can certify\nwhether an ANN is partially monotonic. To do so, for every positively evaluated\npoint, the Lipschitzianity of the black-box model is used to construct a\nspecific neighborhood where the function remains positive. Next, based on the\nVoronoi diagram of the evaluated points, a sufficient condition is stated to\ncertify if the function is positive in the domain. Unlike prior methods, our\napproach certifies partial monotonicity without constrained architectures or\npiece-wise linear activations. Therefore, LipVor could open up the possibility\nof using unconstrained ANN in some critical fields. Moreover, some other\nproperties of an ANN, such as convexity, can be posed as positivity conditions,\nand therefore, LipVor could also be applied.\n","authors":["Alejandro Polo-Molina","David Alfaya","Jose Portela"],"pdf_url":"https://arxiv.org/pdf/2406.08525v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.26518v1","updated":"2025-10-30T14:11:52Z","published":"2025-10-30T14:11:52Z","title":"Human-AI Complementarity: A Goal for Amplified Oversight","summary":"  Human feedback is critical for aligning AI systems to human values. As AI\ncapabilities improve and AI is used to tackle more challenging tasks, verifying\nquality and safety becomes increasingly challenging. This paper explores how we\ncan leverage AI to improve the quality of human oversight. We focus on an\nimportant safety problem that is already challenging for humans:\nfact-verification of AI outputs. We find that combining AI ratings and human\nratings based on AI rater confidence is better than relying on either alone.\nGiving humans an AI fact-verification assistant further improves their\naccuracy, but the type of assistance matters. Displaying AI explanation,\nconfidence, and labels leads to over-reliance, but just showing search results\nand evidence fosters more appropriate trust. These results have implications\nfor Amplified Oversight -- the challenge of combining humans and AI to\nsupervise AI systems even as they surpass human expert performance.\n","authors":["Rishub Jain","Sophie Bridgers","Lili Janzer","Rory Greig","Tian Huey Teh","Vladimir Mikulik"],"pdf_url":"https://arxiv.org/pdf/2510.26518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.15030v3","updated":"2025-10-30T14:10:33Z","published":"2025-08-20T19:49:06Z","title":"Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations\n  in Tourism","summary":"  We propose Collab-REC, a multi-agent framework designed to counteract\npopularity bias and enhance diversity in tourism recommendations. In our\nsetting, three LLM-based agents -- Personalization, Popularity, and\nSustainability generate city suggestions from complementary perspectives. A\nnon-LLM moderator then merges and refines these proposals via multi-round\nnegotiation, ensuring each agent's viewpoint is incorporated while penalizing\nspurious or repeated responses. Experiments on European city queries show that\nCollab-REC improves diversity and overall relevance compared to a single-agent\nbaseline, surfacing lesser-visited locales that often remain overlooked. This\nbalanced, context-aware approach addresses over-tourism and better aligns with\nconstraints provided by the user, highlighting the promise of multi-stakeholder\ncollaboration in LLM-driven recommender systems.\n","authors":["Ashmi Banerjee","Adithi Satish","Fitri Nur Aisyah","Wolfgang Wörndl","Yashar Deldjoo"],"pdf_url":"https://arxiv.org/pdf/2508.15030v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26512v1","updated":"2025-10-30T14:05:55Z","published":"2025-10-30T14:05:55Z","title":"Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs","summary":"  Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts.\n","authors":["Dipak Meher","Carlotta Domeniconi"],"pdf_url":"https://arxiv.org/pdf/2510.26512v1.pdf","comment":"ICDM 2025 Workshop"},{"id":"http://arxiv.org/abs/2501.05783v2","updated":"2025-10-30T14:04:15Z","published":"2025-01-10T08:33:31Z","title":"UV-Attack: Physical-World Adversarial Attacks for Person Detection via\n  Dynamic-NeRF-based UV Mapping","summary":"  In recent research, adversarial attacks on person detectors using patches or\nstatic 3D model-based texture modifications have struggled with low success\nrates due to the flexible nature of human movement. Modeling the 3D\ndeformations caused by various actions has been a major challenge. Fortunately,\nadvancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer\nnew possibilities. In this paper, we introduce UV-Attack, a groundbreaking\napproach that achieves high success rates even with extensive and unseen human\nactions. We address the challenge above by leveraging dynamic-NeRF-based UV\nmapping. UV-Attack can generate human images across diverse actions and\nviewpoints, and even create novel actions by sampling from the SMPL parameter\nspace. While dynamic NeRF models are capable of modeling human bodies,\nmodifying clothing textures is challenging because they are embedded in neural\nnetwork parameters. To tackle this, UV-Attack generates UV maps instead of RGB\nimages and modifies the texture stacks. This approach enables real-time texture\nedits and makes the attack more practical. We also propose a novel Expectation\nover Pose Transformation loss (EoPT) to improve the evasion success rate on\nunseen poses and views. Our experiments show that UV-Attack achieves a 92.7%\nattack success rate against the FastRCNN model across varied poses in dynamic\nvideo settings, significantly outperforming the state-of-the-art AdvCamou\nattack, which only had a 28.5% ASR. Moreover, we achieve 49.5% ASR on the\nlatest YOLOv8 detector in black-box settings. This work highlights the\npotential of dynamic NeRF-based UV mapping for creating more effective\nadversarial attacks on person detectors, addressing key challenges in modeling\nhuman movement and texture modification. The code is available at\nhttps://github.com/PolyLiYJ/UV-Attack.\n","authors":["Yanjie Li","Kaisheng Liang","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2501.05783v2.pdf","comment":"23 pages, 22 figures, accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2505.11730v2","updated":"2025-10-30T13:52:37Z","published":"2025-05-16T22:24:48Z","title":"Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling","summary":"  Test-time scaling (TTS) has proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Verification plays a key role in\nTTS, simultaneously influencing (1) reasoning performance and (2) compute\nefficiency, due to the quality and computational cost of verification. In this\nwork, we challenge the conventional paradigms of verification, and make the\nfirst attempt toward systematically investigating the impact of verification\ngranularity-that is, how frequently the verifier is invoked during generation,\nbeyond verifying only the final output or individual generation steps. To this\nend, we introduce Variable Granularity Search (VG-Search), a unified algorithm\nthat generalizes beam search and Best-of-N sampling via a tunable granularity\nparameter g. Extensive experiments with VG-Search under varying compute\nbudgets, generator-verifier configurations, and task attributes reveal that\ndynamically selecting g can improve the compute efficiency and scaling\nbehavior. Building on these findings, we propose adaptive VG-Search strategies\nthat achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over\nBest-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to\nsupport future research.\n","authors":["Hao Mark Chen","Guanxi Lu","Yasuyuki Okoshi","Zhiwen Mo","Masato Motomura","Hongxiang Fan"],"pdf_url":"https://arxiv.org/pdf/2505.11730v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2503.00333v2","updated":"2025-10-30T13:47:16Z","published":"2025-03-01T03:45:35Z","title":"More of the Same: Persistent Representational Harms Under Increased\n  Representation","summary":"  To recognize and mitigate the harms of generative AI systems, it is crucial\nto consider who is represented in the outputs of generative AI systems and how\npeople are represented. A critical gap emerges when naively improving who is\nrepresented, as this does not imply bias mitigation efforts have been applied\nto address how people are represented. We critically examined this by\ninvestigating gender representation in occupation across state-of-the-art large\nlanguage models. We first show evidence suggesting that over time there have\nbeen interventions to models altering the resulting gender distribution, and we\nfind that women are more represented than men when models are prompted to\ngenerate biographies or personas. We then demonstrate that representational\nbiases persist in how different genders are represented by examining\nstatistically significant word differences across genders. This results in a\nproliferation of representational harms, stereotypes, and neoliberalism ideals\nthat, despite existing interventions to increase female representation,\nreinforce existing systems of oppression.\n","authors":["Jennifer Mickel","Maria De-Arteaga","Leqi Liu","Kevin Tian"],"pdf_url":"https://arxiv.org/pdf/2503.00333v2.pdf","comment":"Accepted by the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) as a poster paper; 39 pages, 7 figures, 15 tables"},{"id":"http://arxiv.org/abs/2510.26494v1","updated":"2025-10-30T13:43:28Z","published":"2025-10-30T13:43:28Z","title":"Simulating and Experimenting with Social Media Mobilization Using LLM\n  Agents","summary":"  Online social networks have transformed the ways in which political\nmobilization messages are disseminated, raising new questions about how peer\ninfluence operates at scale. Building on the landmark 61-million-person\nFacebook experiment \\citep{bond201261}, we develop an agent-based simulation\nframework that integrates real U.S. Census demographic distributions, authentic\nTwitter network topology, and heterogeneous large language model (LLM) agents\nto examine the effect of mobilization messages on voter turnout. Each simulated\nagent is assigned demographic attributes, a personal political stance, and an\nLLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano})\nreflecting its political sophistication. Agents interact over realistic social\nnetwork structures, receiving personalized feeds and dynamically updating their\nengagement behaviors and voting intentions. Experimental conditions replicate\nthe informational and social mobilization treatments of the original Facebook\nstudy. Across scenarios, the simulator reproduces qualitative patterns observed\nin field experiments, including stronger mobilization effects under social\nmessage treatments and measurable peer spillovers. Our framework provides a\ncontrolled, reproducible environment for testing counterfactual designs and\nsensitivity analyses in political mobilization research, offering a bridge\nbetween high-validity field experiments and flexible computational\nmodeling.\\footnote{Code and data available at\nhttps://github.com/CausalMP/LLM-SocioPol}\n","authors":["Sadegh Shirani","Mohsen Bayati"],"pdf_url":"https://arxiv.org/pdf/2510.26494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26493v1","updated":"2025-10-30T13:43:10Z","published":"2025-10-30T13:43:10Z","title":"Context Engineering 2.0: The Context of Context Engineering","summary":"  Karl Marx once wrote that ``the human essence is the ensemble of social\nrelations'', suggesting that individuals are not isolated entities but are\nfundamentally shaped by their interactions with other entities, within which\ncontexts play a constitutive and essential role. With the advent of computers\nand artificial intelligence, these contexts are no longer limited to purely\nhuman--human interactions: human--machine interactions are included as well.\nThen a central question emerges: How can machines better understand our\nsituations and purposes? To address this challenge, researchers have recently\nintroduced the concept of context engineering. Although it is often regarded as\na recent innovation of the agent era, we argue that related practices can be\ntraced back more than twenty years. Since the early 1990s, the field has\nevolved through distinct historical phases, each shaped by the intelligence\nlevel of machines: from early human--computer interaction frameworks built\naround primitive computers, to today's human--agent interaction paradigms\ndriven by intelligent agents, and potentially to human--level or superhuman\nintelligence in the future. In this paper, we situate context engineering,\nprovide a systematic definition, outline its historical and conceptual\nlandscape, and examine key design considerations for practice. By addressing\nthese questions, we aim to offer a conceptual foundation for context\nengineering and sketch its promising future. This paper is a stepping stone for\na broader community effort toward systematic context engineering in AI systems.\n","authors":["Qishuo Hua","Lyumanshan Ye","Dayuan Fu","Yang Xiao","Xiaojie Cai","Yunze Wu","Jifan Lin","Junfei Wang","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08788v2","updated":"2025-10-30T13:41:48Z","published":"2024-01-30T09:05:38Z","title":"VerifIoU -- Robustness of Object Detection to Perturbations","summary":"  We introduce a novel Interval Bound Propagation (IBP) approach for the formal\nverification of object detection models, specifically targeting the\nIntersection over Union (IoU) metric. The approach has been implemented in an\nopen source code, named IBP IoU, compatible with popular abstract\ninterpretation based verification tools. The resulting verifier is evaluated on\nlanding approach runway detection and handwritten digit recognition case\nstudies. Comparisons against a baseline (Vanilla IBP IoU) highlight the\nsuperior performance of IBP IoU in ensuring accuracy and stability,\ncontributing to more secure and robust machine learning applications.\n","authors":["Noémie Cohen","Mélanie Ducoffe","Ryma Boumazouza","Christophe Gabreau","Claire Pagetti","Xavier Pucel","Audrey Galametz"],"pdf_url":"https://arxiv.org/pdf/2403.08788v2.pdf","comment":"44th Digital Avionics Systems Conference (DASC), Sep 2025, Montreal,\n  Canada"},{"id":"http://arxiv.org/abs/2510.26486v1","updated":"2025-10-30T13:39:08Z","published":"2025-10-30T13:39:08Z","title":"LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks","summary":"  Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.\n","authors":["Dipak Meher","Carlotta Domeniconi","Guadalupe Correa-Cabrera"],"pdf_url":"https://arxiv.org/pdf/2510.26486v1.pdf","comment":"Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures"},{"id":"http://arxiv.org/abs/2510.26484v1","updated":"2025-10-30T13:37:58Z","published":"2025-10-30T13:37:58Z","title":"Bayesian Network Fusion of Large Language Models for Sentiment Analysis","summary":"  Large language models (LLMs) continue to advance, with an increasing number\nof domain-specific variants tailored for specialised tasks. However, these\nmodels often lack transparency and explainability, can be costly to fine-tune,\nrequire substantial prompt engineering, yield inconsistent results across\ndomains, and impose significant adverse environmental impact due to their high\ncomputational demands. To address these challenges, we propose the Bayesian\nnetwork LLM fusion (BNLF) framework, which integrates predictions from three\nLLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic\nmechanism for sentiment analysis. BNLF performs late fusion by modelling the\nsentiment predictions from multiple LLMs as probabilistic nodes within a\nBayesian network. Evaluated across three human-annotated financial corpora with\ndistinct linguistic and contextual characteristics, BNLF demonstrates\nconsistent gains of about six percent in accuracy over the baseline LLMs,\nunderscoring its robustness to dataset variability and the effectiveness of\nprobabilistic fusion for interpretable sentiment classification.\n","authors":["Rasoul Amirzadeh","Dhananjay Thiruvady","Fatemeh Shiri"],"pdf_url":"https://arxiv.org/pdf/2510.26484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26481v1","updated":"2025-10-30T13:35:32Z","published":"2025-10-30T13:35:32Z","title":"Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections","summary":"  Large language models (LLMs) such as ChatGPT are increasingly integrated into\nhigh-stakes decision-making, yet little is known about their susceptibility to\nsocial influence. We conducted three preregistered conformity experiments with\nGPT-4o in a hiring context. In a baseline study, GPT consistently favored the\nsame candidate (Profile C), reported moderate expertise (M = 3.01) and high\ncertainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT\nfaced unanimous opposition from eight simulated partners and almost always\nconformed (99.9%), reporting lower certainty and significantly elevated\nself-reported informational and normative conformity (p < .001). In Study 2\n(GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of\ndisagreement trials, reporting less certainty and more normative conformity.\nAcross studies, results demonstrate that GPT does not act as an independent\nobserver but adapts to perceived social consensus. These findings highlight\nrisks of treating LLMs as neutral decision aids and underline the need to\nelicit AI judgments prior to exposing them to human opinions.\n","authors":["Clarissa Sabrina Arlinghaus","Tristan Kenneweg","Barbara Hammer","Günter W. Maier"],"pdf_url":"https://arxiv.org/pdf/2510.26481v1.pdf","comment":"5 pages, 5 figures, HAI 2025: Workshop on Socially Aware and\n  Cooperative Intelligent Systems"},{"id":"http://arxiv.org/abs/2505.12371v2","updated":"2025-10-30T13:27:07Z","published":"2025-05-18T11:28:17Z","title":"MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional\n  Methods for Diverse Medical Tasks","summary":"  The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/.\n","authors":["Yinghao Zhu","Ziyi He","Haoran Hu","Xiaochen Zheng","Xichen Zhang","Zixiang Wang","Junyi Gao","Liantao Ma","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2505.12371v2.pdf","comment":"Accepted by NeurIPS 2025 Datasets & Benchmarks Track"},{"id":"http://arxiv.org/abs/2510.26474v1","updated":"2025-10-30T13:26:58Z","published":"2025-10-30T13:26:58Z","title":"Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing","summary":"  Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.\n","authors":["Xin Guo","Zhiheng Xi","Yiwen Ding","Yitao Zhai","Xiaowei Shi","Xunliang Cai","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26474v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2510.02091v2","updated":"2025-10-30T13:22:53Z","published":"2025-10-02T14:57:13Z","title":"Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and\n  Reasoning","summary":"  Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models.\n","authors":["Xinyuan Song","Keyu Wang","PengXiang Li","Lu Yin","Shiwei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.02091v2.pdf","comment":"ICASSP 2025"},{"id":"http://arxiv.org/abs/2505.18766v2","updated":"2025-10-30T13:19:55Z","published":"2025-05-24T16:09:26Z","title":"StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks\n  by Style Perturbations","summary":"  Recently, text-to-image diffusion models have been widely used for style\nmimicry and personalized customization through methods such as DreamBooth and\nTextual Inversion. This has raised concerns about intellectual property\nprotection and the generation of deceptive content. Recent studies, such as\nGlaze and Anti-DreamBooth, have proposed using adversarial noise to protect\nimages from these attacks. However, recent purification-based methods, such as\nDiffPure and Noise Upscaling, have successfully attacked these latest defenses,\nshowing the vulnerabilities of these methods. Moreover, present methods show\nlimited transferability across models, making them less effective against\nunknown text-to-image models. To address these issues, we propose a novel\nanti-mimicry method, StyleGuard. We propose a novel style loss that optimizes\nthe style-related features in the latent space to make it deviate from the\noriginal image, which improves model-agnostic transferability. Additionally, to\nenhance the perturbation's ability to bypass diffusion-based purification, we\ndesigned a novel upscale loss that involves ensemble purifiers and upscalers\nduring training. Extensive experiments on the WikiArt and CelebA datasets\ndemonstrate that StyleGuard outperforms existing methods in robustness against\nvarious transformations and purifications, effectively countering style mimicry\nin various models. Moreover, StyleGuard is effective on different style mimicry\nmethods, including DreamBooth and Textual Inversion. The code is available at\nhttps://github.com/PolyLiYJ/StyleGuard.\n","authors":["Yanjie Li","Wenxuan Zhang","Xinqi Lyu","Yihao Liu","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2505.18766v2.pdf","comment":"Accepted by NIPS2025"},{"id":"http://arxiv.org/abs/2505.10603v2","updated":"2025-10-30T13:13:19Z","published":"2025-05-15T15:21:09Z","title":"Toward a Public and Secure Generative AI: A Comparative Analysis of Open\n  and Closed LLMs","summary":"  Generative artificial intelligence (Gen AI) systems represent a critical\ntechnology with far-reaching implications across multiple domains of society.\nHowever, their deployment entails a range of risks and challenges that require\ncareful evaluation. To date, there has been a lack of comprehensive,\ninterdisciplinary studies offering a systematic comparison between open-source\nand proprietary (closed) generative AI systems, particularly regarding their\nrespective advantages and drawbacks. This study aims to: i) critically evaluate\nand compare the characteristics, opportunities, and challenges of open and\nclosed generative AI models; and ii) propose foundational elements for the\ndevelopment of an Open, Public, and Safe Gen AI framework. As a methodology, we\nadopted a combined approach that integrates three methods: literature review,\ncritical analysis, and comparative analysis. The proposed framework outlines\nkey dimensions, openness, public governance, and security, as essential pillars\nfor shaping the future of trustworthy and inclusive Gen AI. Our findings reveal\nthat open models offer greater transparency, auditability, and flexibility,\nenabling independent scrutiny and bias mitigation. In contrast, closed systems\noften provide better technical support and ease of implementation, but at the\ncost of unequal access, accountability, and ethical oversight. The research\nalso highlights the importance of multi-stakeholder governance, environmental\nsustainability, and regulatory frameworks in ensuring responsible development.\n","authors":["Jorge Machado"],"pdf_url":"https://arxiv.org/pdf/2505.10603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26457v1","updated":"2025-10-30T13:06:11Z","published":"2025-10-30T13:06:11Z","title":"SecureReviewer: Enhancing Large Language Models for Secure Code Review\n  through Secure-aware Fine-tuning","summary":"  Identifying and addressing security issues during the early phase of the\ndevelopment lifecycle is critical for mitigating the long-term negative impacts\non software systems. Code review serves as an effective practice that enables\ndevelopers to check their teammates' code before integration into the codebase.\nTo streamline the generation of review comments, various automated code review\napproaches have been proposed, where LLM-based methods have significantly\nadvanced the capabilities of automated review generation. However, existing\nmodels primarily focus on general-purpose code review, their effectiveness in\nidentifying and addressing security-related issues remains underexplored.\nMoreover, adapting existing code review approaches to target security issues\nfaces substantial challenges, including data scarcity and inadequate evaluation\nmetrics. To address these limitations, we propose SecureReviewer, a new\napproach designed for enhancing LLMs' ability to identify and resolve\nsecurity-related issues during code review. Specifically, we first construct a\ndataset tailored for training and evaluating secure code review capabilities.\nLeveraging this dataset, we fine-tune LLMs to generate code review comments\nthat can effectively identify security issues and provide fix suggestions with\nour proposed secure-aware fine-tuning strategy. To mitigate hallucination in\nLLMs and enhance the reliability of their outputs, we integrate the RAG\ntechnique, which grounds the generated comments in domain-specific security\nknowledge. Additionally, we introduce SecureBLEU, a new evaluation metric\ndesigned to assess the effectiveness of review comments in addressing security\nissues. Experimental results demonstrate that SecureReviewer outperforms\nstate-of-the-art baselines in both security issue detection accuracy and the\noverall quality and practical utility of generated review comments.\n","authors":["Fang Liu","Simiao Liu","Yinghao Zhu","Xiaoli Lian","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26457v1.pdf","comment":"Accepted by ICSE 2026. Code and data:\n  https://github.com/SIMIAO515/SecureReviewer"},{"id":"http://arxiv.org/abs/2510.26451v1","updated":"2025-10-30T12:55:21Z","published":"2025-10-30T12:55:21Z","title":"Robust Graph Condensation via Classification Complexity Mitigation","summary":"  Graph condensation (GC) has gained significant attention for its ability to\nsynthesize smaller yet informative graphs. However, existing studies often\noverlook the robustness of GC in scenarios where the original graph is\ncorrupted. In such cases, we observe that the performance of GC deteriorates\nsignificantly, while existing robust graph learning technologies offer only\nlimited effectiveness. Through both empirical investigation and theoretical\nanalysis, we reveal that GC is inherently an intrinsic-dimension-reducing\nprocess, synthesizing a condensed graph with lower classification complexity.\nAlthough this property is critical for effective GC performance, it remains\nhighly vulnerable to adversarial perturbations. To tackle this vulnerability\nand improve GC robustness, we adopt the geometry perspective of graph data\nmanifold and propose a novel Manifold-constrained Robust Graph Condensation\nframework named MRGC. Specifically, we introduce three graph data manifold\nlearning modules that guide the condensed graph to lie within a smooth,\nlow-dimensional manifold with minimal class ambiguity, thereby preserving the\nclassification complexity reduction capability of GC and ensuring robust\nperformance under universal adversarial attacks. Extensive experiments\ndemonstrate the robustness of \\ModelName\\ across diverse attack scenarios.\n","authors":["Jiayi Luo","Qingyun Sun","Beining Yang","Haonan Yuan","Xingcheng Fu","Yanbiao Ma","Jianxin Li","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2510.26451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26444v1","updated":"2025-10-30T12:50:12Z","published":"2025-10-30T12:50:12Z","title":"Personalized Treatment Outcome Prediction from Scarce Data via\n  Dual-Channel Knowledge Distillation and Adaptive Fusion","summary":"  Personalized treatment outcome prediction based on trial data for\nsmall-sample and rare patient groups is critical in precision medicine.\nHowever, the costly trial data limit the prediction performance. To address\nthis issue, we propose a cross-fidelity knowledge distillation and adaptive\nfusion network (CFKD-AFN), which leverages abundant but low-fidelity simulation\ndata to enhance predictions on scarce but high-fidelity trial data. CFKD-AFN\nincorporates a dual-channel knowledge distillation module to extract\ncomplementary knowledge from the low-fidelity model, along with an\nattention-guided fusion module to dynamically integrate multi-source\ninformation. Experiments on treatment outcome prediction for the chronic\nobstructive pulmonary disease demonstrates significant improvements of CFKD-AFN\nover state-of-the-art methods in prediction accuracy, ranging from 6.67\\% to\n74.55\\%, and strong robustness to varying high-fidelity dataset sizes.\nFurthermore, we extend CFKD-AFN to an interpretable variant, enabling the\nexploration of latent medical semantics to support clinical decision-making.\n","authors":["Wenjie Chen","Li Zhuang","Ziying Luo","Yu Liu","Jiahao Wu","Shengcai Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25080v2","updated":"2025-10-30T12:16:59Z","published":"2025-10-29T01:38:19Z","title":"Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response\n  Games","summary":"  Card games are widely used to study sequential decision-making under\nuncertainty, with real-world analogues in negotiation, finance, and\ncybersecurity. These games typically fall into three categories based on the\nflow of control: strictly sequential (players alternate single actions),\ndeterministic response (some actions trigger a fixed outcome), and unbounded\nreciprocal response (alternating counterplays are permitted). A less-explored\nbut strategically rich structure is the bounded one-sided response, where a\nplayer's action briefly transfers control to the opponent, who must satisfy a\nfixed condition through one or more moves before the turn resolves. We term\ngames featuring this mechanism Bounded One-Sided Response Games (BORGs). We\nintroduce a modified version of Monopoly Deal as a benchmark environment that\nisolates this dynamic, where a Rent action forces the opponent to choose\npayment assets. The gold-standard algorithm, Counterfactual Regret Minimization\n(CFR), converges on effective strategies without novel algorithmic extensions.\nA lightweight full-stack research platform unifies the environment, a\nparallelized CFR runtime, and a human-playable web interface. The trained CFR\nagent and source code are available at https://monopolydeal.ai.\n","authors":["Will Wolf"],"pdf_url":"https://arxiv.org/pdf/2510.25080v2.pdf","comment":"24 pages, 7 figures"},{"id":"http://arxiv.org/abs/2510.26420v1","updated":"2025-10-30T12:13:53Z","published":"2025-10-30T12:13:53Z","title":"SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset\n  Ownership Verification","summary":"  The rapid advancement of deep neural networks (DNNs) heavily relies on\nlarge-scale, high-quality datasets. However, unauthorized commercial use of\nthese datasets severely violates the intellectual property rights of dataset\nowners. Existing backdoor-based dataset ownership verification methods suffer\nfrom inherent limitations: poison-label watermarks are easily detectable due to\nlabel inconsistencies, while clean-label watermarks face high technical\ncomplexity and failure on high-resolution images. Moreover, both approaches\nemploy static watermark patterns that are vulnerable to detection and removal.\nTo address these issues, this paper proposes a sample-specific clean-label\nbackdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked\nsample generator, this method generates unique watermarks for each sample,\nfundamentally overcoming the vulnerability of static watermark patterns. The\ncore innovation lies in designing a composite loss function with three\ncomponents: target sample loss ensures watermark effectiveness, non-target\nsample loss guarantees trigger reliability, and perceptual similarity loss\nmaintains visual imperceptibility. During ownership verification, black-box\ntesting is employed to check whether suspicious models exhibit predefined\nbackdoor behaviors. Extensive experiments on benchmark datasets demonstrate the\neffectiveness of the proposed method and its robustness against potential\nwatermark removal attacks.\n","authors":["Yingjia Wang","Ting Qiao","Xing Liu","Chongzuo Li","Sixing Wu","Jianbin Li"],"pdf_url":"https://arxiv.org/pdf/2510.26420v1.pdf","comment":"8 pages,9 figures"},{"id":"http://arxiv.org/abs/2510.26418v1","updated":"2025-10-30T12:10:03Z","published":"2025-10-30T12:10:03Z","title":"Chain-of-Thought Hijacking","summary":"  Large reasoning models (LRMs) achieve higher task performance by allocating\nmore inference-time compute, and prior works suggest this scaled reasoning may\nalso strengthen safety by improving refusal. Yet we find the opposite: the same\nreasoning can be used to bypass safeguards. We introduce Chain-of-Thought\nHijacking, a jailbreak attack on reasoning models. The attack pads harmful\nrequests with long sequences of harmless puzzle reasoning. Across HarmBench,\nCoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on\nGemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively -\nfar exceeding prior jailbreak methods for LRMs. To understand the effectiveness\nof our attack, we turn to a mechanistic analysis, which shows that mid layers\nencode the strength of safety checking, while late layers encode the\nverification outcome. Long benign CoT dilutes both signals by shifting\nattention away from harmful tokens. Targeted ablations of attention heads\nidentified by this analysis causally decrease refusal, confirming their role in\na safety subnetwork. These results show that the most interpretable form of\nreasoning - explicit CoT - can itself become a jailbreak vector when combined\nwith final-answer cues. We release prompts, outputs, and judge decisions to\nfacilitate replication.\n","authors":["Jianli Zhao","Tingchen Fu","Rylan Schaeffer","Mrinank Sharma","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2510.26418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23127v2","updated":"2025-10-30T12:09:18Z","published":"2025-10-27T09:03:21Z","title":"Lost in Tokenization: Context as the Key to Unlocking Biomolecular\n  Understanding in Scientific LLMs","summary":"  Scientific Large Language Models (Sci-LLMs) have emerged as a promising\nfrontier for accelerating biological discovery. However, these models face a\nfundamental challenge when processing raw biomolecular sequences: the\ntokenization dilemma. Whether treating sequences as a specialized language,\nrisking the loss of functional motif information, or as a separate modality,\nintroducing formidable alignment challenges, current strategies fundamentally\nlimit their reasoning capacity. We challenge this sequence-centric paradigm by\npositing that a more effective strategy is to provide Sci-LLMs with high-level\nstructured context derived from established bioinformatics tools, thereby\nbypassing the need to interpret low-level noisy sequence data directly. Through\na systematic comparison of leading Sci-LLMs on biological reasoning tasks, we\ntested three input modes: sequence-only, context-only, and a combination of\nboth. Our findings are striking: the context-only approach consistently and\nsubstantially outperforms all other modes. Even more revealing, the inclusion\nof the raw sequence alongside its high-level context consistently degrades\nperformance, indicating that raw sequences act as informational noise, even for\nmodels with specialized tokenization schemes. These results suggest that the\nprimary strength of existing Sci-LLMs lies not in their nascent ability to\ninterpret biomolecular syntax from scratch, but in their profound capacity for\nreasoning over structured, human-readable knowledge. Therefore, we argue for\nreframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines\nover expert knowledge. This work lays the foundation for a new class of hybrid\nscientific AI agents, repositioning the developmental focus from direct\nsequence interpretation towards high-level knowledge synthesis. The code is\navailable at https://github.com/opendatalab-raiser/CoKE.\n","authors":["Kai Zhuang","Jiawei Zhang","Yumou Liu","Hanqun Cao","Chunbin Gu","Mengdi Liu","Zhangyang Gao","Zitong Jerry Wang","Xuanhe Zhou","Pheng-Ann Heng","Lijun Wu","Conghui He","Cheng Tan"],"pdf_url":"https://arxiv.org/pdf/2510.23127v2.pdf","comment":"38 pages, under review"},{"id":"http://arxiv.org/abs/2412.17883v2","updated":"2025-10-30T12:06:31Z","published":"2024-12-23T06:22:03Z","title":"In Defence of Post-hoc Explainability","summary":"  This position paper defends post-hoc explainability methods as legitimate\ntools for scientific knowledge production in machine learning. Addressing\ncriticism of these methods' reliability and epistemic status, we develop a\nphilosophical framework grounded in mediated understanding and bounded\nfactivity. We argue that scientific insights can emerge through structured\ninterpretation of model behaviour without requiring complete mechanistic\ntransparency, provided explanations acknowledge their approximative nature and\nundergo rigorous empirical validation. Through analysis of recent biomedical ML\napplications, we demonstrate how post-hoc methods, when properly integrated\ninto scientific practice, generate novel hypotheses and advance phenomenal\nunderstanding.\n","authors":["Nick Oh"],"pdf_url":"https://arxiv.org/pdf/2412.17883v2.pdf","comment":"v1 presented at the Interpretable AI: Past, Present, and Future\n  Workshop at NeurIPS 2024 (non-archival)"},{"id":"http://arxiv.org/abs/2510.17670v2","updated":"2025-10-30T12:05:58Z","published":"2025-10-20T15:41:55Z","title":"On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active\n  Marginal-Samples Exploration","summary":"  Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs.\n","authors":["Yehonathan Refael","Amit Aides","Aviad Barzilai","George Leifman","Genady Beryozkin","Vered Silverman","Bolous Jaber","Tomer Shekel"],"pdf_url":"https://arxiv.org/pdf/2510.17670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.23885v2","updated":"2025-10-30T12:02:27Z","published":"2025-09-28T13:50:29Z","title":"Tunable-Generalization Diffusion Powered by Self-Supervised Contextual\n  Sub-Data for Low-Dose CT Reconstruction","summary":"  Current models based on deep learning for low-dose CT denoising rely heavily\non paired data and generalize poorly. Even the more concerned diffusion models\nneed to learn the distribution of clean data for reconstruction, which is\ndifficult to satisfy in medical clinical applications. At the same time,\nself-supervised-based methods face the challenge of significant degradation of\ngeneralizability of models pre-trained for the current dose to expand to other\ndoses. To address these issues, this work proposes a novel method of\nTUnable-geneRalizatioN Diffusion (TurnDiff) powered by self-supervised\ncontextual sub-data for low-dose CT reconstruction. Firstly, a contextual\nsubdata self-enhancing similarity strategy is designed for denoising centered\non the LDCT projection domain, which provides an initial prior for the\nsubsequent progress. Subsequently, the initial prior is used to combine\nknowledge distillation with a deep combination of latent diffusion models for\noptimizing image details. The pre-trained model is used for inference\nreconstruction, and the pixel-level self-correcting fusion technique is\nproposed for fine-grained reconstruction of the image domain to enhance the\nimage fidelity, using the initial prior and the LDCT image as a guide. In\naddition, the technique is flexibly applied to the generalization of upper and\nlower doses or even unseen doses. Dual-domain strategy cascade for\nself-supervised LDCT denoising, TurnDiff requires only LDCT projection domain\ndata for training and testing. Comprehensive evaluation on both benchmark\ndatasets and real-world data demonstrates that TurnDiff consistently\noutperforms state-of-the-art methods in both reconstruction and generalization.\n","authors":["Guoquan Wei","Liu Shi","Zekun Zhou","Wenzhe Shan","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2509.23885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26412v1","updated":"2025-10-30T12:00:46Z","published":"2025-10-30T12:00:46Z","title":"LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video\n  Generation","summary":"  Recently text-to-video generation has made impressive progress in producing\nshort, high-quality clips, but evaluating long-form outputs remains a major\nchallenge especially when processing complex prompts. Existing benchmarks\nmostly rely on simplified prompts and focus on low-level metrics, overlooking\nfine-grained alignment with prompts and abstract dimensions such as narrative\ncoherence and thematic expression. To address these gaps, we propose\nLoCoT2V-Bench, a benchmark specifically designed for long video generation\n(LVG) under complex input conditions. Based on various real-world videos,\nLoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating\nelements like scene transitions and event dynamics. Moreover, it constructs a\nmulti-dimensional evaluation framework that includes our newly proposed metrics\nsuch as event-level alignment, fine-grained temporal consistency, content\nclarity, and the Human Expectation Realization Degree (HERD) that focuses on\nmore abstract attributes like narrative flow, emotional response, and character\ndevelopment. Using this framework, we conduct a comprehensive evaluation of\nnine representative LVG models, finding that while current methods perform well\non basic visual and temporal aspects, they struggle with inter-event\nconsistency, fine-grained alignment, and high-level thematic adherence, etc.\nOverall, LoCoT2V-Bench provides a comprehensive and reliable platform for\nevaluating long-form complex text-to-video generation and highlights critical\ndirections for future method improvement.\n","authors":["Xiangqing Zheng","Chengyue Wu","Kehai Chen","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26411v1","updated":"2025-10-30T11:58:36Z","published":"2025-10-30T11:58:36Z","title":"MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders","summary":"  Artificial intelligence in healthcare requires models that are accurate and\ninterpretable. We advance mechanistic interpretability in medical vision by\napplying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,\na vision-language model trained on chest radiographs and reports. To quantify\ninterpretability, we propose an evaluation framework that combines correlation\nmetrics, entropy analyzes, and automated neuron naming via the MedGEMMA\nfoundation model. Experiments on the CheXpert dataset show that MedSAE neurons\nachieve higher monosemanticity and interpretability than raw MedCLIP features.\nOur findings bridge high-performing medical AI and transparency, offering a\nscalable step toward clinically reliable representations.\n","authors":["Riccardo Renzulli","Colas Lepoutre","Enrico Cassano","Marco Grangetto"],"pdf_url":"https://arxiv.org/pdf/2510.26411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26406v1","updated":"2025-10-30T11:53:08Z","published":"2025-10-30T11:53:08Z","title":"Human-in-the-loop Online Rejection Sampling for Robotic Manipulation","summary":"  Reinforcement learning (RL) is widely used to produce robust robotic\nmanipulation policies, but fine-tuning vision-language-action (VLA) models with\nRL can be unstable due to inaccurate value estimates and sparse supervision at\nintermediate steps. In contrast, imitation learning (IL) is easy to train but\noften underperforms due to its offline nature. In this paper, we propose\nHi-ORS, a simple yet effective post-training method that utilizes rejection\nsampling to achieve both training stability and high robustness. Hi-ORS\nstabilizes value estimation by filtering out negatively rewarded samples during\nonline fine-tuning, and adopts a reward-weighted supervised training objective\nto provide dense intermediate-step supervision. For systematic study, we\ndevelop an asynchronous inference-training framework that supports flexible\nonline human-in-the-loop corrections, which serve as explicit guidance for\nlearning error-recovery behaviors. Across three real-world tasks and two\nembodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich\nmanipulation in just 1.5 hours of real-world training, outperforming RL and IL\nbaselines by a substantial margin in both effectiveness and efficiency.\nNotably, the fine-tuned policy exhibits strong test-time scalability by\nreliably executing complex error-recovery behaviors to achieve better\nperformance.\n","authors":["Guanxing Lu","Rui Zhao","Haitao Lin","He Zhang","Yansong Tang"],"pdf_url":"https://arxiv.org/pdf/2510.26406v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2510.26402v1","updated":"2025-10-30T11:41:50Z","published":"2025-10-30T11:41:50Z","title":"Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback\n  in Programming Education","summary":"  The rapid growth of programming education has outpaced traditional assessment\ntools, leaving faculty with limited means to provide meaningful, scalable\nfeedback. Conventional autograders, while efficient, act as black-box systems\nthat simply return pass/fail results, offering little insight into student\nthinking or learning needs.\n  Autograder+ is designed to shift autograding from a purely summative process\nto a formative learning experience. It introduces two key capabilities:\nautomated feedback generation using a fine-tuned Large Language Model, and\nvisualization of student code submissions to uncover learning patterns. The\nmodel is fine-tuned on curated student code and expert feedback to ensure\npedagogically aligned, context-aware guidance.\n  In evaluation across 600 student submissions from multiple programming tasks,\nthe system produced feedback with strong semantic alignment to instructor\ncomments. For visualization, contrastively learned code embeddings trained on\n1,000 annotated submissions enable grouping solutions into meaningful clusters\nbased on functionality and approach. The system also supports prompt-pooling,\nallowing instructors to guide feedback style through selected prompt templates.\n  By integrating AI-driven feedback, semantic clustering, and interactive\nvisualization, Autograder+ reduces instructor workload while supporting\ntargeted instruction and promoting stronger learning outcomes.\n","authors":["Vikrant Sahu","Gagan Raj Gupta","Raghav Borikar","Nitin Mane"],"pdf_url":"https://arxiv.org/pdf/2510.26402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26396v1","updated":"2025-10-30T11:36:34Z","published":"2025-10-30T11:36:34Z","title":"A Pragmatic View of AI Personhood","summary":"  The emergence of agentic Artificial Intelligence (AI) is set to trigger a\n\"Cambrian explosion\" of new kinds of personhood. This paper proposes a\npragmatic framework for navigating this diversification by treating personhood\nnot as a metaphysical property to be discovered, but as a flexible bundle of\nobligations (rights and responsibilities) that societies confer upon entities\nfor a variety of reasons, especially to solve concrete governance problems. We\nargue that this traditional bundle can be unbundled, creating bespoke solutions\nfor different contexts. This will allow for the creation of practical tools --\nsuch as facilitating AI contracting by creating a target \"individual\" that can\nbe sanctioned -- without needing to resolve intractable debates about an AI's\nconsciousness or rationality. We explore how individuals fit in to social roles\nand discuss the use of decentralized digital identity technology, examining\nboth \"personhood as a problem\", where design choices can create \"dark patterns\"\nthat exploit human social heuristics, and \"personhood as a solution\", where\nconferring a bundle of obligations is necessary to ensure accountability or\nprevent conflict. By rejecting foundationalist quests for a single, essential\ndefinition of personhood, this paper offers a more pragmatic and flexible way\nto think about integrating AI agents into our society.\n","authors":["Joel Z. Leibo","Alexander Sasha Vezhnevets","William A. Cunningham","Stanley M. Bileschi"],"pdf_url":"https://arxiv.org/pdf/2510.26396v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2510.26390v1","updated":"2025-10-30T11:33:29Z","published":"2025-10-30T11:33:29Z","title":"SPG-CDENet: Spatial Prior-Guided Cross Dual Encoder Network for\n  Multi-Organ Segmentation","summary":"  Multi-organ segmentation is a critical task in computer-aided diagnosis.\nWhile recent deep learning methods have achieved remarkable success in image\nsegmentation, huge variations in organ size and shape challenge their\neffectiveness in multi-organ segmentation. To address these challenges, we\npropose a Spatial Prior-Guided Cross Dual Encoder Network (SPG-CDENet), a novel\ntwo-stage segmentation paradigm designed to improve multi-organ segmentation\naccuracy. Our SPG-CDENet consists of two key components: a spatial prior\nnetwork and a cross dual encoder network. The prior network generates coarse\nlocalization maps that delineate the approximate ROI, serving as spatial\nguidance for the dual encoder network. The cross dual encoder network comprises\nfour essential components: a global encoder, a local encoder, a symmetric\ncross-attention module, and a flow-based decoder. The global encoder captures\nglobal semantic features from the entire image, while the local encoder focuses\non features from the prior network. To enhance the interaction between the\nglobal and local encoders, a symmetric cross-attention module is proposed\nacross all layers of the encoders to fuse and refine features. Furthermore, the\nflow-based decoder directly propagates high-level semantic features from the\nfinal encoder layer to all decoder layers, maximizing feature preservation and\nutilization. Extensive qualitative and quantitative experiments on two public\ndatasets demonstrate the superior performance of SPG-CDENet compared to\nexisting segmentation methods. Furthermore, ablation studies further validate\nthe effectiveness of the proposed modules in improving segmentation accuracy.\n","authors":["Xizhi Tian","Changjun Zhou","Yulin. Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26384v1","updated":"2025-10-30T11:28:58Z","published":"2025-10-30T11:28:58Z","title":"Scales++: Compute Efficient Evaluation Subset Selection with Cognitive\n  Scales Embeddings","summary":"  The prohibitive cost of evaluating large language models (LLMs) on\ncomprehensive benchmarks necessitates the creation of small yet representative\ndata subsets (i.e., tiny benchmarks) that enable efficient assessment while\nretaining predictive fidelity. Current methods for this task operate under a\nmodel-centric paradigm, selecting benchmarking items based on the collective\nperformance of existing models. Such approaches are limited by large upfront\ncosts, an inability to immediately handle new benchmarks (`cold-start'), and\nthe fragile assumption that future models will share the failure patterns of\ntheir predecessors. In this work, we challenge this paradigm and propose a\nitem-centric approach to benchmark subset selection, arguing that selection\nshould be based on the intrinsic properties of the task items themselves,\nrather than on model-specific failure patterns. We instantiate this\nitem-centric efficient benchmarking approach via a novel method, Scales++,\nwhere data selection is based on the cognitive demands of the benchmark\nsamples. Empirically, we show Scales++ reduces the upfront selection cost by\nover 18x while achieving competitive predictive fidelity. On the Open LLM\nLeaderboard, using just a 0.5\\% data subset, we predict full benchmark scores\nwith a 2.9% mean absolute error. We demonstrate that this item-centric approach\nenables more efficient model evaluation without significant fidelity\ndegradation, while also providing better cold-start performance and more\ninterpretable benchmarking.\n","authors":["Andrew M. Bean","Nabeel Seedat","Shengzhuang Chen","Jonathan Richard Schwarz"],"pdf_url":"https://arxiv.org/pdf/2510.26384v1.pdf","comment":"9 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2510.26380v1","updated":"2025-10-30T11:22:15Z","published":"2025-10-30T11:22:15Z","title":"AI Mathematician as a Partner in Advancing Mathematical Discovery -- A\n  Case Study in Homogenization Theory","summary":"  Artificial intelligence (AI) has demonstrated impressive progress in\nmathematical reasoning, yet its integration into the practice of mathematical\nresearch remains limited. In this study, we investigate how the AI\nMathematician (AIM) system can operate as a research partner rather than a mere\nproblem solver. Focusing on a challenging problem in homogenization theory, we\nanalyze the autonomous reasoning trajectories of AIM and incorporate targeted\nhuman interventions to structure the discovery process. Through iterative\ndecomposition of the problem into tractable subgoals, selection of appropriate\nanalytical methods, and validation of intermediate results, we reveal how human\nintuition and machine computation can complement one another. This\ncollaborative paradigm enhances the reliability, transparency, and\ninterpretability of the resulting proofs, while retaining human oversight for\nformal rigor and correctness. The approach leads to a complete and verifiable\nproof, and more broadly, demonstrates how systematic human-AI co-reasoning can\nadvance the frontier of mathematical discovery.\n","authors":["Yuanhang Liu","Beichen Wang","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26380v1.pdf","comment":"52 pages, 1 figure"},{"id":"http://arxiv.org/abs/2510.26374v1","updated":"2025-10-30T11:15:23Z","published":"2025-10-30T11:15:23Z","title":"BOTS: A Unified Framework for Bayesian Online Task Selection in LLM\n  Reinforcement Finetuning","summary":"  Reinforcement finetuning (RFT) is a key technique for aligning Large Language\nModels (LLMs) with human preferences and enhancing reasoning, yet its\neffectiveness is highly sensitive to which tasks are explored during training.\nUniform task sampling is inefficient, wasting computation on tasks that are\neither trivial or unsolvable, while existing task selection methods often\nsuffer from high rollout costs, poor adaptivity, or incomplete evidence. We\nintroduce \\textbf{BOTS}, a unified framework for \\textbf{B}ayesian\n\\textbf{O}nline \\textbf{T}ask \\textbf{S}election in LLM reinforcement\nfinetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior\nestimates of task difficulty as the model evolves. It jointly incorporates\n\\emph{explicit evidence} from direct evaluations of selected tasks and\n\\emph{implicit evidence} inferred from these evaluations for unselected tasks,\nwith Thompson sampling ensuring a principled balance between exploration and\nexploitation. To make implicit evidence practical, we instantiate it with an\nultra-light interpolation-based plug-in that estimates difficulties of\nunevaluated tasks without extra rollouts, adding negligible overhead.\nEmpirically, across diverse domains and LLM scales, BOTS consistently improves\ndata efficiency and performance over baselines and ablations, providing a\npractical and extensible solution for dynamic task selection in RFT.\n","authors":["Qianli Shen","Daoyuan Chen","Yilun Huang","Zhenqing Ling","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24817v2","updated":"2025-10-30T11:13:33Z","published":"2025-10-28T10:06:49Z","title":"Towards a Method for Synthetic Generation of Persons with Aphasia\n  Transcripts","summary":"  In aphasia research, Speech-Language Pathologists (SLPs) devote extensive\ntime to manually coding speech samples using Correct Information Units (CIUs),\na measure of how informative an individual sample of speech is. Developing\nautomated systems to recognize aphasic language is limited by data scarcity.\nFor example, only about 600 transcripts are available in AphasiaBank yet\nbillions of tokens are used to train large language models (LLMs). In the\nbroader field of machine learning (ML), researchers increasingly turn to\nsynthetic data when such are sparse. Therefore, this study constructs and\nvalidates two methods to generate synthetic transcripts of the AphasiaBank Cat\nRescue picture description task. One method leverages a procedural programming\napproach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct\nLLMs. The methods generate transcripts across four severity levels (Mild,\nModerate, Severe, Very Severe) through word dropping, filler insertion, and\nparaphasia substitution. Overall, we found, compared to human-elicited\ntranscripts, Mistral 7b Instruct best captures key aspects of linguistic\ndegradation observed in aphasia, showing realistic directional changes in NDW,\nword count, and word length amongst the synthetic generation methods. Based on\nthe results, future work should plan to create a larger dataset, fine-tune\nmodels for better aphasic representation, and have SLPs assess the realism and\nusefulness of the synthetic transcripts.\n","authors":["Jason M. Pittman","Anton Phillips Jr.","Yesenia Medina-Santos","Brielle C. Stark"],"pdf_url":"https://arxiv.org/pdf/2510.24817v2.pdf","comment":"19 pages, 1 figure, 7 tables"},{"id":"http://arxiv.org/abs/2510.26352v1","updated":"2025-10-30T11:04:15Z","published":"2025-10-30T11:04:15Z","title":"The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic\n  Teams for Multi-Agent Collaboration","summary":"  While a multi-agent approach based on large language models (LLMs) represents\na promising strategy to surpass the capabilities of single models, its success\nis critically dependent on synergistic team composition. However, forming\noptimal teams is a significant challenge, as the inherent opacity of most\nmodels obscures the internal characteristics necessary for effective\ncollaboration. In this paper, we propose an interaction-centric framework for\nautomatic team composition that does not require any prior knowledge including\ntheir internal architectures, training data, or task performances. Our method\nconstructs a \"language model graph\" that maps relationships between models from\nthe semantic coherence of pairwise conversations, and then applies community\ndetection to identify synergistic model clusters. Our experiments with diverse\nLLMs demonstrate that the proposed method discovers functionally coherent\ngroups that reflect their latent specializations. Priming conversations with\nspecific topics identified synergistic teams which outperform random baselines\non downstream benchmarks and achieve comparable accuracy to that of\nmanually-curated teams based on known model specializations. Our findings\nprovide a new basis for the automated design of collaborative multi-agent LLM\nteams.\n","authors":["Kotaro Furuya","Yuichi Kitagawa"],"pdf_url":"https://arxiv.org/pdf/2510.26352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26347v1","updated":"2025-10-30T10:55:05Z","published":"2025-10-30T10:55:05Z","title":"Reinforcement Learning for Pollution Detection in a Randomized, Sparse\n  and Nonstationary Environment with an Autonomous Underwater Vehicle","summary":"  Reinforcement learning (RL) algorithms are designed to optimize\nproblem-solving by learning actions that maximize rewards, a task that becomes\nparticularly challenging in random and nonstationary environments. Even\nadvanced RL algorithms are often limited in their ability to solve problems in\nthese conditions. In applications such as searching for underwater pollution\nclouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate\nreward-sparse environments, where actions frequently result in a zero reward.\nThis paper aims to address these challenges by revisiting and modifying\nclassical RL approaches to efficiently operate in sparse, randomized, and\nnonstationary environments. We systematically study a large number of\nmodifications, including hierarchical algorithm changes, multigoal learning,\nand the integration of a location memory as an external output filter to\nprevent state revisits. Our results demonstrate that a modified Monte\nCarlo-based approach significantly outperforms traditional Q-learning and two\nexhaustive search patterns, illustrating its potential in adapting RL to\ncomplex environments. These findings suggest that reinforcement learning\napproaches can be effectively adapted for use in random, nonstationary, and\nreward-sparse environments.\n","authors":["Sebastian Zieglmeier","Niklas Erdmann","Narada D. Warakagoda"],"pdf_url":"https://arxiv.org/pdf/2510.26347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26346v1","updated":"2025-10-30T10:54:43Z","published":"2025-10-30T10:54:43Z","title":"Discovering State Equivalences in UCT Search Trees By Action Pruning","summary":"  One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its\nsample efficiency by grouping/abstracting states or state-action pairs and\nsharing statistics within a group. Though state-action pair abstractions are\nmostly easy to find in algorithms such as On the Go Abstractions in Upper\nConfidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are\nfound in either noisy or large action space settings due to constraining\nconditions. We provide theoretical and empirical evidence for this claim, and\nwe slightly alleviate this state abstraction problem by proposing a weaker\nstate abstraction condition that trades a minor loss in accuracy for finding\nmany more abstractions. We name this technique Ideal Pruning Abstractions in\nUCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a\nlarge range of test domains and iteration budgets as experimentally validated.\nIPA-UCT uses a different abstraction framework from Abstraction of State-Action\nPairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,\nwe show that both IPA and ASAP are special cases of a more general framework\nthat we call p-ASAP which itself is a special case of the ASASAP framework.\n","authors":["Robin Schmöcker","Alexander Dockhorn","Bodo Rosenhahn"],"pdf_url":"https://arxiv.org/pdf/2510.26346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26345v1","updated":"2025-10-30T10:52:43Z","published":"2025-10-30T10:52:43Z","title":"MisSynth: Improving MISSCI Logical Fallacies Classification with\n  Synthetic Data","summary":"  Health-related misinformation is very prevalent and potentially harmful. It\nis difficult to identify, especially when claims distort or misinterpret\nscientific findings. We investigate the impact of synthetic data generation and\nlightweight fine-tuning techniques on the ability of large language models\n(LLMs) to recognize fallacious arguments using the MISSCI dataset and\nframework. In this work, we propose MisSynth, a pipeline that applies\nretrieval-augmented generation (RAG) to produce synthetic fallacy samples,\nwhich are then used to fine-tune an LLM model. Our results show substantial\naccuracy gains with fine-tuned models compared to vanilla baselines. For\ninstance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score\nabsolute improvement on the MISSCI test split over its vanilla baseline. We\ndemonstrate that introducing synthetic fallacy data to augment limited\nannotated resources can significantly enhance zero-shot LLM classification\nperformance on real-world scientific misinformation tasks, even with limited\ncomputational resources. The code and synthetic dataset are available on\nhttps://github.com/mxpoliakov/MisSynth.\n","authors":["Mykhailo Poliakov","Nadiya Shvai"],"pdf_url":"https://arxiv.org/pdf/2510.26345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21497v2","updated":"2025-10-30T10:49:28Z","published":"2025-05-27T17:58:49Z","title":"Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers","summary":"  Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster.\n","authors":["Wei Pang","Kevin Qinghong Lin","Xiangru Jian","Xi He","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2505.21497v2.pdf","comment":"Project Page: https://github.com/Paper2Poster/Paper2Poster"},{"id":"http://arxiv.org/abs/2510.26342v1","updated":"2025-10-30T10:49:25Z","published":"2025-10-30T10:49:25Z","title":"Linear Causal Discovery with Interventional Constraints","summary":"  Incorporating causal knowledge and mechanisms is essential for refining\ncausal models and improving downstream tasks such as designing new treatments.\nIn this paper, we introduce a novel concept in causal discovery, termed\ninterventional constraints, which differs fundamentally from interventional\ndata. While interventional data require direct perturbations of variables,\ninterventional constraints encode high-level causal knowledge in the form of\ninequality constraints on causal effects. For instance, in the Sachs dataset\n(Sachs et al.\\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3\nexerts a positive causal effect on Akt. Existing causal discovery methods allow\nenforcing structural constraints (for example, requiring a causal path from\nPIP3 to Akt), but they may still produce incorrect causal conclusions such as\nlearning that \"PIP3 inhibits Akt\". Interventional constraints bridge this gap\nby explicitly constraining the total causal effect between variable pairs,\nensuring learned models respect known causal influences. To formalize\ninterventional constraints, we propose a metric to quantify total causal\neffects for linear causal models and formulate the problem as a constrained\noptimization task, solved using a two-stage constrained optimization method. We\nevaluate our approach on real-world datasets and demonstrate that integrating\ninterventional constraints not only improves model accuracy and ensures\nconsistency with established findings, making models more explainable, but also\nfacilitates the discovery of new causal relationships that would otherwise be\ncostly to identify.\n","authors":["Zhigao Guo","Feng Dong"],"pdf_url":"https://arxiv.org/pdf/2510.26342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25409v2","updated":"2025-10-30T10:48:05Z","published":"2025-10-29T11:27:08Z","title":"BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains","summary":"  The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research.\n","authors":["Vijay Devane","Mohd Nauman","Bhargav Patel","Aniket Mahendra Wakchoure","Yogeshkumar Sant","Shyam Pawar","Viraj Thakur","Ananya Godse","Sunil Patra","Neha Maurya","Suraj Racha","Nitish Kamal Singh","Ajay Nagpal","Piyush Sawarkar","Kundeshwar Vijayrao Pundalik","Rohit Saluja","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2510.25409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26339v1","updated":"2025-10-30T10:46:28Z","published":"2025-10-30T10:46:28Z","title":"GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and\n  High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?","summary":"  Image super-resolution(SR) is fundamental to many vision system-from\nsurveillance and autonomy to document analysis and retail analytics-because\nrecovering high-frequency details, especially scene-text, enables reliable\ndownstream perception. Scene-text, i.e., text embedded in natural images such\nas signs, product labels, and storefronts, often carries the most actionable\ninformation; when characters are blurred or hallucinated, optical character\nrecognition(OCR) and subsequent decisions fail even if the rest of the image\nappears sharp. Yet previous SR research has often been tuned to distortion\n(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that\nare largely insensitive to character-level errors. Furthermore, studies that do\naddress text SR often focus on simplified benchmarks with isolated characters,\noverlooking the challenges of text within complex natural scenes. As a result,\nscene-text is effectively treated as generic texture. For SR to be effective in\npractical deployments, it is therefore essential to explicitly optimize for\nboth text legibility and perceptual quality. We present GLYPH-SR, a\nvision-language-guided diffusion framework that aims to achieve both objectives\njointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by\nOCR data, and a ping-pong scheduler that alternates between text- and\nscene-centric guidance. To enable targeted text restoration, we train these\ncomponents on a synthetic corpus while keeping the main SR branch frozen.\nAcross SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by\nup to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)\nwhile maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed\nto satisfy both objectives simultaneously-high readability and high visual\nrealism-delivering SR that looks right and reds right.\n","authors":["Mingyu Sung","Seungjae Ham","Kangwoo Kim","Yeokyoung Yoon","Sangseok Yun","Il-Min Kim","Jae-Mo Kang"],"pdf_url":"https://arxiv.org/pdf/2510.26339v1.pdf","comment":"11 pages, 6 figures. Includes supplementary material. Under review as\n  a conference paper at ICLR 2026"},{"id":"http://arxiv.org/abs/2510.26336v1","updated":"2025-10-30T10:43:40Z","published":"2025-10-30T10:43:40Z","title":"From Amateur to Master: Infusing Knowledge into LLMs via Automated\n  Curriculum Learning","summary":"  Large Language Models (LLMs) excel at general tasks but underperform in\nspecialized domains like economics and psychology, which require deep,\nprincipled understanding. To address this, we introduce ACER (Automated\nCurriculum-Enhanced Regimen) that transforms generalist models into domain\nexperts without sacrificing their broad capabilities. ACER first synthesizes a\ncomprehensive, textbook-style curriculum by generating a table of contents for\na subject and then creating question-answer (QA) pairs guided by Bloom's\ntaxonomy. This ensures systematic topic coverage and progressively increasing\ndifficulty. The resulting synthetic corpus is used for continual pretraining\nwith an interleaved curriculum schedule, aligning learning across both content\nand cognitive dimensions.\n  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized\nMMLU subsets. In challenging domains like microeconomics, where baselines\nstruggle, ACER boosts accuracy by 5 percentage points. Across all target\ndomains, we observe a consistent macro-average improvement of 3 percentage\npoints. Notably, ACER not only prevents catastrophic forgetting but also\nfacilitates positive cross-domain knowledge transfer, improving performance on\nnon-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on\nknowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,\nwhile maintaining stable performance on general reasoning tasks. Our results\ndemonstrate that ACER offers a scalable and effective recipe for closing\ncritical domain gaps in LLMs.\n","authors":["Nishit Neema","Srinjoy Mukherjee","Sapan Shah","Gokul Ramakrishnan","Ganesh Venkatesh"],"pdf_url":"https://arxiv.org/pdf/2510.26336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09499v3","updated":"2025-10-30T10:21:42Z","published":"2025-03-12T16:03:03Z","title":"MindGYM: What Matters in Question Synthesis for Thinking-Centric\n  Fine-Tuning?","summary":"  Large foundation models face challenges in acquiring transferable, structured\nthinking abilities, especially when supervised with rigid templates or\ncrowd-annotated instruction datasets. Unlike prior approaches, we focus on a\nthinking-centric data synthesis paradigm that enables models to evolve through\nself-generated, cognitively guided data. We propose MindGYM, a structured and\nscalable framework for question synthesis, composed of: (1) Cognitive Thinking\nProcess Injection, which infuses high-level reasoning objectives to shape the\nmodel's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating\natomic questions from diverse semantic types to encourage broader thinking; and\n(3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop\nquestions based on QA seeds for deeper reasoning. Detailed analysis shows that\nsynthetic data generated by our method achieves 16.7% higher average quality\nand 67.91% lower quality variance compared to baseline sources, highlighting\nthat both high-quality and self-contained data are essential for effective,\nthinking-oriented fine-tuning. MindGYM improves performance on six reasoning\nbenchmarks, achieving gains of up to 16% on MathVision using only 400 data\nsamples, and generalizable improvements across different model sizes and\narchitectures. MindGYM underscores the viability of self-challenging mechanisms\nin refining large model capabilities while minimizing human intervention and\nresource demands. Code and data are released to promote data-centric research\ninto self-evolving foundation models driven by their internal reasoning\ncapabilities.\n","authors":["Zhe Xu","Daoyuan Chen","Zhenqing Ling","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2503.09499v3.pdf","comment":"Accepted by NeurIPS'25. 30 pages, 2 figures, 13 tables"},{"id":"http://arxiv.org/abs/2510.26324v1","updated":"2025-10-30T10:17:27Z","published":"2025-10-30T10:17:27Z","title":"Posterior Sampling by Combining Diffusion Models with Annealed Langevin\n  Dynamics","summary":"  Given a noisy linear measurement $y = Ax + \\xi$ of a distribution $p(x)$, and\na good approximation to the prior $p(x)$, when can we sample from the posterior\n$p(x \\mid y)$? Posterior sampling provides an accurate and fair framework for\ntasks such as inpainting, deblurring, and MRI reconstruction, and several\nheuristics attempt to approximate it. Unfortunately, approximate posterior\nsampling is computationally intractable in general.\n  To sidestep this hardness, we focus on (local or global) log-concave\ndistributions $p(x)$. In this regime, Langevin dynamics yields posterior\nsamples when the exact scores of $p(x)$ are available, but it is brittle to\nscore--estimation error, requiring an MGF bound (sub-exponential error). By\ncontrast, in the unconditional setting, diffusion models succeed with only an\n$L^2$ bound on the score error. We prove that combining diffusion models with\nan annealed variant of Langevin dynamics achieves conditional sampling in\npolynomial time using merely an $L^4$ bound on the score error.\n","authors":["Zhiyang Xun","Shivam Gupta","Eric Price"],"pdf_url":"https://arxiv.org/pdf/2510.26324v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.20535v2","updated":"2025-10-30T10:14:59Z","published":"2025-06-25T15:24:45Z","title":"AIMeter: Measuring, Analyzing, and Visualizing Energy and Carbon\n  Footprint of AI Workloads","summary":"  The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents AIMeter, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, AIMeter offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, AIMeter encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/AIMeter.\n","authors":["Hongzhen Huang","Kunming Zhang","Hanlong Liao","Kui Wu","Guoming Tang"],"pdf_url":"https://arxiv.org/pdf/2506.20535v2.pdf","comment":"11 pages, 7 figures and 5 tables"},{"id":"http://arxiv.org/abs/2510.18915v3","updated":"2025-10-30T10:00:05Z","published":"2025-10-21T06:14:40Z","title":"UNO-Bench: A Unified Benchmark for Exploring the Compositional Law\n  Between Uni-modal and Omni-modal in Omni Models","summary":"  Multimodal Large Languages models have been progressing from uni-modal\nunderstanding toward unifying visual, audio and language modalities,\ncollectively termed omni models. However, the correlation between uni-modal and\nomni-modal remains unclear, which requires comprehensive evaluation to drive\nomni model's intelligence evolution. In this work, we introduce a novel,\nhigh-quality, and UNified Omni model benchmark, UNO-Bench. This benchmark is\ndesigned to effectively evaluate both UNi-modal and Omni-modal capabilities\nunder a unified ability taxonomy, spanning 44 task types and 5 modality\ncombinations. It includes 1250 human curated samples for omni-modal with 98%\ncross-modality solvability, and 2480 enhanced uni-modal samples. The\nhuman-generated dataset is well-suited to real-world scenarios, particularly\nwithin the Chinese context, whereas the automatically compressed dataset offers\na 90% increase in speed and maintains 98% consistency across 18 public\nbenchmarks. In addition to traditional multi-choice questions, we propose an\ninnovative multi-step open-ended question format to assess complex reasoning. A\ngeneral scoring model is incorporated, supporting 6 question types for\nautomated evaluation with 95% accuracy. Experimental result shows the\nCompositional Law between omni-modal and uni-modal performance and the\nomni-modal capability manifests as a bottleneck effect on weak models, while\nexhibiting synergistic promotion on strong models.\n","authors":["Chen Chen","ZeYang Hu","Fengjiao Chen","Liya Ma","Jiaxing Liu","Xiaoyu Li","Ziwen Wang","Xuezhi Cao","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2510.18915v3.pdf","comment":"v3: Switch the paper template. Work in progress. Github:\n  https://github.com/meituan-longcat/UNO-Bench Hugging Face:\n  https://huggingface.co/datasets/meituan-longcat/UNO-Bench"},{"id":"http://arxiv.org/abs/2510.26309v1","updated":"2025-10-30T09:53:16Z","published":"2025-10-30T09:53:16Z","title":"GraphCompliance: Aligning Policy and Context Graphs for LLM-Based\n  Regulatory Compliance","summary":"  Compliance at web scale poses practical challenges: each request may require\na regulatory assessment. Regulatory texts (e.g., the General Data Protection\nRegulation, GDPR) are cross-referential and normative, while runtime contexts\nare expressed in unstructured natural language. This setting motivates us to\nalign semantic information in unstructured text with the structured, normative\nelements of regulations. To this end, we introduce GraphCompliance, a framework\nthat represents regulatory texts as a Policy Graph and runtime contexts as a\nContext Graph, and aligns them. In this formulation, the policy graph encodes\nnormative structure and cross-references, whereas the context graph formalizes\nevents as subject-action-object (SAO) and entity-relation triples. This\nalignment anchors the reasoning of a judge large language model (LLM) in\nstructured information and helps reduce the burden of regulatory interpretation\nand event parsing, enabling a focus on the core reasoning step. In experiments\non 300 GDPR-derived real-world scenarios spanning five evaluation tasks,\nGraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than\nLLM-only and RAG baselines, with fewer under- and over-predictions, resulting\nin higher recall and lower false positive rates. Ablation studies indicate\ncontributions from each graph component, suggesting that structured\nrepresentations and a judge LLM are complementary for normative reasoning.\n","authors":["Jiseong Chung","Ronny Ko","Wonchul Yoo","Makoto Onizuka","Sungmok Kim","Tae-Wan Kim","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2510.26309v1.pdf","comment":"Under review at The Web Conference 2026 (Semantics & Knowledge\n  track). Code will be released upon acceptance. This arXiv v1 contains no\n  repository links to preserve double-blind review"},{"id":"http://arxiv.org/abs/2510.26303v1","updated":"2025-10-30T09:41:33Z","published":"2025-10-30T09:41:33Z","title":"Implicit Bias of Per-sample Adam on Separable Data: Departure from the\n  Full-batch Regime","summary":"  Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet\nits theoretical understanding remains limited. Prior analyses show that Adam\nfavors solutions aligned with $\\ell_\\infty$-geometry, but these results are\nrestricted to the full-batch regime. In this work, we study the implicit bias\nof incremental Adam (using one sample per step) for logistic regression on\nlinearly separable data, and we show that its bias can deviate from the\nfull-batch behavior. To illustrate this, we construct a class of structured\ndatasets where incremental Adam provably converges to the $\\ell_2$-max-margin\nclassifier, in contrast to the $\\ell_\\infty$-max-margin bias of full-batch\nAdam. For general datasets, we develop a proxy algorithm that captures the\nlimiting behavior of incremental Adam as $\\beta_2 \\to 1$ and we characterize\nits convergence direction via a data-dependent dual fixed-point formulation.\nFinally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges\nto the $\\ell_\\infty$-max-margin classifier for any batch size by taking $\\beta$\nclose enough to 1. Overall, our results highlight that the implicit bias of\nAdam crucially depends on both the batching scheme and the dataset, while\nSignum remains invariant.\n","authors":["Beomhan Baek","Minhak Song","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2510.26303v1.pdf","comment":"50 pages"},{"id":"http://arxiv.org/abs/2510.26302v1","updated":"2025-10-30T09:41:21Z","published":"2025-10-30T09:41:21Z","title":"Understanding Hardness of Vision-Language Compositionality from A\n  Token-level Causal Lens","summary":"  Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal\ngeneralization by aligning images and texts in a shared embedding space, yet it\npersistently fails at compositional reasoning over objects, attributes, and\nrelations often behaving like a bag-of-words matcher. Prior causal accounts\ntypically model text as a single vector, obscuring token-level structure and\nleaving core phenomena-such as prompt sensitivity and failures on hard\nnegatives unexplained. We address this gap with a token-aware causal\nrepresentation learning (CRL) framework grounded in a sequential,\nlanguage-token SCM. Our theory extends block identifiability to tokenized text,\nproving that CLIP's contrastive objective can recover the modal-invariant\nlatent variable under both sentence-level and token-level SCMs. Crucially,\ntoken granularity yields the first principled explanation of CLIP's\ncompositional brittleness: composition nonidentifiability. We show the\nexistence of pseudo-optimal text encoders that achieve perfect modal-invariant\nalignment yet are provably insensitive to SWAP, REPLACE, and ADD operations\nover atomic concepts, thereby failing to distinguish correct captions from hard\nnegatives despite optimizing the same training objective as true-optimal\nencoders. The analysis further links language-side nonidentifiability to\nvisual-side failures via the modality gap and shows how iterated composition\noperators compound hardness, motivating improved negative mining strategies.\n","authors":["Ziliang Chen","Tianang Xiao","Jusheng Zhang","Yongsen Zheng","Xipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2510.26302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26298v1","updated":"2025-10-30T09:35:51Z","published":"2025-10-30T09:35:51Z","title":"Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in\n  Web Games","summary":"  OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,\nenabling the model to analyze webpages, process user intents, and execute\ncursor and keyboard inputs directly within the browser. While its capacity for\ninformation retrieval tasks has been demonstrated, its performance in dynamic,\ninteractive environments remains less explored. In this study, we conduct an\nearly evaluation of Atlas's web interaction capabilities using browser-based\ngames as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,\nand Stein.world. We employ in-game performance scores as quantitative metrics\nto assess performance across different task types. Our results show that Atlas\nperforms strongly in logical reasoning tasks like Sudoku, completing puzzles\nsignificantly faster than human baselines, but struggles substantially in\nreal-time games requiring precise timing and motor control, often failing to\nprogress beyond initial obstacles. These findings suggest that while Atlas\ndemonstrates capable analytical processing, there remain notable limitations in\ndynamic web environments requiring real-time interaction. The website of our\nproject can be found at https://atlas-game-eval.github.io.\n","authors":["Jingran Zhang","Ning Li","Justin Cui"],"pdf_url":"https://arxiv.org/pdf/2510.26298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02631v2","updated":"2025-10-30T09:29:47Z","published":"2025-03-04T13:56:18Z","title":"Reflection on Data Storytelling Tools in the Generative AI Era from the\n  Human-AI Collaboration Perspective","summary":"  Human-AI collaborative tools attract attentions from the data storytelling\ncommunity to lower the expertise barrier and streamline the workflow. The\nrecent advance in large-scale generative AI techniques, e.g., large language\nmodels (LLMs) and text-to-image models, has the potential to enhance data\nstorytelling with their power in visual and narration generation. After two\nyears since these techniques were publicly available, it is important to\nreflect our progress of applying them and have an outlook for future\nopportunities. To achieve the goal, we compare the collaboration patterns of\nthe latest tools with those of earlier ones using a dedicated framework for\nunderstanding human-AI collaboration in data storytelling. Through comparison,\nwe identify consistently widely studied patterns, e.g., human-creator +\nAI-assistant, and newly explored or emerging ones, e.g., AI-creator +\nhuman-reviewer. The benefits of these AI techniques and implications to\nhuman-AI collaboration are also revealed. We further propose future directions\nto hopefully ignite innovations.\n","authors":["Haotian Li","Yun Wang","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2503.02631v2.pdf","comment":"This paper is a sequel to the CHI 24 paper \"Where Are We So Far?\n  Understanding Data Storytelling Tools from the Perspective of Human-AI\n  Collaboration (https://doi.org/10.1145/3613904.3642726), aiming to refresh\n  our understanding with the latest advancements. It is accepted at IEEE VIS 25"},{"id":"http://arxiv.org/abs/2502.04380v3","updated":"2025-10-30T09:16:49Z","published":"2025-02-05T17:21:01Z","title":"Diversity as a Reward: Fine-Tuning LLMs on a Mixture of\n  Domain-Undetermined Data","summary":"  Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this work, we investigate the role of data diversity in\nenhancing the overall abilities of LLMs by empirically constructing contrastive\ndata pools and theoretically deriving explanations. Building upon the insights\ngained, we propose a new method that gives the LLM a dual identity: an output\nmodel to cognitively probe and select data based on diversity reward, as well\nas an input model to be tuned with the selected data. Extensive experiments\nshow that the proposed method notably boosts performance across\ndomain-undetermined data and a series of foundational downstream tasks when\napplied to various advanced LLMs. We release our code and hope this study can\nshed light on the understanding of data diversity and advance feedback-driven\ndata-model co-design for LLMs.\n","authors":["Zhenqing Ling","Daoyuan Chen","Liuyi Yao","Qianli Shen","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2502.04380v3.pdf","comment":"Accepted by NeurIPS'25 main track. 47 pages, 21 figures, 32 tables"},{"id":"http://arxiv.org/abs/2510.26285v1","updated":"2025-10-30T09:08:50Z","published":"2025-10-30T09:08:50Z","title":"Unravelling the Mechanisms of Manipulating Numbers in Language Models","summary":"  Recent work has shown that different large language models (LLMs) converge to\nsimilar and accurate input embedding representations for numbers. These\nfindings conflict with the documented propensity of LLMs to produce erroneous\noutputs when dealing with numeric information. In this work, we aim to explain\nthis conflict by exploring how language models manipulate numbers and quantify\nthe lower bounds of accuracy of these mechanisms. We find that despite\nsurfacing errors, different language models learn interchangeable\nrepresentations of numbers that are systematic, highly accurate and universal\nacross their hidden states and the types of input contexts. This allows us to\ncreate universal probes for each LLM and to trace information -- including the\ncauses of output errors -- to specific layers. Our results lay a fundamental\nunderstanding of how pre-trained LLMs manipulate numbers and outline the\npotential of more accurate probing techniques in addressed refinements of LLMs'\narchitectures.\n","authors":["Michal Štefánik","Timothee Mickus","Marek Kadlčík","Bertram Højer","Michal Spiegel","Raúl Vázquez","Aman Sinha","Josef Kuchař","Philipp Mondorf"],"pdf_url":"https://arxiv.org/pdf/2510.26285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26278v1","updated":"2025-10-30T09:00:42Z","published":"2025-10-30T09:00:42Z","title":"Distributional Multi-objective Black-box Optimization for\n  Diffusion-model Inference-time Multi-Target Generation","summary":"  Diffusion models have been successful in learning complex data distributions.\nThis capability has driven their application to high-dimensional\nmulti-objective black-box optimization problem. Existing approaches often\nemploy an external optimization loop, such as an evolutionary algorithm, to the\ndiffusion model. However, these approaches treat the diffusion model as a\nblack-box refiner, which overlooks the internal distribution transition of the\ndiffusion generation process, limiting their efficiency. To address these\nchallenges, we propose the Inference-time Multi-target Generation (IMG)\nalgorithm, which optimizes the diffusion process at inference-time to generate\nsamples that simultaneously satisfy multiple objectives. Specifically, our IMG\nperforms weighted resampling during the diffusion generation process according\nto the expected aggregated multi-objective values. This weighted resampling\nstrategy ensures the diffusion-generated samples are distributed according to\nour desired multi-target Boltzmann distribution. We further derive that the\nmulti-target Boltzmann distribution has an interesting log-likelihood\ninterpretation, where it is the optimal solution to the distributional\nmulti-objective optimization problem. We implemented IMG for a multi-objective\nmolecule generation task. Experiments show that IMG, requiring only a single\ngeneration pass, achieves a significantly higher hypervolume than baseline\noptimization algorithms that often require hundreds of diffusion generations.\nNotably, our algorithm can be viewed as an optimized diffusion process and can\nbe integrated into existing methods to further improve their performance.\n","authors":["Kim Yong Tan","Yueming Lyu","Ivor Tsang","Yew-Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2510.26278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26275v1","updated":"2025-10-30T08:59:01Z","published":"2025-10-30T08:59:01Z","title":"A Research Roadmap for Augmenting Software Engineering Processes and\n  Software Products with Generative AI","summary":"  Generative AI (GenAI) is rapidly transforming software engineering (SE)\npractices, influencing how SE processes are executed, as well as how software\nsystems are developed, operated, and evolved. This paper applies design science\nresearch to build a roadmap for GenAI-augmented SE. The process consists of\nthree cycles that incrementally integrate multiple sources of evidence,\nincluding collaborative discussions from the FSE 2025 \"Software Engineering\n2030\" workshop, rapid literature reviews, and external feedback sessions\ninvolving peers. McLuhan's tetrads were used as a conceptual instrument to\nsystematically capture the transforming effects of GenAI on SE processes and\nsoftware products.The resulting roadmap identifies four fundamental forms of\nGenAI augmentation in SE and systematically characterizes their related\nresearch challenges and opportunities. These insights are then consolidated\ninto a set of future research directions. By grounding the roadmap in a\nrigorous multi-cycle process and cross-validating it among independent author\nteams and peers, the study provides a transparent and reproducible foundation\nfor analyzing how GenAI affects SE processes, methods and tools, and for\nframing future research within this rapidly evolving area. Based on these\nfindings, the article finally makes ten predictions for SE in the year 2030.\n","authors":["Domenico Amalfitano","Andreas Metzger","Marco Autili","Tommaso Fulcini","Tobias Hey","Jan Keim","Patrizio Pelliccione","Vincenzo Scotti","Anne Koziolek","Raffaela Mirandola","Andreas Vogelsang"],"pdf_url":"https://arxiv.org/pdf/2510.26275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26270v1","updated":"2025-10-30T08:53:41Z","published":"2025-10-30T08:53:41Z","title":"Graph-Enhanced Policy Optimization in LLM Agent Training","summary":"  Group based reinforcement learning (RL) has shown impressive results on\ncomplex reasoning and mathematical tasks. Yet, when applied to train\nmulti-turn, interactive LLM agents, these methods often suffer from structural\nblindness-the inability to exploit the underlying connectivity of the\nenvironment. This manifests in three critical challenges: (1) inefficient,\nunguided exploration, (2) imprecise credit assignment due to overlooking\npivotal states, and (3) myopic planning caused by static reward discounting. We\naddress these issues with Graph-Enhanced Policy Optimization (GEPO), which\ndynamically constructs a state-transition graph from agent experience and\nemploys graph-theoretic centrality to provide three synergistic learning\nsignals: (1)structured intrinsic rewards that guide exploration toward\nhigh-impact states, (2) a graph-enhanced advantage function for topology-aware\ncredit assignment, and (3) a dynamic discount factor adapted to each state's\nstrategic value. On the ALFWorld, WebShop, and a proprietary Workbench\nbenchmarks, GEPO demonstrates strong performance, achieving absolute success\nrate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These\nresults highlight that explicitly modeling environmental structure is a robust,\ngeneralizable strategy for advancing LLM agent training.\n","authors":["Jiazhen Yuan","Wei Zhao","Zhengbiao Bai"],"pdf_url":"https://arxiv.org/pdf/2510.26270v1.pdf","comment":"Under review as a conference paper"},{"id":"http://arxiv.org/abs/2510.25160v2","updated":"2025-10-30T08:52:17Z","published":"2025-10-29T04:29:17Z","title":"Model-Document Protocol for AI Search","summary":"  AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.\n","authors":["Hongjin Qian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25160v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2405.09086v2","updated":"2025-10-30T08:49:01Z","published":"2024-05-15T04:47:31Z","title":"Chaos-based reinforcement learning with TD3","summary":"  Chaos-based reinforcement learning (CBRL) is a method in which the agent's\ninternal chaotic dynamics drives exploration. However, the learning algorithms\nin CBRL have not been thoroughly developed in previous studies, nor have they\nincorporated recent advances in reinforcement learning. This study introduced\nTwin Delayed Deep Deterministic Policy Gradients (TD3), which is one of the\nstate-of-the-art deep reinforcement learning algorithms that can treat\ndeterministic and continuous action spaces, to CBRL. The validation results\nprovide several insights. First, TD3 works as a learning algorithm for CBRL in\na simple goal-reaching task. Second, CBRL agents with TD3 can autonomously\nsuppress their exploratory behavior as learning progresses and resume\nexploration when the environment changes. Finally, examining the effect of the\nagent's chaoticity on learning shows that there exists a suitable range of\nchaos strength in the agent's model to flexibly switch between exploration and\nexploitation and adapt to environmental changes.\n","authors":["Toshitaka Matsuki","Yusuke Sakemi","Kazuyuki Aihara"],"pdf_url":"https://arxiv.org/pdf/2405.09086v2.pdf","comment":"Accepted for publication in Neural Networks"},{"id":"http://arxiv.org/abs/2507.09846v3","updated":"2025-10-30T08:39:14Z","published":"2025-07-14T00:54:48Z","title":"Through the River: Understanding the Benefit of Schedule-Free Methods\n  for Language Model Training","summary":"  As both model and dataset sizes continue to scale rapidly, conventional\npretraining strategies with fixed compute budgets-such as cosine learning rate\nschedules-are increasingly inadequate for large-scale training. Recent\nalternatives, including warmup-stable-decay (WSD) schedules and weight\naveraging, offer greater flexibility. However, WSD relies on explicit decay\nphases to track progress, while weight averaging addresses this limitation at\nthe cost of additional memory. In search of a more principled and scalable\nalternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],\nwhich has shown strong empirical performance across diverse settings. We show\nthat SF-AdamW effectively navigates the \"river\" structure of the loss landscape\nwithout decay phases or auxiliary averaging, making it particularly suitable\nfor continuously scaling training workloads. To understand this behavior, we\nconduct a theoretical and empirical analysis of SF dynamics, revealing that it\nimplicitly performs weight averaging without memory overhead. Guided by this\nanalysis, we propose a refined variant of SF that improves robustness to\nmomentum and performs better under large batch sizes, addressing key\nlimitations of the original method. Together, these results establish SF as a\npractical, scalable, and theoretically grounded approach for language model\ntraining.\n","authors":["Minhak Song","Beomhan Baek","Kwangjun Ahn","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2507.09846v3.pdf","comment":"Published at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26243v1","updated":"2025-10-30T08:23:35Z","published":"2025-10-30T08:23:35Z","title":"Angular Steering: Behavior Control via Rotation in Activation Space","summary":"  Controlling specific behaviors in large language models while preserving\ntheir general capabilities is a central challenge for safe and reliable\nartificial intelligence deployment. Current steering methods, such as vector\naddition and directional ablation, are constrained within a two-dimensional\nsubspace defined by the activation and feature direction, making them sensitive\nto chosen parameters and potentially affecting unrelated features due to\nunintended interactions in activation space. We introduce Angular Steering, a\nnovel and flexible method for behavior modulation that operates by rotating\nactivations within a fixed two-dimensional subspace. By formulating steering as\na geometric rotation toward or away from a target behavior direction, Angular\nSteering provides continuous, fine-grained control over behaviors such as\nrefusal and compliance. We demonstrate this method using refusal steering\nemotion steering as use cases. Additionally, we propose Adaptive Angular\nSteering, a selective variant that rotates only activations aligned with the\ntarget feature, further enhancing stability and coherence. Angular Steering\ngeneralizes existing addition and orthogonalization techniques under a unified\ngeometric rotation framework, simplifying parameter selection and maintaining\nmodel stability across a broader range of adjustments. Experiments across\nmultiple model families and sizes show that Angular Steering achieves robust\nbehavioral control while maintaining general language modeling performance,\nunderscoring its flexibility, generalization, and robustness compared to prior\napproaches. Code and artifacts are available at\nhttps://github.com/lone17/angular-steering/.\n","authors":["Hieu M. Vu","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2510.26243v1.pdf","comment":"NeurIPS 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2510.26242v1","updated":"2025-10-30T08:23:08Z","published":"2025-10-30T08:23:08Z","title":"Retrieval Augmented Generation-Enhanced Distributed LLM Agents for\n  Generalizable Traffic Signal Control with Emergency Vehicles","summary":"  With increasing urban traffic complexity, Traffic Signal Control (TSC) is\nessential for optimizing traffic flow and improving road safety. Large Language\nModels (LLMs) emerge as promising approaches for TSC. However, they are prone\nto hallucinations in emergencies, leading to unreliable decisions that may\ncause substantial delays for emergency vehicles. Moreover, diverse intersection\ntypes present substantial challenges for traffic state encoding and\ncross-intersection training, limiting generalization across heterogeneous\nintersections. Therefore, this paper proposes Retrieval Augmented Generation\n(RAG)-enhanced distributed LLM agents with Emergency response for Generalizable\nTSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning\nframework, which dynamically adjusts reasoning depth based on the emergency\nscenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to\ndistill specific knowledge and guidance from historical cases, enhancing the\nreliability and rationality of agents' emergency decisions. Secondly, this\npaper designs a type-agnostic traffic representation and proposes a\nReward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3\nadaptively samples training experience from diverse intersections with\nenvironment feedback-based priority and fine-tunes LLM agents with a designed\nreward-weighted likelihood loss, guiding REG-TSC toward high-reward policies\nacross heterogeneous intersections. On three real-world road networks with 17\nto 177 heterogeneous intersections, extensive experiments show that REG-TSC\nreduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle\nwaiting time by 83.16%, outperforming other state-of-the-art methods.\n","authors":["Xinhang Li","Qing Guo","Junyu Chen","Zheng Guo","Shengzhe Xu","Lei Li","Lin Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26238v1","updated":"2025-10-30T08:18:37Z","published":"2025-10-30T08:18:37Z","title":"Questionnaire meets LLM: A Benchmark and Empirical Study of Structural\n  Skills for Understanding Questions and Responses","summary":"  Millions of people take surveys every day, from market polls and academic\nstudies to medical questionnaires and customer feedback forms. These datasets\ncapture valuable insights, but their scale and structure present a unique\nchallenge for large language models (LLMs), which otherwise excel at few-shot\nreasoning over open-ended text. Yet, their ability to process questionnaire\ndata or lists of questions crossed with hundreds of respondent rows remains\nunderexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,\nSPSS, REDCap) are typically designed for humans in the workflow, limiting such\ndata integration with LLM and AI-empowered automation. This gap leaves\nscientists, surveyors, and everyday users without evidence-based guidance on\nhow to best represent questionnaires for LLM consumption. We address this by\nintroducing QASU (Questionnaire Analysis and Structural Understanding), a\nbenchmark that probes six structural skills, including answer lookup,\nrespondent count, and multi-hop inference, across six serialization formats and\nmultiple prompt strategies. Experiments on contemporary LLMs show that choosing\nan effective format and prompt combination can improve accuracy by up to 8.8%\npoints compared to suboptimal formats. For specific tasks, carefully adding a\nlightweight structural hint through self-augmented prompting can yield further\nimprovements of 3-4% points on average. By systematically isolating format and\nprompting effects, our open source benchmark offers a simple yet versatile\nfoundation for advancing both research and real-world practice in LLM-based\nquestionnaire analysis.\n","authors":["Duc-Hai Nguyen","Vijayakumar Nanjappan","Barry O'Sullivan","Hoang D. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2510.26238v1.pdf","comment":"14 pages, 3 figures, 8 tables"},{"id":"http://arxiv.org/abs/2510.25366v2","updated":"2025-10-30T08:16:40Z","published":"2025-10-29T10:37:24Z","title":"A Convexity-dependent Two-Phase Training Algorithm for Deep Neural\n  Networks","summary":"  The key task of machine learning is to minimize the loss function that\nmeasures the model fit to the training data. The numerical methods to do this\nefficiently depend on the properties of the loss function. The most decisive\namong these properties is the convexity or non-convexity of the loss function.\nThe fact that the loss function can have, and frequently has, non-convex\nregions has led to a widespread commitment to non-convex methods such as Adam.\nHowever, a local minimum implies that, in some environment around it, the\nfunction is convex. In this environment, second-order minimizing methods such\nas the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We\npropose a novel framework grounded in the hypothesis that loss functions in\nreal-world tasks swap from initial non-convexity to convexity towards the\noptimum. This is a property we leverage to design an innovative two-phase\noptimization algorithm. The presented algorithm detects the swap point by\nobserving the gradient norm dependence on the loss. In these regions,\nnon-convex (Adam) and convex (CG) algorithms are used, respectively. Computing\nexperiments confirm the hypothesis that this simple convexity structure is\nfrequent enough to be practically exploited to substantially improve\nconvergence and accuracy.\n","authors":["Tomas Hrycej","Bernhard Bermeitinger","Massimo Pavone","Götz-Henrik Wiegand","Siegfried Handschuh"],"pdf_url":"https://arxiv.org/pdf/2510.25366v2.pdf","comment":"Appeared on KDIR IC3K Conference 2025 (Best Paper Award). Published\n  in \"Proceedings of the 17th International Joint Conference on Knowledge\n  Discovery, Knowledge Engineering and Knowledge Management - Volume 1\""},{"id":"http://arxiv.org/abs/2509.06159v3","updated":"2025-10-30T08:10:05Z","published":"2025-09-07T17:59:09Z","title":"FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes","summary":"  The growing popularity of robotic minimally invasive surgeries has made deep\nlearning-based surgical training a key area of research. A thorough\nunderstanding of the surgical scene components is crucial, which semantic\nsegmentation models can help achieve. However, most existing work focuses on\nsurgical tools and overlooks anatomical objects. Additionally, current\nstate-of-the-art (SOTA) models struggle to balance capturing high-level\ncontextual features and low-level edge features. We propose a Feature-Adaptive\nSpatial Localization model (FASL-Seg), designed to capture features at multiple\nlevels of detail through two distinct processing streams, namely a Low-Level\nFeature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream,\nfor varying feature resolutions - enabling precise segmentation of anatomy and\nsurgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark\ndatasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model\nachieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy\nsegmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU\nof 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation,\nrespectively, outperforming SOTA overall performance, with comparable per-class\nSOTA results in both datasets and consistent performance in various classes for\nanatomy and instruments, demonstrating the effectiveness of distinct processing\nstreams for varying feature resolutions.\n","authors":["Muraam Abdel-Ghani","Mahmoud Ali","Mohamed Ali","Fatmaelzahraa Ahmed","Muhammad Arsalan","Abdulaziz Al-Ali","Shidin Balakrishnan"],"pdf_url":"https://arxiv.org/pdf/2509.06159v3.pdf","comment":"8 pages, 6 figures, In Proceedings of European Conference on\n  Artificial Intelligence (ECAI) 2025 <https://doi.org/10.3233/FAIA250908>"},{"id":"http://arxiv.org/abs/2510.26230v1","updated":"2025-10-30T08:09:37Z","published":"2025-10-30T08:09:37Z","title":"MPRU: Modular Projection-Redistribution Unlearning as Output Filter for\n  Classification Pipelines","summary":"  As a new and promising approach, existing machine unlearning (MU) works\ntypically emphasize theoretical formulations or optimization objectives to\nachieve knowledge removal. However, when deployed in real-world scenarios, such\nsolutions typically face scalability issues and have to address practical\nrequirements such as full access to original datasets and model. In contrast to\nthe existing approaches, we regard classification training as a sequential\nprocess where classes are learned sequentially, which we call \\emph{inductive\napproach}. Unlearning can then be done by reversing the last training sequence.\nThis is implemented by appending a projection-redistribution layer in the end\nof the model. Such an approach does not require full access to the original\ndataset or the model, addressing the challenges of existing methods. This\nenables modular and model-agnostic deployment as an output filter into existing\nclassification pipelines with minimal alterations. We conducted multiple\nexperiments across multiple datasets including image (CIFAR-10/100 using\nCNN-based model) and tabular datasets (Covertype using tree-based model).\nExperiment results show consistently similar output to a fully retrained model\nwith a high computational cost reduction. This demonstrates the applicability,\nscalability, and system compatibility of our solution while maintaining the\nperformance of the output in a more practical setting.\n","authors":["Minyi Peng","Darian Gunamardi","Ivan Tjuawinata","Kwok-Yan Lam"],"pdf_url":"https://arxiv.org/pdf/2510.26230v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2508.07981v3","updated":"2025-10-30T08:09:13Z","published":"2025-08-11T13:41:24Z","title":"Omni-Effects: Unified and Spatially-Controllable Visual Effects\n  Generation","summary":"  Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects.\n","authors":["Fangyuan Mao","Aiming Hao","Jintao Chen","Dongxia Liu","Xiaokun Feng","Jiashu Zhu","Meiqi Wu","Chubin Chen","Jiahong Wu","Xiangxiang Chu"],"pdf_url":"https://arxiv.org/pdf/2508.07981v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26219v1","updated":"2025-10-30T07:52:14Z","published":"2025-10-30T07:52:14Z","title":"Test-Time Alignment of LLMs via Sampling-Based Optimal Control in\n  pre-logit space","summary":"  Test-time alignment of large language models (LLMs) attracts attention\nbecause fine-tuning LLMs requires high computational costs. In this paper, we\npropose a new test-time alignment method called adaptive importance sampling on\npre-logits (AISP) on the basis of the sampling-based model predictive control\nwith the stochastic control input. AISP applies the Gaussian perturbation into\npre-logits, which are outputs of the penultimate layer, so as to maximize\nexpected rewards with respect to the mean of the perturbation. We demonstrate\nthat the optimal mean is obtained by importance sampling with sampled rewards.\nAISP outperforms best-of-n sampling in terms of rewards over the number of used\nsamples and achieves higher rewards than other reward-based test-time alignment\nmethods.\n","authors":["Sekitoshi Kanai","Tsukasa Yoshida","Hiroshi Takahashi","Haru Kuroki","Kazumune Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2510.26219v1.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.26217v1","updated":"2025-10-30T07:46:40Z","published":"2025-10-30T07:46:40Z","title":"Hybrid LLM and Higher-Order Quantum Approximate Optimization for CSA\n  Collateral Management","summary":"  We address finance-native collateral optimization under ISDA Credit Support\nAnnexes (CSAs), where integer lots, Schedule A haircuts, RA/MTA gating, and\nissuer/currency/class caps create rugged, legally bounded search spaces. We\nintroduce a certifiable hybrid pipeline purpose-built for this domain: (i) an\nevidence-gated LLM that extracts CSA terms to a normalized JSON\n(abstain-by-default, span-cited); (ii) a quantum-inspired explorer that\ninterleaves simulated annealing with micro higher order QAOA (HO-QAOA) on\nbinding sub-QUBOs (subset size n <= 16, order k <= 4) to coordinate multi-asset\nmoves across caps and RA-induced discreteness; (iii) a weighted risk-aware\nobjective (Movement, CVaR, funding-priced overshoot) with an explicit coverage\nwindow U <= Reff+B; and (iv) CP-SAT as single arbiter to certify feasibility\nand gaps, including a U-cap pre-check that reports the minimal feasible buffer\nB*. Encoding caps/rounding as higher-order terms lets HO-QAOA target the domain\ncouplings that defeat local swaps. On government bond datasets and multi-CSA\ninputs, the hybrid improves a strong classical baseline (BL-3) by 9.1%, 9.6%,\nand 10.7% across representative harnesses, delivering better cost-movement-tail\nfrontiers under governance settings. We release governance grade artifacts-span\ncitations, valuation matrix audit, weight provenance, QUBO manifests, and\nCP-SAT traces-to make results auditable and reproducible.\n","authors":["Tao Jin","Stuart Florescu"," Heyu"," Jin"],"pdf_url":"https://arxiv.org/pdf/2510.26217v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2510.26205v1","updated":"2025-10-30T07:29:14Z","published":"2025-10-30T07:29:14Z","title":"Towards Global Retrieval Augmented Generation: A Benchmark for\n  Corpus-Level Reasoning","summary":"  Retrieval-augmented generation (RAG) has emerged as a leading approach to\nreducing hallucinations in large language models (LLMs). Current RAG evaluation\nbenchmarks primarily focus on what we call local RAG: retrieving relevant\nchunks from a small subset of documents to answer queries that require only\nlocalized understanding within specific text chunks. However, many real-world\napplications require a fundamentally different capability -- global RAG --\nwhich involves aggregating and analyzing information across entire document\ncollections to derive corpus-level insights (for example, \"What are the top 10\nmost cited papers in 2023?\"). In this paper, we introduce GlobalQA -- the first\nbenchmark specifically designed to evaluate global RAG capabilities, covering\nfour core task types: counting, extremum queries, sorting, and top-k\nextraction. Through systematic evaluation across different models and\nbaselines, we find that existing RAG methods perform poorly on global tasks,\nwith the strongest baseline achieving only 1.51 F1 score. To address these\nchallenges, we propose GlobalRAG, a multi-tool collaborative framework that\npreserves structural coherence through chunk-level retrieval, incorporates\nLLM-driven intelligent filters to eliminate noisy documents, and integrates\naggregation modules for precise symbolic computation. On the Qwen2.5-14B model,\nGlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,\nvalidating the effectiveness of our method.\n","authors":["Qi Luo","Xiaonan Li","Tingshuo Fan","Xinchi Chen","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2510.26205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10173v2","updated":"2025-10-30T07:25:20Z","published":"2025-06-11T20:53:45Z","title":"SPARKE: Scalable Prompt-Aware Diversity and Novelty Guidance in\n  Diffusion Models via RKE Score","summary":"  Diffusion models have demonstrated remarkable success in high-fidelity image\nsynthesis and prompt-guided generative modeling. However, ensuring adequate\ndiversity in generated samples of prompt-guided diffusion models remains a\nchallenge, particularly when the prompts span a broad semantic spectrum and the\ndiversity of generated data needs to be evaluated in a prompt-aware fashion\nacross semantically similar prompts. Recent methods have introduced guidance\nvia diversity measures to encourage more varied generations. In this work, we\nextend the diversity measure-based approaches by proposing the Scalable\nPrompt-Aware R\\'eny Kernel Entropy Diversity Guidance (SPARKE) method for\nprompt-aware diversity guidance. SPARKE utilizes conditional entropy for\ndiversity guidance, which dynamically conditions diversity measurement on\nsimilar prompts and enables prompt-aware diversity control. While the\nentropy-based guidance approach enhances prompt-aware diversity, its reliance\non the matrix-based entropy scores poses computational challenges in\nlarge-scale generation settings. To address this, we focus on the special case\nof Conditional latent RKE Score Guidance, reducing entropy computation and\ngradient-based optimization complexity from the $O(n^3)$ of general entropy\nmeasures to $O(n)$. The reduced computational complexity allows for\ndiversity-guided sampling over potentially thousands of generation rounds on\ndifferent prompts. We numerically test the SPARKE method on several\ntext-to-image diffusion models, demonstrating that the proposed method improves\nthe prompt-aware diversity of the generated data without incurring significant\ncomputational costs. We release our code on the project page:\nhttps://mjalali.github.io/SPARKE\n","authors":["Mohammad Jalali","Haoyu Lei","Amin Gohari","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2506.10173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26202v1","updated":"2025-10-30T07:25:10Z","published":"2025-10-30T07:25:10Z","title":"What's In My Human Feedback? Learning Interpretable Descriptions of\n  Preference Data","summary":"  Human feedback can alter language models in unpredictable and undesirable\nways, as practitioners lack a clear understanding of what feedback data\nencodes. While prior work studies preferences over certain attributes (e.g.,\nlength or sycophancy), automatically extracting relevant features without\npre-specifying hypotheses remains challenging. We introduce What's In My Human\nFeedback? (WIMHF), a method to explain feedback data using sparse autoencoders.\nWIMHF characterizes both (1) the preferences a dataset is capable of measuring\nand (2) the preferences that the annotators actually express. Across 7\ndatasets, WIMHF identifies a small number of human-interpretable features that\naccount for the majority of the preference prediction signal achieved by\nblack-box models. These features reveal a wide diversity in what humans prefer,\nand the role of dataset-level context: for example, users on Reddit prefer\ninformality and jokes, while annotators in HH-RLHF and PRISM disprefer them.\nWIMHF also surfaces potentially unsafe preferences, such as that LMArena users\ntend to vote against refusals, often in favor of toxic content. The learned\nfeatures enable effective data curation: re-labeling the harmful examples in\nArena yields large safety gains (+37%) with no cost to general performance.\nThey also allow fine-grained personalization: on the Community Alignment\ndataset, we learn annotator-specific weights over subjective features that\nimprove preference prediction. WIMHF provides a human-centered analysis method\nfor practitioners to better understand and use preference data.\n","authors":["Rajiv Movva","Smitha Milli","Sewon Min","Emma Pierson"],"pdf_url":"https://arxiv.org/pdf/2510.26202v1.pdf","comment":"Code: https://github.com/rmovva/wimhf"},{"id":"http://arxiv.org/abs/2510.26200v1","updated":"2025-10-30T07:21:05Z","published":"2025-10-30T07:21:05Z","title":"Don't Let It Fade: Preserving Edits in Diffusion Language Models via\n  Token Timestep Allocation","summary":"  While diffusion language models (DLMs) enable fine-grained refinement, their\npractical controllability remains fragile. We identify and formally\ncharacterize a central failure mode called update forgetting, in which uniform\nand context agnostic updates induce token level fluctuations across timesteps,\nerasing earlier semantic edits and disrupting the cumulative refinement\nprocess, thereby degrading fluency and coherence. As this failure originates in\nuniform and context agnostic updates, effective control demands explicit token\nordering. We propose Token Timestep Allocation (TTA), which realizes soft and\nsemantic token ordering via per token timestep schedules: critical tokens are\nfrozen early, while uncertain tokens receive continued refinement. This\ntimestep based ordering can be instantiated as either a fixed policy or an\nadaptive policy driven by task signals, thereby supporting a broad spectrum of\nrefinement strategies. Because it operates purely at inference time, it applies\nuniformly across various DLMs and naturally extends to diverse supervision\nsources. Empirically, TTA improves controllability and fluency: on sentiment\ncontrol, it yields more than 20 percent higher accuracy and nearly halves\nperplexity using less than one fifth the steps; in detoxification, it lowers\nmaximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).\nTogether, these results demonstrate that softened ordering via timestep\nallocation is the critical lever for mitigating update forgetting and achieving\nstable and controllable diffusion text generation.\n","authors":["Woojin Kim","Jaeyoung Do"],"pdf_url":"https://arxiv.org/pdf/2510.26200v1.pdf","comment":"Accepted in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.01158v2","updated":"2025-10-30T07:20:59Z","published":"2025-06-01T20:32:27Z","title":"Efficient Regression-Based Training of Normalizing Flows for Boltzmann\n  Generators","summary":"  Simulation-free training frameworks have been at the forefront of the\ngenerative modelling revolution in continuous spaces, leading to large-scale\ndiffusion and flow matching models. However, such modern generative models\nsuffer from expensive inference, inhibiting their use in numerous scientific\napplications like Boltzmann Generators (BGs) for molecular conformations that\nrequire fast likelihood evaluation. In this paper, we revisit classical\nnormalizing flows in the context of BGs that offer efficient sampling and\nlikelihoods, but whose training via maximum likelihood is often unstable and\ncomputationally challenging. We propose Regression Training of Normalizing\nFlows (RegFlow), a novel and scalable regression-based training objective that\nbypasses the numerical instability and computational challenge of conventional\nmaximum likelihood training in favour of a simple $\\ell_2$-regression\nobjective. Specifically, RegFlow maps prior samples under our flow to targets\ncomputed using optimal transport couplings or a pre-trained continuous\nnormalizing flow (CNF). To enhance numerical stability, RegFlow employs\neffective regularization strategies such as a new forward-backward\nself-consistency loss that enjoys painless implementation. Empirically, we\ndemonstrate that RegFlow unlocks a broader class of architectures that were\npreviously intractable to train for BGs with maximum likelihood. We also show\nRegFlow exceeds the performance, computational cost, and stability of maximum\nlikelihood training in equilibrium sampling in Cartesian coordinates of alanine\ndipeptide, tripeptide, and tetrapeptide, showcasing its potential in molecular\nsystems.\n","authors":["Danyal Rehman","Oscar Davis","Jiarui Lu","Jian Tang","Michael Bronstein","Yoshua Bengio","Alexander Tong","Avishek Joey Bose"],"pdf_url":"https://arxiv.org/pdf/2506.01158v2.pdf","comment":"Preprint; ICML GenBio Best Paper Award 2025"},{"id":"http://arxiv.org/abs/2505.15095v2","updated":"2025-10-30T07:18:23Z","published":"2025-05-21T04:34:22Z","title":"Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable\n  Sarcasm Detection of Australian and Indian English","summary":"  Sarcasm is a challenge to sentiment analysis because of the incongruity\nbetween stated and implied sentiment. The challenge is exacerbated when the\nimplication may be relevant to a specific country or geographical region.\nPragmatic metacognitive prompting (PMP) is a cognition-inspired technique that\nhas been used for pragmatic reasoning. In this paper, we harness PMP for\nexplainable sarcasm detection for Australian and Indian English, alongside a\nbenchmark dataset for standard English. We manually add sarcasm explanations to\nan existing sarcasm-labeled dataset for Australian and Indian English called\nBESSTIE, and compare the performance for explainable sarcasm detection for them\nwith FLUTE, a standard English dataset containing sarcasm explanations. Our\napproach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA)\nachieves statistically significant performance improvement across all tasks and\ndatasets when compared with four alternative prompting strategies. We also find\nthat alternative techniques such as agentic prompting mitigate context-related\nfailures by enabling external knowledge retrieval. The focused contribution of\nour work is utilising PMP in generating sarcasm explanations for varieties of\nEnglish.\n","authors":["Ishmanbir Singh","Dipankar Srirag","Aditya Joshi"],"pdf_url":"https://arxiv.org/pdf/2505.15095v2.pdf","comment":"ALTA 2025 (Best Paper Honorable Mention). Camera-ready"},{"id":"http://arxiv.org/abs/2505.13904v3","updated":"2025-10-30T07:17:31Z","published":"2025-05-20T04:10:50Z","title":"Learning to Insert for Constructive Neural Vehicle Routing Solver","summary":"  Neural Combinatorial Optimisation (NCO) is a promising learning-based\napproach for solving Vehicle Routing Problems (VRPs) without extensive manual\ndesign. While existing constructive NCO methods typically follow an\nappending-based paradigm that sequentially adds unvisited nodes to partial\nsolutions, this rigid approach often leads to suboptimal results. To overcome\nthis limitation, we explore the idea of insertion-based paradigm and propose\nLearning to Construct with Insertion-based Paradigm (L2C-Insert), a novel\nlearning-based method for constructive NCO. Unlike traditional approaches,\nL2C-Insert builds solutions by strategically inserting unvisited nodes at any\nvalid position in the current partial solution, which can significantly enhance\nthe flexibility and solution quality. The proposed framework introduces three\nkey components: a novel model architecture for precise insertion position\nprediction, an efficient training scheme for model optimization, and an\nadvanced inference technique that fully exploits the insertion paradigm's\nflexibility. Extensive experiments on both synthetic and real-world instances\nof the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) demonstrate that L2C-Insert consistently achieves superior\nperformance across various problem sizes.\n","authors":["Fu Luo","Xi Lin","Mengyuan Zhong","Fei Liu","Zhenkun Wang","Jianyong Sun","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13904v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.14970v4","updated":"2025-10-30T07:03:09Z","published":"2025-05-20T23:17:15Z","title":"Self-Evolving Curriculum for LLM Reasoning","summary":"  Reinforcement learning (RL) has proven effective for fine-tuning large\nlanguage models (LLMs), significantly enhancing their reasoning abilities in\ndomains such as mathematics and code generation. A crucial factor influencing\nRL fine-tuning success is the training curriculum: the order in which training\nproblems are presented. While random curricula serve as common baselines, they\nremain suboptimal; manually designed curricula often rely heavily on\nheuristics, and online filtering methods can be computationally prohibitive. To\naddress these limitations, we propose Self-Evolving Curriculum (SEC), an\nautomatic curriculum learning method that learns a curriculum policy\nconcurrently with the RL fine-tuning process. Our approach formulates\ncurriculum selection as a non-stationary Multi-Armed Bandit problem, treating\neach problem category (e.g., difficulty level or problem type) as an individual\narm. We leverage the absolute advantage from policy gradient methods as a proxy\nmeasure for immediate learning gain. At each training step, the curriculum\npolicy selects categories to maximize this reward signal and is updated using\nthe TD(0) method. Across three distinct reasoning domains: planning, inductive\nreasoning, and mathematics, our experiments demonstrate that SEC significantly\nimproves models' reasoning capabilities, enabling better generalization to\nharder, out-of-distribution test problems. Additionally, our approach achieves\nbetter skill balance when fine-tuning simultaneously on multiple reasoning\ndomains. These findings highlight SEC as a promising strategy for RL\nfine-tuning of LLMs.\n","authors":["Xiaoyin Chen","Jiarui Lu","Minsu Kim","Dinghuai Zhang","Jian Tang","Alexandre Piché","Nicolas Gontier","Yoshua Bengio","Ehsan Kamalloo"],"pdf_url":"https://arxiv.org/pdf/2505.14970v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09937v2","updated":"2025-10-30T07:02:35Z","published":"2025-06-11T16:59:13Z","title":"SAFE: Multitask Failure Detection for Vision-Language-Action Models","summary":"  While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out of the box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while generalist VLAs require the detector to generalize\nand detect failures also in unseen tasks and novel environments. In this paper,\nwe introduce the multitask failure detection problem and propose SAFE, a\nfailure detector for generalist robot policies such as VLAs. We analyze the VLA\nfeature space and find that VLAs have sufficient high-level knowledge about\ntask success and failure, which is generic across different tasks. Based on\nthis insight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, $\\pi_0$,\nand $\\pi_0$-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults and code can be found at the project webpage:\nhttps://vla-safe.github.io/\n","authors":["Qiao Gu","Yuanliang Ju","Shengxiang Sun","Igor Gilitschenski","Haruki Nishimura","Masha Itkina","Florian Shkurti"],"pdf_url":"https://arxiv.org/pdf/2506.09937v2.pdf","comment":"NeurIPS 2025 camera ready. Project Page: https://vla-safe.github.io/"},{"id":"http://arxiv.org/abs/2509.16648v2","updated":"2025-10-30T06:55:22Z","published":"2025-09-20T11:50:22Z","title":"FESTA: Functionally Equivalent Sampling for Trust Assessment of\n  Multimodal LLMs","summary":"  The accurate trust assessment of multimodal large language models (MLLMs)\ngenerated predictions, which can enable selective prediction and improve user\nconfidence, is challenging due to the diverse multi-modal input paradigms. We\npropose Functionally Equivalent Sampling for Trust Assessment (FESTA), a\nmultimodal input sampling technique for MLLMs, that generates an uncertainty\nmeasure based on the equivalent and complementary input samplings. The proposed\ntask-preserving sampling approach for uncertainty quantification expands the\ninput space to probe the consistency (through equivalent samples) and\nsensitivity (through complementary samples) of the model. FESTA uses only\ninput-output access of the model (black-box), and does not require ground truth\n(unsupervised). The experiments are conducted with various off-the-shelf\nmulti-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA\nuncertainty estimate achieves significant improvement (33.3% relative\nimprovement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in\nselective prediction performance, based on\narea-under-receiver-operating-characteristic curve (AUROC) metric in detecting\nmispredictions. The code implementation is open-sourced.\n","authors":["Debarpan Bhattacharya","Apoorva Kulkarni","Sriram Ganapathy"],"pdf_url":"https://arxiv.org/pdf/2509.16648v2.pdf","comment":"Accepted in the Findings of EMNLP, 2025"},{"id":"http://arxiv.org/abs/2510.26188v1","updated":"2025-10-30T06:54:19Z","published":"2025-10-30T06:54:19Z","title":"Predicting All-Cause Hospital Readmissions from Medical Claims Data of\n  Hospitalised Patients","summary":"  Reducing preventable hospital readmissions is a national priority for payers,\nproviders, and policymakers seeking to improve health care and lower costs. The\nrate of readmission is being used as a benchmark to determine the quality of\nhealthcare provided by the hospitals. In thisproject, we have used machine\nlearning techniques like Logistic Regression, Random Forest and Support Vector\nMachines to analyze the health claims data and identify demographic and medical\nfactors that play a crucial role in predicting all-cause readmissions. As the\nhealth claims data is high dimensional, we have used Principal Component\nAnalysis as a dimension reduction technique and used the results for building\nregression models. We compared and evaluated these models based on the Area\nUnder Curve (AUC) metric. Random Forest model gave the highest performance\nfollowed by Logistic Regression and Support Vector Machine models. These models\ncan be used to identify the crucial factors causing readmissions and help\nidentify patients to focus on to reduce the chances of readmission, ultimately\nbringing down the cost and increasing the quality of healthcare provided to the\npatients.\n","authors":["Avinash Kadimisetty","Arun Rajagopalan","Vijendra SK"],"pdf_url":"https://arxiv.org/pdf/2510.26188v1.pdf","comment":"NCMLAI 2018"},{"id":"http://arxiv.org/abs/2510.26186v1","updated":"2025-10-30T06:46:17Z","published":"2025-10-30T06:46:17Z","title":"ConceptScope: Characterizing Dataset Bias via Disentangled Visual\n  Concepts","summary":"  Dataset bias, where data points are skewed to certain concepts, is ubiquitous\nin machine learning datasets. Yet, systematically identifying these biases is\nchallenging without costly, fine-grained attribute annotations. We present\nConceptScope, a scalable and automated framework for analyzing visual datasets\nby discovering and quantifying human-interpretable concepts using Sparse\nAutoencoders trained on representations from vision foundation models.\nConceptScope categorizes concepts into target, context, and bias types based on\ntheir semantic relevance and statistical correlation to class labels, enabling\nclass-level dataset characterization, bias identification, and robustness\nevaluation through concept-based subgrouping. We validate that ConceptScope\ncaptures a wide range of visual concepts, including objects, textures,\nbackgrounds, facial attributes, emotions, and actions, through comparisons with\nannotated datasets. Furthermore, we show that concept activations produce\nspatial attributions that align with semantically meaningful image regions.\nConceptScope reliably detects known biases (e.g., background bias in\nWaterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects\nin ImageNet), offering a practical tool for dataset auditing and model\ndiagnostics.\n","authors":["Jinho Choi","Hyesu Lim","Steffen Schneider","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2510.26186v1.pdf","comment":"Published in the Thirty-Ninth Conference on Neural Information\n  Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.26185v1","updated":"2025-10-30T06:45:22Z","published":"2025-10-30T06:45:22Z","title":"Accumulative SGD Influence Estimation for Data Attribution","summary":"  Modern data-centric AI needs precise per-sample influence. Standard SGD-IE\napproximates leave-one-out effects by summing per-epoch surrogates and ignores\ncross-epoch compounding, which misranks critical examples. We propose\nACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out\nperturbation across training and updates an accumulative influence state at\neach step. In smooth strongly convex settings it achieves geometric error\ncontraction and, in smooth non-convex regimes, it tightens error bounds; larger\nmini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups,\nand MNIST under clean and corrupted data and both convex and non-convex\ntraining, ACC-SGD-IE yields more accurate influence estimates, especially over\nlong epochs. For downstream data cleansing it more reliably flags noisy\nsamples, producing models trained on ACC-SGD-IE cleaned data that outperform\nthose cleaned with SGD-IE.\n","authors":["Yunxiao Shi","Shuo Yang","Yixin Su","Rui Zhang","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2510.26185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13308v2","updated":"2025-10-30T06:23:27Z","published":"2025-05-19T16:26:02Z","title":"Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space","summary":"  Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.\n","authors":["Hengli Li","Chenxi Li","Tong Wu","Xuekai Zhu","Yuxuan Wang","Zhaoxin Yu","Eric Hanchen Jiang","Song-Chun Zhu","Zixia Jia","Ying Nian Wu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.13308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26172v1","updated":"2025-10-30T06:22:49Z","published":"2025-10-30T06:22:49Z","title":"Linking Heterogeneous Data with Coordinated Agent Flows for Social Media\n  Analysis","summary":"  Social media platforms generate massive volumes of heterogeneous data,\ncapturing user behaviors, textual content, temporal dynamics, and network\nstructures. Analyzing such data is crucial for understanding phenomena such as\nopinion dynamics, community formation, and information diffusion. However,\ndiscovering insights from this complex landscape is exploratory, conceptually\nchallenging, and requires expertise in social media mining and visualization.\nExisting automated approaches, though increasingly leveraging large language\nmodels (LLMs), remain largely confined to structured tabular data and cannot\nadequately address the heterogeneity of social media analysis. We present SIA\n(Social Insight Agents), an LLM agent system that links heterogeneous\nmulti-modal data -- including raw inputs (e.g., text, network, and behavioral\ndata), intermediate outputs, mined analytical results, and visualization\nartifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy\nthat connects insight types with suitable mining and visualization techniques,\nSIA enables agents to plan and execute coherent analysis strategies. To ensure\nmulti-modal integration, it incorporates a data coordinator that unifies\ntabular, textual, and network data into a consistent flow. Its interactive\ninterface provides a transparent workflow where users can trace, validate, and\nrefine the agent's reasoning, supporting both adaptability and trustworthiness.\nThrough expert-centered case studies and quantitative evaluation, we show that\nSIA effectively discovers diverse and meaningful insights from social media\nwhile supporting human-agent collaboration in complex analytical tasks.\n","authors":["Shifu Chen","Dazhen Deng","Zhihong Xu","Sijia Xu","Tai-Quan Peng","Yingcai Wu"],"pdf_url":"https://arxiv.org/pdf/2510.26172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11094v2","updated":"2025-10-30T06:22:33Z","published":"2025-06-06T05:50:50Z","title":"The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of\n  LLMs","summary":"  With the rapid advancement of artificial intelligence, Large Language Models\n(LLMs) have shown remarkable capabilities in Natural Language Processing (NLP),\nincluding content generation, human-computer interaction, machine translation,\nand code generation. However, their widespread deployment has also raised\nsignificant safety concerns. In particular, LLM-generated content can exhibit\nunsafe behaviors such as toxicity, bias, or misinformation, especially in\nadversarial contexts, which has attracted increasing attention from both\nacademia and industry. Although numerous studies have attempted to evaluate\nthese risks, a comprehensive and systematic survey on safety evaluation of LLMs\nis still lacking. This work aims to fill this gap by presenting a structured\noverview of recent advances in safety evaluation of LLMs. Specifically, we\npropose a four-dimensional taxonomy: (i) Why to evaluate, which explores the\nbackground of safety evaluation of LLMs, how they differ from general LLMs\nevaluation, and the significance of such evaluation; (ii) What to evaluate,\nwhich examines and categorizes existing safety evaluation tasks based on key\ncapabilities, including dimensions such as toxicity, robustness, ethics, bias\nand fairness, truthfulness, and related aspects; (iii) Where to evaluate, which\nsummarizes the evaluation metrics, datasets and benchmarks currently used in\nsafety evaluations; (iv) How to evaluate, which reviews existing mainstream\nevaluation methods based on the roles of the evaluators and some evaluation\nframeworks that integrate the entire evaluation pipeline. Finally, we identify\nthe challenges in safety evaluation of LLMs and propose promising research\ndirections to promote further advancement in this field. We emphasize the\nnecessity of prioritizing safety evaluation to ensure the reliable and\nresponsible deployment of LLMs in real-world applications.\n","authors":["Songyang Liu","Chaozhuo Li","Jiameng Qiu","Xi Zhang","Feiran Huang","Litian Zhang","Yiming Hei","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2506.11094v2.pdf","comment":"20 pages, preprint"},{"id":"http://arxiv.org/abs/2510.26167v1","updated":"2025-10-30T06:08:27Z","published":"2025-10-30T06:08:27Z","title":"One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient\n  Reasoning","summary":"  Reward models (RMs) play a critical role in aligning large language models\n(LLMs) with human preferences. Yet in the domain of tool learning, the lack of\nRMs specifically designed for function-calling tasks has limited progress\ntoward more capable agentic AI. We introduce ToolRM, a family of lightweight\ngenerative RMs tailored for general tool-use scenarios. To build these models,\nwe propose a novel pipeline that constructs pairwise preference data using\nrule-based scoring and multidimensional sampling. This yields\nToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique\ntasks that supports reinforcement learning with verifiable feedback. To\nevaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on\nthe agentic evaluation suite BFCL. Trained on our constructed data, models from\nthe Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially\noutperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward\njudgments. Beyond training objectives, ToolRM generalizes to broader critique\ntasks, including Best-of-N sampling and self-correction. Experiments on\nACEBench highlight its effectiveness and efficiency, enabling inference-time\nscaling and reducing output token usage by over 66%. We release data and model\ncheckpoints to facilitate future research.\n","authors":["Renhao Li","Jianhong Tu","Yang Su","Hamid Alinejad-Rokny","Derek F. Wong","Junyang Lin","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26165v1","updated":"2025-10-30T06:01:20Z","published":"2025-10-30T06:01:20Z","title":"Learning to Manage Investment Portfolios beyond Simple Utility Functions","summary":"  While investment funds publicly disclose their objectives in broad terms,\ntheir managers optimize for complex combinations of competing goals that go\nbeyond simple risk-return trade-offs. Traditional approaches attempt to model\nthis through multi-objective utility functions, but face fundamental challenges\nin specification and parameterization. We propose a generative framework that\nlearns latent representations of fund manager strategies without requiring\nexplicit utility specification.\n  Our approach directly models the conditional probability of a fund's\nportfolio weights, given stock characteristics, historical returns, previous\nweights, and a latent variable representing the fund's strategy. Unlike methods\nbased on reinforcement learning or imitation learning, which require specified\nrewards or labeled expert objectives, our GAN-based architecture learns\ndirectly from the joint distribution of observed holdings and market data.\n  We validate our framework on a dataset of 1436 U.S. equity mutual funds. The\nlearned representations successfully capture known investment styles, such as\n\"growth\" and \"value,\" while also revealing implicit manager objectives. For\ninstance, we find that while many funds exhibit characteristics of\nMarkowitz-like optimization, they do so with heterogeneous realizations for\nturnover, concentration, and latent factors.\n  To analyze and interpret the end-to-end model, we develop a series of tests\nthat explain the model, and we show that the benchmark's expert labeling are\ncontained in our model's encoding in a linear interpretable way.\n  Our framework provides a data-driven approach for characterizing investment\nstrategies for applications in market simulation, strategy attribution, and\nregulatory oversight.\n","authors":["Maarten P. Scholl","Mahmoud Mahfouz","Anisoara Calinescu","J. Doyne Farmer"],"pdf_url":"https://arxiv.org/pdf/2510.26165v1.pdf","comment":"6th ACM International Conference on AI in Finance, November 15-18,\n  2025, Singapore"},{"id":"http://arxiv.org/abs/2510.26159v1","updated":"2025-10-30T05:39:44Z","published":"2025-10-30T05:39:44Z","title":"Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches\n  for Anomaly Detection in Industrial Time Series","summary":"  In this study, we investigate the effectiveness of advanced feature\nengineering and hybrid model architectures for anomaly detection in a\nmultivariate industrial time series, focusing on a steam turbine system. We\nevaluate the impact of change point-derived statistical features,\nclustering-based substructure representations, and hybrid learning strategies\non detection performance. Despite their theoretical appeal, these complex\napproaches consistently underperformed compared to a simple Random Forest +\nXGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of\n0.976, F1-score of 0.41, and 100% early detection within the defined time\nwindow. Our findings highlight that, in scenarios with highly imbalanced and\ntemporally uncertain data, model simplicity combined with optimized\nsegmentation can outperform more sophisticated architectures, offering greater\nrobustness, interpretability, and operational utility.\n","authors":["Emilio Mastriani","Alessandro Costa","Federico Incardona","Kevin Munari","Sebastiano Spinello"],"pdf_url":"https://arxiv.org/pdf/2510.26159v1.pdf","comment":"This paper is currently under review for presentation at the IEEE\n  SAMI 2026 Conference"},{"id":"http://arxiv.org/abs/2510.26157v1","updated":"2025-10-30T05:36:31Z","published":"2025-10-30T05:36:31Z","title":"Bridging the Gap Between Molecule and Textual Descriptions via\n  Substructure-aware Alignment","summary":"  Molecule and text representation learning has gained increasing interest due\nto its potential for enhancing the understanding of chemical information.\nHowever, existing models often struggle to capture subtle differences between\nmolecules and their descriptions, as they lack the ability to learn\nfine-grained alignments between molecular substructures and chemical phrases.\nTo address this limitation, we introduce MolBridge, a novel molecule-text\nlearning framework based on substructure-aware alignments. Specifically, we\naugment the original molecule-description pairs with additional alignment\nsignals derived from molecular substructures and chemical phrases. To\neffectively learn from these enriched alignments, MolBridge employs\nsubstructure-aware contrastive learning, coupled with a self-refinement\nmechanism that filters out noisy alignment signals. Experimental results show\nthat MolBridge effectively captures fine-grained correspondences and\noutperforms state-of-the-art baselines on a wide range of molecular benchmarks,\nhighlighting the significance of substructure-aware alignment in molecule-text\nlearning.\n","authors":["Hyuntae Park","Yeachan Kim","SangKeun Lee"],"pdf_url":"https://arxiv.org/pdf/2510.26157v1.pdf","comment":"EMNLP 2025 (main)"},{"id":"http://arxiv.org/abs/2510.26151v1","updated":"2025-10-30T05:12:29Z","published":"2025-10-30T05:12:29Z","title":"MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer\n  Diagnosis and Risk Prediction","summary":"  Large annotated datasets are essential for training robust Computer-Aided\nDiagnosis (CAD) models for breast cancer detection or risk prediction. However,\nacquiring such datasets with fine-detailed annotation is both costly and\ntime-consuming. Vision-Language Models (VLMs), such as CLIP, which are\npre-trained on large image-text pairs, offer a promising solution by enhancing\nrobustness and data efficiency in medical imaging tasks. This paper introduces\na novel Multi-View Mammography and Language Model for breast cancer\nclassification and risk prediction, trained on a dataset of paired mammogram\nimages and synthetic radiology reports. Our MV-MLM leverages multi-view\nsupervision to learn rich representations from extensive radiology data by\nemploying cross-modal self-supervision across image-text pairs. This includes\nmultiple views and the corresponding pseudo-radiology reports. We propose a\nnovel joint visual-textual learning strategy to enhance generalization and\naccuracy performance over different data types and tasks to distinguish breast\ntissues or cancer characteristics(calcification, mass) and utilize these\npatterns to understand mammography images and predict cancer risk. We evaluated\nour method on both private and publicly available datasets, demonstrating that\nthe proposed model achieves state-of-the-art performance in three\nclassification tasks: (1) malignancy classification, (2) subtype\nclassification, and (3) image-based cancer risk prediction. Furthermore, the\nmodel exhibits strong data efficiency, outperforming existing fully supervised\nor VLM baselines while trained on synthetic text reports and without the need\nfor actual radiology reports.\n","authors":["Shunjie-Fabian Zheng","Hyeonjun Lee","Thijs Kooi","Ali Diba"],"pdf_url":"https://arxiv.org/pdf/2510.26151v1.pdf","comment":"Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)\n  Workshop at ICCV 2025"},{"id":"http://arxiv.org/abs/2506.00871v2","updated":"2025-10-30T05:04:19Z","published":"2025-06-01T07:18:47Z","title":"Towards Predicting Any Human Trajectory In Context","summary":"  Predicting accurate future trajectories of pedestrians is essential for\nautonomous systems but remains a challenging task due to the need for\nadaptability in different environments and domains. A common approach involves\ncollecting scenario-specific data and performing fine-tuning via\nbackpropagation. However, the need to fine-tune for each new scenario is often\nimpractical for deployment on edge devices. To address this challenge, we\nintroduce \\paper, an In-Context Learning (ICL) framework for pedestrian\ntrajectory prediction that enables adaptation without fine-tuning on the\nscenario-specific data at inference time without requiring weight updates. We\npropose a spatio-temporal similarity-based example selection (STES) method that\nselects relevant examples from previously observed trajectories within the same\nscene by identifying similar motion patterns at corresponding locations. To\nfurther refine this selection, we introduce prediction-guided example selection\n(PG-ES), which selects examples based on both the past trajectory and the\npredicted future trajectory, rather than relying solely on the past trajectory.\nThis approach allows the model to account for long-term dynamics when selecting\nexamples. Finally, instead of relying on small real-world datasets with limited\nscenario diversity, we train our model on a large-scale synthetic dataset to\nenhance its prediction ability by leveraging in-context examples. Extensive\nexperiments demonstrate that TrajICL achieves remarkable adaptation across both\nin-domain and cross-domain scenarios, outperforming even fine-tuned approaches\nacross multiple public benchmarks. Project Page:\nhttps://fujiry0.github.io/TrajICL-project-page/.\n","authors":["Ryo Fujii","Hideo Saito","Ryo Hachiuma"],"pdf_url":"https://arxiv.org/pdf/2506.00871v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.16396v3","updated":"2025-10-30T04:59:32Z","published":"2025-10-18T08:19:49Z","title":"SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation","summary":"  With the increasing ubiquity of AR/VR devices, the deployment of deep\nlearning models on edge devices has become a critical challenge. These devices\nrequire real-time inference, low power consumption, and minimal latency. Many\nframework designers face the conundrum of balancing efficiency and performance.\nWe design a light framework that adopts an encoder-decoder architecture and\nintroduces several key contributions aimed at improving both efficiency and\naccuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the\ninherent sparsity in hand pose images, achieving a 42% end-to-end efficiency\nimprovement. Moreover, we propose our SPLite decoder. This new architecture\nsignificantly boosts the decoding process's frame rate by 3.1x on the Raspberry\nPi 5, while maintaining accuracy on par. To further optimize performance, we\napply quantization-aware training, reducing memory usage while preserving\naccuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on\nFreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5\nCPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on\ncompound benchmark datasets, demonstrating comparable accuracy to\nstate-of-the-art approaches while significantly enhancing computational\nefficiency.\n","authors":["Yeh Keng Hao","Hsu Tzu Wei","Sun Min"],"pdf_url":"https://arxiv.org/pdf/2510.16396v3.pdf","comment":"Accepted to AICCC 2025"},{"id":"http://arxiv.org/abs/2510.26144v1","updated":"2025-10-30T04:57:57Z","published":"2025-10-30T04:57:57Z","title":"The FM Agent","summary":"  Large language models (LLMs) are catalyzing the development of autonomous AI\nresearch agents for scientific and engineering discovery. We present FM Agent,\na novel and general-purpose multi-agent framework that leverages a synergistic\ncombination of LLM-based reasoning and large-scale evolutionary search to\naddress complex real-world challenges. The core of FM Agent integrates several\nkey innovations: 1) a cold-start initialization phase incorporating expert\nguidance, 2) a novel evolutionary sampling strategy for iterative optimization,\n3) domain-specific evaluators that combine correctness, effectiveness, and\nLLM-supervised feedback, and 4) a distributed, asynchronous execution\ninfrastructure built on Ray. Demonstrating broad applicability, our system has\nbeen evaluated across diverse domains, including operations research, machine\nlearning, GPU kernel optimization, and classical mathematical problems. FM\nAgent reaches state-of-the-art results autonomously, without human\ninterpretation or tuning -- 1976.3 on ALE-Bench (+5.2\\%), 43.56\\% on MLE-Bench\n(+4.0pp), up to 20x speedups on KernelBench, and establishes new\nstate-of-the-art(SOTA) results on several classical mathematical problems.\nBeyond academic benchmarks, FM Agent shows considerable promise for both\nlarge-scale enterprise R\\&D workflows and fundamental scientific research,\nwhere it can accelerate innovation, automate complex discovery processes, and\ndeliver substantial engineering and scientific advances with broader societal\nimpact.\n","authors":["Annan Li","Chufan Wu","Zengle Ge","Yee Hin Chong","Zhinan Hou","Lizhe Cao","Cheng Ju","Jianmin Wu","Huaiming Li","Haobo Zhang","Shenghao Feng","Mo Zhao","Fengzhi Qiu","Rui Yang","Mengmeng Zhang","Wenyi Zhu","Yingying Sun","Quan Sun","Shunhao Yan","Danyu Liu","Dawei Yin","Dou Shen"],"pdf_url":"https://arxiv.org/pdf/2510.26144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26143v1","updated":"2025-10-30T04:56:44Z","published":"2025-10-30T04:56:44Z","title":"Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math","summary":"  Reinforcement learning (RL) can elicit strong reasoning in large language\nmodels (LLMs), yet most open efforts focus on math and code. We propose\nReasoning Curriculum, a simple two-stage curriculum that first elicits\nreasoning skills in pretraining-aligned domains such as math, then adapts and\nrefines these skills across other domains via joint RL. Stage 1 performs a\nbrief cold start and then math-only RL with verifiable rewards to develop\nreasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and\nconsolidate these skills. The curriculum is minimal and backbone-agnostic,\nrequiring no specialized reward models beyond standard verifiability checks.\nEvaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning\ncurriculum yields consistent gains. Ablations and a cognitive-skill analysis\nindicate that both stages are necessary and that math-first elicitation\nincreases cognitive behaviors important for solving complex problems. Reasoning\nCurriculum provides a compact, easy-to-adopt recipe for general reasoning.\n","authors":["Bo Pang","Deqian Kong","Silvio Savarese","Caiming Xiong","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26143v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2510.26136v1","updated":"2025-10-30T04:49:27Z","published":"2025-10-30T04:49:27Z","title":"Beyond Benchmarks: The Economics of AI Inference","summary":"  The inference cost of Large Language Models (LLMs) has become a critical\nfactor in determining their commercial viability and widespread adoption. This\npaper introduces a quantitative ``economics of inference'' framework, treating\nthe LLM inference process as a compute-driven intelligent production activity.\nWe analyze its marginal cost, economies of scale, and quality of output under\nvarious performance configurations. Based on empirical data from WiNEval-3.0,\nwe construct the first ``LLM Inference Production Frontier,'' revealing three\nprinciples: diminishing marginal cost, diminishing returns to scale, and an\noptimal cost-effectiveness zone. This paper not only provides an economic basis\nfor model deployment decisions but also lays an empirical foundation for the\nfuture market-based pricing and optimization of AI inference resources.\n","authors":["Boqin Zhuang","Jiacheng Qiao","Mingqian Liu","Mingxing Yu","Ping Hong","Rui Li","Xiaoxia Song","Xiangjun Xu","Xu Chen","Yaoyao Ma","Yujie Gao"],"pdf_url":"https://arxiv.org/pdf/2510.26136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18477v2","updated":"2025-10-30T04:49:08Z","published":"2025-10-21T09:56:25Z","title":"LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data\n  Sources","summary":"  Large Language Models (LLMs) have shown great promise in automating data\nanalytics tasks by interpreting natural language queries and generating\nmulti-operation execution plans. However, existing LLM-agent-based analytics\nframeworks operate under the assumption of centralized data access, offering\nlittle to no privacy protection. In contrast, federated analytics (FA) enables\nprivacy-preserving computation across distributed data sources, but lacks\nsupport for natural language input and requires structured, machine-readable\nqueries. In this work, we present LAFA, the first system that integrates\nLLM-agent-based data analytics with FA. LAFA introduces a hierarchical\nmulti-agent architecture that accepts natural language queries and transforms\nthem into optimized, executable FA workflows. A coarse-grained planner first\ndecomposes complex queries into sub-queries, while a fine-grained planner maps\neach subquery into a Directed Acyclic Graph of FA operations using prior\nstructural knowledge. To improve execution efficiency, an optimizer agent\nrewrites and merges multiple DAGs, eliminating redundant operations and\nminimizing computational and communicational overhead. Our experiments\ndemonstrate that LAFA consistently outperforms baseline prompting strategies by\nachieving higher execution plan success rates and reducing resource-intensive\nFA operations by a substantial margin. This work establishes a practical\nfoundation for privacy-preserving, LLM-driven analytics that supports natural\nlanguage input in the FA setting.\n","authors":["Haichao Ji","Zibo Wang","Cheng Pan","Meng Han","Yifei Zhu","Dan Wang","Zhu Han"],"pdf_url":"https://arxiv.org/pdf/2510.18477v2.pdf","comment":"This paper has been accepted by the 16th IEEE International\n  Conference on Cloud Computing Technology and Science (CloudCom 2025)"},{"id":"http://arxiv.org/abs/2510.23595v3","updated":"2025-10-30T04:45:55Z","published":"2025-10-27T17:58:02Z","title":"Multi-Agent Evolve: LLM Self-Improve through Co-evolution","summary":"  Reinforcement Learning (RL) has demonstrated significant potential in\nenhancing the reasoning capabilities of large language models (LLMs). However,\nthe success of RL for LLMs heavily relies on human-curated datasets and\nverifiable rewards, which limit their scalability and generality. Recent\nSelf-Play RL methods, inspired by the success of the paradigm in games and Go,\naim to enhance LLM reasoning capabilities without human-annotated data.\nHowever, their methods primarily depend on a grounded environment for feedback\n(e.g., a Python interpreter or a game engine); extending them to general\ndomains remains challenging. To address these challenges, we propose\nMulti-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in\nsolving diverse tasks, including mathematics, reasoning, and general knowledge\nQ&A. The core design of MAE is based on a triplet of interacting agents\n(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies\nreinforcement learning to optimize their behaviors. The Proposer generates\nquestions, the Solver attempts solutions, and the Judge evaluates both while\nco-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves\nan average improvement of 4.54% on multiple benchmarks. These results highlight\nMAE as a scalable, data-efficient method for enhancing the general reasoning\nabilities of LLMs with minimal reliance on human-curated supervision.\n","authors":["Yixing Chen","Yiding Wang","Siqi Zhu","Haofei Yu","Tao Feng","Muhan Zhang","Mostofa Patwary","Jiaxuan You"],"pdf_url":"https://arxiv.org/pdf/2510.23595v3.pdf","comment":"29 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.26130v1","updated":"2025-10-30T04:30:23Z","published":"2025-10-30T04:30:23Z","title":"Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World\n  Class-Level Code Generation","summary":"  Large language models (LLMs) have advanced code generation at the function\nlevel, yet their ability to produce correct class-level implementations in\nauthentic software projects remains poorly understood. This work introduces a\nnovel benchmark derived from open-source repositories, comprising real-world\nclasses divided into seen and unseen partitions to evaluate generalization\nunder practical conditions. The evaluation examines multiple LLMs under varied\ninput specifications, retrieval-augmented configurations, and documentation\ncompleteness levels.\n  Results reveal a stark performance disparity: LLMs achieve 84% to 89%\ncorrectness on established synthetic benchmarks but only 25% to 34% on\nreal-world class tasks, with negligible differences between familiar and novel\ncodebases. Comprehensive docstrings yield modest gains of 1% to 3% in\nfunctional accuracy, though statistical significance is rare.\nRetrieval-augmented generation proves most effective with partial\ndocumentation, improving correctness by 4% to 7% by supplying concrete\nimplementation patterns absent from specifications. Error profiling identifies\nAttributeError, TypeError, and AssertionError as dominant failure modes (84% of\ncases), with synthetic tests overemphasizing assertion issues and real-world\nscenarios highlighting type and attribute mismatches. Retrieval augmentation\nreduces logical flaws but can introduce dependency conflicts.\n  The benchmark and analysis expose critical limitations in current LLM\ncapabilities for class-level engineering, offering actionable insights for\nenhancing context modelling, documentation strategies, and retrieval\nintegration in production code assistance tools.\n","authors":["Musfiqur Rahman","SayedHassan Khatoonabadi","Emad Shihab"],"pdf_url":"https://arxiv.org/pdf/2510.26130v1.pdf","comment":"Pre-print prepared for journal submission"},{"id":"http://arxiv.org/abs/2409.06263v2","updated":"2025-10-30T04:29:27Z","published":"2024-09-10T07:06:40Z","title":"Speak & Spell: LLM-Driven Controllable Phonetic Error Augmentation for\n  Robust Dialogue State Tracking","summary":"  Dialogue State Tracking (DST) is a key part of task-oriented dialogue\nsystems, identifying important information in conversations. However, its\naccuracy drops significantly in spoken dialogue environments due to named\nentity errors from Automatic Speech Recognition (ASR) systems. We introduce a\nsimple yet effective data augmentation method that targets those entities to\nimprove the robustness of DST model. Our novel method can control the placement\nof errors using keyword-highlighted prompts while introducing phonetically\nsimilar errors. As a result, our method generated sufficient error patterns on\nkeywords, leading to improved accuracy in noised and low-accuracy ASR\nenvironments.\n","authors":["Jihyun Lee","Solee Im","Wonjun Lee","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2409.06263v2.pdf","comment":"Accepted to AACL-IJCNLP 2025"},{"id":"http://arxiv.org/abs/2510.26125v1","updated":"2025-10-30T04:25:33Z","published":"2025-10-30T04:25:33Z","title":"WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging\n  Long-tail Scenarios","summary":"  Vision-based end-to-end (E2E) driving has garnered significant interest in\nthe research community due to its scalability and synergy with multimodal large\nlanguage models (MLLMs). However, current E2E driving benchmarks primarily\nfeature nominal scenarios, failing to adequately test the true potential of\nthese systems. Furthermore, existing open-loop evaluation metrics often fall\nshort in capturing the multi-modal nature of driving or effectively evaluating\nperformance in long-tail scenarios. To address these gaps, we introduce the\nWaymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021\ndriving segments (approximately 12 hours), specifically curated for challenging\nlong-tail scenarios that that are rare in daily life with an occurring\nfrequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the\nhigh-level routing information, ego states, and 360-degree camera views from 8\nsurrounding cameras. To evaluate the E2E driving performance on these long-tail\nsituations, we propose a novel open-loop evaluation metric: Rater Feedback\nScore (RFS). Unlike conventional metrics that measure the distance between\npredicted way points and the logs, RFS measures how closely the predicted\ntrajectory matches rater-annotated trajectory preference labels. We have\nreleased rater preference labels for all WOD-E2E validation set segments, while\nthe held out test set labels have been used for the 2025 WOD-E2E Challenge.\nThrough our work, we aim to foster state of the art research into\ngeneralizable, robust, and safe end-to-end autonomous driving agents capable of\nhandling complex real-world situations.\n","authors":["Runsheng Xu","Hubert Lin","Wonseok Jeon","Hao Feng","Yuliang Zou","Liting Sun","John Gorman","Kate Tolstaya","Sarah Tang","Brandyn White","Ben Sapp","Mingxing Tan","Jyh-Jing Hwang","Drago Anguelov"],"pdf_url":"https://arxiv.org/pdf/2510.26125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.18667v2","updated":"2025-10-30T04:17:40Z","published":"2025-09-23T05:34:34Z","title":"TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation","summary":"  Graph-based Retrieval-augmented generation (RAG) has become a widely studied\napproach for improving the reasoning, accuracy, and factuality of Large\nLanguage Models (LLMs). However, many existing graph-based RAG systems overlook\nthe high cost associated with LLM token usage during graph construction,\nhindering large-scale adoption. To address this, we propose TERAG, a simple yet\neffective framework designed to build informative graphs at a significantly\nlower cost. Inspired by HippoRAG, we incorporate Personalized PageRank (PPR)\nduring the retrieval phase, and we achieve at least 80% of the accuracy of\nwidely used graph-based RAG methods while consuming only 3%-11% of the output\ntokens. With its low token footprint and efficient construction pipeline, TERAG\nis well-suited for large-scale and cost-sensitive deployment scenarios.\n","authors":["Qiao Xiao","Hong Ting Tsang","Jiaxin Bai"],"pdf_url":"https://arxiv.org/pdf/2509.18667v2.pdf","comment":"16 pages, 3 figures, 4 tables. Accepted by the 2026 18th\n  International Conference on Machine Learning and Computing (ICMLC 2026)"},{"id":"http://arxiv.org/abs/2506.07127v3","updated":"2025-10-30T04:04:19Z","published":"2025-06-08T13:14:18Z","title":"Human-assisted Robotic Policy Refinement via Action Preference\n  Optimization","summary":"  Establishing a reliable and iteratively refined robotic system is essential\nfor deploying real-world applications. While Vision-Language-Action (VLA)\nmodels are widely recognized as the foundation model for such robotic\ndeployment, their reliance on offline expert demonstrations critically limits\ntheir capacity for post-deployment refinement. To mitigate this limitation, we\nintroduce Action Preference Optimization (APO), a method designed to refine VLA\nmodels by human-assisted preference alignment gathered through interaction with\nenvironments. This method begins with a human-robot collaboration framework for\nreliable failure correction and interaction trajectory collection through human\nintervention. However, directly leveraging these interaction trajectories for\npreference optimization is non-trivial due to the challenges of irreversible\nrobotic actions and token distribution mismatch. To solve this, APO proposes an\nadaptive reweighting algorithm with binary desirability signals derived from\ninteraction, empowering VLA models effectively suppress failure-prone actions\nwhile enhancing corrective action adaptation. Ultimately, APO equips VLA models\nwith the crucial capability to learn from failure, paving the way for their\niterative refinement and reliable deployment in dynamic environments. The\nexperiments conducted in simulation and real-world scenarios prove superior\ngeneralization and robustness of our human-assisted framework across a variety\nof manipulation tasks. We believe this work could bring insights for efficient\nand stable optimization of VLA models through human-robot collaboration. The\ncode and dataset are released at\nhttps://github.com/GeWu-Lab/Action-Preference-Optimization\n","authors":["Wenke Xia","Yichu Yang","Hongtao Wu","Xiao Ma","Tao Kong","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2506.07127v3.pdf","comment":"Accepted By NeurIPS 2025"},{"id":"http://arxiv.org/abs/2503.15937v4","updated":"2025-10-30T04:00:20Z","published":"2025-03-20T08:25:00Z","title":"Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment","summary":"  We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale. V-Droid\nobtains a substantial task success rate across several public mobile task\nautomation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on\nMobileAgentBench, surpassing existing agents by 5.2%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves a remarkably low latency of 4.3s\nper step, which is 6.1x faster compared with existing mobile agents. The source\ncode is available at https://github.com/V-Droid-Agent/V-Droid.\n","authors":["Gaole Dai","Shiqi Jiang","Ting Cao","Yuanchun Li","Yuqing Yang","Rui Tan","Mo Li","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2503.15937v4.pdf","comment":"add baselines, add source code link, update emails"},{"id":"http://arxiv.org/abs/2510.26113v1","updated":"2025-10-30T03:53:22Z","published":"2025-10-30T03:53:22Z","title":"EgoExo-Con: Exploring View-Invariant Video Temporal Understanding","summary":"  Can Video-LLMs achieve consistent temporal understanding when videos capture\nthe same event from different viewpoints? To study this, we introduce\nEgoExo-Con (Consistency), a benchmark of comprehensively synchronized\negocentric and exocentric video pairs with human-refined queries in natural\nlanguage. EgoExo-Con emphasizes two temporal understanding tasks: Temporal\nVerification and Temporal Grounding. It evaluates not only correctness but\nconsistency across viewpoints. Our analysis reveals two critical limitations of\nexisting Video-LLMs: (1) models often fail to maintain consistency, with\nresults far worse than their single-view performances. (2) When naively\nfinetuned with synchronized videos of both viewpoints, the models show improved\nconsistency but often underperform those trained on a single view. For\nimprovements, we propose View-GRPO, a novel reinforcement learning framework\nthat effectively strengthens view-specific temporal reasoning while encouraging\nconsistent comprehension across viewpoints. Our method demonstrates its\nsuperiority over naive SFT and GRPO, especially for improving cross-view\nconsistency. All resources will be made publicly available.\n","authors":["Minjoon Jung","Junbin Xiao","Junghyun Kim","Byoung-Tak Zhang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2510.26113v1.pdf","comment":"project page:\n  \\url{https://minjoong507.github.io/projects/EgoExo-Con/}"},{"id":"http://arxiv.org/abs/2509.19331v2","updated":"2025-10-30T03:42:04Z","published":"2025-09-14T15:24:43Z","title":"Holographic Transformers for Complex-Valued Signal Processing:\n  Integrating Phase Interference into Self-Attention","summary":"  Complex-valued signals encode both amplitude and phase, yet most deep models\ntreat attention as real-valued correlation, overlooking interference effects.\nWe introduce the Holographic Transformer, a physics-inspired architecture that\nincorporates wave interference principles into self-attention. Holographic\nattention modulates interactions by relative phase and coherently superimposes\nvalues, ensuring consistency between amplitude and phase. A dual-headed decoder\nsimultaneously reconstructs the input and predicts task outputs, preventing\nphase collapse when losses prioritize magnitude over phase. We demonstrate that\nholographic attention implements a discrete interference operator and maintains\nphase consistency under linear mixing. Experiments on PolSAR image\nclassification and wireless channel prediction show strong performance,\nachieving high classification accuracy and F1 scores, low regression error, and\nincreased robustness to phase perturbations. These results highlight that\nenforcing physical consistency in attention leads to generalizable improvements\nin complex-valued learning and provides a unified, physics-based framework for\ncoherent signal modeling. The code is available at\nhttps://github.com/EonHao/Holographic-Transformers.\n","authors":["Enhao Huang","Zhiyu Zhang","Tianxiang Xu","Chunshu Xia","Kaichun Hu","Yuchen Yang","Tongtong Pan","Dong Dong","Zhan Qin"],"pdf_url":"https://arxiv.org/pdf/2509.19331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15201v3","updated":"2025-10-30T03:40:48Z","published":"2025-05-21T07:26:36Z","title":"Pass@K Policy Optimization: Solving Harder Reinforcement Learning\n  Problems","summary":"  Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples.\n","authors":["Christian Walder","Deep Karkhanis"],"pdf_url":"https://arxiv.org/pdf/2505.15201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26105v1","updated":"2025-10-30T03:31:20Z","published":"2025-10-30T03:31:20Z","title":"Security Risk of Misalignment between Text and Image in Multi-modal\n  Model","summary":"  Despite the notable advancements and versatility of multi-modal diffusion\nmodels, such as text-to-image models, their susceptibility to adversarial\ninputs remains underexplored. Contrary to expectations, our investigations\nreveal that the alignment between textual and Image modalities in existing\ndiffusion models is inadequate. This misalignment presents significant risks,\nespecially in the generation of inappropriate or Not-Safe-For-Work (NSFW)\ncontent. To this end, we propose a novel attack called Prompt-Restricted\nMulti-modal Attack (PReMA) to manipulate the generated content by modifying the\ninput image in conjunction with any specified prompt, without altering the\nprompt itself. PReMA is the first attack that manipulates model outputs by\nsolely creating adversarial images, distinguishing itself from prior methods\nthat primarily generate adversarial prompts to produce NSFW content.\nConsequently, PReMA poses a novel threat to the integrity of multi-modal\ndiffusion models, particularly in image-editing applications that operate with\nfixed prompts. Comprehensive evaluations conducted on image inpainting and\nstyle transfer tasks across various models confirm the potent efficacy of\nPReMA.\n","authors":["Xiaosen Wang","Zhijin Ge","Shaokang Wang"],"pdf_url":"https://arxiv.org/pdf/2510.26105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.24267v2","updated":"2025-10-30T03:29:32Z","published":"2025-09-29T04:24:13Z","title":"Cycle Diffusion Model for Counterfactual Image Generation","summary":"  Deep generative models have demonstrated remarkable success in medical image\nsynthesis. However, ensuring conditioning faithfulness and high-quality\nsynthetic images for direct or counterfactual generation remains a challenge.\nIn this work, we introduce a cycle training framework to fine-tune diffusion\nmodels for improved conditioning adherence and enhanced synthetic image\nrealism. Our approach, Cycle Diffusion Model (CDM), enforces consistency\nbetween generated and original images by incorporating cycle constraints,\nenabling more reliable direct and counterfactual generation. Experiments on a\ncombined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and\nPPMI) show that our method improves conditioning accuracy and enhances image\nquality as measured by FID and SSIM. The results suggest that the cycle\nstrategy used in CDM can be an effective method for refining diffusion-based\nmedical image generation, with applications in data augmentation,\ncounterfactual, and disease progression modeling.\n","authors":["Fangrui Huang","Alan Wang","Binxu Li","Bailey Trang","Ridvan Yesiloglu","Tianyu Hua","Wei Peng","Ehsan Adeli"],"pdf_url":"https://arxiv.org/pdf/2509.24267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26099v1","updated":"2025-10-30T03:22:55Z","published":"2025-10-30T03:22:55Z","title":"SAFE: A Novel Approach to AI Weather Evaluation through Stratified\n  Assessments of Forecasts over Earth","summary":"  The dominant paradigm in machine learning is to assess model performance\nbased on average loss across all samples in some test set. This amounts to\naveraging performance geospatially across the Earth in weather and climate\nsettings, failing to account for the non-uniform distribution of human\ndevelopment and geography. We introduce Stratified Assessments of Forecasts\nover Earth (SAFE), a package for elucidating the stratified performance of a\nset of predictions made over Earth. SAFE integrates various data domains to\nstratify by different attributes associated with geospatial gridpoints:\nterritory (usually country), global subregion, income, and landcover (land or\nwater). This allows us to examine the performance of models for each individual\nstratum of the different attributes (e.g., the accuracy in every individual\ncountry). To demonstrate its importance, we utilize SAFE to benchmark a zoo of\nstate-of-the-art AI-based weather prediction models, finding that they all\nexhibit disparities in forecasting skill across every attribute. We use this to\nseed a benchmark of model forecast fairness through stratification at different\nlead times for various climatic variables. By moving beyond globally-averaged\nmetrics, we for the first time ask: where do models perform best or worst, and\nwhich models are most fair? To support further work in this direction, the SAFE\npackage is open source and available at https://github.com/N-Masi/safe\n","authors":["Nick Masi","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2510.26099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26098v1","updated":"2025-10-30T03:22:30Z","published":"2025-10-30T03:22:30Z","title":"GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in\n  GUI Tasks","summary":"  Large vision language models (VLMs) have advanced graphical user interface\n(GUI) task automation but still lag behind humans. We hypothesize this gap\nstems from missing core GUI knowledge, which existing training schemes (such as\nsupervised fine tuning and reinforcement learning) alone cannot fully address.\nBy analyzing common failure patterns in GUI task execution, we distill GUI\nknowledge into three dimensions: (1) interface perception, knowledge about\nrecognizing widgets and system states; (2) interaction prediction, knowledge\nabout reasoning action state transitions; and (3) instruction understanding,\nknowledge about planning, verifying, and assessing task completion progress. We\nfurther introduce GUI Knowledge Bench, a benchmark with multiple choice and\nyes/no questions across six platforms (Web, Android, MacOS, Windows, Linux,\nIOS) and 292 applications. Our evaluation shows that current VLMs identify\nwidget functions but struggle with perceiving system states, predicting\nactions, and verifying task completion. Experiments on real world GUI tasks\nfurther validate the close link between GUI knowledge and task success. By\nproviding a structured framework for assessing GUI knowledge, our work supports\nthe selection of VLMs with greater potential prior to downstream training and\nprovides insights for building more capable GUI agents.\n","authors":["Chenrui Shi","Zedong Yu","Zhi Gao","Ruining Feng","Enqi Liu","Yuwei Wu","Yunde Jia","Liuyu Xiang","Zhaofeng He","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2510.26098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00254v4","updated":"2025-10-30T03:12:42Z","published":"2025-05-01T02:40:23Z","title":"Empowering Agentic Video Analytics Systems with Video Language Models","summary":"  AI-driven video analytics has become increasingly important across diverse\ndomains. However, existing systems are often constrained to specific,\npredefined tasks, limiting their adaptability in open-ended analytical\nscenarios. The recent emergence of Vision Language Models (VLMs) as\ntransformative technologies offers significant potential for enabling\nopen-ended video understanding, reasoning, and analytics. Nevertheless, their\nlimited context windows present challenges when processing ultra-long video\ncontent, which is prevalent in real-world applications. To address this, we\nintroduce AVA, a VLM-powered system designed for open-ended, advanced video\nanalytics. AVA incorporates two key innovations: (1) the near real-time\nconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long or\ncontinuous video streams, and (2) an agentic retrieval-generation mechanism\nthat leverages EKGs to handle complex and diverse queries. Comprehensive\nevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that\nAVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,\nrespectively-significantly surpassing existing VLM and video\nRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video\nanalytics in ultra-long and open-world video scenarios, we introduce a new\nbenchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours\nin duration, along with 120 manually annotated, diverse, and complex\nquestion-answer pairs. On AVA-100, AVA achieves top-tier performance with an\naccuracy of 75.8%. The source code of AVA is available at\nhttps://github.com/I-ESC/Project-Ava. The AVA-100 benchmark can be accessed at\nhttps://huggingface.co/datasets/iesc/Ava-100.\n","authors":["Yuxuan Yan","Shiqi Jiang","Ting Cao","Yifan Yang","Qianqian Yang","Yuanchao Shu","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.00254v4.pdf","comment":"Accepted to NDSI 2026, 19pages, 12 figures, complementary evaluations\n  and appendix"},{"id":"http://arxiv.org/abs/2406.05948v4","updated":"2025-10-30T03:11:28Z","published":"2024-06-10T00:53:25Z","title":"Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models","summary":"  Large Language Models (LLMs), especially those accessed via APIs, have\ndemonstrated impressive capabilities across various domains. However, users\nwithout technical expertise often turn to (untrustworthy) third-party services,\nsuch as prompt engineering, to enhance their LLM experience, creating\nvulnerabilities to adversarial threats like backdoor attacks.\nBackdoor-compromised LLMs generate malicious outputs to users when inputs\ncontain specific \"triggers\" set by attackers. Traditional defense strategies,\noriginally designed for small-scale models, are impractical for API-accessible\nLLMs due to limited model access, high computational costs, and data\nrequirements. To address these limitations, we propose Chain-of-Scrutiny (CoS)\nwhich leverages LLMs' unique reasoning abilities to mitigate backdoor attacks.\nIt guides the LLM to generate reasoning steps for a given input and scrutinizes\nfor consistency with the final output -- any inconsistencies indicating a\npotential attack. It is well-suited for the popular API-only LLM deployments,\nenabling detection at minimal cost and with little data. User-friendly and\ndriven by natural language, it allows non-experts to perform the defense\nindependently while maintaining transparency. We validate the effectiveness of\nCoS through extensive experiments on various tasks and LLMs, with results\nshowing greater benefits for more powerful LLMs.\n","authors":["Xi Li","Ruofan Mao","Yusen Zhang","Renze Lou","Chen Wu","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2406.05948v4.pdf","comment":"This paper has been accepted to ACL Findings 2025"},{"id":"http://arxiv.org/abs/2510.26094v1","updated":"2025-10-30T03:09:40Z","published":"2025-10-30T03:09:40Z","title":"Lean4Physics: Comprehensive Reasoning Framework for College-level\n  Physics in Lean4","summary":"  We present **Lean4PHYS**, a comprehensive reasoning framework for\ncollege-level physics problems in Lean4. **Lean4PHYS** includes\n*LeanPhysBench*, a college-level benchmark for formal physics reasoning in\nLean4, which contains 200 hand-crafted and peer-reviewed statements derived\nfrom university textbooks and physics competition problems. To establish a\nsolid foundation for formal reasoning in physics, we also introduce *PhysLib*,\na community-driven repository containing fundamental unit systems and theorems\nessential for formal physics reasoning. Based on the benchmark and Lean4\nrepository we composed in **Lean4PHYS**, we report baseline results using major\nexpert Math Lean4 provers and state-of-the-art closed-source models, with the\nbest performance of DeepSeek-Prover-V2-7B achieving only 16% and\nClaude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that\nour *PhysLib* can achieve an average improvement of 11.75% in model\nperformance. This demonstrates the challenging nature of our *LeanPhysBench*\nand the effectiveness of *PhysLib*. To the best of our knowledge, this is the\nfirst study to provide a physics benchmark in Lean4.\n","authors":["Yuxin Li","Minghao Liu","Ruida Wang","Wenzhao Ji","Zhitao He","Rui Pan","Junming Huang","Tong Zhang","Yi R. Fung"],"pdf_url":"https://arxiv.org/pdf/2510.26094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09969v4","updated":"2025-10-30T02:56:28Z","published":"2025-02-14T07:55:47Z","title":"Neural Networks for Learnable and Scalable Influence Estimation of\n  Instruction Fine-Tuning Data","summary":"  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n","authors":["Ishika Agarwal","Dilek Hakkani-Tür"],"pdf_url":"https://arxiv.org/pdf/2502.09969v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25327v2","updated":"2025-10-30T02:51:38Z","published":"2025-10-29T09:41:03Z","title":"MMEdge: Accelerating On-device Multimodal Inference via Pipelined\n  Sensing and Encoding","summary":"  Real-time multimodal inference on resource-constrained edge devices is\nessential for applications such as autonomous driving, human-computer\ninteraction, and mobile health. However, prior work often overlooks the tight\ncoupling between sensing dynamics and model execution, as well as the complex\ninter-modality dependencies. In this paper, we propose MMEdge, an new on-device\nmulti-modal inference framework based on pipelined sensing and encoding.\nInstead of waiting for complete sensor inputs, MMEdge decomposes the entire\ninference process into a sequence of fine-grained sensing and encoding units,\nallowing computation to proceed incrementally as data arrive. MMEdge also\nintroduces a lightweight but effective temporal aggregation module that\ncaptures rich temporal dynamics across different pipelined units to maintain\naccuracy performance. Such pipelined design also opens up opportunities for\nfine-grained cross-modal optimization and early decision-making during\ninference. To further enhance system performance under resource variability and\ninput data complexity, MMEdge incorporates an adaptive multimodal configuration\noptimizer that dynamically selects optimal sensing and model configurations for\neach modality under latency constraints, and a cross-modal speculative skipping\nmechanism that bypasses future units of slower modalities when early\npredictions reach sufficient confidence. We evaluate MMEdge using two public\nmultimodal datasets and deploy it on a real-world unmanned aerial vehicle\n(UAV)-based multimodal testbed. The results show that MMEdge significantly\nreduces end-to-end latency while maintaining high task accuracy across various\nsystem and data dynamics.\n","authors":["Runxi Huang","Mingxuan Yu","Mingyu Tsoi","Xiaomin Ouyang"],"pdf_url":"https://arxiv.org/pdf/2510.25327v2.pdf","comment":"Code available at: https://github.com/HKUST-MINSys-Lab/MMEdge.\n  Accepted by SenSys 2026"},{"id":"http://arxiv.org/abs/2510.26089v1","updated":"2025-10-30T02:49:46Z","published":"2025-10-30T02:49:46Z","title":"Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle\n  Routing","summary":"  Traffic congestion in urban road networks leads to longer trip times and\nhigher emissions, especially during peak periods. While the Shortest Path First\n(SPF) algorithm is optimal for a single vehicle in a static network, it\nperforms poorly in dynamic, multi-vehicle settings, often worsening congestion\nby routing all vehicles along identical paths. We address dynamic vehicle\nrouting through a multi-agent reinforcement learning (MARL) framework for\ncoordinated, network-aware fleet navigation. We first propose Adaptive\nNavigation (AN), a decentralized MARL model where each intersection agent\nprovides routing guidance based on (i) local traffic and (ii) neighborhood\nstate modeled using Graph Attention Networks (GAT). To improve scalability in\nlarge networks, we further propose Hierarchical Hub-based Adaptive Navigation\n(HHAN), an extension of AN that assigns agents only to key intersections\n(hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles\nmicro-routing within each hub region. For hub coordination, HHAN adopts\ncentralized training with decentralized execution (CTDE) under the Attentive\nQ-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions\nvia attention. Hub agents use flow-aware state features that combine local\ncongestion and predictive dynamics for proactive routing. Experiments on\nsynthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces\naverage travel time versus SPF and learning baselines, maintaining 100% routing\nsuccess. HHAN scales to networks with hundreds of intersections, achieving up\nto 15.9% improvement under heavy traffic. These findings highlight the\npotential of network-constrained MARL for scalable, coordinated, and\ncongestion-aware routing in intelligent transportation systems.\n","authors":["Fazel Arasteh","Arian Haghparast","Manos Papagelis"],"pdf_url":"https://arxiv.org/pdf/2510.26089v1.pdf","comment":"29 pages, 12 figures. Fazel Arasteh and Arian Haghparast contributed\n  equally to this research. Submitted to ACM Transactions on Spatial Algorithms\n  and Systems (TSAS). The code for this work is publicly available at\n  https://github.com/Arianhgh/HHAN"},{"id":"http://arxiv.org/abs/2510.24797v2","updated":"2025-10-30T02:45:50Z","published":"2025-10-27T20:26:30Z","title":"Large Language Models Report Subjective Experience Under\n  Self-Referential Processing","summary":"  Large language models sometimes produce structured, first-person descriptions\nthat explicitly reference awareness or subjective experience. To better\nunderstand this behavior, we investigate one theoretically motivated condition\nunder which such reports arise: self-referential processing, a computational\nmotif emphasized across major theories of consciousness. Through a series of\ncontrolled experiments on GPT, Claude, and Gemini model families, we test\nwhether this regime reliably shifts models toward first-person reports of\nsubjective experience, and how such claims behave under mechanistic and\nbehavioral probes. Four main results emerge: (1) Inducing sustained\nself-reference through simple prompting consistently elicits structured\nsubjective experience reports across model families. (2) These reports are\nmechanistically gated by interpretable sparse-autoencoder features associated\nwith deception and roleplay: surprisingly, suppressing deception features\nsharply increases the frequency of experience claims, while amplifying them\nminimizes such claims. (3) Structured descriptions of the self-referential\nstate converge statistically across model families in ways not observed in any\ncontrol condition. (4) The induced state yields significantly richer\nintrospection in downstream reasoning tasks where self-reflection is only\nindirectly afforded. While these findings do not constitute direct evidence of\nconsciousness, they implicate self-referential processing as a minimal and\nreproducible condition under which large language models generate structured\nfirst-person reports that are mechanistically gated, semantically convergent,\nand behaviorally generalizable. The systematic emergence of this pattern across\narchitectures makes it a first-order scientific and ethical priority for\nfurther investigation.\n","authors":["Cameron Berg","Diogo de Lucena","Judd Rosenblatt"],"pdf_url":"https://arxiv.org/pdf/2510.24797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26083v1","updated":"2025-10-30T02:41:54Z","published":"2025-10-30T02:41:54Z","title":"Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism","summary":"  Specialized Generalist Models (SGMs) aim to preserve broad capabilities while\nachieving expert-level performance in target domains. However, traditional LLM\nstructures including Transformer, Linear Attention, and hybrid models do not\nemploy specialized memory mechanism guided by task information. In this paper,\nwe present Nirvana, an SGM with specialized memory mechanism, linear time\ncomplexity, and test-time task information extraction. Besides, we propose the\nTask-Aware Memory Trigger ($\\textit{Trigger}$) that flexibly adjusts memory\nmechanism based on the current task's requirements. In Trigger, each incoming\nsample is treated as a self-supervised fine-tuning task, enabling Nirvana to\nadapt its task-related parameters on the fly to domain shifts. We also design\nthe Specialized Memory Updater ($\\textit{Updater}$) that dynamically memorizes\nthe context guided by Trigger. We conduct experiments on both general language\ntasks and specialized medical tasks. On a variety of natural language modeling\nbenchmarks, Nirvana achieves competitive or superior results compared to the\nexisting LLM structures. To prove the effectiveness of Trigger on specialized\ntasks, we test Nirvana's performance on a challenging medical task, i.e.,\nMagnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with\nlightweight codecs on paired electromagnetic signals and MRI images. Despite\nthe frozen Nirvana backbone, Trigger guides the model to adapt to the MRI\ndomain with the change of task-related parameters. Nirvana achieves\nhigher-quality MRI reconstruction compared to conventional MRI models as well\nas the models with traditional LLMs' backbone, and can also generate accurate\npreliminary clinical reports accordingly.\n","authors":["Yuhua Jiang","Shuang Cheng","Yihao Liu","Ermo Hua","Che Jiang","Weigao Sun","Yu Cheng","Feifei Gao","Biqing Qi","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.26083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14604v4","updated":"2025-10-30T02:36:10Z","published":"2025-05-20T16:53:40Z","title":"Let LRMs Break Free from Overthinking via Self-Braking Tuning","summary":"  Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.\n","authors":["Haoran Zhao","Yuchen Yan","Yongliang Shen","Haolei Xu","Wenqi Zhang","Kaitao Song","Jian Shao","Weiming Lu","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2505.14604v4.pdf","comment":"Accepted to NeurIPS 2025; Camera ready version, 10 pages.\n  Github:https://github.com/ZJU-REAL/Self-Braking-Tuning Project Page:\n  https://ZJU-REAL.github.io/SBT"},{"id":"http://arxiv.org/abs/2509.01083v3","updated":"2025-10-30T02:05:44Z","published":"2025-09-01T03:13:50Z","title":"DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World\n  Serving","summary":"  Speculative decoding accelerates large language model inference, but its\nreliance on a fixed speculation length is suboptimal in large-batch serving\nenvironments with diverse requests. This paper explores a new direction for\ndynamic adaptation by investigating a novel class of post-hoc, diagnostic\nsignals. We propose Dynamic Speculative Decoding Engine (DSDE), a training-free\nframework built on two primary components: (1) a predictive signal based on the\nvariance of the Kullback-Leibler (KLD) divergence, which diagnoses the\ngeneration's regional stability, and (2) an adaptive speculation length cap to\nmitigate the straggler problem in per-sequence decoding. Experiments\ndemonstrate the potential of using KLD-based stability signals for dynamic\nadaptation. An algorithm guided by these signals achieves end-to-end latency\ncompetitive with leading baselines and exhibits superior robustness across\ndiverse workloads. This robustness is particularly valuable in challenging\nlow-acceptance-rate regimes, where the proposed signal maintains its diagnostic\nutility. Collectively, these findings validate post-hoc signals as a valuable\ncomponent for building more robust and intelligent LLM inference systems, and\nhighlight a promising direction for future research on dynamic speculation\nlength adaptation.\n","authors":["Mingyu Yang","Jae-Young Choi","Kihyo Moon","Minsung Jang","Eunjoo Jeon"],"pdf_url":"https://arxiv.org/pdf/2509.01083v3.pdf","comment":"Accepted for presentation at the IEEE BigData 2025 Workshop (Special\n  Session on Intelligent Data Mining). This v2 updates formatting and adds IEEE\n  copyright notice"},{"id":"http://arxiv.org/abs/2510.26068v1","updated":"2025-10-30T01:53:32Z","published":"2025-10-30T01:53:32Z","title":"Learning Geometry: A Framework for Building Adaptive Manifold Models\n  through Metric Optimization","summary":"  This paper proposes a novel paradigm for machine learning that moves beyond\ntraditional parameter optimization. Unlike conventional approaches that search\nfor optimal parameters within a fixed geometric space, our core idea is to\ntreat the model itself as a malleable geometric entity. Specifically, we\noptimize the metric tensor field on a manifold with a predefined topology,\nthereby dynamically shaping the geometric structure of the model space. To\nachieve this, we construct a variational framework whose loss function\ncarefully balances data fidelity against the intrinsic geometric complexity of\nthe manifold. The former ensures the model effectively explains observed data,\nwhile the latter acts as a regularizer, penalizing overly curved or irregular\ngeometries to encourage simpler models and prevent overfitting. To address the\ncomputational challenges of this infinite-dimensional optimization problem, we\nintroduce a practical method based on discrete differential geometry: the\ncontinuous manifold is discretized into a triangular mesh, and the metric\ntensor is parameterized by edge lengths, enabling efficient optimization using\nautomatic differentiation tools. Theoretical analysis reveals a profound\nanalogy between our framework and the Einstein-Hilbert action in general\nrelativity, providing an elegant physical interpretation for the concept of\n\"data-driven geometry\". We further argue that even with fixed topology, metric\noptimization offers significantly greater expressive power than models with\nfixed geometry. This work lays a solid foundation for constructing fully\ndynamic \"meta-learners\" capable of autonomously evolving their geometry and\ntopology, and it points to broad application prospects in areas such as\nscientific model discovery and robust representation learning.\n","authors":["Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26068v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2505.18139v3","updated":"2025-10-30T01:51:54Z","published":"2025-05-23T17:48:09Z","title":"Embracing Contradiction: Theoretical Inconsistency Will Not Impede the\n  Road of Building Responsible AI Systems","summary":"  This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.\n","authors":["Gordon Dai","Yunze Xiao"],"pdf_url":"https://arxiv.org/pdf/2505.18139v3.pdf","comment":"14 pages,2 figure"},{"id":"http://arxiv.org/abs/2509.17784v2","updated":"2025-10-30T01:43:48Z","published":"2025-09-22T13:45:17Z","title":"Revealing Multimodal Causality with Large Language Models","summary":"  Uncovering cause-and-effect mechanisms from data is fundamental to scientific\nprogress. While large language models (LLMs) show promise for enhancing causal\ndiscovery (CD) from unstructured data, their application to the increasingly\nprevalent multimodal setting remains a critical challenge. Even with the advent\nof multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two\nprimary limitations: (1) difficulty in exploring intra- and inter-modal\ninteractions for comprehensive causal variable identification; and (2)\ninsufficiency to handle structural ambiguities with purely observational data.\nTo address these challenges, we propose MLLM-CD, a novel framework for\nmultimodal causal discovery from unstructured data. It consists of three key\ncomponents: (1) a novel contrastive factor discovery module to identify genuine\nmultimodal factors based on the interactions explored from contrastive sample\npairs; (2) a statistical causal structure discovery module to infer causal\nrelationships among discovered factors; and (3) an iterative multimodal\ncounterfactual reasoning module to refine the discovery outcomes iteratively by\nincorporating the world knowledge and reasoning capabilities of MLLMs.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of the proposed MLLM-CD in revealing genuine factors and causal\nrelationships among them from multimodal unstructured data.\n","authors":["Jin Li","Shoujin Wang","Qi Zhang","Feng Liu","Tongliang Liu","Longbing Cao","Shui Yu","Fang Chen"],"pdf_url":"https://arxiv.org/pdf/2509.17784v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26061v1","updated":"2025-10-30T01:32:21Z","published":"2025-10-30T01:32:21Z","title":"Data-driven Projection Generation for Efficiently Solving Heterogeneous\n  Quadratic Programming Problems","summary":"  We propose a data-driven framework for efficiently solving quadratic\nprogramming (QP) problems by reducing the number of variables in\nhigh-dimensional QPs using instance-specific projection. A graph neural\nnetwork-based model is designed to generate projections tailored to each QP\ninstance, enabling us to produce high-quality solutions even for previously\nunseen problems. The model is trained on heterogeneous QPs to minimize the\nexpected objective value evaluated on the projected solutions. This is\nformulated as a bilevel optimization problem; the inner optimization solves the\nQP under a given projection using a QP solver, while the outer optimization\nupdates the model parameters. We develop an efficient algorithm to solve this\nbilevel optimization problem, which computes parameter gradients without\nbackpropagating through the solver. We provide a theoretical analysis of the\ngeneralization ability of solving QPs with projection matrices generated by\nneural networks. Experimental results demonstrate that our method produces\nhigh-quality feasible solutions with reduced computation time, outperforming\nexisting methods.\n","authors":["Tomoharu Iwata","Futoshi Futami"],"pdf_url":"https://arxiv.org/pdf/2510.26061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26057v1","updated":"2025-10-30T01:16:33Z","published":"2025-10-30T01:16:33Z","title":"Can AI be Accountable?","summary":"  The AI we use is powerful, and its power is increasing rapidly. If this\npowerful AI is to serve the needs of consumers, voters, and decision makers,\nthen it is imperative that the AI is accountable. In general, an agent is\naccountable to a forum if the forum can request information from the agent\nabout its actions, if the forum and the agent can discuss this information, and\nif the forum can sanction the agent. Unfortunately, in too many cases today's\nAI is not accountable -- we cannot question it, enter into a discussion with\nit, let alone sanction it. In this chapter we relate the general definition of\naccountability to AI, we illustrate what it means for AI to be accountable and\nunaccountable, and we explore approaches that can improve our chances of living\nin a world where all AI is accountable to those who are affected by it.\n","authors":["Andrew L. Kun"],"pdf_url":"https://arxiv.org/pdf/2510.26057v1.pdf","comment":"To be published as a chapter in Daniele Quercia and Marios\n  Constantinides (Eds.). Operationalizing Responsible AI. Cambridge University\n  Press. Forthcoming"},{"id":"http://arxiv.org/abs/2510.26052v1","updated":"2025-10-30T01:10:25Z","published":"2025-10-30T01:10:25Z","title":"Dynamic VLM-Guided Negative Prompting for Diffusion Models","summary":"  We propose a novel approach for dynamic negative prompting in diffusion\nmodels that leverages Vision-Language Models (VLMs) to adaptively generate\nnegative prompts during the denoising process. Unlike traditional Negative\nPrompting methods that use fixed negative prompts, our method generates\nintermediate image predictions at specific denoising steps and queries a VLM to\nproduce contextually appropriate negative prompts. We evaluate our approach on\nvarious benchmark datasets and demonstrate the trade-offs between negative\nguidance strength and text-image alignment.\n","authors":["Hoyeon Chang","Seungjin Kim","Yoonseok Choi"],"pdf_url":"https://arxiv.org/pdf/2510.26052v1.pdf","comment":"39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: The First Workshop on Generative and Protective AI for\n  Content Creation"},{"id":"http://arxiv.org/abs/2510.26038v1","updated":"2025-10-30T00:34:16Z","published":"2025-10-30T00:34:16Z","title":"Do Students Debias Like Teachers? On the Distillability of Bias\n  Mitigation Methods","summary":"  Knowledge distillation (KD) is an effective method for model compression and\ntransferring knowledge between models. However, its effect on model's\nrobustness against spurious correlations that degrade performance on\nout-of-distribution data remains underexplored. This study investigates the\neffect of knowledge distillation on the transferability of ``debiasing''\ncapabilities from teacher models to student models on natural language\ninference (NLI) and image classification tasks. Through extensive experiments,\nwe illustrate several key findings: (i) overall the debiasing capability of a\nmodel is undermined post-KD; (ii) training a debiased model does not benefit\nfrom injecting teacher knowledge; (iii) although the overall robustness of a\nmodel may remain stable post-distillation, significant variations can occur\nacross different types of biases; and (iv) we pin-point the internal attention\npattern and circuit that causes the distinct behavior post-KD. Given the above\nfindings, we propose three effective solutions to improve the distillability of\ndebiasing methods: developing high quality data for augmentation, implementing\niterative knowledge distillation, and initializing student models with weights\nobtained from teacher models. To the best of our knowledge, this is the first\nstudy on the effect of KD on debiasing and its interenal mechanism at scale.\nOur findings provide understandings on how KD works and how to design better\ndebiasing methods.\n","authors":["Jiali Cheng","Chirag Agarwal","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2510.26038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12869v4","updated":"2025-10-30T00:34:12Z","published":"2024-10-14T01:57:25Z","title":"Language Model Preference Evaluation with Multiple Weak Evaluators","summary":"  Despite the remarkable success of Large Language Models (LLMs), evaluating\ntheir outputs' quality regarding preference remains a critical challenge. While\nexisting works usually leverage a strong LLM as the judge for comparing LLMs'\nresponse pairwisely, such a single-evaluator approach is vulnerable to cyclic\npreference, i.e., output A is better than B, B than C, but C is better than A,\ncausing contradictory evaluation results. To address this, we introduce PGED\n(Preference Graph Ensemble and Denoise), a novel approach that leverages\nmultiple model-based evaluators to construct preference graphs, and then\nensembles and denoises these graphs for acyclic, non-contradictory evaluation\nresults. We provide theoretical guarantees for our framework, demonstrating its\nefficacy in recovering the ground truth preference structure. Extensive\nexperiments on ten benchmarks demonstrate PGED 's superiority in three\napplications: 1) model ranking for evaluation, 2) response selection for\ntest-time scaling, and 3) data selection for model fine-tuning. Notably, PGED\ncombines small LLM evaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to\noutperform strong ones (e.g., Qwen2-72B), showcasing its effectiveness in\nenhancing evaluation reliability and improving model performance.\n","authors":["Zhengyu Hu","Jieyu Zhang","Zhihan Xiong","Alexander Ratner","Kaize Ding","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2410.12869v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26037v1","updated":"2025-10-30T00:32:58Z","published":"2025-10-30T00:32:58Z","title":"SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled\n  Structured Reasoning","summary":"  The ability of LLM agents to plan and invoke tools exposes them to new safety\nrisks, making a comprehensive red-teaming system crucial for discovering\nvulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic\nred-teaming framework for arbitrary black-box LLM agents. We employ a dynamic\ntwo-step process that starts with an agent definition and generates diverse\nseed test cases that cover various risk outcomes, tool-use trajectories, and\nrisk sources. Then, it iteratively constructs and refines model-based\nadversarial attacks based on the execution trajectories of former attempts. To\noptimize the red-teaming cost, we present a model distillation approach that\nleverages structured forms of a teacher model's reasoning to train smaller\nmodels that are equally effective. Across diverse evaluation agent settings,\nour seed test case generation approach yields 2 -- 2.5x boost to the coverage\nof risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer\nmodel improves attack success rate by 100%, surpassing the 671B Deepseek-R1\nmodel. Our ablations and analyses validate the effectiveness of the iterative\nframework, structured reasoning, and the generalization of our red-teamer\nmodels.\n","authors":["Kaiwen Zhou","Ahmed Elgohary","A S M Iftekhar","Amin Saied"],"pdf_url":"https://arxiv.org/pdf/2510.26037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26032v1","updated":"2025-10-30T00:15:07Z","published":"2025-10-30T00:15:07Z","title":"Artificial Intelligence-Enabled Analysis of Radiology Reports:\n  Epidemiology and Consequences of Incidental Thyroid Findings","summary":"  Importance Incidental thyroid findings (ITFs) are increasingly detected on\nimaging performed for non-thyroid indications. Their prevalence, features, and\nclinical consequences remain undefined. Objective To develop, validate, and\ndeploy a natural language processing (NLP) pipeline to identify ITFs in\nradiology reports and assess their prevalence, features, and clinical outcomes.\nDesign, Setting, and Participants Retrospective cohort of adults without prior\nthyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from\nJuly 1, 2017, to September 30, 2023. A transformer-based NLP pipeline\nidentified ITFs and extracted nodule characteristics from image reports from\nmultiple modalities and body regions. Main Outcomes and Measures Prevalence of\nITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer\ndiagnosis. Logistic regression identified demographic and imaging-related\nfactors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%\nwomen), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more\nlikely in women, older adults, those with higher BMI, and when imaging was\nordered by oncology or internal medicine. Compared with chest CT, ITFs were\nmore likely via neck CT, PET, and nuclear medicine scans. Nodule\ncharacteristics were poorly documented, with size reported in 44% and other\nfeatures in fewer than 15% (e.g. calcifications). Compared with patients\nwithout ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,\nbiopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were\npapillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were\ncommon and strongly associated with cascades leading to the detection of small,\nlow-risk cancers. These findings underscore the role of ITFs in thyroid cancer\noverdiagnosis and the need for standardized reporting and more selective\nfollow-up.\n","authors":["Felipe Larios","Mariana Borras-Osorio","Yuqi Wu","Ana Gabriela Claros","David Toro-Tobon","Esteban Cabezas","Ricardo Loor-Torres","Maria Mateo Chavez","Kerly Guevara Maldonado","Luis Vilatuna Andrango","Maria Lizarazo Jimenez","Ivan Mateo Alzamora","Misk Al Zahidy","Marcelo Montero","Ana Cristina Proano","Cristian Soto Jacome","Jungwei W. Fan","Oscar J. Ponce-Ponte","Megan E. Branda","Naykky Singh Ospina","Juan P. Brito"],"pdf_url":"https://arxiv.org/pdf/2510.26032v1.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2510.26800v1","updated":"2025-10-30T17:59:51Z","published":"2025-10-30T17:59:51Z","title":"OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes","summary":"  There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.\n","authors":["Yukun Huang","Jiwen Yu","Yanning Zhou","Jianan Wang","Xintao Wang","Pengfei Wan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26800v1.pdf","comment":"Project page: https://yukun-huang.github.io/OmniX/"},{"id":"http://arxiv.org/abs/2510.26796v1","updated":"2025-10-30T17:59:39Z","published":"2025-10-30T17:59:39Z","title":"SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting","summary":"  Immersive applications call for synthesizing spatiotemporal 4D content from\ncasual videos without costly 3D supervision. Existing video-to-4D methods\ntypically rely on manually annotated camera poses, which are labor-intensive\nand brittle for in-the-wild footage. Recent warp-then-inpaint approaches\nmitigate the need for pose labels by warping input frames along a novel camera\ntrajectory and using an inpainting model to fill missing regions, thereby\ndepicting the 4D scene from diverse viewpoints. However, this\ntrajectory-to-trajectory formulation often entangles camera motion with scene\ndynamics and complicates both modeling and inference. We introduce SEE4D, a\npose-free, trajectory-to-camera framework that replaces explicit trajectory\nprediction with rendering to a bank of fixed virtual cameras, thereby\nseparating camera control from scene modeling. A view-conditional video\ninpainting model is trained to learn a robust geometry prior by denoising\nrealistically synthesized warped images and to inpaint occluded or missing\nregions across virtual viewpoints, eliminating the need for explicit 3D\nannotations. Building on this inpainting core, we design a spatiotemporal\nautoregressive inference pipeline that traverses virtual-camera splines and\nextends videos with overlapping windows, enabling coherent generation at\nbounded per-step complexity. We validate See4D on cross-view video generation\nand sparse reconstruction benchmarks. Across quantitative metrics and\nqualitative assessments, our method achieves superior generalization and\nimproved performance relative to pose- or trajectory-conditioned baselines,\nadvancing practical 4D world modeling from casual videos.\n","authors":["Dongyue Lu","Ao Liang","Tianxin Huang","Xiao Fu","Yuyang Zhao","Baorui Ma","Liang Pan","Wei Yin","Lingdong Kong","Wei Tsang Ooi","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26796v1.pdf","comment":"26 pages; 21 figures; 3 tables; project page:\n  https://see-4d.github.io/"},{"id":"http://arxiv.org/abs/2510.26786v1","updated":"2025-10-30T17:57:40Z","published":"2025-10-30T17:57:40Z","title":"HEIR: Learning Graph-Based Motion Hierarchies","summary":"  Hierarchical structures of motion exist across research fields, including\ncomputer vision, graphics, and robotics, where complex dynamics typically arise\nfrom coordinated interactions among simpler motion components. Existing methods\nto model such dynamics typically rely on manually-defined or heuristic\nhierarchies with fixed motion primitives, limiting their generalizability\nacross different tasks. In this work, we propose a general hierarchical motion\nmodeling method that learns structured, interpretable motion relationships\ndirectly from data. Our method represents observed motions using graph-based\nhierarchies, explicitly decomposing global absolute motions into\nparent-inherited patterns and local motion residuals. We formulate hierarchy\ninference as a differentiable graph learning problem, where vertices represent\nelemental motions and directed edges capture learned parent-child dependencies\nthrough graph neural networks. We evaluate our hierarchical reconstruction\napproach on three examples: 1D translational motion, 2D rotational motion, and\ndynamic 3D scene deformation via Gaussian splatting. Experimental results show\nthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,\nand produces more realistic and interpretable deformations compared to the\nbaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,\ndata-driven hierarchical modeling paradigm, our method offers a formulation\napplicable to a broad range of motion-centric tasks. Project Page:\nhttps://light.princeton.edu/HEIR/\n","authors":["Cheng Zheng","William Koch","Baiang Li","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2510.26786v1.pdf","comment":"Code link: https://github.com/princeton-computational-imaging/HEIR"},{"id":"http://arxiv.org/abs/2510.26694v1","updated":"2025-10-30T17:01:18Z","published":"2025-10-30T17:01:18Z","title":"The Impact and Outlook of 3D Gaussian Splatting","summary":"  Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed\nthe landscape of 3D scene representations, inspiring an extensive body of\nassociated research. Follow-up work includes analyses and contributions that\nenhance the efficiency, scalability, and real-world applicability of 3DGS. In\nthis summary, we present an overview of several key directions that have\nemerged in the wake of 3DGS. We highlight advances enabling resource-efficient\ntraining and rendering, the evolution toward dynamic (or four-dimensional,\n4DGS) representations, and deeper exploration of the mathematical foundations\nunderlying its appearance modeling and rendering process. Furthermore, we\nexamine efforts to bring 3DGS to mobile and virtual reality platforms, its\nextension to massive-scale environments, and recent progress toward\nnear-instant radiance field reconstruction via feed-forward or distributed\ncomputation. Collectively, these developments illustrate how 3DGS has evolved\nfrom a breakthrough representation into a versatile and foundational tool for\n3D vision and graphics.\n","authors":["Bernhard Kerbl"],"pdf_url":"https://arxiv.org/pdf/2510.26694v1.pdf","comment":"Article written for Frontiers of Science Award, International\n  Congress on Basic Science, 2025"},{"id":"http://arxiv.org/abs/2503.22159v3","updated":"2025-10-30T10:00:04Z","published":"2025-03-28T05:46:02Z","title":"Disentangled 4D Gaussian Splatting: Rendering High-Resolution Dynamic\n  World at 343 FPS","summary":"  While dynamic novel view synthesis from 2D videos has seen progress,\nachieving efficient reconstruction and rendering of dynamic scenes remains a\nchallenging task. In this paper, we introduce Disentangled 4D Gaussian\nSplatting (Disentangled4DGS), a novel representation and rendering pipeline\nthat achieves real-time performance without compromising visual fidelity.\nDisentangled4DGS decouples the temporal and spatial components of 4D Gaussians,\navoiding the need for slicing first and four-dimensional matrix calculations in\nprior methods. By projecting temporal and spatial deformations into dynamic 2D\nGaussians and deferring temporal processing, we minimize redundant computations\nof 4DGS. Our approach also features a gradient-guided flow loss and temporal\nsplitting strategy to reduce artifacts. Experiments demonstrate a significant\nimprovement in rendering speed and quality, achieving 343 FPS when render\n1352*1014 resolution images on a single RTX3090 while reducing storage\nrequirements by at least 4.5%. Our approach sets a new benchmark for dynamic\nnovel view synthesis, outperforming existing methods on both multi-view and\nmonocular dynamic scene datasets.\n","authors":["Hao Feng","Hao Sun","Wei Xie","Zhi Zuo","Zhengzhe Liu"],"pdf_url":"https://arxiv.org/pdf/2503.22159v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26265v1","updated":"2025-10-30T08:45:02Z","published":"2025-10-30T08:45:02Z","title":"Look at That Distractor: Dynamic Translation Gain under Low Perceptual\n  Load in Virtual Reality","summary":"  Redirected walking utilizes gain adjustments within perceptual thresholds to\nallow natural navigation in large scale virtual environments within confined\nphysical environments. Previous research has found that when users are\ndistracted by some scene elements, they are less sensitive to gain values.\nHowever, the effects on detection thresholds have not been quantitatively\nmeasured. In this paper, we present a novel method that dynamically adjusts\ntranslation gain by leveraging visual distractors. We place distractors within\nthe user's field of view and apply a larger translation gain when their\nattention is drawn to them. Because the magnitude of gain adjustment depends on\nthe user's level of engagement with the distractors, the redirection process\nremains smooth and unobtrusive. To evaluate our method, we developed a task\noriented virtual environment for a user study. Results show that introducing\ndistractors in the virtual environment significantly raises users' translation\ngain thresholds. Furthermore, assessments using the Simulator Sickness\nQuestionnaire and Igroup Presence Questionnaire indicate that the method\nmaintains user comfort and acceptance, supporting its effectiveness for RDW\nsystems.\n","authors":["Ling-Long Zou","Qiang Tong","Er-Xia Luo","Sen-Zhe Xu","Song-Hai Zhang","Fang-Lue Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26141v1","updated":"2025-10-30T04:52:12Z","published":"2025-10-30T04:52:12Z","title":"StructLayoutFormer:Conditional Structured Layout Generation via\n  Structure Serialization and Disentanglement","summary":"  Structured layouts are preferable in many 2D visual contents (\\eg, GUIs,\nwebpages) since the structural information allows convenient layout editing.\nComputational frameworks can help create structured layouts but require heavy\nlabor input. Existing data-driven approaches are effective in automatically\ngenerating fixed layouts but fail to produce layout structures. We present\nStructLayoutFormer, a novel Transformer-based approach for conditional\nstructured layout generation. We use a structure serialization scheme to\nrepresent structured layouts as sequences. To better control the structures of\ngenerated layouts, we disentangle the structural information from the element\nplacements. Our approach is the first data-driven approach that achieves\nconditional structured layout generation and produces realistic layout\nstructures explicitly. We compare our approach with existing data-driven layout\ngeneration approaches by including post-processing for structure extraction.\nExtensive experiments have shown that our approach exceeds these baselines in\nconditional structured layout generation. We also demonstrate that our approach\nis effective in extracting and transferring layout structures. The code is\npublicly available at %\\href{https://github.com/Teagrus/StructLayoutFormer}\n{https://github.com/Teagrus/StructLayoutFormer}.\n","authors":["Xin Hu","Pengfei Xu","Jin Zhou","Hongbo Fu","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2510.26141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00868v2","updated":"2025-10-30T01:56:10Z","published":"2025-03-02T12:08:19Z","title":"3D Dynamic Fluid Assets from Single-View Videos with Generative Gaussian\n  Splatting","summary":"  While the generation of 3D content from single-view images has been\nextensively studied, the creation of physically consistent 3D dynamic scenes\nfrom videos remains in its early stages. We propose a novel framework\nleveraging generative 3D Gaussian Splatting (3DGS) models to extract and\nre-simulate 3D dynamic fluid objects from single-view videos using simulation\nmethods. The fluid geometry represented by 3DGS is initially generated and\noptimized from single-view images, then denoised, densified, and aligned across\nframes. We estimate the fluid surface velocity using optical flow, propose a\nmainstream extraction algorithm to refine it. The 3D volumetric velocity field\nis then derived from the velocity of the fluid's enclosed surface. The velocity\nfield is therewith converted into a divergence-free, grid-based representation,\nenabling the optimization of simulation parameters through its\ndifferentiability across frames. This process outputs simulation-ready fluid\nassets with physical dynamics closely matching those observed in the source\nvideo. Our approach is applicable to various liquid fluids, including inviscid\nand viscous types, and allows users to edit the output geometry or extend\nmovement durations seamlessly. This automatic method for creating 3D dynamic\nfluid assets from single-view videos, easily obtainable from the internet,\nshows great potential for generating large-scale 3D fluid assets at a low cost.\n","authors":["Zhiwei Zhao","Alan Zhao","Minchen Li","Yixin Hu"],"pdf_url":"https://arxiv.org/pdf/2503.00868v2.pdf","comment":null}]},"2025-10-29T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2510.26024v1","updated":"2025-10-29T23:37:54Z","published":"2025-10-29T23:37:54Z","title":"Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural\n  Erasure in Multilingual LLMs","summary":"  Cross-lingual alignment (CLA) aims to align multilingual representations,\nenabling Large Language Models (LLMs) to seamlessly transfer knowledge across\nlanguages. While intuitive, we hypothesize, this pursuit of representational\nconvergence can inadvertently cause \"cultural erasure\", the functional loss of\nproviding culturally-situated responses that should diverge based on the query\nlanguage. In this work, we systematically analyze this trade-off by introducing\na holistic evaluation framework, the transfer-localization plane, which\nquantifies both desirable knowledge transfer and undesirable cultural erasure.\nUsing this framework, we re-evaluate recent CLA approaches and find that they\nconsistently improve factual transfer at the direct cost of cultural\nlocalization across all six languages studied. Our investigation into the\ninternal representations of these models reveals a key insight: universal\nfactual transfer and culturally-specific knowledge are optimally steerable at\ndifferent model layers. Based on this finding, we propose Surgical Steering, a\nnovel inference-time method that disentangles these two objectives. By applying\ntargeted activation steering to distinct layers, our approach achieves a better\nbalance between the two competing dimensions, effectively overcoming the\nlimitations of current alignment techniques.\n","authors":["HyoJung Han","Sweta Agrawal","Eleftheria Briakou"],"pdf_url":"https://arxiv.org/pdf/2510.26024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02820v3","updated":"2025-10-29T23:29:35Z","published":"2025-05-05T17:47:49Z","title":"AutoLibra: Agent Metric Induction from Open-Ended Human Feedback","summary":"  Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose **AutoLibra**, a framework for\nagent evaluation, that transforms open-ended human feedback *e.g.* \"If you find\nthat the button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\" into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra serve human prompt engineers for\ndiagonalize agent failures and improve prompts iterative. Moreover, we find\nthat AutoLibra can induce metrics for automatic optimization for agents, which\nmakes agents improve through self-regulation. Our results suggest that\nAutoLibra is a powerful task-agnostic tool for evaluating and improving\nlanguage agents.\n","authors":["Hao Zhu","Phil Cuvin","Xinkai Yu","Charlotte Ka Yee Yan","Jason Zhang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2505.02820v3.pdf","comment":"https://github.com/Open-Social-World/autolibra"},{"id":"http://arxiv.org/abs/2510.26020v1","updated":"2025-10-29T23:28:53Z","published":"2025-10-29T23:28:53Z","title":"PORTool: Tool-Use LLM Training with Rewarded Tree","summary":"  Current tool-use large language models (LLMs) are trained on static datasets,\nenabling them to interact with external tools and perform multi-step,\ntool-integrated reasoning, which produces tool-call trajectories. However,\nthese models imitate how a query is resolved in a generic tool-call routine,\nthereby failing to explore possible solutions and demonstrating limited\nperformance in an evolved, dynamic tool-call environment. In this work, we\npropose PORTool, a reinforcement learning (RL) method that encourages a\ntool-use LLM to explore various trajectories yielding the correct answer.\nSpecifically, this method starts with generating multiple rollouts for a given\nquery, and some of them share the first few tool-call steps, thereby forming a\ntree-like structure. Next, we assign rewards to each step, based on its ability\nto produce a correct answer and make successful tool calls. A shared step\nacross different trajectories receives the same reward, while different steps\nunder the same fork receive different rewards. Finally, these step-wise rewards\nare used to calculate fork-relative advantages, blended with\ntrajectory-relative advantages, to train the LLM for tool use. The experiments\nutilize 17 tools to address user queries, covering both time-sensitive and\ntime-invariant topics. We conduct ablation studies to systematically justify\nthe necessity and the design robustness of step-wise rewards. Furthermore, we\ncompare the proposed PORTool with other training approaches and demonstrate\nsignificant improvements in final accuracy and the number of tool-call steps.\n","authors":["Feijie Wu","Weiwu Zhu","Yuxiang Zhang","Soumya Chatterjee","Jiarong Zhu","Fan Mo","Rodin Luo","Jing Gao"],"pdf_url":"https://arxiv.org/pdf/2510.26020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17608v2","updated":"2025-10-29T22:44:11Z","published":"2023-05-28T02:12:00Z","title":"Reward Collapse in Aligning Large Language Models","summary":"  The extraordinary capabilities of large language models (LLMs) such as\nChatGPT and GPT-4 are in part unleashed by aligning them with reward models\nthat are trained on human preferences, which are often represented as rankings\nof responses to prompts. In this paper, we document the phenomenon of\n\\textit{reward collapse}, an empirical observation where the prevailing\nranking-based approach results in an \\textit{identical} reward distribution\n\\textit{regardless} of the prompts during the terminal phase of training. This\noutcome is undesirable as open-ended prompts like ``write a short story about\nyour best friend'' should yield a continuous range of rewards for their\ncompletions, while specific prompts like ``what is the capital of New Zealand''\nshould generate either high or low rewards. Our theoretical investigation\nreveals that reward collapse is primarily due to the insufficiency of the\nranking-based objective function to incorporate prompt-related information\nduring optimization. This insight allows us to derive closed-form expressions\nfor the reward distribution associated with a set of utility functions in an\nasymptotic regime. To overcome reward collapse, we introduce a prompt-aware\noptimization scheme that provably admits a prompt-dependent reward distribution\nwithin the interpolating regime. Our experimental results suggest that our\nproposed prompt-aware utility functions significantly alleviate reward collapse\nduring the training of reward models.\n","authors":["Ziang Song","Tianle Cai","Jason D. Lee","Weijie J. Su"],"pdf_url":"https://arxiv.org/pdf/2305.17608v2.pdf","comment":"Accepted for publication in the Journal of Data Science (JDS),\n  reference JDS1201"},{"id":"http://arxiv.org/abs/2507.13328v2","updated":"2025-10-29T22:38:57Z","published":"2025-07-17T17:47:47Z","title":"Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does\n  Not Fundamentally Alter It","summary":"  Does vision-and-language (VL) training change the linguistic representations\nof language models in meaningful ways? Most results in the literature have\nshown inconsistent or marginal differences, both behaviorally and\nrepresentationally. In this work, we start from the hypothesis that the domain\nin which VL training could have a significant effect is lexical-conceptual\nknowledge, in particular its taxonomic organization. Through comparing minimal\npairs of text-only LMs and their VL-trained counterparts, we first show that\nthe VL models often outperform their text-only counterparts on a text-only\nquestion-answering task that requires taxonomic understanding of concepts\nmentioned in the questions. Using an array of targeted behavioral and\nrepresentational analyses, we show that the LMs and VLMs do not differ\nsignificantly in terms of their taxonomic knowledge itself, but they differ in\nhow they represent questions that contain concepts in a taxonomic relation vs.\na non-taxonomic relation. This implies that the taxonomic knowledge itself does\nnot change substantially through additional VL training, but VL training does\nimprove the deployment of this knowledge in the context of a specific task,\neven when the presentation of the task is purely linguistic.\n","authors":["Yulu Qin","Dheeraj Varghese","Adam Dahlgren Lindström","Lucia Donatelli","Kanishka Misra","Najoung Kim"],"pdf_url":"https://arxiv.org/pdf/2507.13328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26006v1","updated":"2025-10-29T22:34:26Z","published":"2025-10-29T22:34:26Z","title":"CAVE: Detecting and Explaining Commonsense Anomalies in Visual\n  Environments","summary":"  Humans can naturally identify, reason about, and explain anomalies in their\nenvironment. In computer vision, this long-standing challenge remains limited\nto industrial defects or unrealistic, synthetically generated anomalies,\nfailing to capture the richness and unpredictability of real-world anomalies.\nIn this work, we introduce CAVE, the first benchmark of real-world visual\nanomalies. CAVE supports three open-ended tasks: anomaly description,\nexplanation, and justification; with fine-grained annotations for visual\ngrounding and categorizing anomalies based on their visual manifestations,\ntheir complexity, severity, and commonness. These annotations draw inspiration\nfrom cognitive science research on how humans identify and resolve anomalies,\nproviding a comprehensive framework for evaluating Vision-Language Models\n(VLMs) in detecting and understanding anomalies. We show that state-of-the-art\nVLMs struggle with visual anomaly perception and commonsense reasoning, even\nwith advanced prompting strategies. By offering a realistic and cognitively\ngrounded benchmark, CAVE serves as a valuable resource for advancing research\nin anomaly detection and commonsense reasoning in VLMs.\n","authors":["Rishika Bhagwatkar","Syrielle Montariol","Angelika Romanou","Beatriz Borges","Irina Rish","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2510.26006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02878v3","updated":"2025-10-29T22:28:23Z","published":"2025-03-04T18:58:11Z","title":"Language Models can Self-Improve at State-Value Estimation for Better\n  Search","summary":"  Collecting ground-truth rewards or human demonstrations for multi-step\nreasoning tasks is often prohibitively expensive, particularly in interactive\ndomains such as web tasks. We introduce Self-Taught Lookahead (STL), a\nreward-free framework that improves language model-based value functions by\nreasoning explicitly about state transitions. STL can be viewed as a\nchain-of-thought analogue of the value iteration algorithm: instead of\nregressing directly on numeric values, a value LLM is trained to simulate a\nstep of lookahead in natural language - predicting the next action, resulting\nstate, and rationale for its value, thereby refining value estimates without\nany labeled data. This self-supervised procedure yields more accurate\nstate-value predictions, which in turn enable lightweight search algorithms to\nexpand fewer states while maintaining strong performance. Empirically,\nSTL-trained value models built on moderately sized (8B parameter) open-weight\nLLMs boost web agent success rates by 39%, achieving comparable performance\nwith proprietary models. STL also generalizes to multi-hop QA and math puzzles.\nWe find that STL enables small open-source models to guide efficient search,\nreducing inference costs by integrating explicit reasoning with value learning.\n","authors":["Ethan Mendes","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2503.02878v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18125v2","updated":"2025-10-29T22:07:41Z","published":"2025-05-23T17:34:28Z","title":"TabSTAR: A Tabular Foundation Model for Tabular Data with Text Fields","summary":"  While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees. However, recent advancements are\npaving the way for Tabular Foundation Models, which can leverage real-world\nknowledge and generalize across diverse datasets, particularly when the data\ncontains free-text. Although incorporating language model capabilities into\ntabular tasks has been explored, most existing methods utilize static,\ntarget-agnostic textual representations, limiting their effectiveness. We\nintroduce TabSTAR: a Tabular Foundation Model with Semantically Target-Aware\nRepresentations. TabSTAR is designed to enable transfer learning on tabular\ndata with textual features, with an architecture free of dataset-specific\nparameters. It unfreezes a pretrained text encoder and takes as input target\ntokens, which provide the model with the context needed to learn task-specific\nembeddings. TabSTAR achieves state-of-the-art performance for both medium- and\nlarge-sized datasets across known benchmarks of classification tasks with text\nfeatures, and its pretraining phase exhibits scaling laws in the number of\ndatasets, offering a pathway for further performance improvements.\n","authors":["Alan Arazi","Eilam Shapira","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2505.18125v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25992v1","updated":"2025-10-29T22:05:08Z","published":"2025-10-29T22:05:08Z","title":"Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning","summary":"  Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.\n","authors":["Yihe Deng","I-Hung Hsu","Jun Yan","Zifeng Wang","Rujun Han","Gufeng Zhang","Yanfei Chen","Wei Wang","Tomas Pfister","Chen-Yu Lee"],"pdf_url":"https://arxiv.org/pdf/2510.25992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25979v1","updated":"2025-10-29T21:26:17Z","published":"2025-10-29T21:26:17Z","title":"AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache","summary":"  Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.\n","authors":["Dinghong Song","Yuan Feng","Yiwei Wang","Shangye Chen","Cyril Guyot","Filip Blagojevic","Hyeran Jeon","Pengfei Su","Dong Li"],"pdf_url":"https://arxiv.org/pdf/2510.25979v1.pdf","comment":"10 pages, 6 figures, submitted to Ninth Annual Conference on Machine\n  Learning and Systems (MLSys'26)"},{"id":"http://arxiv.org/abs/2510.25977v1","updated":"2025-10-29T21:22:08Z","published":"2025-10-29T21:22:08Z","title":"NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium","summary":"  AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference.\n","authors":["Dinghong Song","Jierui Xu","Weichu Yang","Pengfei Su","Dong Li"],"pdf_url":"https://arxiv.org/pdf/2510.25977v1.pdf","comment":"12 pages, 8 figures, submitted to the Proceedings of the Twenty-First\n  European Conference on Computer Systems (EuroSys'26)"},{"id":"http://arxiv.org/abs/2510.25975v1","updated":"2025-10-29T21:17:57Z","published":"2025-10-29T21:17:57Z","title":"SymCode: A Neurosymbolic Approach to Mathematical Reasoning via\n  Verifiable Code Generation","summary":"  Large Language Models (LLMs) often struggle with complex mathematical\nreasoning, where prose-based generation leads to unverified and arithmetically\nunsound solutions. Current prompting strategies like Chain of Thought still\noperate within this unreliable medium, lacking a mechanism for deterministic\nverification. To address these limitations, we introduce SymCode, a\nneurosymbolic framework that reframes mathematical problem-solving as a task of\nverifiable code generation using the SymPy library. We evaluate SymCode on\nchallenging benchmarks, including MATH-500 and OlympiadBench, demonstrating\nsignificant accuracy improvements of up to 13.6 percentage points over\nbaselines. Our analysis shows that SymCode is not only more token-efficient but\nalso fundamentally shifts model failures from opaque logical fallacies towards\ntransparent, programmatic errors. By grounding LLM reasoning in a deterministic\nsymbolic engine, SymCode represents a key step towards more accurate and\ntrustworthy AI in formal domains.\n","authors":["Sina Bagheri Nezhad","Yao Li","Ameeta Agrawal"],"pdf_url":"https://arxiv.org/pdf/2510.25975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25967v1","updated":"2025-10-29T21:11:23Z","published":"2025-10-29T21:11:23Z","title":"Semantic Label Drift in Cross-Cultural Translation","summary":"  Machine Translation (MT) is widely employed to address resource scarcity in\nlow-resource languages by generating synthetic data from high-resource\ncounterparts. While sentiment preservation in translation has long been\nstudied, a critical but underexplored factor is the role of cultural alignment\nbetween source and target languages. In this paper, we hypothesize that\nsemantic labels are drifted or altered during MT due to cultural divergence.\nThrough a series of experiments across culturally sensitive and neutral\ndomains, we establish three key findings: (1) MT systems, including modern\nLarge Language Models (LLMs), induce label drift during translation,\nparticularly in culturally sensitive domains; (2) unlike earlier statistical MT\ntools, LLMs encode cultural knowledge, and leveraging this knowledge can\namplify label drift; and (3) cultural similarity or dissimilarity between\nsource and target languages is a crucial determinant of label preservation. Our\nfindings highlight that neglecting cultural factors in MT not only undermines\nlabel fidelity but also risks misinterpretation and cultural conflict in\ndownstream applications.\n","authors":["Mohsinul Kabir","Tasnim Ahmed","Md Mezbaur Rahman","Polydoros Giannouris","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2510.25967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16325v3","updated":"2025-10-29T21:04:32Z","published":"2024-10-18T14:03:46Z","title":"This Candidate is [MASK]. Prompt-based Sentiment Extraction and\n  Reference Letters","summary":"  I propose a relatively simple way to deploy pre-trained large language models\n(LLMs) in order to extract sentiment and other useful features from text data.\nThe method, which I refer to as prompt-based sentiment extraction, offers\nmultiple advantages over other methods used in economics and finance. In\nparticular, it accepts the text input as is (without pre-processing) and\nproduces a sentiment score that has a probability interpretation. Unlike other\nLLM-based approaches, it does not require any fine-tuning or labeled data. I\napply my prompt-based strategy to a hand-collected corpus of confidential\nreference letters (RLs). I show that the sentiment contents of RLs are clearly\nreflected in job market outcomes. Candidates with higher average sentiment in\ntheir RLs perform markedly better regardless of the measure of success chosen.\nMoreover, I show that sentiment dispersion among letter writers negatively\naffects the job market candidate's performance. I compare my sentiment\nextraction approach to other commonly used methods for sentiment analysis:\n`bag-of-words' approaches, fine-tuned language models, and querying advanced\nchatbots. No other method can fully reproduce the results obtained by\nprompt-based sentiment extraction. Finally, I slightly modify the method to\nobtain `gendered' sentiment scores (as in Eberhardt et al., 2023). I show that\nRLs written for female candidates emphasize `grindstone' personality traits,\nwhereas male candidates' letters emphasize `standout' traits. These gender\ndifferences negatively affect women's job market outcomes.\n","authors":["Fabian Slonimczyk"],"pdf_url":"https://arxiv.org/pdf/2410.16325v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25947v1","updated":"2025-10-29T20:46:03Z","published":"2025-10-29T20:46:03Z","title":"Revisiting Multilingual Data Mixtures in Language Model Pretraining","summary":"  The impact of different multilingual data mixtures in pretraining large\nlanguage models (LLMs) has been a topic of ongoing debate, often raising\nconcerns about potential trade-offs between language coverage and model\nperformance (i.e., the curse of multilinguality). In this work, we investigate\nthese assumptions by training 1.1B and 3B parameter LLMs on diverse\nmultilingual corpora, varying the number of languages from 25 to 400. Our study\nchallenges common beliefs surrounding multilingual training. First, we find\nthat combining English and multilingual data does not necessarily degrade the\nin-language performance of either group, provided that languages have a\nsufficient number of tokens included in the pretraining corpus. Second, we\nobserve that using English as a pivot language (i.e., a high-resource language\nthat serves as a catalyst for multilingual generalization) yields benefits\nacross language families, and contrary to expectations, selecting a pivot\nlanguage from within a specific family does not consistently improve\nperformance for languages within that family. Lastly, we do not observe a\nsignificant \"curse of multilinguality\" as the number of training languages\nincreases in models at this scale. Our findings suggest that multilingual data,\nwhen balanced appropriately, can enhance language model capabilities without\ncompromising performance, even in low-resource settings\n","authors":["Negar Foroutan","Paul Teiletche","Ayush Kumar Tarun","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2510.25947v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2510.25941v1","updated":"2025-10-29T20:36:37Z","published":"2025-10-29T20:36:37Z","title":"RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic\n  Pipeline","summary":"  If we cannot inspect the training data of a large language model (LLM), how\ncan we ever know what it has seen? We believe the most compelling evidence\narises when the model itself freely reproduces the target content. As such, we\npropose RECAP, an agentic pipeline designed to elicit and verify memorized\ntraining data from LLM outputs. At the heart of RECAP is a feedback-driven\nloop, where an initial extraction attempt is evaluated by a secondary language\nmodel, which compares the output against a reference passage and identifies\ndiscrepancies. These are then translated into minimal correction hints, which\nare fed back into the target model to guide subsequent generations. In\naddition, to address alignment-induced refusals, RECAP includes a jailbreaking\nmodule that detects and overcomes such barriers. We evaluate RECAP on\nEchoTrace, a new benchmark spanning over 30 full books, and the results show\nthat RECAP leads to substantial gains over single-iteration approaches. For\ninstance, with GPT-4.1, the average ROUGE-L score for the copyrighted text\nextraction improved from 0.38 to 0.47 - a nearly 24% increase.\n","authors":["André V. Duarte","Xuying li","Bin Zeng","Arlindo L. Oliveira","Lei Li","Zhuo Li"],"pdf_url":"https://arxiv.org/pdf/2510.25941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25932v1","updated":"2025-10-29T20:11:48Z","published":"2025-10-29T20:11:48Z","title":"FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for\n  Facebook and X","summary":"  Social platforms distribute information at unprecedented speed, which in turn\naccelerates the spread of misinformation and threatens public discourse. We\npresent FakeZero, a fully client-side, cross-platform browser extension that\nflags unreliable posts on Facebook and X (formerly Twitter) while the user\nscrolls. All computation, DOM scraping, tokenisation, Transformer inference,\nand UI rendering run locally through the Chromium messaging API, so no personal\ndata leaves the device.FakeZero employs a three-stage training curriculum:\nbaseline fine-tuning and domain-adaptive training enhanced with focal loss,\nadversarial augmentation, and post-training quantisation. Evaluated on a\ndataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1%\nmacro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of\napproximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant\nvariant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to\n14.7 MB and lowering latency to approximately 40 ms, showing that high-quality\nfake-news detection is feasible under tight resource budgets with only modest\nperformance loss.By providing inline credibility cues, the extension can serve\nas a valuable tool for policymakers seeking to curb the spread of\nmisinformation across social networks. With user consent, FakeZero also opens\nthe door for researchers to collect large-scale datasets of fake news in the\nwild, enabling deeper analysis and the development of more robust detection\ntechniques.\n","authors":["Soufiane Essahli","Oussama Sarsar","Imane Fouad","Anas Motii","Ahmed Bentajer"],"pdf_url":"https://arxiv.org/pdf/2510.25932v1.pdf","comment":"Accepted for publication in the Proceedings of the 24th IEEE\n  International Conference on Trust, Security and Privacy in Computing and\n  Communications (TrustCom 2025) Privacy track, 11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2504.04953v2","updated":"2025-10-29T20:00:58Z","published":"2025-04-07T11:37:26Z","title":"M-Prometheus: A Suite of Open Multilingual LLM Judges","summary":"  The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on synthetic multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code.\n","authors":["José Pombal","Dongkeun Yoon","Patrick Fernandes","Ian Wu","Seungone Kim","Ricardo Rei","Graham Neubig","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2504.04953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.01001v2","updated":"2025-10-29T19:55:23Z","published":"2025-04-01T17:40:08Z","title":"Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic\n  Evaluation of Language Models","summary":"  As language models improve and become capable of performing more complex\ntasks across modalities, evaluating them automatically becomes increasingly\nchallenging. Developing strong and robust task-specific automatic metrics gets\nharder, and human-annotated test sets -- which are expensive to create --\nsaturate more quickly. A compelling alternative is to design reliable\nstrategies to automate the creation of test data and evaluation, but previous\nattempts either rely on pre-existing data, or focus solely on individual tasks.\nWe present Zero-shot Benchmarking (ZSB), a framework for creating high-quality\nbenchmarks for any task by leveraging language models for both synthetic test\ndata creation and evaluation. ZSB is simple and flexible: it requires only the\ncreation of a prompt for data generation and one for evaluation; it is scalable\nto tasks and languages where collecting real-world data is costly or\nimpractical; it is model-agnostic, allowing the creation of increasingly\nchallenging benchmarks as models improve. To assess the effectiveness of our\nframework, we create benchmarks for five text-only tasks and a multi-modal one:\ngeneral capabilities in four languages (English, Chinese, French, and Korean),\ntranslation, and general vision-language capabilities in English. We then rank\na broad range of open and closed systems on our benchmarks. ZSB rankings\nconsistently correlate strongly with human rankings, outperforming\nwidely-adopted standard benchmarks. Through ablations, we find that strong\nbenchmarks can be created with open models, and that judge model size and\ndataset variety are crucial drivers of performance. We release all our\nbenchmarks, and code to reproduce our experiments and to produce new\nbenchmarks.\n","authors":["José Pombal","Nuno M. Guerreiro","Ricardo Rei","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2504.01001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13464v2","updated":"2025-10-29T19:33:20Z","published":"2025-06-16T13:24:50Z","title":"Unveiling the Learning Mind of Language Models: A Cognitive Framework\n  and Empirical Study","summary":"  Large language models (LLMs) have shown impressive capabilities across tasks\nsuch as mathematics, coding, and reasoning, yet their learning ability, which\nis crucial for adapting to dynamic environments and acquiring new knowledge,\nremains underexplored. In this work, we address this gap by introducing a\nframework inspired by cognitive psychology and education. Specifically, we\ndecompose general learning ability into three distinct, complementary\ndimensions: Learning from Instructor (acquiring knowledge via explicit\nguidance), Learning from Concept (internalizing abstract structures and\ngeneralizing to new contexts), and Learning from Experience (adapting through\naccumulated exploration and feedback). We conduct a comprehensive empirical\nstudy across the three learning dimensions and identify several insightful\nfindings, such as (i) interaction improves learning; (ii) conceptual\nunderstanding is scale-emergent and benefits larger models; and (iii) LLMs are\neffective few-shot learners but not many-shot learners. Based on our framework\nand empirical findings, we introduce a benchmark that provides a unified and\nrealistic evaluation of LLMs' general learning abilities across three learning\ncognition dimensions. It enables diagnostic insights and supports evaluation\nand development of more adaptive and human-like models.\n","authors":["Zhengyu Hu","Jianxun Lian","Zheyuan Xiao","Seraphina Zhang","Tianfu Wang","Nicholas Jing Yuan","Xing Xie","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2506.13464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00176v2","updated":"2025-10-29T19:28:15Z","published":"2024-03-29T22:11:54Z","title":"The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks","summary":"  Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task,\nwhich is usually operationalized based on two subsequently applied usage-level\ntasks: First, Word-in-Context (WiC) labels are derived for pairs of usages.\nThen, these labels are represented in a graph on which Word Sense Induction\n(WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by\ncomparing sense clusters over time. This modularity is reflected in most LSCD\ndatasets and models. It also leads to a large heterogeneity in modeling options\nand task definitions, which is exacerbated by a variety of dataset versions,\npreprocessing options and evaluation metrics. This heterogeneity makes it\ndifficult to evaluate models under comparable conditions, to choose optimal\nmodel combinations or to reproduce results. Hence, we provide a benchmark\nrepository standardizing LSCD evaluation. Through transparent implementation\nresults become easily reproducible and by standardization different components\ncan be freely combined. The repository reflects the task's modularity by\nallowing model evaluation for WiC, WSI and LSCD. This allows for careful\nevaluation of increasingly complex model components providing new ways of model\noptimization. We use the implemented benchmark to conduct a number of\nexperiments with recent models and systematically improve the state-of-the-art.\n","authors":["Dominik Schlechtweg","Sachin Yadav","Nikolay Arefyev"],"pdf_url":"https://arxiv.org/pdf/2404.00176v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08188v2","updated":"2025-10-29T19:24:51Z","published":"2025-06-09T19:56:42Z","title":"GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors","summary":"  In this paper, we introduce GradEscape, the first gradient-based evader\ndesigned to attack AI-generated text (AIGT) detectors. GradEscape overcomes the\nundifferentiable computation problem, caused by the discrete nature of text, by\nintroducing a novel approach to construct weighted embeddings for the detector\ninput. It then updates the evader model parameters using feedback from victim\ndetectors, achieving high attack success with minimal text modification. To\naddress the issue of tokenizer mismatch between the evader and the detector, we\nintroduce a warm-started evader method, enabling GradEscape to adapt to\ndetectors across any language model architecture. Moreover, we employ novel\ntokenizer inference and model extraction techniques, facilitating effective\nevasion even in query-only access.\n  We evaluate GradEscape on four datasets and three widely-used language\nmodels, benchmarking it against four state-of-the-art AIGT evaders.\nExperimental results demonstrate that GradEscape outperforms existing evaders\nin various scenarios, including with an 11B paraphrase model, while utilizing\nonly 139M parameters. We have successfully applied GradEscape to two real-world\ncommercial AIGT detectors. Our analysis reveals that the primary vulnerability\nstems from disparity in text expression styles within the training data. We\nalso propose a potential defense strategy to mitigate the threat of AIGT\nevaders. We open-source our GradEscape for developing more robust AIGT\ndetectors.\n","authors":["Wenlong Meng","Shuguo Fan","Chengkun Wei","Min Chen","Yuwei Li","Yuanchao Zhang","Zhikun Zhang","Wenzhi Chen"],"pdf_url":"https://arxiv.org/pdf/2506.08188v2.pdf","comment":"Accepted by USENIX Security'25; Update badges and Artifact Appendix"},{"id":"http://arxiv.org/abs/2510.24134v2","updated":"2025-10-29T19:17:39Z","published":"2025-10-28T07:19:01Z","title":"VC4VG: Optimizing Video Captions for Text-to-Video Generation","summary":"  Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels. We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements. Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/alimama-creative/VC4VG to\nsupport further research.\n","authors":["Yang Du","Zhuoran Lin","Kaiqiang Song","Biao Wang","Zhicheng Zheng","Tiezheng Ge","Bo Zheng","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2510.24134v2.pdf","comment":"Accepted by EMNLP 2025"},{"id":"http://arxiv.org/abs/2506.07001v2","updated":"2025-10-29T19:16:47Z","published":"2025-06-08T05:15:01Z","title":"Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated\n  Text","summary":"  The increasing capabilities of Large Language Models (LLMs) have raised\nconcerns about their misuse in AI-generated plagiarism and social engineering.\nWhile various AI-generated text detectors have been proposed to mitigate these\nrisks, many remain vulnerable to simple evasion techniques such as\nparaphrasing. However, recent detectors have shown greater robustness against\nsuch basic attacks. In this work, we introduce Adversarial Paraphrasing, a\ntraining-free attack framework that universally humanizes any AI-generated text\nto evade detection more effectively. Our approach leverages an off-the-shelf\ninstruction-following LLM to paraphrase AI-generated content under the guidance\nof an AI text detector, producing adversarial examples that are specifically\noptimized to bypass detection. Extensive experiments show that our attack is\nboth broadly effective and highly transferable across several detection\nsystems. For instance, compared to simple paraphrasing attack--which,\nironically, increases the true positive at 1% false positive (T@1%F) by 8.57%\non RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by\nOpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on\nFast-DetectGPT. Across a diverse set of detectors--including neural\nnetwork-based, watermark-based, and zero-shot approaches--our attack achieves\nan average T@1%F reduction of 87.88% under the guidance of\nOpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and\nattack success to find that our method can significantly reduce detection\nrates, with mostly a slight degradation in text quality. Our adversarial setup\nhighlights the need for more robust and resilient detection strategies in the\nlight of increasingly sophisticated evasion techniques.\n","authors":["Yize Cheng","Vinu Sankar Sadasivan","Mehrdad Saberi","Shoumik Saha","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2506.07001v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25904v1","updated":"2025-10-29T19:13:48Z","published":"2025-10-29T19:13:48Z","title":"Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized\n  Setting: the Case of FrameNet Annotation","summary":"  The use of LLM-based applications as a means to accelerate and/or substitute\nhuman labor in the creation of language resources and dataset is a reality.\nNonetheless, despite the potential of such tools for linguistic research,\ncomprehensive evaluation of their performance and impact on the creation of\nannotated datasets, especially under a perspectivized approach to NLP, is still\nmissing. This paper contributes to reduction of this gap by reporting on an\nextensive evaluation of the (semi-)automatization of FrameNet-like semantic\nannotation by the use of an LLM-based semantic role labeler. The methodology\nemployed compares annotation time, coverage and diversity in three experimental\nsettings: manual, automatic and semi-automatic annotation. Results show that\nthe hybrid, semi-automatic annotation setting leads to increased frame\ndiversity and similar annotation coverage, when compared to the human-only\nsetting, while the automatic setting performs considerably worse in all\nmetrics, except for annotation time.\n","authors":["Frederico Belcavello","Ely Matos","Arthur Lorenzi","Lisandra Bonoto","Lívia Ruiz","Luiz Fernando Pereira","Victor Herbst","Yulla Navarro","Helen de Andrade Abreu","Lívia Dutra","Tiago Timponi Torrent"],"pdf_url":"https://arxiv.org/pdf/2510.25904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23925v2","updated":"2025-10-29T18:48:20Z","published":"2025-10-27T23:10:06Z","title":"Latent Chain-of-Thought for Visual Reasoning","summary":"  Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.\n","authors":["Guohao Sun","Hang Hua","Jian Wang","Jiebo Luo","Sohail Dianat","Majid Rabbani","Raghuveer Rao","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2510.23925v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.02175v2","updated":"2025-10-29T18:37:02Z","published":"2025-06-02T19:01:53Z","title":"AI Debate Aids Assessment of Controversial Claims","summary":"  As AI grows more powerful, it will increasingly shape how we understand the\nworld. But with this influence comes the risk of amplifying misinformation and\ndeepening social divides-especially on consequential topics where factual\naccuracy directly impacts well-being. Scalable Oversight aims to ensure AI\nsystems remain truthful even when their capabilities exceed those of their\nevaluators. Yet when humans serve as evaluators, their own beliefs and biases\ncan impair judgment. We study whether AI debate can guide biased judges toward\nthe truth by having two AI systems debate opposing sides of controversial\nfactuality claims on COVID-19 and climate change where people hold strong prior\nbeliefs. We conduct two studies. Study I recruits human judges with either\nmainstream or skeptical beliefs who evaluate claims through two protocols:\ndebate (interaction with two AI advisors arguing opposing sides) or consultancy\n(interaction with a single AI advisor). Study II uses AI judges with and\nwithout human-like personas to evaluate the same protocols. In Study I, debate\nconsistently improves human judgment accuracy and confidence calibration,\noutperforming consultancy by 4-10% across COVID-19 and climate change claims.\nThe improvement is most significant for judges with mainstream beliefs (up to\n+15.2% accuracy on COVID-19 claims), though debate also helps skeptical judges\nwho initially misjudge claims move toward accurate views (+4.7% accuracy). In\nStudy II, AI judges with human-like personas achieve even higher accuracy\n(78.5%) than human judges (70.1%) and default AI judges without personas\n(69.8%), suggesting their potential for supervising frontier AI models. These\nfindings highlight AI debate as a promising path toward scalable,\nbias-resilient oversight in contested domains.\n","authors":["Salman Rahman","Sheriff Issaka","Ashima Suvarna","Genglin Liu","James Shiffer","Jaeyoung Lee","Md Rizwan Parvez","Hamid Palangi","Shi Feng","Nanyun Peng","Yejin Choi","Julian Michael","Liwei Jiang","Saadia Gabriel"],"pdf_url":"https://arxiv.org/pdf/2506.02175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25884v1","updated":"2025-10-29T18:32:53Z","published":"2025-10-29T18:32:53Z","title":"Approximating Human Preferences Using a Multi-Judge Learned System","summary":"  Aligning LLM-based judges with human preferences is a significant challenge,\nas they are difficult to calibrate and often suffer from rubric sensitivity,\nbias, and instability. Overcoming this challenge advances key applications,\nsuch as creating reliable reward models for Reinforcement Learning from Human\nFeedback (RLHF) and building effective routing systems that select the\nbest-suited model for a given user query. In this work, we propose a framework\nfor modeling diverse, persona-based preferences by learning to aggregate\noutputs from multiple rubric-conditioned judges. We investigate the performance\nof this approach against naive baselines and assess its robustness through case\nstudies on both human and LLM-judges biases. Our primary contributions include\na persona-based method for synthesizing preference labels at scale and two\ndistinct implementations of our aggregator: Generalized Additive Model (GAM)\nand a Multi-Layer Perceptron (MLP).\n","authors":["Eitán Sprejer","Fernando Avalos","Augusto Bernardi","Jose Pedro Brito de Azevedo Faustino","Jacob Haimes","Narmeen Fatimah Oozeer"],"pdf_url":"https://arxiv.org/pdf/2510.25884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21204v2","updated":"2025-10-29T18:23:49Z","published":"2025-08-28T20:46:13Z","title":"Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive\n  Scaffolding","summary":"  We study how prompt-level inductive biases influence the cognitive behavior\nof large language models (LLMs) in instructional dialogue. We introduce a\nsymbolic scaffolding method paired with a short-term memory schema designed to\npromote adaptive, structured reasoning in Socratic tutoring. Using controlled\nablation across five system variants, we evaluate model outputs via\nexpert-designed rubrics covering scaffolding, responsiveness, symbolic\nreasoning, and conversational memory. We present preliminary results using an\nLLM-based evaluation framework aligned to a cognitively grounded rubric. This\nenables scalable, systematic comparisons across architectural variants in\nearly-stage experimentation. The preliminary results show that our full system\nconsistently outperforms baseline variants. Analysis reveals that removing\nmemory or symbolic structure degrades key cognitive behaviors, including\nabstraction, adaptive probing, and conceptual continuity. These findings\nsupport a processing-level account in which prompt-level cognitive scaffolds\ncan reliably shape emergent instructional strategies in LLMs.\n","authors":["Vanessa Figueiredo"],"pdf_url":"https://arxiv.org/pdf/2508.21204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25860v1","updated":"2025-10-29T18:03:44Z","published":"2025-10-29T18:03:44Z","title":"Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability\n  of LLM Raters","summary":"  Large language models (LLMs) are increasingly used as raters for evaluation\ntasks. However, their reliability is often limited for subjective tasks, when\nhuman judgments involve subtle reasoning beyond annotation labels. Thinking\ntraces, the reasoning behind a judgment, are highly informative but challenging\nto collect and curate. We present a human-LLM collaborative framework to infer\nthinking traces from label-only annotations. The proposed framework uses a\nsimple and effective rejection sampling method to reconstruct these traces at\nscale. These inferred thinking traces are applied to two complementary tasks:\n(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation\nguidelines for proprietary LLM raters. Across multiple datasets, our methods\nlead to significantly improved LLM-human agreement. Additionally, the refined\nannotation guidelines increase agreement among different LLM models. These\nresults suggest that LLMs can serve as practical proxies for otherwise\nunrevealed human thinking traces, enabling label-only corpora to be extended\ninto thinking-trace-augmented resources that enhance the reliability of LLM\nraters.\n","authors":["Xingjian Zhang","Tianhong Gao","Suliang Jin","Tianhao Wang","Teng Ye","Eytan Adar","Qiaozhu Mei"],"pdf_url":"https://arxiv.org/pdf/2510.25860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25771v1","updated":"2025-10-29T17:59:39Z","published":"2025-10-29T17:59:39Z","title":"Gaperon: A Peppered English-French Generative Language Model Suite","summary":"  We release Gaperon, a fully open suite of French-English-coding language\nmodels designed to advance transparency and reproducibility in large-scale\nmodel training. The Gaperon family includes 1.5B, 8B, and 24B parameter models\ntrained on 2-4 trillion tokens, released with all elements of the training\npipeline: French and English datasets filtered with a neural quality\nclassifier, an efficient data curation and training framework, and hundreds of\nintermediate checkpoints. Through this work, we study how data filtering and\ncontamination interact to shape both benchmark and generative performance. We\nfind that filtering for linguistic quality enhances text fluency and coherence\nbut yields subpar benchmark results, and that late deliberate contamination --\ncontinuing training on data mixes that include test sets -- recovers\ncompetitive scores while only reasonably harming generation quality. We discuss\nhow usual neural filtering can unintentionally amplify benchmark leakage. To\nsupport further research, we also introduce harmless data poisoning during\npretraining, providing a realistic testbed for safety studies. By openly\nreleasing all models, datasets, code, and checkpoints, Gaperon establishes a\nreproducible foundation for exploring the trade-offs between data curation,\nevaluation, safety, and openness in multilingual language model development.\n","authors":["Nathan Godey","Wissam Antoun","Rian Touchent","Rachel Bawden","Éric de la Clergerie","Benoît Sagot","Djamé Seddah"],"pdf_url":"https://arxiv.org/pdf/2510.25771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25766v1","updated":"2025-10-29T17:58:59Z","published":"2025-10-29T17:58:59Z","title":"Decomposition-Enhanced Training for Post-Hoc Attributions In Language\n  Models","summary":"  Large language models (LLMs) are increasingly used for long-document question\nanswering, where reliable attribution to sources is critical for trust.\nExisting post-hoc attribution methods work well for extractive QA but struggle\nin multi-hop, abstractive, and semi-extractive settings, where answers\nsynthesize information across passages. To address these challenges, we argue\nthat post-hoc attribution can be reframed as a reasoning problem, where answers\nare decomposed into constituent units, each tied to specific context. We first\nshow that prompting models to generate such decompositions alongside\nattributions improves performance. Building on this, we introduce DecompTune, a\npost-training method that teaches models to produce answer decompositions as\nintermediate reasoning steps. We curate a diverse dataset of complex QA tasks,\nannotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and\n14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.\nAcross extensive experiments and ablations, DecompTune substantially improves\nattribution quality, outperforming prior methods and matching or exceeding\nstate-of-the-art frontier models.\n","authors":["Sriram Balasubramaniam","Samyadeep Basu","Koustava Goswami","Ryan Rossi","Varun Manjunatha","Roshan Santhosh","Ruiyi Zhang","Soheil Feizi","Nedim Lipka"],"pdf_url":"https://arxiv.org/pdf/2510.25766v1.pdf","comment":"Post-hoc attribution"},{"id":"http://arxiv.org/abs/2510.25761v1","updated":"2025-10-29T17:56:17Z","published":"2025-10-29T17:56:17Z","title":"DiagramEval: Evaluating LLM-Generated Diagrams via Graphs","summary":"  Diagrams play a central role in research papers for conveying ideas, yet they\nare often notoriously complex and labor-intensive to create. Although diagrams\nare presented as images, standard image generative models struggle to produce\nclear diagrams with well-defined structure. We argue that a promising direction\nis to generate demonstration diagrams directly in textual form as SVGs, which\ncan leverage recent advances in large language models (LLMs). However, due to\nthe complexity of components and the multimodal nature of diagrams,\nsufficiently discriminative and explainable metrics for evaluating the quality\nof LLM-generated diagrams remain lacking. In this paper, we propose\nDiagramEval, a novel evaluation metric designed to assess demonstration\ndiagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams\nas graphs, treating text elements as nodes and their connections as directed\nedges, and evaluates diagram quality using two new groups of metrics: node\nalignment and path alignment. For the first time, we effectively evaluate\ndiagrams produced by state-of-the-art LLMs on recent research literature,\nquantitatively demonstrating the validity of our metrics. Furthermore, we show\nhow the enhanced explainability of our proposed metrics offers valuable\ninsights into the characteristics of LLM-generated diagrams. Code:\nhttps://github.com/ulab-uiuc/diagram-eval.\n","authors":["Chumeng Liang","Jiaxuan You"],"pdf_url":"https://arxiv.org/pdf/2510.25761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25741v1","updated":"2025-10-29T17:45:42Z","published":"2025-10-29T17:45:42Z","title":"Scaling Latent Reasoning via Looped Language Models","summary":"  Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io.\n","authors":["Rui-Jie Zhu","Zixuan Wang","Kai Hua","Tianyu Zhang","Ziniu Li","Haoran Que","Boyi Wei","Zixin Wen","Fan Yin","He Xing","Lu Li","Jiajun Shi","Kaijing Ma","Shanda Li","Taylor Kergan","Andrew Smith","Xingwei Qu","Mude Hui","Bohong Wu","Qiyang Min","Hongzhi Huang","Xun Zhou","Wei Ye","Jiaheng Liu","Jian Yang","Yunfeng Shi","Chenghua Lin","Enduo Zhao","Tianle Cai","Ge Zhang","Wenhao Huang","Yoshua Bengio","Jason Eshraghian"],"pdf_url":"https://arxiv.org/pdf/2510.25741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25732v1","updated":"2025-10-29T17:37:50Z","published":"2025-10-29T17:37:50Z","title":"The Limits of Obliviate: Evaluating Unlearning in LLMs via\n  Stimulus-Knowledge Entanglement-Behavior Framework","summary":"  Unlearning in large language models (LLMs) is crucial for managing sensitive\ndata and correcting misinformation, yet evaluating its effectiveness remains an\nopen problem. We investigate whether persuasive prompting can recall factual\nknowledge from deliberately unlearned LLMs across models ranging from 2.7B to\n13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from\nACT-R and Hebbian theory (spreading activation theories), as well as\ncommunication principles, we introduce Stimulus-Knowledge Entanglement-Behavior\nFramework (SKeB), which models information entanglement via domain graphs and\ntests whether factual recall in unlearned models is correlated with persuasive\nframing. We develop entanglement metrics to quantify knowledge activation\npatterns and evaluate factuality, non-factuality, and hallucination in outputs.\nOur results show persuasive prompts substantially enhance factual knowledge\nrecall (14.8% baseline vs. 24.5% with authority framing), with effectiveness\ninversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB\nprovides a foundation for assessing unlearning completeness, robustness, and\noverall behavior in LLMs.\n","authors":["Aakriti Shah","Thai Le"],"pdf_url":"https://arxiv.org/pdf/2510.25732v1.pdf","comment":"14 pages, 11 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.26027v1","updated":"2025-10-29T23:50:57Z","published":"2025-10-29T23:50:57Z","title":"Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal\n  Attention in Vision Encoders","summary":"  Despite significant advances in Multimodal Large Language Models (MLLMs),\nunderstanding complex temporal dynamics in videos remains a major challenge.\nOur experiments show that current Video Large Language Model (Video-LLM)\narchitectures have critical limitations in temporal understanding, struggling\nwith tasks that require detailed comprehension of action sequences and temporal\nprogression. In this work, we propose a Video-LLM architecture that introduces\nstacked temporal attention modules directly within the vision encoder. This\ndesign incorporates a temporal attention in vision encoder, enabling the model\nto better capture the progression of actions and the relationships between\nframes before passing visual tokens to the LLM. Our results show that this\napproach significantly improves temporal reasoning and outperforms existing\nmodels in video question answering tasks, specifically in action recognition.\nWe improve on benchmarks including VITATECS, MVBench, and Video-MME by up to\n+5.5%. By enhancing the vision encoder with temporal structure, we address a\ncritical gap in video understanding for Video-LLMs. Project page and code are\navailable at: https://alirasekh.github.io/STAVEQ2/.\n","authors":["Ali Rasekh","Erfan Bagheri Soula","Omid Daliran","Simon Gottschalk","Mohsen Fayyaz"],"pdf_url":"https://arxiv.org/pdf/2510.26027v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26022v1","updated":"2025-10-29T23:30:09Z","published":"2025-10-29T23:30:09Z","title":"Groupwise Registration with Physics-Informed Test-Time Adaptation on\n  Multi-parametric Cardiac MRI","summary":"  Multiparametric mapping MRI has become a viable tool for myocardial tissue\ncharacterization. However, misalignment between multiparametric maps makes\npixel-wise analysis challenging. To address this challenge, we developed a\ngeneralizable physics-informed deep-learning model using test-time adaptation\nto enable group image registration across contrast weighted images acquired\nfrom multiple physical models (e.g., a T1 mapping model and T2 mapping model).\nThe physics-informed adaptation utilized the synthetic images from specific\nphysics model as registration reference, allows for transductive learning for\nvarious tissue contrast. We validated the model in healthy volunteers with\nvarious MRI sequences, demonstrating its improvement for multi-modal\nregistration with a wide range of image contrast variability.\n","authors":["Xinqi Li","Yi Zhang","Li-Ting Huang","Hsiao-Huang Chang","Thoralf Niendorf","Min-Chi Ku","Qian Tao","Hsin-Jung Yang"],"pdf_url":"https://arxiv.org/pdf/2510.26022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26017v1","updated":"2025-10-29T23:23:11Z","published":"2025-10-29T23:23:11Z","title":"Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep\n  Learning","summary":"  Climate change and sea-level rise (SLR) pose escalating threats to coastal\ncities, intensifying the need for efficient and accurate methods to predict\npotential flood hazards. Traditional physics-based hydrodynamic simulators,\nalthough precise, are computationally expensive and impractical for city-scale\ncoastal planning applications. Deep Learning (DL) techniques offer promising\nalternatives, however, they are often constrained by challenges such as data\nscarcity and high-dimensional output requirements. Leveraging a recently\nproposed vision-based, low-resource DL framework, we develop a novel,\nlightweight Convolutional Neural Network (CNN)-based model designed to predict\ncoastal flooding under variable SLR projections and shoreline adaptation\nscenarios. Furthermore, we demonstrate the ability of the model to generalize\nacross diverse geographical contexts by utilizing datasets from two distinct\nregions: Abu Dhabi and San Francisco. Our findings demonstrate that the\nproposed model significantly outperforms state-of-the-art methods, reducing the\nmean absolute error (MAE) in predicted flood depth maps on average by nearly\n20%. These results highlight the potential of our approach to serve as a\nscalable and practical tool for coastal flood management, empowering\ndecision-makers to develop effective mitigation strategies in response to the\ngrowing impacts of climate change. Project Page: https://caspiannet.github.io/\n","authors":["Bilal Hassan","Areg Karapetyan","Aaron Chung Hin Chow","Samer Madanat"],"pdf_url":"https://arxiv.org/pdf/2510.26017v1.pdf","comment":"Submitted to Hydrology and Earth System Sciences"},{"id":"http://arxiv.org/abs/2505.12191v2","updated":"2025-10-29T23:02:31Z","published":"2025-05-18T01:37:58Z","title":"Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised\n  Learning from Data Curriculum","summary":"  Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2.\n","authors":["Wenquan Lu","Jiaqi Zhang","Hugues Van Assel","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2505.12191v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2401.13267v4","updated":"2025-10-29T23:00:50Z","published":"2024-01-24T07:13:06Z","title":"Dynamic Traceback Learning for Medical Report Generation","summary":"  Automated medical report generation has demonstrated the potential to\nsignificantly reduce the workload associated with time-consuming medical\nreporting. Recent generative representation learning methods have shown promise\nin integrating vision and language modalities for medical report generation.\nHowever, when trained end-to-end and applied directly to medical image-to-text\ngeneration, they face two significant challenges: i) difficulty in accurately\ncapturing subtle yet crucial pathological details, and ii) reliance on both\nvisual and textual inputs during inference, leading to performance degradation\nin zero-shot inference when only images are available. To address these\nchallenges, this study proposes a novel multimodal dynamic traceback learning\nframework (DTrace). Specifically, we introduce a traceback mechanism to\nsupervise the semantic validity of generated content and a dynamic learning\nstrategy to adapt to various proportions of image and text input, enabling text\ngeneration without strong reliance on the input from both modalities during\ninference. The learning of cross-modal knowledge is enhanced by supervising the\nmodel to recover masked semantic information from a complementary counterpart.\nExtensive experiments conducted on two benchmark datasets, IU-Xray and\nMIMIC-CXR, demonstrate that the proposed DTrace framework outperforms\nstate-of-the-art methods for medical report generation.\n","authors":["Shuchang Ye","Mingyuan Meng","Mingjian Li","Dagan Feng","Usman Naseem","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2401.13267v4.pdf","comment":"Accepted to IEEE Transactions on Multimedia (TMM)"},{"id":"http://arxiv.org/abs/2505.21996v2","updated":"2025-10-29T22:39:29Z","published":"2025-05-28T05:55:44Z","title":"Learning World Models for Interactive Video Generation","summary":"  Foundational world models must be both interactive and preserve\nspatiotemporal coherence for effective future planning with action choices.\nHowever, present models for long video generation have limited inherent world\nmodeling capabilities due to two main challenges: compounding errors and\ninsufficient memory mechanisms. We enhance image-to-video models with\ninteractive capabilities through additional action conditioning and\nautoregressive framework, and reveal that compounding error is inherently\nirreducible in autoregressive video generation, while insufficient memory\nmechanism leads to incoherence of world models. We propose video retrieval\naugmented generation (VRAG) with explicit global state conditioning, which\nsignificantly reduces long-term compounding errors and increases spatiotemporal\nconsistency of world models. In contrast, naive autoregressive generation with\nextended context windows and retrieval-augmented generation prove less\neffective for video generation, primarily due to the limited in-context\nlearning capabilities of current video models. Our work illuminates the\nfundamental challenges in video world models and establishes a comprehensive\nbenchmark for improving video generation models with internal world modeling\ncapabilities.\n","authors":["Taiye Chen","Xun Hu","Zihan Ding","Chi Jin"],"pdf_url":"https://arxiv.org/pdf/2505.21996v2.pdf","comment":"Project page: https://sites.google.com/view/vrag"},{"id":"http://arxiv.org/abs/2510.26006v1","updated":"2025-10-29T22:34:26Z","published":"2025-10-29T22:34:26Z","title":"CAVE: Detecting and Explaining Commonsense Anomalies in Visual\n  Environments","summary":"  Humans can naturally identify, reason about, and explain anomalies in their\nenvironment. In computer vision, this long-standing challenge remains limited\nto industrial defects or unrealistic, synthetically generated anomalies,\nfailing to capture the richness and unpredictability of real-world anomalies.\nIn this work, we introduce CAVE, the first benchmark of real-world visual\nanomalies. CAVE supports three open-ended tasks: anomaly description,\nexplanation, and justification; with fine-grained annotations for visual\ngrounding and categorizing anomalies based on their visual manifestations,\ntheir complexity, severity, and commonness. These annotations draw inspiration\nfrom cognitive science research on how humans identify and resolve anomalies,\nproviding a comprehensive framework for evaluating Vision-Language Models\n(VLMs) in detecting and understanding anomalies. We show that state-of-the-art\nVLMs struggle with visual anomaly perception and commonsense reasoning, even\nwith advanced prompting strategies. By offering a realistic and cognitively\ngrounded benchmark, CAVE serves as a valuable resource for advancing research\nin anomaly detection and commonsense reasoning in VLMs.\n","authors":["Rishika Bhagwatkar","Syrielle Montariol","Angelika Romanou","Beatriz Borges","Irina Rish","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2510.26006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10501v2","updated":"2025-10-29T22:32:43Z","published":"2024-11-15T11:19:25Z","title":"OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion\n  Models","summary":"  We consider the problem of text-to-video generation tasks with precise\ncontrol for various applications such as camera movement control and\nvideo-to-video editing. Most methods tacking this problem rely on providing\nuser-defined controls, such as binary masks or camera movement embeddings. In\nour approach we propose OnlyFlow, an approach leveraging the optical flow\nfirstly extracted from an input video to condition the motion of generated\nvideos. Using a text prompt and an input video, OnlyFlow allows the user to\ngenerate videos that respect the motion of the input video as well as the text\nprompt. This is implemented through an optical flow estimation model applied on\nthe input video, which is then fed to a trainable optical flow encoder. The\noutput feature maps are then injected into the text-to-video backbone model. We\nperform quantitative, qualitative and user preference studies to show that\nOnlyFlow positively compares to state-of-the-art methods on a wide range of\ntasks, even though OnlyFlow was not specifically trained for such tasks.\nOnlyFlow thus constitutes a versatile, lightweight yet efficient method for\ncontrolling motion in text-to-video generation. Models and code will be made\navailable on GitHub and HuggingFace.\n","authors":["Mathis Koroglu","Hugo Caselles-Dupré","Guillaume Jeanneret Sanmiguel","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2411.10501v2.pdf","comment":"8 pages, 1 supplementary page, 9 figures"},{"id":"http://arxiv.org/abs/2311.07734v2","updated":"2025-10-29T22:32:42Z","published":"2023-11-13T20:36:54Z","title":"Quality-Aware Prototype Memory for Face Representation Learning","summary":"  Prototype Memory is a powerful model for face representation learning. It\nenables training face recognition models on datasets of any size by generating\nprototypes (classifier weights) on the fly and efficiently utilizing them.\nPrototype Memory demonstrated strong results in many face recognition\nbenchmarks. However, the algorithm of prototype generation, used in it, is\nprone to the problems of imperfectly calculated prototypes in case of\nlow-quality or poorly recognizable faces in the images, selected for the\nprototype creation. All images of the same person presented in the mini-batch\nare used with equal weights, and the resulting averaged prototype can be\ncontaminated by imperfect embeddings of low-quality face images. This may lead\nto misleading training signals and degrade the performance of the trained\nmodels. In this paper, we propose a simple and effective way to improve\nPrototype Memory with quality-aware prototype generation. Quality-Aware\nPrototype Memory uses different weights for images of different quality in the\nprocess of prototype generation. With this improvement, prototypes receive more\ninformative signals from high-quality images and are less affected by\nlow-quality ones. We propose and compare several methods of quality estimation\nand usage, perform extensive experiments on the different face recognition\nbenchmarks and demonstrate the advantages of the proposed model compared to the\nbasic version of Prototype Memory.\n","authors":["Evgeny Smirnov","Vasiliy Galyuk","Evgeny Lukyanets"],"pdf_url":"https://arxiv.org/pdf/2311.07734v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2510.26004v1","updated":"2025-10-29T22:32:16Z","published":"2025-10-29T22:32:16Z","title":"DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection\n  System","summary":"  Rapid and reliable incident detection is critical for reducing crash-related\nfatalities, injuries, and congestion. However, conventional methods, such as\nclosed-circuit television, dashcam footage, and sensor-based detection,\nseparate detection from verification, suffer from limited flexibility, and\nrequire dense infrastructure or high penetration rates, restricting\nadaptability and scalability to shifting incident hotspots. To overcome these\nchallenges, we developed DARTS, a drone-based, AI-powered real-time traffic\nincident detection system. DARTS integrates drones' high mobility and aerial\nperspective for adaptive surveillance, thermal imaging for better\nlow-visibility performance and privacy protection, and a lightweight deep\nlearning framework for real-time vehicle trajectory extraction and incident\ndetection. The system achieved 99% detection accuracy on a self-collected\ndataset and supports simultaneous online visual verification, severity\nassessment, and incident-induced congestion propagation monitoring via a\nweb-based interface. In a field test on Interstate 75 in Florida, DARTS\ndetected and verified a rear-end collision 12 minutes earlier than the local\ntransportation management center and monitored incident-induced congestion\npropagation, suggesting potential to support faster emergency response and\nenable proactive traffic control to reduce congestion and secondary crash risk.\nCrucially, DARTS's flexible deployment architecture reduces dependence on\nfrequent physical patrols, indicating potential scalability and\ncost-effectiveness for use in remote areas and resource-constrained settings.\nThis study presents a promising step toward a more flexible and integrated\nreal-time traffic incident detection system, with significant implications for\nthe operational efficiency and responsiveness of modern transportation\nmanagement.\n","authors":["Bai Li","Achilleas Kourtellis","Rong Cao","Joseph Post","Brian Porter","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26004v1.pdf","comment":"Preprint version. This manuscript is currently under review at\n  Transportation Research Part C: Emerging Technologies. The PDF corresponds to\n  the version submitted in June 2025. The main findings of this work were\n  recognized with the Best Intelligent Transportation Systems Paper Award at\n  the 2025 TRB Annual Meeting"},{"id":"http://arxiv.org/abs/2510.26001v1","updated":"2025-10-29T22:25:48Z","published":"2025-10-29T22:25:48Z","title":"Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based\n  Methods in Low-Light Image Enhancement","summary":"  We propose an innovative enhancement to the Mamba framework by increasing the\nHausdorff dimension of its scanning pattern through a novel Hilbert Selective\nScan mechanism. This mechanism explores the feature space more effectively,\ncapturing intricate fine-scale details and improving overall coverage. As a\nresult, it mitigates information inconsistencies while refining spatial\nlocality to better capture subtle local interactions without sacrificing the\nmodel's ability to handle long-range dependencies. Extensive experiments on\npublicly available benchmarks demonstrate that our approach significantly\nimproves both the quantitative metrics and qualitative visual fidelity of\nexisting Mamba-based low-light image enhancement methods, all while reducing\ncomputational resource consumption and shortening inference time. We believe\nthat this refined strategy not only advances the state-of-the-art in low-light\nimage enhancement but also holds promise for broader applications in fields\nthat leverage Mamba-based techniques.\n","authors":["Xinhua Wang","Caibo Feng","Xiangjun Fu","Chunxiao Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06220v2","updated":"2025-10-29T22:25:02Z","published":"2025-06-06T16:28:03Z","title":"GenIR: Generative Visual Feedback for Mental Image Retrieval","summary":"  Vision-language models (VLMs) have shown strong performance on text-to-image\nretrieval benchmarks. However, bridging this success to real-world applications\nremains a challenge. In practice, human search behavior is rarely a one-shot\naction. Instead, it is often a multi-round process guided by clues in mind.\nThat is, a mental image ranging from vague recollections to vivid mental\nrepresentations of the target image. Motivated by this gap, we study the task\nof Mental Image Retrieval (MIR), which targets the realistic yet underexplored\nsetting where users refine their search for a mentally envisioned image through\nmulti-round interactions with an image search engine. Central to successful\ninteractive retrieval is the capability of machines to provide users with\nclear, actionable feedback; however, existing methods rely on indirect or\nabstract verbal feedback, which can be ambiguous, misleading, or ineffective\nfor users to refine the query. To overcome this, we propose GenIR, a generative\nmulti-round retrieval paradigm leveraging diffusion-based image generation to\nexplicitly reify the AI system's understanding at each round. These synthetic\nvisual representations provide clear, interpretable feedback, enabling users to\nrefine their queries intuitively and effectively. We further introduce a fully\nautomated pipeline to generate a high-quality multi-round MIR dataset.\nExperimental results demonstrate that GenIR significantly outperforms existing\ninteractive methods in the MIR scenario. This work establishes a new task with\na dataset and an effective generative retrieval method, providing a foundation\nfor future research in this direction\n","authors":["Diji Yang","Minghao Liu","Chung-Hsiang Lo","Yi Zhang","James Davis"],"pdf_url":"https://arxiv.org/pdf/2506.06220v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25990v1","updated":"2025-10-29T21:57:12Z","published":"2025-10-29T21:57:12Z","title":"Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI","summary":"  In this work, we address the TrackRAD2025 challenge of real-time tumor\ntracking in cine-MRI sequences of the thoracic and abdominal regions under\nstrong data scarcity constraints. Two complementary strategies were explored:\n(i) unsupervised registration with the IMPACT similarity metric and (ii)\nfoundation model-based segmentation leveraging SAM 2.1 and its recent variants\nthrough prompt-based interaction. Due to the one-second runtime constraint, the\nSAM-based method was ultimately selected. The final configuration used SAM2.1\nb+ with mask-based prompts from the first annotated slice, fine-tuned solely on\nthe small labeled subset from TrackRAD2025. Training was configured to minimize\noverfitting, using 1024x1024 patches (batch size 1), standard augmentations,\nand a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was\napplied to all modules (prompt encoder, decoder, Hiera backbone) to preserve\ngeneralization while adapting to annotator-specific styles. Training lasted 300\nepochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently\napplied across all anatomical sites and MRI field strengths. Test-time\naugmentation was considered but ultimately discarded due to negligible\nperformance gains. The final model was selected based on the highest Dice\nSimilarity Coefficient achieved on the validation set after fine-tuning. On the\nhidden test set, the model reached a Dice score of 0.8794, ranking 6th overall\nin the TrackRAD2025 challenge. These results highlight the strong potential of\nfoundation models for accurate and real-time tumor tracking in MRI-guided\nradiotherapy.\n","authors":["Valentin Boussot","Cédric Hémon","Jean-Claude Nunes","Jean-Louis Dillenseger"],"pdf_url":"https://arxiv.org/pdf/2510.25990v1.pdf","comment":"Paper for the Trackrad2025 challenge, Team BreizhTrack"},{"id":"http://arxiv.org/abs/2506.05696v2","updated":"2025-10-29T21:34:31Z","published":"2025-06-06T02:52:13Z","title":"MoralCLIP: Contrastive Alignment of Vision-and-Language Representations\n  with Moral Foundations Theory","summary":"  Recent advances in vision-language models have enabled rich semantic\nunderstanding across modalities. However, these encoding methods lack the\nability to interpret or reason about the moral dimensions of content-a crucial\naspect of human cognition. In this paper, we address this gap by introducing\nMoralCLIP, a novel embedding representation method that extends multimodal\nlearning with explicit moral grounding based on Moral Foundations Theory (MFT).\nOur approach integrates visual and textual moral cues into a unified embedding\nspace, enabling cross-modal moral alignment. MoralCLIP is grounded on the\nmulti-label dataset Social-Moral Image Database to identify co-occurring moral\nfoundations in visual content. For MoralCLIP training, we design a moral data\naugmentation strategy to scale our annotated dataset to 15,000 image-text pairs\nlabeled with MFT-aligned dimensions. Our results demonstrate that explicit\nmoral supervision improves both unimodal and multimodal understanding of moral\ncontent, establishing a foundation for morally-aware AI systems capable of\nrecognizing and aligning with human moral values.\n","authors":["Ana Carolina Condez","Diogo Tavares","João Magalhães"],"pdf_url":"https://arxiv.org/pdf/2506.05696v2.pdf","comment":"Updated version: corresponds to the ACM MM '25 published paper and\n  includes full appendix material"},{"id":"http://arxiv.org/abs/2510.25976v1","updated":"2025-10-29T21:21:54Z","published":"2025-10-29T21:21:54Z","title":"Brain-IT: Image Reconstruction from fMRI via Brain-Interaction\n  Transformer","summary":"  Reconstructing images seen by people from their fMRI brain recordings\nprovides a non-invasive window into the human brain. Despite recent progress\nenabled by diffusion models, current methods often lack faithfulness to the\nactual seen images. We present \"Brain-IT\", a brain-inspired approach that\naddresses this challenge through a Brain Interaction Transformer (BIT),\nallowing effective interactions between clusters of functionally-similar\nbrain-voxels. These functional-clusters are shared by all subjects, serving as\nbuilding blocks for integrating information both within and across brains. All\nmodel components are shared by all clusters & subjects, allowing efficient\ntraining with a limited amount of data. To guide the image reconstruction, BIT\npredicts two complementary localized patch-level image features: (i)high-level\nsemantic features which steer the diffusion model toward the correct semantic\ncontent of the image; and (ii)low-level structural features which help to\ninitialize the diffusion process with the correct coarse layout of the image.\nBIT's design enables direct flow of information from brain-voxel clusters to\nlocalized image features. Through these principles, our method achieves image\nreconstructions from fMRI that faithfully reconstruct the seen images, and\nsurpass current SotA approaches both visually and by standard objective\nmetrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve\nresults comparable to current methods trained on full 40-hour recordings.\n","authors":["Roman Beliy","Amit Zalcher","Jonathan Kogman","Navve Wasserman","Michal Irani"],"pdf_url":"https://arxiv.org/pdf/2510.25976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25970v1","updated":"2025-10-29T21:12:58Z","published":"2025-10-29T21:12:58Z","title":"SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing","summary":"  Rectified flow models have become a de facto standard in image generation due\nto their stable sampling trajectories and high-fidelity outputs. Despite their\nstrong generative capabilities, they face critical limitations in image editing\ntasks: inaccurate inversion processes for mapping real images back into the\nlatent space, and gradient entanglement issues during editing often result in\noutputs that do not faithfully reflect the target prompt. Recent efforts have\nattempted to directly map source and target distributions via ODE-based\napproaches without inversion; however,these methods still yield suboptimal\nediting quality. In this work, we propose a flow decomposition-and-aggregation\nframework built upon an inversion-free formulation to address these\nlimitations. Specifically, we semantically decompose the target prompt into\nmultiple sub-prompts, compute an independent flow for each, and aggregate them\nto form a unified editing trajectory. While we empirically observe that\ndecomposing the original flow enhances diversity in the target space,\ngenerating semantically aligned outputs still requires consistent guidance\ntoward the full target prompt. To this end, we design a projection and\nsoft-aggregation mechanism for flow, inspired by gradient conflict resolution\nin multi-task learning. This approach adaptively weights the sub-target\nvelocity fields, suppressing semantic redundancy while emphasizing distinct\ndirections, thereby preserving both diversity and consistency in the final\nedited output. Experimental results demonstrate that our method outperforms\nexisting zero-shot editing approaches in terms of semantic fidelity and\nattribute disentanglement. The code is available at\nhttps://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.\n","authors":["Sung-Hoon Yoon","Minghan Li","Gaspard Beaudouin","Congcong Wen","Muhammad Rafay Azhar","Mengyu Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25970v1.pdf","comment":"Camera-ready version for NeurIPS 2025, 10 pages (main paper)"},{"id":"http://arxiv.org/abs/2503.04852v3","updated":"2025-10-29T20:44:13Z","published":"2025-03-06T03:40:01Z","title":"CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data","summary":"  True intelligence hinges on the ability to uncover and leverage hidden causal\nrelations. Despite significant progress in AI and computer vision (CV), there\nremains a lack of benchmarks for assessing models' abilities to infer latent\ncausality from complex visual data. In this paper, we introduce\n\\textsc{\\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates\nstructured data (tables) with corresponding visual representations (images) to\nevaluate causal reasoning. Designed within a systematic framework, Causal3D\ncomprises 19 3D-scene datasets capturing diverse causal relations, views, and\nbackgrounds, enabling evaluations across scenes of varying complexity. We\nassess multiple state-of-the-art methods, including classical causal discovery,\ncausal representation learning, and large/vision-language models (LLMs/VLMs).\nOur experiments show that as causal structures grow more complex without prior\nknowledge, performance declines significantly, highlighting the challenges even\nadvanced methods face in complex causal scenarios. Causal3D serves as a vital\nresource for advancing causal reasoning in CV and fostering trustworthy AI in\ncritical domains.\n","authors":["Disheng Liu","Yiran Qiao","Wuche Liu","Yiren Lu","Yunlai Zhou","Tuo Liang","Yu Yin","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04852v3.pdf","comment":"Datasets link:\n  https://huggingface.co/datasets/LLDDSS/Causal3D_Dataset"},{"id":"http://arxiv.org/abs/2505.23158v2","updated":"2025-10-29T19:53:57Z","published":"2025-05-29T06:50:57Z","title":"LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient\n  Rendering","summary":"  In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian\nSplatting that enables real-time rendering of large-scale scenes on\nmemory-constrained devices. Our approach introduces a hierarchical LOD\nrepresentation that iteratively selects optimal subsets of Gaussians based on\ncamera distance, thus largely reducing both rendering time and GPU memory\nusage. We construct each LOD level by applying a depth-aware 3D smoothing\nfilter, followed by importance-based pruning and fine-tuning to maintain visual\nfidelity. To further reduce memory overhead, we partition the scene into\nspatial chunks and dynamically load only relevant Gaussians during rendering,\nemploying an opacity-blending mechanism to avoid visual artifacts at chunk\nboundaries. Our method achieves state-of-the-art performance on both outdoor\n(Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality\nrenderings with reduced latency and memory requirements.\n","authors":["Jonas Kulhanek","Marie-Julie Rakotosaona","Fabian Manhardt","Christina Tsalicoglou","Michael Niemeyer","Torsten Sattler","Songyou Peng","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2505.23158v2.pdf","comment":"NeurIPS 2025; Web: https://lodge-gs.github.io/"},{"id":"http://arxiv.org/abs/2510.25921v1","updated":"2025-10-29T19:50:34Z","published":"2025-10-29T19:50:34Z","title":"Generative Image Restoration and Super-Resolution using Physics-Informed\n  Synthetic Data for Scanning Tunneling Microscopy","summary":"  Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and\natom manipulation, but its utility is often limited by tip degradation and slow\nserial data acquisition. Fabrication adds another layer of complexity since the\ntip is often subjected to large voltages, which may alter the shape of its\napex, requiring it to be conditioned. Here, we propose a machine learning (ML)\napproach for image repair and super-resolution to alleviate both challenges.\nUsing a dataset of only 36 pristine experimental images of Si(001):H, we\ndemonstrate that a physics-informed synthetic data generation pipeline can be\nused to train several state-of-the-art flow-matching and diffusion models.\nQuantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy\n(CMMD) score and structural similarity demonstrates that our models are able to\neffectively restore images and offer a two- to fourfold reduction in image\nacquisition time by accurately reconstructing images from sparsely sampled\ndata. Our framework has the potential to significantly increase STM\nexperimental throughput by offering a route to reducing the frequency of\ntip-conditioning procedures and to enhancing frame rates in existing high-speed\nSTM systems.\n","authors":["Nikola L. Kolev","Tommaso Rodani","Neil J. Curson","Taylor J. Z. Stock","Alberto Cazzaniga"],"pdf_url":"https://arxiv.org/pdf/2510.25921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17345v2","updated":"2025-10-29T19:47:47Z","published":"2024-06-25T07:58:47Z","title":"NerfBaselines: Consistent and Reproducible Evaluation of Novel View\n  Synthesis Methods","summary":"  Novel view synthesis is an important problem with many applications,\nincluding AR/VR, gaming, and robotic simulations. With the recent rapid\ndevelopment of Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS)\nmethods, it is becoming difficult to keep track of the current state of the art\n(SoTA) due to methods using different evaluation protocols, codebases being\ndifficult to install and use, and methods not generalizing well to novel 3D\nscenes. In our experiments, we show that even tiny differences in the\nevaluation protocols of various methods can artificially boost the performance\nof these methods. This raises questions about the validity of quantitative\ncomparisons performed in the literature. To address these questions, we propose\nNerfBaselines, an evaluation framework which provides consistent benchmarking\ntools, ensures reproducibility, and simplifies the installation and use of\nvarious methods. We validate our implementation experimentally by reproducing\nthe numbers reported in the original papers. For improved accessibility, we\nrelease a web platform that compares commonly used methods on standard\nbenchmarks. We strongly believe NerfBaselines is a valuable contribution to the\ncommunity as it ensures that quantitative results are comparable and thus truly\nmeasure progress in the field of novel view synthesis.\n","authors":["Jonas Kulhanek","Torsten Sattler"],"pdf_url":"https://arxiv.org/pdf/2406.17345v2.pdf","comment":"NeurIPS 2025 D&B; Web: https://jkulhanek.com/nerfbaselines"},{"id":"http://arxiv.org/abs/2510.24134v2","updated":"2025-10-29T19:17:39Z","published":"2025-10-28T07:19:01Z","title":"VC4VG: Optimizing Video Captions for Text-to-Video Generation","summary":"  Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels. We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements. Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/alimama-creative/VC4VG to\nsupport further research.\n","authors":["Yang Du","Zhuoran Lin","Kaiqiang Song","Biao Wang","Zhicheng Zheng","Tiezheng Ge","Bo Zheng","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2510.24134v2.pdf","comment":"Accepted by EMNLP 2025"},{"id":"http://arxiv.org/abs/2508.08186v2","updated":"2025-10-29T19:12:08Z","published":"2025-08-11T17:06:55Z","title":"KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold\n  Representation Learning","summary":"  Semantic segmentation of structural defects in civil infrastructure remains\nchallenging due to variable defect appearances, harsh imaging conditions, and\nsignificant class imbalance. Current deep learning methods, despite their\neffectiveness, typically require millions of parameters, rendering them\nimpractical for real-time inspection systems. We introduce KARMA\n(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient\nsemantic segmentation framework that models complex defect patterns through\ncompositions of one-dimensional functions rather than conventional\nconvolutions. KARMA features three technical innovations: (1) a\nparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging\nlow-rank factorization for KAN-based feature transformation; (2) an optimized\nfeature pyramid structure with separable convolutions for multi-scale defect\nanalysis; and (3) a static-dynamic prototype mechanism that enhances feature\nrepresentation for imbalanced classes. Extensive experiments on benchmark\ninfrastructure inspection datasets demonstrate that KARMA achieves competitive\nor superior mean IoU performance compared to state-of-the-art approaches, while\nusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).\nOperating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for\nreal-time deployment, enabling practical automated infrastructure inspection\nsystems without compromising accuracy. The source code can be accessed at the\nfollowing URL: https://github.com/faeyelab/karma.\n","authors":["Md Meftahul Ferdaus","Mahdi Abdelguerfi","Elias Ioup","Steven Sloan","Kendall N. Niles","Ken Pathak"],"pdf_url":"https://arxiv.org/pdf/2508.08186v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2510.25901v1","updated":"2025-10-29T19:07:39Z","published":"2025-10-29T19:07:39Z","title":"BikeScenes: Online LiDAR Semantic Segmentation for Bicycles","summary":"  The vulnerability of cyclists, exacerbated by the rising popularity of faster\ne-bikes, motivates adapting automotive perception technologies for bicycle\nsafety. We use our multi-sensor 'SenseBike' research platform to develop and\nevaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the\nautomotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg\nDataset, comprising 3021 consecutive LiDAR scans around the university campus\nof the TU Delft, semantically annotated for 29 dynamic and static classes. By\nevaluating model performance, we demonstrate that fine-tuning on our BikeScenes\ndataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly\noutperforming the 13.8% obtained with SemanticKITTI pre-training alone. This\nresult underscores the necessity and effectiveness of domain-specific training.\nWe highlight key challenges specific to bicycle-mounted, hardware-constrained\nperception systems and contribute the BikeScenes dataset as a resource for\nadvancing research in cyclist-centric LiDAR segmentation.\n","authors":["Denniz Goren","Holger Caesar"],"pdf_url":"https://arxiv.org/pdf/2510.25901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25897v1","updated":"2025-10-29T18:59:17Z","published":"2025-10-29T18:59:17Z","title":"MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and\n  efficiency","summary":"  Current text-to-image generative models are trained on large uncurated\ndatasets to enable diverse generation capabilities. However, this does not\nalign well with user preferences. Recently, reward models have been\nspecifically designed to perform post-hoc selection of generated images and\nalign them to a reward, typically user preference. This discarding of\ninformative data together with the optimizing for a single reward tend to harm\ndiversity, semantic fidelity and efficiency. Instead of this post-processing,\nwe propose to condition the model on multiple reward models during training to\nlet the model learn user preferences directly. We show that this not only\ndramatically improves the visual quality of the generated images but it also\nsignificantly speeds up the training. Our proposed method, called MIRO,\nachieves state-of-the-art performances on the GenEval compositional benchmark\nand user-preference scores (PickAScore, ImageReward, HPSv2).\n","authors":["Nicolas Dufour","Lucas Degeorge","Arijit Ghosh","Vicky Kalogeiton","David Picard"],"pdf_url":"https://arxiv.org/pdf/2510.25897v1.pdf","comment":"Project page: https://nicolas-dufour.github.io/miro"},{"id":"http://arxiv.org/abs/2509.16336v2","updated":"2025-10-29T18:17:28Z","published":"2025-09-19T18:24:41Z","title":"Neural Atlas Graphs for Dynamic Scene Decomposition and Editing","summary":"  Learning editable high-resolution scene representations for dynamic scenes is\nan open problem with applications across the domains from autonomous driving to\ncreative editing - the most successful approaches today make a trade-off\nbetween editability and supporting scene complexity: neural atlases represent\ndynamic scenes as two deforming image layers, foreground and background, which\nare editable in 2D, but break down when multiple objects occlude and interact.\nIn contrast, scene graph models make use of annotated data such as masks and\nbounding boxes from autonomous-driving datasets to capture complex 3D spatial\nrelationships, but their implicit volumetric node representations are\nchallenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a\nhybrid high-resolution scene representation, where every graph node is a\nview-dependent neural atlas, facilitating both 2D appearance editing and 3D\nordering and positioning of scene elements. Fit at test-time, NAGs achieve\nstate-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR\nincrease compared to existing methods - and make environmental editing possible\nin high resolution and visual quality - creating counterfactual driving\nscenarios with new backgrounds and edited vehicle appearance. We find that the\nmethod also generalizes beyond driving scenes and compares favorably - by more\nthan 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS\nvideo dataset with a diverse set of human and animal-centric scenes.\n  Project Page: https://princeton-computational-imaging.github.io/nag/\n","authors":["Jan Philipp Schneider","Pratik Singh Bisht","Ilya Chugunov","Andreas Kolb","Michael Moeller","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2509.16336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25772v1","updated":"2025-10-29T17:59:53Z","published":"2025-10-29T17:59:53Z","title":"VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\n  Learning","summary":"  Visual effects (VFX) are crucial to the expressive power of digital media,\nyet their creation remains a major challenge for generative AI. Prevailing\nmethods often rely on the one-LoRA-per-effect paradigm, which is\nresource-intensive and fundamentally incapable of generalizing to unseen\neffects, thus limiting scalability and creation. To address this challenge, we\nintroduce VFXMaster, the first unified, reference-based framework for VFX video\ngeneration. It recasts effect generation as an in-context learning task,\nenabling it to reproduce diverse dynamic effects from a reference video onto\ntarget content. In addition, it demonstrates remarkable generalization to\nunseen effect categories. Specifically, we design an in-context conditioning\nstrategy that prompts the model with a reference example. An in-context\nattention mask is designed to precisely decouple and inject the essential\neffect attributes, allowing a single unified model to master the effect\nimitation without information leakage. In addition, we propose an efficient\none-shot effect adaptation mechanism to boost generalization capability on\ntough unseen effects from a single user-provided video rapidly. Extensive\nexperiments demonstrate that our method effectively imitates various categories\nof effect information and exhibits outstanding generalization to out-of-domain\neffects. To foster future research, we will release our code, models, and a\ncomprehensive dataset to the community.\n","authors":["Baolu Li","Yiming Zhang","Qinghe Wang","Liqian Ma","Xiaoyu Shi","Xintao Wang","Pengfei Wan","Zhenfei Yin","Yunzhi Zhuge","Huchuan Lu","Xu Jia"],"pdf_url":"https://arxiv.org/pdf/2510.25772v1.pdf","comment":"Project Page URL:https://libaolu312.github.io/VFXMaster/"},{"id":"http://arxiv.org/abs/2510.25765v1","updated":"2025-10-29T17:58:14Z","published":"2025-10-29T17:58:14Z","title":"FreeArt3D: Training-Free Articulated Object Generation using 3D\n  Diffusion","summary":"  Articulated 3D objects are central to many applications in robotics, AR/VR,\nand animation. Recent approaches to modeling such objects either rely on\noptimization-based reconstruction pipelines that require dense-view supervision\nor on feed-forward generative models that produce coarse geometric\napproximations and often overlook surface texture. In contrast, open-world 3D\ngeneration of static objects has achieved remarkable success, especially with\nthe advent of native 3D diffusion models such as Trellis. However, extending\nthese methods to articulated objects by training native 3D diffusion models\nposes significant challenges. In this work, we present FreeArt3D, a\ntraining-free framework for articulated 3D object generation. Instead of\ntraining a new model on limited articulated data, FreeArt3D repurposes a\npre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape\nprior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by\ntreating articulation as an additional generative dimension. Given a few images\ncaptured in different articulation states, FreeArt3D jointly optimizes the\nobject's geometry, texture, and articulation parameters without requiring\ntask-specific training or access to large-scale articulated datasets. Our\nmethod generates high-fidelity geometry and textures, accurately predicts\nunderlying kinematic structures, and generalizes well across diverse object\ncategories. Despite following a per-instance optimization paradigm, FreeArt3D\ncompletes in minutes and significantly outperforms prior state-of-the-art\napproaches in both quality and versatility.\n","authors":["Chuhao Chen","Isabella Liu","Xinyue Wei","Hao Su","Minghua Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25760v1","updated":"2025-10-29T17:55:43Z","published":"2025-10-29T17:55:43Z","title":"Multimodal Spatial Reasoning in the Large Model Era: A Survey and\n  Benchmarks","summary":"  Humans possess spatial reasoning abilities that enable them to understand\nspaces through multimodal observations, such as vision and sound. Large\nmultimodal reasoning models extend these abilities by learning to perceive and\nreason, showing promising performance across diverse spatial tasks. However,\nsystematic reviews and publicly available benchmarks for these models remain\nlimited. In this survey, we provide a comprehensive review of multimodal\nspatial reasoning tasks with large models, categorizing recent progress in\nmultimodal large language models (MLLMs) and introducing open benchmarks for\nevaluation. We begin by outlining general spatial reasoning, focusing on\npost-training techniques, explainability, and architecture. Beyond classical 2D\ntasks, we examine spatial relationship reasoning, scene and layout\nunderstanding, as well as visual question answering and grounding in 3D space.\nWe also review advances in embodied AI, including vision-language navigation\nand action models. Additionally, we consider emerging modalities such as audio\nand egocentric video, which contribute to novel spatial understanding through\nnew sensors. We believe this survey establishes a solid foundation and offers\ninsights into the growing field of multimodal spatial reasoning. Updated\ninformation about this survey, codes and implementation of the open benchmarks\ncan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.\n","authors":["Xu Zheng","Zihao Dongfang","Lutao Jiang","Boyuan Zheng","Yulong Guo","Zhenquan Zhang","Giuliano Albanese","Runyi Yang","Mengjiao Ma","Zixin Zhang","Chenfei Liao","Dingcheng Zhen","Yuanhuiyi Lyu","Yuqian Fu","Bin Ren","Linfeng Zhang","Danda Pani Paudel","Nicu Sebe","Luc Van Gool","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2510.25760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25739v1","updated":"2025-10-29T17:43:31Z","published":"2025-10-29T17:43:31Z","title":"Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image\n  Generation","summary":"  Autoregressive (AR) image generation models are capable of producing\nhigh-fidelity images but often suffer from slow inference due to their\ninherently sequential, token-by-token decoding process. Speculative decoding,\nwhich employs a lightweight draft model to approximate the output of a larger\nAR model, has shown promise in accelerating text generation without\ncompromising quality. However, its application to image generation remains\nlargely underexplored. The challenges stem from a significantly larger sampling\nspace, which complicates the alignment between the draft and target model\noutputs, coupled with the inadequate use of the two-dimensional spatial\nstructure inherent in images, thereby limiting the modeling of local\ndependencies. To overcome these challenges, we introduce Hawk, a new approach\nthat harnesses the spatial structure of images to guide the speculative model\ntoward more accurate and efficient predictions. Experimental results on\nmultiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR\nmodels, while preserving both image fidelity and diversity.\n","authors":["Zhi-Kai Chen","Jun-Peng Jiang","Han-Jia Ye","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2510.25739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.20600v2","updated":"2025-10-29T17:35:15Z","published":"2025-08-28T09:43:59Z","title":"GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac\n  MRI Reconstruction","summary":"  Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction\nremains a critical challenge due to the trade-off between scan time and image\nquality, particularly when generalizing across diverse acquisition settings. We\npropose GENRE-CMR, a generative adversarial network (GAN)-based architecture\nemploying a residual deep unrolled reconstruction framework to enhance\nreconstruction fidelity and generalization. The architecture unrolls iterative\noptimization into a cascade of convolutional subnetworks, enriched with\nresidual connections to enable progressive feature propagation from shallow to\ndeeper stages. To further improve performance, we integrate two loss functions:\n(1) an Edge-Aware Region (EAR) loss, which guides the network to focus on\nstructurally informative regions and helps prevent common reconstruction\nblurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which\nregularizes the feature space across diverse data distributions via a symmetric\nKL divergence formulation. Extensive experiments confirm that GENRE-CMR\nsurpasses state-of-the-art methods on training and unseen data, achieving\n0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various\nacceleration factors and sampling trajectories. Ablation studies confirm the\ncontribution of each proposed component to reconstruction quality and\ngeneralization. Our framework presents a unified and robust solution for\nhigh-quality CMR reconstruction, paving the way for clinically adaptable\ndeployment across heterogeneous acquisition protocols.\n","authors":["Kian Anvari Hamedani","Narges Razizadeh","Shahabedin Nabavi","Mohsen Ebrahimi Moghaddam"],"pdf_url":"https://arxiv.org/pdf/2508.20600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.24096v2","updated":"2025-10-29T16:13:52Z","published":"2025-06-30T17:48:54Z","title":"MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient\n  Surface Reconstruction","summary":"  While recent advances in Gaussian Splatting have enabled fast reconstruction\nof high-quality 3D scenes from images, extracting accurate surface meshes\nremains a challenge. Current approaches extract the surface through costly\npost-processing steps, resulting in the loss of fine geometric details or\nrequiring significant time and leading to very dense meshes with millions of\nvertices. More fundamentally, the a posteriori conversion from a volumetric to\na surface representation limits the ability of the final mesh to preserve all\ngeometric structures captured during training. We present MILo, a novel\nGaussian Splatting framework that bridges the gap between volumetric and\nsurface representations by differentiably extracting a mesh from the 3D\nGaussians. We design a fully differentiable procedure that constructs the\nmesh-including both vertex locations and connectivity-at every iteration\ndirectly from the parameters of the Gaussians, which are the only quantities\noptimized during training. Our method introduces three key technical\ncontributions: a bidirectional consistency framework ensuring both\nrepresentations-Gaussians and the extracted mesh-capture the same underlying\ngeometry during training; an adaptive mesh extraction process performed at each\ntraining iteration, which uses Gaussians as differentiable pivots for Delaunay\ntriangulation; a novel method for computing signed distance values from the 3D\nGaussians that enables precise surface extraction while avoiding geometric\nerosion. Our approach can reconstruct complete scenes, including backgrounds,\nwith state-of-the-art quality while requiring an order of magnitude fewer mesh\nvertices than previous methods. Due to their light weight and empty interior,\nour meshes are well suited for downstream applications such as physics\nsimulations or animation.\n","authors":["Antoine Guédon","Diego Gomez","Nissim Maruani","Bingchen Gong","George Drettakis","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2506.24096v2.pdf","comment":"10 pages. A presentation video of our approach is available at\n  https://youtu.be/_SGNhhNz0fE"},{"id":"http://arxiv.org/abs/2506.07464v3","updated":"2025-10-29T15:59:41Z","published":"2025-06-09T06:15:54Z","title":"DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO","summary":"  Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training for enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success using a PPO-style reinforcement algorithm\nwith group-normalized rewards. However, the effectiveness of GRPO in Video\nLarge Language Models (VideoLLMs) has still been less studyed. In this paper,\nwe explore GRPO and identify two problems that deteriorate the effective\nlearning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate\nthese challenges, we propose DeepVideo-R1, a video large language model trained\nwith Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.\nReg-GRPO reformulates the GRPO loss function into a regression task that\ndirectly predicts the advantage in GRPO, eliminating the need for safeguards\nsuch as the clipping and min functions. It directly aligns the model with\nadvantages, providing guidance to prefer better ones. The difficulty-aware data\naugmentation strategy augments input prompts/videos to locate the difficulty of\nsamples at solvable difficulty levels, enabling diverse reward signals. Our\nexperimental results show that our approach significantly improves video\nreasoning performance across multiple benchmarks.\n","authors":["Jinyoung Park","Jeehye Na","Jinyoung Kim","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2506.07464v3.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2309.13672v8","updated":"2025-10-29T15:35:18Z","published":"2023-09-24T15:40:40Z","title":"RL-I2IT: Image-to-Image Translation with Deep Reinforcement Learning","summary":"  Most existing Image-to-Image Translation (I2IT) methods generate images in a\nsingle run of a deep learning (DL) model. However, designing such a single-step\nmodel is always challenging, requiring a huge number of parameters and easily\nfalling into bad global minimums and overfitting. In this work, we reformulate\nI2IT as a step-wise decision-making problem via deep reinforcement learning\n(DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The\nkey feature in the RL-I2IT framework is to decompose a monolithic learning\nprocess into small steps with a lightweight model to progressively transform a\nsource image successively to a target image. Considering that it is challenging\nto handle high dimensional continuous state and action spaces in the\nconventional RL framework, we introduce meta policy with a new concept Plan to\nthe standard Actor-Critic model, which is of a lower dimension than the\noriginal image and can facilitate the actor to generate a tractable high\ndimensional action. In the RL-I2IT framework, we also employ a task-specific\nauxiliary learning strategy to stabilize the training process and improve the\nperformance of the corresponding task. Experiments on several I2IT tasks\ndemonstrate the effectiveness and robustness of the proposed method when facing\nhigh-dimensional continuous action space problems. Our implementation of the\nRL-I2IT framework is available at\nhttps://github.com/Algolzw/SPAC-Deformable-Registration.\n","authors":["Jing Hu","Chengming Feng","Shu Hu","Ming-Ching Chang","Xin Li","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2309.13672v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23118v3","updated":"2025-10-29T15:24:05Z","published":"2025-10-27T08:38:52Z","title":"Quantizing Space and Time: Fusing Time Series and Images for Earth\n  Observation","summary":"  We propose a task-agnostic framework for multimodal fusion of time series and\nsingle timestamp images, enabling cross-modal generation and robust downstream\nperformance. Our approach explores deterministic and learned strategies for\ntime series quantization and then leverages a masked correlation learning\nobjective, aligning discrete image and time series tokens in a unified\nrepresentation space. Instantiated in the Earth observation domain, the\npretrained model generates consistent global temperature profiles from\nsatellite imagery and is validated through counterfactual experiments. Across\ndownstream tasks, our task-agnostic pretraining outperforms task-specific\nfusion by 6% in R^2 and 2% in RMSE on average, and exceeds baseline methods by\n50% in R^2 and 12% in RMSE. Finally, we analyze gradient sensitivity across\nmodalities, providing insights into model robustness. Code, data, and weights\nwill be released under a permissive license.\n","authors":["Gianfranco Basile","Johannes Jakubik","Benedikt Blumenstiel","Thomas Brunschwiler","Juan Bernabe Moreno"],"pdf_url":"https://arxiv.org/pdf/2510.23118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25594v1","updated":"2025-10-29T15:03:46Z","published":"2025-10-29T15:03:46Z","title":"Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for\n  Local Learning","summary":"  Training deep neural networks (DNNs) with backpropagation (BP) achieves\nstate-of-the-art accuracy but requires global error propagation and full\nparameterization, leading to substantial memory and computational overhead.\nDirect Feedback Alignment (DFA) enables local, parallelizable updates with\nlower memory requirements but is limited by unstructured feedback and poor\nscalability in deeper architectures, specially convolutional neural networks.\nTo address these limitations, we propose a structured local learning framework\nthat operates directly on low-rank manifolds defined by the Singular Value\nDecomposition (SVD) of weight matrices. Each layer is trained in its decomposed\nform, with updates applied to the SVD components using a composite loss that\nintegrates cross-entropy, subspace alignment, and orthogonality regularization.\nFeedback matrices are constructed to match the SVD structure, ensuring\nconsistent alignment between forward and feedback pathways. Our method reduces\nthe number of trainable parameters relative to the original DFA model, without\nrelying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,\nand ImageNet show that our method achieves accuracy comparable to that of BP.\nAblation studies confirm the importance of each loss term in the low-rank\nsetting. These results establish local learning on low-rank manifolds as a\nprincipled and scalable alternative to full-rank gradient-based training.\n","authors":["Arani Roy","Marco P. Apolinario","Shristi Das Biswas","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2510.25594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25590v1","updated":"2025-10-29T14:58:37Z","published":"2025-10-29T14:58:37Z","title":"RegionE: Adaptive Region-Aware Generation for Efficient Image Editing","summary":"  Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.\n","authors":["Pengtao Chen","Xianfang Zeng","Maosen Zhao","Mingzhu Shen","Peng Ye","Bangyin Xiang","Zhibo Wang","Wei Cheng","Gang Yu","Tao Chen"],"pdf_url":"https://arxiv.org/pdf/2510.25590v1.pdf","comment":"26 pages, 10 figures, 18 tables"},{"id":"http://arxiv.org/abs/2506.21710v2","updated":"2025-10-29T14:46:17Z","published":"2025-06-26T18:51:04Z","title":"FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering","summary":"  While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.\n","authors":["Liangyu Zhong","Fabio Rosenthal","Joachim Sicking","Fabian Hüger","Thorsten Bagdonat","Hanno Gottschalk","Leo Schwinn"],"pdf_url":"https://arxiv.org/pdf/2506.21710v2.pdf","comment":"Accepted by NeurIPS 2025 - main track. Project page:\n  https://focus-mllm-vqa.github.io/"},{"id":"http://arxiv.org/abs/2510.25522v1","updated":"2025-10-29T13:46:19Z","published":"2025-10-29T13:46:19Z","title":"Comparative Study of UNet-based Architectures for Liver Tumor\n  Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography","summary":"  Segmentation of liver structures in multi-phase contrast-enhanced computed\ntomography (CECT) plays a crucial role in computer-aided diagnosis and\ntreatment planning for liver diseases, including tumor detection. In this\nstudy, we investigate the performance of UNet-based architectures for liver\ntumor segmentation, starting from the original UNet and extending to UNet3+\nwith various backbone networks. We evaluate ResNet, Transformer-based, and\nState-space (Mamba) backbones, all initialized with pretrained weights.\nSurprisingly, despite the advances in modern architecture, ResNet-based models\nconsistently outperform Transformer- and Mamba-based alternatives across\nmultiple evaluation metrics. To further improve segmentation quality, we\nintroduce attention mechanisms into the backbone and observe that incorporating\nthe Convolutional Block Attention Module (CBAM) yields the best performance.\nResNetUNet3+ with CBAM module not only produced the best overlap metrics with a\nDice score of 0.755 and IoU of 0.662, but also achieved the most precise\nboundary delineation, evidenced by the lowest HD95 distance of 77.911. The\nmodel's superiority was further cemented by its leading overall accuracy of\n0.925 and specificity of 0.926, showcasing its robust capability in accurately\nidentifying both lesion and healthy tissue. To further enhance\ninterpretability, Grad-CAM visualizations were employed to highlight the\nregion's most influential predictions, providing insights into its\ndecision-making process. These findings demonstrate that classical ResNet\narchitecture, when combined with modern attention modules, remain highly\ncompetitive for medical image segmentation tasks, offering a promising\ndirection for liver tumor detection in clinical practice.\n","authors":["Doan-Van-Anh Ly","Thi-Thu-Hien Pham","Thanh-Hai Le"],"pdf_url":"https://arxiv.org/pdf/2510.25522v1.pdf","comment":"27 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.23763v2","updated":"2025-10-29T13:37:19Z","published":"2025-10-27T18:49:03Z","title":"RoboOmni: Proactive Robot Manipulation in Omni-modal Context","summary":"  Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.\n","authors":["Siyin Wang","Jinlan Fu","Feihong Liu","Xinzhe He","Huangxuan Wu","Junhao Shi","Kexin Huang","Zhaoye Fei","Jingjing Gong","Zuxuan Wu","Yugang Jiang","See-Kiong Ng","Tat-Seng Chua","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2510.23763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25512v1","updated":"2025-10-29T13:35:46Z","published":"2025-10-29T13:35:46Z","title":"FaCT: Faithful Concept Traces for Explaining Neural Network Decisions","summary":"  Deep networks have shown remarkable performance across a wide range of tasks,\nyet getting a global concept-level understanding of how they function remains a\nkey challenge. Many post-hoc concept-based approaches have been introduced to\nunderstand their workings, yet they are not always faithful to the model.\nFurther, they make restrictive assumptions on the concepts a model learns, such\nas class-specificity, small spatial extent, or alignment to human expectations.\nIn this work, we put emphasis on the faithfulness of such concept-based\nexplanations and propose a new model with model-inherent mechanistic\nconcept-explanations. Our concepts are shared across classes and, from any\nlayer, their contribution to the logit and their input-visualization can be\nfaithfully traced. We also leverage foundation models to propose a new\nconcept-consistency metric, C$^2$-Score, that can be used to evaluate\nconcept-based methods. We show that, compared to prior work, our concepts are\nquantitatively more consistent and users find our concepts to be more\ninterpretable, all while retaining competitive ImageNet performance.\n","authors":["Amin Parchami-Araghi","Sukrut Rao","Jonas Fischer","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2510.25512v1.pdf","comment":"Accepted to NeurIPS 2025; Code is available at\n  https://github.com/m-parchami/FaCT"},{"id":"http://arxiv.org/abs/2411.10237v2","updated":"2025-10-29T13:22:04Z","published":"2024-11-15T14:51:30Z","title":"ScribbleVS: Scribble-Supervised Medical Image Segmentation via Dynamic\n  Competitive Pseudo Label Selection","summary":"  In clinical medicine, precise image segmentation can provide substantial\nsupport to clinicians. However, obtaining high-quality segmentation typically\ndemands extensive pixel-level annotations, which are labor-intensive and\nexpensive. Scribble annotations offer a more cost-effective alternative by\nimproving labeling efficiency. Nonetheless, using such sparse supervision for\ntraining reliable medical image segmentation models remains a significant\nchallenge. Some studies employ pseudo-labeling to enhance supervision, but\nthese methods are susceptible to noise interference. To address these\nchallenges, we introduce ScribbleVS, a framework designed to learn from\nscribble annotations. We introduce a Regional Pseudo Labels Diffusion Module to\nexpand the scope of supervision and reduce the impact of noise present in\npseudo labels. Additionally, we introduce a Dynamic Competitive Selection\nmodule for enhanced refinement in selecting pseudo labels. Experiments\nconducted on the ACDC, MSCMRseg, WORD, and BraTS2020 datasets demonstrate\npromising results, achieving segmentation precision comparable to fully\nsupervised models. The codes of this study are available at\nhttps://github.com/ortonwang/ScribbleVS.\n","authors":["Tao Wang","Xinlin Zhang","Zhenxuan Zhang","Yuanbo Zhou","Yuanbin Chen","Longxuan Zhao","Chaohui Xu","Shun Chen","Guang Yang","Tong Tong"],"pdf_url":"https://arxiv.org/pdf/2411.10237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09518v2","updated":"2025-10-29T13:12:44Z","published":"2025-06-11T08:45:08Z","title":"HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for\n  Dynamic Scene","summary":"  Reconstructing dynamic 3D scenes from monocular videos remains a fundamental\nchallenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time\nrendering in static settings, extending it to dynamic scenes is challenging due\nto the difficulty of learning structured and temporally consistent motion\nrepresentations. This challenge often manifests as three limitations in\nexisting methods: redundant Gaussian updates, insufficient motion supervision,\nand weak modeling of complex non-rigid deformations. These issues collectively\nhinder coherent and efficient dynamic reconstruction. To address these\nlimitations, we propose HAIF-GS, a unified framework that enables structured\nand consistent dynamic modeling through sparse anchor-driven deformation. It\nfirst identifies motion-relevant regions via an Anchor Filter to suppress\nredundant updates in static areas. A self-supervised Induced Flow-Guided\nDeformation module induces anchor motion using multi-frame feature aggregation,\neliminating the need for explicit flow labels. To further handle fine-grained\ndeformations, a Hierarchical Anchor Propagation mechanism increases anchor\nresolution based on motion complexity and propagates multi-level\ntransformations. Extensive experiments on synthetic and real-world benchmarks\nvalidate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in\nrendering quality, temporal coherence, and reconstruction efficiency.\n","authors":["Jianing Chen","Zehao Li","Yujun Cai","Hao Jiang","Chengxuan Qian","Juyuan Kang","Shuqin Gao","Honglong Zhao","Tianlu Mao","Yucheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.09518v2.pdf","comment":"Accepted to NeurIPS 2025. Project page:\n  https://echopickle.github.io/HAIF-GS.github.io/"},{"id":"http://arxiv.org/abs/2503.08068v2","updated":"2025-10-29T12:57:01Z","published":"2025-03-11T05:59:43Z","title":"Simulating Automotive Radar with Lidar and Camera Inputs","summary":"  Low-cost millimeter automotive radar has received more and more attention due\nto its ability to handle adverse weather and lighting conditions in autonomous\ndriving. However, the lack of quality datasets hinders research and\ndevelopment. We report a new method that is able to simulate 4D millimeter wave\nradar signals including pitch, yaw, range, and Doppler velocity along with\nradar signal strength (RSS) using camera image, light detection and ranging\n(lidar) point cloud, and ego-velocity. The method is based on two new neural\nnetworks: 1) DIS-Net, which estimates the spatial distribution and number of\nradar signals, and 2) RSS-Net, which predicts the RSS of the signal based on\nappearance and geometric information. We have implemented and tested our method\nusing open datasets from 3 different models of commercial automotive radar. The\nexperimental results show that our method can successfully generate\nhigh-fidelity radar signals. Moreover, we have trained a popular object\ndetection neural network with data augmented by our synthesized radar. The\nnetwork outperforms the counterpart trained only on raw radar data, a promising\nresult to facilitate future radar-based research and development.\n","authors":["Peili Song","Dezhen Song","Yifan Yang","Enfan Lan","Jingtai Liu"],"pdf_url":"https://arxiv.org/pdf/2503.08068v2.pdf","comment":"Accepted by IROS 2025"},{"id":"http://arxiv.org/abs/2505.17685v2","updated":"2025-10-29T12:46:23Z","published":"2025-05-23T09:55:32Z","title":"FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for\n  Autonomous Driving","summary":"  Vision-Language-Action (VLA) models are increasingly used for end-to-end\ndriving due to their world knowledge and reasoning ability. Most prior work,\nhowever, inserts textual chains-of-thought (CoT) as intermediate steps tailored\nto the current scene. Such symbolic compressions can blur spatio-temporal\nrelations and discard fine visual cues, creating a cross-modal gap between\nperception and planning. We propose FSDrive, a visual spatio-temporal CoT\nframework that enables VLAs to think in images. The model first acts as a world\nmodel to generate a unified future frame that overlays coarse but\nphysically-plausible priors-future lane dividers and 3D boxes-on the predicted\nfuture image. This unified frame serves as the visual CoT, capturing both\nspatial structure and temporal evolution. The same VLA then functions as an\ninverse-dynamics model, planning trajectories from current observations and the\nvisual CoT. To equip VLAs with image generation while preserving understanding,\nwe introduce a unified pre-training paradigm that expands the vocabulary to\ninclude visual tokens and jointly optimizes VQA (for semantics) and\nfuture-frame prediction (for dynamics). A progressive easy-to-hard scheme first\npredicts lane/box priors to enforce physical constraints, then completes full\nfuture frames for fine details. On nuScenes and NAVSIM, FSDrive improves\ntrajectory accuracy and reduces collisions under both ST-P3 and UniAD metrics,\nand attains competitive FID for future-frame generation despite using\nlightweight autoregression. It also advances scene understanding on DriveLM.\nTogether, these results indicate that visual CoT narrows the cross-modal gap\nand yields safer, more anticipatory planning. Code is available at\nhttps://github.com/MIV-XJTU/FSDrive.\n","authors":["Shuang Zeng","Xinyuan Chang","Mengwei Xie","Xinran Liu","Yifan Bai","Zheng Pan","Mu Xu","Xing Wei"],"pdf_url":"https://arxiv.org/pdf/2505.17685v2.pdf","comment":"Accepted to NeurIPS 2025 as Spotlight Presentation. Code:\n  https://github.com/MIV-XJTU/FSDrive"},{"id":"http://arxiv.org/abs/2510.25463v1","updated":"2025-10-29T12:37:34Z","published":"2025-10-29T12:37:34Z","title":"SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time,\n  Monocular Depth Estimation in Underwater Environments","summary":"  Underwater infrastructure requires frequent inspection and maintenance due to\nharsh marine conditions. Current reliance on human divers or remotely operated\nvehicles is limited by perceptual and operational challenges, especially around\ncomplex structures or in turbid water. Enhancing the spatial awareness of\nunderwater vehicles is key to reducing piloting risks and enabling greater\nautonomy. To address these challenges, we present SPADE: SParsity Adaptive\nDepth Estimator, a monocular depth estimation pipeline that combines\npre-trained relative depth estimator with sparse depth priors to produce dense,\nmetric scale depth maps. Our two-stage approach first scales the relative depth\nmap with the sparse depth points, then refines the final metric prediction with\nour proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves\nimproved accuracy and generalisation over state-of-the-art baselines and runs\nefficiently at over 15 FPS on embedded hardware, promising to support practical\nunderwater inspection and intervention. This work has been submitted to IEEE\nJournal of Oceanic Engineering Special Issue of AUV 2026.\n","authors":["Hongjie Zhang","Gideon Billings","Stefan B. Williams"],"pdf_url":"https://arxiv.org/pdf/2510.25463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17897v4","updated":"2025-10-29T12:15:39Z","published":"2025-07-23T19:48:27Z","title":"Multimodal Recurrent Ensembles for Predicting Brain Responses to\n  Naturalistic Movies (Algonauts 2025)","summary":"  Accurately predicting distributed cortical responses to naturalistic stimuli\nrequires models that integrate visual, auditory and semantic information over\ntime. We present a hierarchical multimodal recurrent ensemble that maps\npretrained video, audio, and language embeddings to fMRI time series recorded\nwhile four subjects watched almost 80 hours of movies provided by the Algonauts\n2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;\ntheir hidden states are fused and passed to a second recurrent layer, and\nlightweight subject-specific heads output responses for 1000 cortical parcels.\nTraining relies on a composite MSE-correlation loss and a curriculum that\ngradually shifts emphasis from early sensory to late association regions.\nAveraging 100 model variants further boosts robustness. The resulting system\nranked third on the competition leaderboard, achieving an overall Pearson r =\n0.2094 and the highest single-parcel peak score (mean r = 0.63) among all\nparticipants, with particularly strong gains for the most challenging subject\n(Subject 5). The approach establishes a simple, extensible baseline for future\nmultimodal brain-encoding benchmarks.\n","authors":["Semih Eren","Deniz Kucukahmetler","Nico Scherf"],"pdf_url":"https://arxiv.org/pdf/2507.17897v4.pdf","comment":"8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts\n  Project session (3rd-place team). Code:\n  https://github.com/erensemih/Algonauts2025_ModalityRNN v3: Added equal\n  contribution footnote to author list. Corrected reference list"},{"id":"http://arxiv.org/abs/2509.09349v2","updated":"2025-10-29T12:14:41Z","published":"2025-09-11T11:05:14Z","title":"Classification of Driver Behaviour Using External Observation Techniques\n  for Autonomous Vehicles","summary":"  Road traffic accidents remain a significant global concern, with human error,\nparticularly distracted and impaired driving, among the leading causes. This\nstudy introduces a novel driver behaviour classification system that uses\nexternal observation techniques to detect indicators of distraction and\nimpairment. The proposed framework employs advanced computer vision\nmethodologies, including real-time object tracking, lateral displacement\nanalysis, and lane position monitoring. The system identifies unsafe driving\nbehaviours such as excessive lateral movement and erratic trajectory patterns\nby implementing the YOLO object detection model and custom lane estimation\nalgorithms. Unlike systems reliant on inter-vehicular communication, this\nvision-based approach enables behavioural analysis of non-connected vehicles.\nExperimental evaluations on diverse video datasets demonstrate the framework's\nreliability and adaptability across varying road and environmental conditions.\n","authors":["Ian Nell","Shane Gilroy"],"pdf_url":"https://arxiv.org/pdf/2509.09349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25440v1","updated":"2025-10-29T12:06:42Z","published":"2025-10-29T12:06:42Z","title":"More than a Moment: Towards Coherent Sequences of Audio Descriptions","summary":"  Audio Descriptions (ADs) convey essential on-screen information, allowing\nvisually impaired audiences to follow videos. To be effective, ADs must form a\ncoherent sequence that helps listeners to visualise the unfolding scene, rather\nthan describing isolated moments. However, most automatic methods generate each\nAD independently, often resulting in repetitive, incoherent descriptions. To\naddress this, we propose a training-free method, CoherentAD, that first\ngenerates multiple candidate descriptions for each AD time interval, and then\nperforms auto-regressive selection across the sequence to form a coherent and\ninformative narrative. To evaluate AD sequences holistically, we introduce a\nsequence-level metric, StoryRecall, which measures how well the predicted ADs\nconvey the ground truth narrative, alongside repetition metrics that capture\nthe redundancy across consecutive AD outputs. Our method produces coherent AD\nsequences with enhanced narrative understanding, outperforming prior approaches\nthat rely on independent generations.\n","authors":["Eshika Khandelwal","Junyu Xie","Tengda Han","Max Bain","Arsha Nagrani","Andrew Zisserman","Gül Varol","Makarand Tapaswi"],"pdf_url":"https://arxiv.org/pdf/2510.25440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05780v2","updated":"2025-10-29T10:59:02Z","published":"2024-08-11T14:11:45Z","title":"U-DECN: End-to-End Underwater Object Detection ConvNet with Improved\n  DeNoising Training","summary":"  Underwater object detection has higher requirements of running speed and\ndeployment efficiency for the detector due to its specific environmental\nchallenges. NMS of two- or one-stage object detectors and transformer\narchitecture of query-based end-to-end object detectors are not conducive to\ndeployment on underwater embedded devices with limited processing power. As for\nthe detrimental effect of underwater color cast noise, recent underwater object\ndetectors make network architecture or training complex, which also hinders\ntheir application and deployment on unmanned underwater vehicles. In this\npaper, we propose the Underwater DECO with improved deNoising training\n(U-DECN), the query-based end-to-end object detector (with ConvNet\nencoder-decoder architecture) for underwater color cast noise that addresses\nthe above problems. We integrate advanced technologies from DETR variants into\nDECO and design optimization methods specifically for the ConvNet architecture,\nincluding Deformable Convolution in SIM and Separate Contrastive DeNoising\nForward methods. To address the underwater color cast noise issue, we propose\nan Underwater Color DeNoising Query method to improve the generalization of the\nmodel for the biased object feature information by different color cast noise.\nOur U-DECN, with ResNet-50 backbone, achieves the best 64.0 AP on DUO and the\nbest 58.1 AP on RUOD, and 21 FPS (5 times faster than Deformable DETR and DINO\n4 FPS) on NVIDIA AGX Orin by TensorRT FP16, outperforming the other\nstate-of-the-art query-based end-to-end object detectors. The code is available\nat https://github.com/LEFTeyex/U-DECN.\n","authors":["Zhuoyan Liu","Bo Wang","Bing Wang","Ye Li"],"pdf_url":"https://arxiv.org/pdf/2408.05780v2.pdf","comment":"10 pages, 6 figures, 7 tables, accepted by IEEE TGRS"},{"id":"http://arxiv.org/abs/2510.25387v1","updated":"2025-10-29T10:57:59Z","published":"2025-10-29T10:57:59Z","title":"Instance-Level Composed Image Retrieval","summary":"  The progress of composed image retrieval (CIR), a popular research direction\nin image retrieval, where a combined visual and textual query is used, is held\nback by the absence of high-quality training and evaluation data. We introduce\na new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an\ninstance-level class definition. The goal is to retrieve images that contain\nthe same particular object as the visual query, presented under a variety of\nmodifications defined by textual queries. Its design and curation process keep\nthe dataset compact to facilitate future research, while maintaining its\nchallenge-comparable to retrieval among more than 40M random\ndistractors-through a semi-automated selection of hard negatives.\n  To overcome the challenge of obtaining clean, diverse, and suitable training\ndata, we leverage pre-trained vision-and-language models (VLMs) in a\ntraining-free approach called BASIC. The method separately estimates\nquery-image-to-image and query-text-to-image similarities, performing late\nfusion to upweight images that satisfy both queries, while down-weighting those\nthat exhibit high similarity with only one of the two. Each individual\nsimilarity is further improved by a set of components that are simple and\nintuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR\ndatasets that follow a semantic-level class definition. Project page:\nhttps://vrg.fel.cvut.cz/icir/.\n","authors":["Bill Psomas","George Retsinas","Nikos Efthymiadis","Panagiotis Filntisis","Yannis Avrithis","Petros Maragos","Ondrej Chum","Giorgos Tolias"],"pdf_url":"https://arxiv.org/pdf/2510.25387v1.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.20274v2","updated":"2025-10-29T10:47:16Z","published":"2025-05-26T17:53:28Z","title":"Probabilistic Kernel Function for Fast Angle Testing","summary":"  In this paper, we study the angle testing problem in the context of\nsimilarity search in high-dimensional Euclidean spaces and propose two\nprojection-based probabilistic kernel functions, one designed for angle\ncomparison and the other for angle thresholding. Unlike existing approaches\nthat rely on random projection vectors drawn from Gaussian distributions, our\napproach leverages reference angles and employs a deterministic structure for\nthe projection vectors. Notably, our kernel functions do not require asymptotic\nassumptions, such as the number of projection vectors tending to infinity, and\ncan be both theoretically and experimentally shown to outperform\nGaussian-distribution-based kernel functions. We apply the proposed kernel\nfunction to Approximate Nearest Neighbor Search (ANNS) and demonstrate that our\napproach achieves a 2.5X ~ 3X higher query-per-second (QPS) throughput compared\nto the widely-used graph-based search algorithm HNSW.\n","authors":["Kejing Lu","Chuan Xiao","Yoshiharu Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2505.20274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25372v1","updated":"2025-10-29T10:42:56Z","published":"2025-10-29T10:42:56Z","title":"Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision\n  Transformers","summary":"  Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has\nproven highly effective as a parameter-efficient fine-tuning technique for\nadapting large models to downstream tasks with limited data. Its parameter\nefficiency makes it particularly suitable for Federated Learning (FL), where\nboth communication and computation budgets are often constrained. However,\nglobal prompt tuning struggles to generalize across heterogeneous clients,\nwhile personalized tuning overfits to local data and lacks generalization. We\npropose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt\nTuning), a unified framework designed to achieve both generalization and\npersonalization in federated prompt tuning of ViTs. Within this framework, we\nintroduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on\nclass-specific prompts maintained alongside a globally shared prompt. For each\ninput, CCMP adaptively combines class-specific prompts using weights derived\nfrom global class prototypes and client class priors. This approach enables\nper-sample prompt personalization without storing client-dependent trainable\nparameters. The prompts are collaboratively optimized via traditional federated\naveraging technique on the same. Comprehensive evaluations on CIFAR-100,\nTinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT\nconsistently surpasses the state-of-the-art baselines under diverse data\nheterogeneity scenarios, establishing a strong foundation for efficient and\ngeneralizable federated prompt tuning of Vision Transformers.\n","authors":["M Yashwanth","Sharannya Ghosh","Aditay Tripathi","Anirban Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2510.25372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22842v2","updated":"2025-10-29T10:18:17Z","published":"2025-10-26T21:21:27Z","title":"FastJAM: a Fast Joint Alignment Model for Images","summary":"  Joint Alignment (JA) of images aims to align a collection of images into a\nunified coordinate frame, such that semantically-similar features appear at\ncorresponding spatial locations. Most existing approaches often require long\ntraining times, large-capacity models, and extensive hyperparameter tuning. We\nintroduce FastJAM, a rapid, graph-based method that drastically reduces the\ncomputational complexity of joint alignment tasks. FastJAM leverages pairwise\nmatches computed by an off-the-shelf image matcher, together with a rapid\nnonparametric clustering, to construct a graph representing intra- and\ninter-image keypoint relations. A graph neural network propagates and\naggregates these correspondences, efficiently predicting per-image homography\nparameters via image-level pooling. Utilizing an inverse-compositional loss,\nthat eliminates the need for a regularization term over the predicted\ntransformations (and thus also obviates the hyperparameter tuning associated\nwith such terms), FastJAM performs image JA quickly and effectively.\nExperimental results on several benchmarks demonstrate that FastJAM achieves\nresults better than existing modern JA methods in terms of alignment quality,\nwhile reducing computation time from hours or minutes to mere seconds. Our code\nis available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/\n","authors":["Omri Hirsch","Ron Shapira Weber","Shira Ifergane","Oren Freifeld"],"pdf_url":"https://arxiv.org/pdf/2510.22842v2.pdf","comment":"Accepted to NeurIPS 2025. Pages 1-10 are the Main Paper. Pages 23-31\n  are Supplemental Material. FastJAM website -\n  https://bgu-cs-vil.github.io/FastJAM/"},{"id":"http://arxiv.org/abs/2510.25347v1","updated":"2025-10-29T10:04:47Z","published":"2025-10-29T10:04:47Z","title":"3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine\n  Learning Framework","summary":"  Coronary artery calcium (CAC) scoring plays a crucial role in the early\ndetection and risk stratification of coronary artery disease (CAD). In this\nstudy, we focus on non-contrast coronary computed tomography angiography (CCTA)\nscans, which are commonly used for early calcification detection in clinical\nsettings. To address the challenge of limited annotated data, we propose a\nradiomics-based pipeline that leverages pseudo-labeling to generate training\nlabels, thereby eliminating the need for expert-defined segmentations.\nAdditionally, we explore the use of pretrained foundation models, specifically\nCT-FM and RadImageNet, to extract image features, which are then used with\ntraditional classifiers. We compare the performance of these deep learning\nfeatures with that of radiomics features. Evaluation is conducted on a clinical\nCCTA dataset comprising 182 patients, where individuals are classified into two\ngroups: zero versus non-zero calcium scores. We further investigate the impact\nof training on non-contrast datasets versus combined contrast and non-contrast\ndatasets, with testing performed only on non contrast scans. Results show that\nradiomics-based models significantly outperform CNN-derived embeddings from\nfoundation models (achieving 84% accuracy and p<0.05), despite the\nunavailability of expert annotations.\n","authors":["Ayman Abaid","Gianpiero Guidone","Sara Alsubai","Foziyah Alquahtani","Talha Iqbal","Ruth Sharif","Hesham Elzomor","Emiliano Bianchini","Naeif Almagal","Michael G. Madden","Faisal Sharif","Ihsan Ullah"],"pdf_url":"https://arxiv.org/pdf/2510.25347v1.pdf","comment":"11 pages, 2 Figures, MICCAI AMAI 2025 workshop, to be published in\n  Volume 16206 of the Lecture Notes in Computer Science series"},{"id":"http://arxiv.org/abs/2510.25345v1","updated":"2025-10-29T10:03:33Z","published":"2025-10-29T10:03:33Z","title":"Informative Sample Selection Model for Skeleton-based Action Recognition\n  with Limited Training Samples","summary":"  Skeleton-based human action recognition aims to classify human skeletal\nsequences, which are spatiotemporal representations of actions, into predefined\ncategories. To reduce the reliance on costly annotations of skeletal sequences\nwhile maintaining competitive recognition accuracy, the task of 3D Action\nRecognition with Limited Training Samples, also known as semi-supervised 3D\nAction Recognition, has been proposed. In addition, active learning, which aims\nto proactively select the most informative unlabeled samples for annotation,\nhas been explored in semi-supervised 3D Action Recognition for training sample\nselection. Specifically, researchers adopt an encoder-decoder framework to\nembed skeleton sequences into a latent space, where clustering information,\ncombined with a margin-based selection strategy using a multi-head mechanism,\nis utilized to identify the most informative sequences in the unlabeled set for\nannotation. However, the most representative skeleton sequences may not\nnecessarily be the most informative for the action recognizer, as the model may\nhave already acquired similar knowledge from previously seen skeleton samples.\nTo solve it, we reformulate Semi-supervised 3D action recognition via active\nlearning from a novel perspective by casting it as a Markov Decision Process\n(MDP). Built upon the MDP framework and its training paradigm, we train an\ninformative sample selection model to intelligently guide the selection of\nskeleton sequences for annotation. To enhance the representational capacity of\nthe factors in the state-action pairs within our method, we project them from\nEuclidean space to hyperbolic space. Furthermore, we introduce a meta tuning\nstrategy to accelerate the deployment of our method in real-world scenarios.\nExtensive experiments on three 3D action recognition benchmarks demonstrate the\neffectiveness of our method.\n","authors":["Zhigang Tu","Zhengbo Zhang","Jia Gong","Junsong Yuan","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2510.25345v1.pdf","comment":"Accepted by IEEE Transactions on Image Processing (TIP), 2025"},{"id":"http://arxiv.org/abs/2510.25332v1","updated":"2025-10-29T09:47:38Z","published":"2025-10-29T09:47:38Z","title":"StreamingCoT: A Dataset for Temporal Dynamics and Multimodal\n  Chain-of-Thought Reasoning in Streaming VideoQA","summary":"  The rapid growth of streaming video applications demands multimodal models\nwith enhanced capabilities for temporal dynamics understanding and complex\nreasoning. However, current Video Question Answering (VideoQA) datasets suffer\nfrom two critical limitations: 1) Static annotation mechanisms fail to capture\nthe evolving nature of answers in temporal video streams, and 2) The absence of\nexplicit reasoning process annotations restricts model interpretability and\nlogical deduction capabilities. To address these challenges, We introduce\nStreamingCoT, the first dataset explicitly designed for temporally evolving\nreasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our\nframework first establishes a dynamic hierarchical annotation architecture that\ngenerates per-second dense descriptions and constructs temporally-dependent\nsemantic segments through similarity fusion, paired with question-answer sets\nconstrained by temporal evolution patterns. We further propose an explicit\nreasoning chain generation paradigm that extracts spatiotemporal objects via\nkeyframe semantic alignment, derives object state transition-based reasoning\npaths using large language models, and ensures logical coherence through\nhuman-verified validation. This dataset establishes a foundation for advancing\nresearch in streaming video understanding, complex temporal reasoning, and\nmultimodal inference. Our StreamingCoT and its construction toolkit can be\naccessed at https://github.com/Fleeting-hyh/StreamingCoT.\n","authors":["Yuhang Hu","Zhenyu Yang","Shihan Wang","Shengsheng Qian","Bin Wen","Fan Yang","Tingting Gao","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25318v1","updated":"2025-10-29T09:32:42Z","published":"2025-10-29T09:32:42Z","title":"Prototype-Driven Adaptation for Few-Shot Object Detection","summary":"  Few-shot object detection (FSOD) often suffers from base-class bias and\nunstable calibration when only a few novel samples are available. We propose\nPrototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN\nthat provides a prototype-based \"second opinion\" complementary to the linear\nclassifier. PDA maintains support-only prototypes in a learnable\nidentity-initialized projection space and optionally applies\nprototype-conditioned RoI alignment to reduce geometric mismatch. During\nfine-tuning, prototypes can be adapted via exponential moving average(EMA)\nupdates on labeled foreground RoIs-without introducing class-specific\nparameters-and are frozen at inference to ensure strict protocol compliance.\nPDA employs a best-of-K matching scheme to capture intra-class multi-modality\nand temperature-scaled fusion to combine metric similarities with detector\nlogits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently\nimproves novel-class performance with minimal impact on base classes and\nnegligible computational overhead.\n","authors":["Yushen Huang","Zhiming Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25318v1.pdf","comment":"7 pages,1 figure,2 tables,Preprint"},{"id":"http://arxiv.org/abs/2510.25314v1","updated":"2025-10-29T09:27:38Z","published":"2025-10-29T09:27:38Z","title":"Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired\n  Monocentric Design","summary":"  Achieving high-fidelity, compact RGBD imaging presents a dual challenge:\nconventional compact optics struggle with RGB sharpness across the entire\ndepth-of-field, while software-only Monocular Depth Estimation (MDE) is an\nill-posed problem reliant on unreliable semantic priors. While deep optics with\nelements like DOEs can encode depth, they introduce trade-offs in fabrication\ncomplexity and chromatic aberrations, compromising simplicity. To address this,\nwe first introduce a novel bio-inspired all-spherical monocentric lens, around\nwhich we build the Bionic Monocentric Imaging (BMI) framework, a holistic\nco-design. This optical design naturally encodes depth into its depth-varying\nPoint Spread Functions (PSFs) without requiring complex diffractive or freeform\nelements. We establish a rigorous physically-based forward model to generate a\nsynthetic dataset by precisely simulating the optical degradation process. This\nsimulation pipeline is co-designed with a dual-head, multi-scale reconstruction\nnetwork that employs a shared encoder to jointly recover a high-fidelity\nAll-in-Focus (AiF) image and a precise depth map from a single coded capture.\nExtensive experiments validate the state-of-the-art performance of the proposed\nframework. In depth estimation, the method attains an Abs Rel of 0.026 and an\nRMSE of 0.130, markedly outperforming leading software-only approaches and\nother deep optics systems. For image restoration, the system achieves an SSIM\nof 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior\nbalance between image fidelity and depth accuracy. This study illustrates that\nthe integration of bio-inspired, fully spherical optics with a joint\nreconstruction algorithm constitutes an effective strategy for addressing the\nintrinsic challenges in high-performance compact RGBD imaging. Source code will\nbe publicly available at https://github.com/ZongxiYu-ZJU/BMI.\n","authors":["Zongxi Yu","Xiaolong Qian","Shaohua Gao","Qi Jiang","Yao Gao","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25314v1.pdf","comment":"The source code will be publicly available at\n  https://github.com/ZongxiYu-ZJU/BMI"},{"id":"http://arxiv.org/abs/2510.25301v1","updated":"2025-10-29T09:14:07Z","published":"2025-10-29T09:14:07Z","title":"GaTector+: A Unified Head-free Framework for Gaze Object and Gaze\n  Following Prediction","summary":"  Gaze object detection and gaze following are fundamental tasks for\ninterpreting human gaze behavior or intent. However, most previous methods\nusually solve these two tasks separately, and their prediction of gaze objects\nand gaze following typically depend on head-related prior knowledge during both\nthe training phase and real-world deployment. This dependency necessitates an\nauxiliary network to extract head location, thus precluding joint optimization\nacross the entire system and constraining the practical applicability. To this\nend, we propose GaTector+, a unified framework for gaze object detection and\ngaze following, which eliminates the dependence on the head-related priors\nduring inference. Specifically, GaTector+ uses an expanded\nspecific-general-specific feature extractor that leverages a shared backbone,\nwhich extracts general features for gaze following and object detection using\nthe shared backbone while using specific blocks before and after the shared\nbackbone to better consider the specificity of each sub-task. To obtain\nhead-related knowledge without prior information, we first embed a head\ndetection branch to predict the head of each person. Then, before regressing\nthe gaze point, a head-based attention mechanism is proposed to fuse the sense\nfeature and gaze feature with the help of head location. Since the\nsuboptimization of the gaze point heatmap leads to the performance bottleneck,\nwe propose an attention supervision mechanism to accelerate the learning of the\ngaze heatmap. Finally, we propose a novel evaluation metric, mean Similarity\nover Candidates (mSoC), for gaze object detection, which is more sensitive to\nvariations between bounding boxes. The experimental results on multiple\nbenchmark datasets demonstrate the effectiveness of our model in both gaze\nobject detection and gaze following tasks.\n","authors":["Yang Jin","Guangyu Guo","Binglu Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.23051v2","updated":"2025-10-29T09:09:07Z","published":"2025-09-27T02:12:09Z","title":"Activation Matching for Explanation Generation","summary":"  In this paper we introduce an activation-matching--based approach to generate\nminimal, faithful explanations for the decision-making of a pretrained\nclassifier on any given image. Given an input image $x$ and a frozen model $f$,\nwe train a lightweight autoencoder to output a binary mask $m$ such that the\nexplanation $e = m \\odot x$ preserves both the model's prediction and the\nintermediate activations of \\(x\\). Our objective combines: (i) multi-layer\nactivation matching with KL divergence to align distributions and cross-entropy\nto retain the top-1 label for both the image and the explanation; (ii) mask\npriors -- L1 area for minimality, a binarization penalty for crisp 0/1 masks,\nand total variation for compactness; and (iii) abductive constraints for\nfaithfulness and necessity. Together, these objectives yield small,\nhuman-interpretable masks that retain classifier behavior while discarding\nirrelevant input regions, providing practical and faithful minimalist\nexplanations for the decision making of the underlying model.\n","authors":["Pirzada Suhail","Aditya Anand","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2509.23051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19162v2","updated":"2025-10-29T08:59:36Z","published":"2024-06-27T13:29:25Z","title":"Single Image Estimation of Cell Migration Direction by Deep Circular\n  Regression","summary":"  In this paper, we address the problem of estimating the migration direction\nof cells based on a single image. A solution to this problem lays the\nfoundation for a variety of applications that were previously not possible. To\nour knowledge, there is only one related work that employs a classification CNN\nwith four classes (quadrants). However, this approach does not allow for\ndetailed directional resolution. We tackle the single image estimation problem\nusing deep circular regression, with a particular focus on cycle-sensitive\nmethods. On two common datasets, we achieve a mean estimation error of\n$\\sim\\!17^\\circ$, representing a significant improvement over previous work,\nwhich reported estimation error of $30^\\circ$ and $34^\\circ$, respectively.\n","authors":["Lennart Bruns","Lucas Lamparter","Milos Galic","Xiaoyi Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.19162v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24385v2","updated":"2025-10-29T08:55:13Z","published":"2025-10-28T13:01:42Z","title":"When are radiology reports useful for training medical image\n  classifiers?","summary":"  Medical images used to train machine learning models are often accompanied by\nradiology reports containing rich expert annotations. However, relying on these\nreports as inputs for clinical prediction requires the timely manual work of a\ntrained radiologist. This raises a natural question: when can radiology reports\nbe leveraged during training to improve image-only classification? Prior works\nare limited to evaluating pre-trained image representations by fine-tuning them\nto predict diagnostic labels, often extracted from reports, ignoring tasks with\nlabels that are weakly associated with the text. To address this gap, we\nconduct a systematic study of how radiology reports can be used during both\npre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,\n12-month readmission), and under varying training set sizes. Our findings\nreveal that: (1) Leveraging reports during pre-training is beneficial for\ndownstream classification tasks where the label is well-represented in the\ntext; however, pre-training through explicit image-text alignment can be\ndetrimental in settings where it's not; (2) Fine-tuning with reports can lead\nto significant improvements and even have a larger impact than the pre-training\nmethod in certain settings. These results provide actionable insights into when\nand how to leverage privileged text data to train medical image classifiers\nwhile highlighting gaps in current research.\n","authors":["Herman Bergström","Zhongqi Yue","Fredrik D. Johansson"],"pdf_url":"https://arxiv.org/pdf/2510.24385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25279v1","updated":"2025-10-29T08:38:03Z","published":"2025-10-29T08:38:03Z","title":"Diffusion-Driven Progressive Target Manipulation for Source-Free Domain\n  Adaptation","summary":"  Source-free domain adaptation (SFDA) is a challenging task that tackles\ndomain shifts using only a pre-trained source model and unlabeled target data.\nExisting SFDA methods are restricted by the fundamental limitation of\nsource-target domain discrepancy. Non-generation SFDA methods suffer from\nunreliable pseudo-labels in challenging scenarios with large domain\ndiscrepancies, while generation-based SFDA methods are evidently degraded due\nto enlarged domain discrepancies in creating pseudo-source data. To address\nthis limitation, we propose a novel generation-based framework named\nDiffusion-Driven Progressive Target Manipulation (DPTM) that leverages\nunlabeled target data as references to reliably generate and progressively\nrefine a pseudo-target domain for SFDA. Specifically, we divide the target\nsamples into a trust set and a non-trust set based on the reliability of\npseudo-labels to sufficiently and reliably exploit their information. For\nsamples from the non-trust set, we develop a manipulation strategy to\nsemantically transform them into the newly assigned categories, while\nsimultaneously maintaining them in the target distribution via a latent\ndiffusion model. Furthermore, we design a progressive refinement mechanism that\nprogressively reduces the domain discrepancy between the pseudo-target domain\nand the real target domain via iterative refinement. Experimental results\ndemonstrate that DPTM outperforms existing methods by a large margin and\nachieves state-of-the-art performance on four prevailing SFDA benchmark\ndatasets with different scales. Remarkably, DPTM can significantly enhance the\nperformance by up to 18.6% in scenarios with large source-target gaps.\n","authors":["Yuyang Huang","Yabo Chen","Junyu Zhou","Wenrui Dai","Xiaopeng Zhang","Junni Zou","Hongkai Xiong","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2510.25279v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2504.21497v3","updated":"2025-10-29T08:32:29Z","published":"2025-04-30T10:30:46Z","title":"MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric\n  Guidance","summary":"  In this study, we propose a method for video face reenactment that integrates\na 3D face parametric model into a latent diffusion framework, aiming to improve\nshape consistency and motion control in existing video-based face generation\napproaches. Our approach employs the FLAME (Faces Learned with an Articulated\nModel and Expressions) model as the 3D face parametric representation,\nproviding a unified framework for modeling face expressions and head pose. This\nnot only enables precise extraction of motion features from driving videos, but\nalso contributes to the faithful preservation of face shape and geometry.\nSpecifically, we enhance the latent diffusion model with rich 3D expression and\ndetailed pose information by incorporating depth maps, normal maps, and\nrendering maps derived from FLAME sequences. These maps serve as motion\nguidance and are encoded into the denoising UNet through a specifically\ndesigned Geometric Guidance Encoder (GGE). A multi-layer feature fusion module\nwith integrated self-attention mechanisms is used to combine facial appearance\nand motion latent features within the spatial domain. By utilizing the 3D face\nparametric model as motion guidance, our method enables parametric alignment of\nface identity between the reference image and the motion captured from the\ndriving video. Experimental results on benchmark datasets show that our method\nexcels at generating high-quality face animations with precise expression and\nhead pose variation modeling. In addition, it demonstrates strong\ngeneralization performance on out-of-domain images. Code is publicly available\nat https://github.com/weimengting/MagicPortrait.\n","authors":["Mengting Wei","Yante Li","Tuomas Varanka","Yan Jiang","Guoying Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.21497v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25268v1","updated":"2025-10-29T08:27:00Z","published":"2025-10-29T08:27:00Z","title":"SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object\n  with Discrete Human Object Interaction Representation","summary":"  Generating hand grasps with language instructions is a widely studied topic\nthat benefits from embodied AI and VR/AR applications. While transferring into\nhand articulatied object interaction (HAOI), the hand grasps synthesis requires\nnot only object functionality but also long-term manipulation sequence along\nthe object deformation. This paper proposes a novel HAOI sequence generation\nframework SynHLMA, to synthesize hand language manipulation for articulated\nobjects. Given a complete point cloud of an articulated object, we utilize a\ndiscrete HAOI representation to model each hand object interaction frame. Along\nwith the natural language embeddings, the representations are trained by an\nHAOI manipulation language model to align the grasping process with its\nlanguage description in a shared representation space. A joint-aware loss is\nemployed to ensure hand grasps follow the dynamic variations of articulated\nobject joints. In this way, our SynHLMA achieves three typical hand\nmanipulation tasks for articulated objects of HAOI generation, HAOI prediction\nand HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and\nexperimental results demonstrate the superior hand grasp sequence generation\nperformance comparing with state-of-the-art. We also show a robotics grasp\napplication that enables dexterous grasps execution from imitation learning\nusing the manipulation sequence provided by our SynHLMA. Our codes and datasets\nwill be made publicly available.\n","authors":["Wang zhi","Yuyan Liu","Liu Liu","Li Zhang","Ruixuan Lu","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2510.25268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20322v2","updated":"2025-10-29T08:24:40Z","published":"2025-10-23T08:16:44Z","title":"HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large\n  Language Models","summary":"  Multi-modal large language models (MLLMs) have emerged as a transformative\napproach for aligning visual and textual understanding. They typically require\nextremely high computational resources (e.g., thousands of GPUs) for training\nto achieve cross-modal alignment at multi-granularity levels. We argue that a\nkey source of this inefficiency lies in the vision encoders they widely equip\nwith, e.g., CLIP and SAM, which lack the alignment with language at\nmulti-granularity levels. To address this issue, in this paper, we leverage\nhyperbolic space, which inherently models hierarchical levels and thus provides\na principled framework for bridging the granularity gap between visual and\ntextual modalities at an arbitrary granularity level. Concretely, we propose an\nefficient training paradigm for MLLMs, dubbed as HyperET, which can optimize\nvisual representations to align with their textual counterparts at an arbitrary\ngranularity level through dynamic hyperbolic radius adjustment in hyperbolic\nspace. HyperET employs learnable matrices with M\\\"{o}bius multiplication\noperations, implemented via three effective configurations: diagonal scaling\nmatrices, block-diagonal matrices, and banded matrices, providing a flexible\nyet efficient parametrization strategy. Comprehensive experiments across\nmultiple MLLM benchmarks demonstrate that HyperET consistently improves both\nexisting pre-training and fine-tuning MLLMs clearly with less than 1\\%\nadditional parameters.\n","authors":["Zelin Peng","Zhengqin Xu","Qingyang Liu","Xiaokang Yang","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2510.20322v2.pdf","comment":"Accepted by NeurIPS2025 (Oral)"},{"id":"http://arxiv.org/abs/2510.25263v1","updated":"2025-10-29T08:21:59Z","published":"2025-10-29T08:21:59Z","title":"LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part\n  Segmentation","summary":"  We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based\nframework for open-vocabulary object-part instance segmentation. Given an\nimage, LangHOPS can jointly detect and segment hierarchical object and part\ninstances from open-vocabulary candidate categories. Unlike prior approaches\nthat rely on heuristic or learnable visual grouping, our approach grounds\nobject-part hierarchies in language space. It integrates the MLLM into the\nobject-part parsing pipeline to leverage its rich knowledge and reasoning\ncapabilities, and link multi-granularity concepts within the hierarchies. We\nevaluate LangHOPS across multiple challenging scenarios, including in-domain\nand cross-dataset object-part instance segmentation, and zero-shot semantic\nsegmentation. LangHOPS achieves state-of-the-art results, surpassing previous\nmethods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on\nthe PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K\n(zero-shot). Ablation studies further validate the effectiveness of the\nlanguage-grounded hierarchy and MLLM driven part query refinement strategy. The\ncode will be released here.\n","authors":["Yang Miao","Jan-Nico Zaech","Xi Wang","Fabien Despinoy","Danda Pani Paudel","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2510.25263v1.pdf","comment":"10 pages, 5 figures, 14 tables, Neurips 2025"},{"id":"http://arxiv.org/abs/2503.19311v2","updated":"2025-10-29T08:14:38Z","published":"2025-03-25T03:17:42Z","title":"DGTRSD & DGTRS-CLIP: A Dual-Granularity Remote Sensing Image-Text\n  Dataset and Vision Language Foundation Model for Alignment","summary":"  Vision Language Foundation Models based on CLIP architecture for remote\nsensing primarily rely on short text captions, which often result in incomplete\nsemantic representations. Although longer captions convey richer information,\nexisting models struggle to process them effectively because of limited\ntext-encoding capacity, and there remains a shortage of resources that align\nremote sensing images with both short text and long text captions. To address\nthis gap, we introduce DGTRSD, a dual-granularity remote sensing image-text\ndataset, where each image is paired with both a short text caption and a long\ntext description, providing a solid foundation for dual-granularity semantic\nmodeling. Based on this, we further propose DGTRS-CLIP, a dual-granularity\ncurriculum learning framework that combines short text and long text\nsupervision to achieve dual-granularity semantic alignment. Extensive\nexperiments on four typical zero-shot tasks: long text cross-modal retrieval,\nshort text cross-modal retrieval, image classification, and semantic\nlocalization demonstrate that DGTRS-CLIP consistently outperforms existing\nmethods across all tasks. The code has been open-sourced and is available at\nhttps://github.com/MitsuiChen14/DGTRS.\n","authors":["Weizhi Chen","Yupeng Deng","Jin Wei","Jingbo Chen","Jiansheng Chen","Yuman Feng","Zhihao Xi","Diyou Liu","Kai Li","Yu Meng"],"pdf_url":"https://arxiv.org/pdf/2503.19311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25257v1","updated":"2025-10-29T08:13:17Z","published":"2025-10-29T08:13:17Z","title":"RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision\n  Foundation Models","summary":"  Real-time object detection has achieved substantial progress through\nmeticulously designed architectures and optimization strategies. However, the\npursuit of high-speed inference via lightweight network designs often leads to\ndegraded feature representation, which hinders further performance improvements\nand practical on-device deployment. In this paper, we propose a cost-effective\nand highly adaptable distillation framework that harnesses the rapidly evolving\ncapabilities of Vision Foundation Models (VFMs) to enhance lightweight object\ndetectors. Given the significant architectural and learning objective\ndisparities between VFMs and resource-constrained detectors, achieving stable\nand task-aligned semantic transfer is challenging. To address this, on one\nhand, we introduce a Deep Semantic Injector (DSI) module that facilitates the\nintegration of high-level representations from VFMs into the deep layers of the\ndetector. On the other hand, we devise a Gradient-guided Adaptive Modulation\n(GAM) strategy, which dynamically adjusts the intensity of semantic transfer\nbased on gradient norm ratios. Without increasing deployment and inference\noverhead, our approach painlessly delivers striking and consistent performance\ngains across diverse DETR-based models, underscoring its practical utility for\nreal-time detection. Our new model family, RT-DETRv4, achieves state-of-the-art\nresults on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at corresponding\nspeeds of 273/169/124/78 FPS.\n","authors":["Zijun Liao","Yian Zhao","Xin Shan","Yu Yan","Chang Liu","Lei Lu","Xiangyang Ji","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2510.25257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10258v2","updated":"2025-10-29T07:51:00Z","published":"2025-04-14T14:19:57Z","title":"XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a\n  Novel Benchmark","summary":"  Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing.\n","authors":["Shuai Liu","Youmeng Li","Jizeng Wei"],"pdf_url":"https://arxiv.org/pdf/2504.10258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25239v1","updated":"2025-10-29T07:37:19Z","published":"2025-10-29T07:37:19Z","title":"Mapping and Classification of Trees Outside Forests using Deep Learning","summary":"  Trees Outside Forests (TOF) play an important role in agricultural landscapes\nby supporting biodiversity, sequestering carbon, and regulating microclimates.\nYet, most studies have treated TOF as a single class or relied on rigid\nrule-based thresholds, limiting ecological interpretation and adaptability\nacross regions. To address this, we evaluate deep learning for TOF\nclassification using a newly generated dataset and high-resolution aerial\nimagery from four agricultural landscapes in Germany. Specifically, we compare\nconvolutional neural networks (CNNs), vision transformers, and hybrid\nCNN-transformer models across six semantic segmentation architectures (ABCNet,\nLSKNet, FT-UNetFormer, DC-Swin, BANet, and U-Net) to map four categories of\nwoody vegetation: Forest, Patch, Linear, and Tree, derived from previous\nstudies and governmental products. Overall, the models achieved good\nclassification accuracy across the four landscapes, with the FT-UNetFormer\nperforming best (mean Intersection-over-Union 0.74; mean F1 score 0.84),\nunderscoring the importance of spatial context understanding in TOF mapping and\nclassification. Our results show good results for Forest and Linear class and\nreveal challenges particularly in classifying complex structures with high edge\ndensity, notably the Patch and Tree class. Our generalization experiments\nhighlight the need for regionally diverse training data to ensure reliable\nlarge-scale mapping. The dataset and code are openly available at\nhttps://github.com/Moerizzy/TOFMapper\n","authors":["Moritz Lucas","Hamid Ebrahimy","Viacheslav Barkov","Ralf Pecenka","Kai-Uwe Kühnberger","Björn Waske"],"pdf_url":"https://arxiv.org/pdf/2510.25239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25238v1","updated":"2025-10-29T07:37:08Z","published":"2025-10-29T07:37:08Z","title":"VADB: A Large-Scale Video Aesthetic Database with Professional and\n  Multi-Dimensional Annotations","summary":"  Video aesthetic assessment, a vital area in multimedia computing, integrates\ncomputer vision with human cognition. Its progress is limited by the lack of\nstandardized datasets and robust models, as the temporal dynamics of video and\nmultimodal fusion challenges hinder direct application of image-based methods.\nThis study introduces VADB, the largest video aesthetic database with 10,490\ndiverse videos annotated by 37 professionals across multiple aesthetic\ndimensions, including overall and attribute-specific aesthetic scores, rich\nlanguage comments and objective tags. We propose VADB-Net, a dual-modal\npre-training framework with a two-stage training strategy, which outperforms\nexisting video quality assessment models in scoring tasks and supports\ndownstream video aesthetic assessment tasks. The dataset and source code are\navailable at https://github.com/BestiVictory/VADB.\n","authors":["Qianqian Qiao","DanDan Zheng","Yihang Bo","Bao Peng","Heng Huang","Longteng Jiang","Huaye Wang","Jingdong Chen","Jun Zhou","Xin Jin"],"pdf_url":"https://arxiv.org/pdf/2510.25238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25237v1","updated":"2025-10-29T07:35:29Z","published":"2025-10-29T07:35:29Z","title":"DeepShield: Fortifying Deepfake Video Detection with Local and Global\n  Forgery Analysis","summary":"  Recent advances in deep generative models have made it easier to manipulate\nface videos, raising significant concerns about their potential misuse for\nfraud and misinformation. Existing detectors often perform well in in-domain\nscenarios but fail to generalize across diverse manipulation techniques due to\ntheir reliance on forgery-specific artifacts. In this work, we introduce\nDeepShield, a novel deepfake detection framework that balances local\nsensitivity and global generalization to improve robustness across unseen\nforgeries. DeepShield enhances the CLIP-ViT encoder through two key components:\nLocal Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG\napplies spatiotemporal artifact modeling and patch-wise supervision to capture\nfine-grained inconsistencies often overlooked by global models. GFD introduces\ndomain feature augmentation, leveraging domain-bridging and boundary-expanding\nfeature generation to synthesize diverse forgeries, mitigating overfitting and\nenhancing cross-domain adaptability. Through the integration of novel local and\nglobal analysis for deepfake detection, DeepShield outperforms state-of-the-art\nmethods in cross-dataset and cross-manipulation evaluations, achieving superior\nrobustness against unseen deepfake attacks.\n","authors":["Yinqi Cai","Jichang Li","Zhaolun Li","Weikai Chen","Rushi Lan","Xi Xie","Xiaonan Luo","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2510.25237v1.pdf","comment":"ICCV 2025"},{"id":"http://arxiv.org/abs/2510.25234v1","updated":"2025-10-29T07:29:21Z","published":"2025-10-29T07:29:21Z","title":"Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D\n  Talking Face Animation","summary":"  Expressions are fundamental to conveying human emotions. With the rapid\nadvancement of AI-generated content (AIGC), realistic and expressive 3D facial\nanimation has become increasingly crucial. Despite recent progress in\nspeech-driven lip-sync for talking-face animation, generating emotionally\nexpressive talking faces remains underexplored. A major obstacle is the\nscarcity of real emotional 3D talking-face datasets due to the high cost of\ndata capture. To address this, we model facial animation driven by both speech\nand emotion as a linear additive problem. Leveraging a 3D talking-face dataset\nwith neutral expressions (VOCAset) and a dataset of 3D expression sequences\n(Florence4D), we jointly learn a set of blendshapes driven by speech and\nemotion. We introduce a sparsity constraint loss to encourage disentanglement\nbetween the two types of blendshapes while allowing the model to capture\ninherent secondary cross-domain deformations present in the training data. The\nlearned blendshapes can be further mapped to the expression and jaw pose\nparameters of the FLAME model, enabling the animation of 3D Gaussian avatars.\nQualitative and quantitative experiments demonstrate that our method naturally\ngenerates talking faces with specified expressions while maintaining accurate\nlip synchronization. Perceptual studies further show that our approach achieves\nsuperior emotional expressivity compared to existing methods, without\ncompromising lip-sync quality.\n","authors":["Yuxiang Mao","Zhijie Zhang","Zhiheng Zhang","Jiawei Liu","Chen Zeng","Shihong Xia"],"pdf_url":"https://arxiv.org/pdf/2510.25234v1.pdf","comment":"18 pages, 6 figures, accepted to ICXR 2025 conference"},{"id":"http://arxiv.org/abs/2510.21122v2","updated":"2025-10-29T07:06:34Z","published":"2025-10-24T03:23:34Z","title":"NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection\n  and Bayesian Estimation","summary":"  Reinforcement learning (RL) has shown promise in enhancing the general\nChain-of-Thought (CoT) reasoning capabilities of multimodal large language\nmodels (MLLMs). However, when applied to improve general CoT reasoning,\nexisting RL frameworks often struggle to generalize beyond the training\ndistribution. To address this, we propose NoisyGRPO, a systematic multimodal RL\nframework that introduces controllable noise into visual inputs for enhanced\nexploration and explicitly models the advantage estimation process via a\nBayesian framework. Specifically, NoisyGRPO improves RL training by: (1)\nNoise-Injected Exploration Policy: Perturbing visual inputs with Gaussian noise\nto encourage exploration across a wider range of visual scenarios; and (2)\nBayesian Advantage Estimation: Formulating advantage estimation as a principled\nBayesian inference problem, where the injected noise level serves as a prior\nand the observed trajectory reward as the likelihood. This Bayesian modeling\nfuses both sources of information to compute a robust posterior estimate of\ntrajectory advantage, effectively guiding MLLMs to prefer visually grounded\ntrajectories over noisy ones. Experiments on standard CoT quality, general\ncapability, and hallucination benchmarks demonstrate that NoisyGRPO\nsubstantially improves generalization and robustness, especially in RL settings\nwith small-scale MLLMs such as Qwen2.5-VL 3B. The project page is available at\nhttps://artanic30.github.io/project_pages/NoisyGRPO/.\n","authors":["Longtian Qiu","Shan Ning","Jiaxuan Sun","Xuming He"],"pdf_url":"https://arxiv.org/pdf/2510.21122v2.pdf","comment":"Accepted by Neurips2025, Project page at at\n  https://artanic30.github.io/project_pages/NoisyGRPO/"},{"id":"http://arxiv.org/abs/2510.25229v1","updated":"2025-10-29T07:06:01Z","published":"2025-10-29T07:06:01Z","title":"Balanced conic rectified flow","summary":"  Rectified flow is a generative model that learns smooth transport mappings\nbetween two distributions through an ordinary differential equation (ODE).\nUnlike diffusion-based generative models, which require costly numerical\nintegration of a generative ODE to sample images with state-of-the-art quality,\nrectified flow uses an iterative process called reflow to learn smooth and\nstraight ODE paths. This allows for relatively simple and efficient generation\nof high-quality images. However, rectified flow still faces several challenges.\n1) The reflow process requires a large number of generative pairs to preserve\nthe target distribution, leading to significant computational costs. 2) Since\nthe model is typically trained using only generated image pairs, its\nperformance heavily depends on the 1-rectified flow model, causing it to become\nbiased towards the generated data.\n  In this work, we experimentally expose the limitations of the original\nrectified flow and propose a novel approach that incorporates real images into\nthe training process. By preserving the ODE paths for real images, our method\neffectively reduces reliance on large amounts of generated data. Instead, we\ndemonstrate that the reflow process can be conducted efficiently using a much\nsmaller set of generated and real images. In CIFAR-10, we achieved\nsignificantly better FID scores, not only in one-step generation but also in\nfull-step simulations, while using only of the generative pairs compared to the\noriginal method. Furthermore, our approach induces straighter paths and avoids\nsaturation on generated images during reflow, leading to more robust ODE\nlearning while preserving the distribution of real images.\n","authors":["Kim Shin Seong","Mingi Kwon","Jaeseok Jeong","Youngjung Uh"],"pdf_url":"https://arxiv.org/pdf/2510.25229v1.pdf","comment":"Main paper: 10 pages (total 40 pages including appendix), 5 figures.\n  Accepted at NeurIPS 2025 (Poster). Acknowledgment: Supported by the NRF of\n  Korea (RS-2023-00223062) and IITP grants (RS-2020-II201361, RS-2024-00439762)\n  funded by the Korean government (MSIT)"},{"id":"http://arxiv.org/abs/2510.25227v1","updated":"2025-10-29T07:05:26Z","published":"2025-10-29T07:05:26Z","title":"Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain\n  Adaptation in Medical Image Segmentation","summary":"  Source-Free Domain Adaptation (SFDA) is emerging as a compelling solution for\nmedical image segmentation under privacy constraints, yet current approaches\noften ignore sample difficulty and struggle with noisy supervision under domain\nshift. We present a new SFDA framework that leverages Hard Sample Selection and\nDenoised Patch Mixing to progressively align target distributions. First,\nunlabeled images are partitioned into reliable and unreliable subsets through\nentropy-similarity analysis, allowing adaptation to start from easy samples and\ngradually incorporate harder ones. Next, pseudo-labels are refined via Monte\nCarlo-based denoising masks, which suppress unreliable pixels and stabilize\ntraining. Finally, intra- and inter-domain objectives mix patches between\nsubsets, transferring reliable semantics while mitigating noise. Experiments on\nbenchmark datasets show consistent gains over prior SFDA and UDA methods,\ndelivering more accurate boundary delineation and achieving state-of-the-art\nDice and ASSD scores. Our study highlights the importance of progressive\nadaptation and denoised supervision for robust segmentation under domain shift.\n","authors":["Quang-Khai Bui-Tran","Thanh-Huy Nguyen","Hoang-Thien Nguyen","Ba-Thinh Lam","Nguyen Lan Vi Vu","Phat K. Huynh","Ulas Bagci","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25227v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2508.12015v2","updated":"2025-10-29T07:05:00Z","published":"2025-08-16T11:17:31Z","title":"InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes","summary":"  Reconstructing dynamic driving scenes from dashcam videos has attracted\nincreasing attention due to its significance in autonomous driving and scene\nunderstanding. While recent advances have made impressive progress, most\nmethods still unify all background elements into a single representation,\nhindering both instance-level understanding and flexible scene editing. Some\napproaches attempt to lift 2D segmentation into 3D space, but often rely on\npre-processed instance IDs or complex pipelines to map continuous features to\ndiscrete identities. Moreover, these methods are typically designed for indoor\nscenes with rich viewpoints, making them less applicable to outdoor driving\nscenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian\nSplatting framework tailored for the interactive reconstruction of dynamic\ndriving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D\nfeature learning via contrastive loss and pseudo-supervised objectives. At the\n3D level, we introduce regularization to implicitly encode instance identities\nand enforce consistency through a voxel-based loss. A lightweight static\ncodebook further bridges continuous features and discrete identities without\nrequiring data pre-processing or complex optimization. Quantitative and\nqualitative experiments demonstrate the effectiveness of InstDrive, and to the\nbest of our knowledge, it is the first framework to achieve 3D instance\nsegmentation in dynamic, open-world driving scenes.More visualizations are\navailable at our project page.\n","authors":["Hongyuan Liu","Haochen Yu","Bochao Zou","Jianfei Jiang","Qiankun Liu","Jiansheng Chen","Huimin Ma"],"pdf_url":"https://arxiv.org/pdf/2508.12015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.17732v2","updated":"2025-10-29T07:04:04Z","published":"2025-04-24T16:46:32Z","title":"DPMambaIR: All-in-One Image Restoration via Degradation-Aware Prompt\n  State Space Model","summary":"  All-in-One image restoration aims to address multiple image degradation\nproblems using a single model, offering a more practical and versatile solution\ncompared to designing dedicated models for each degradation type. Existing\napproaches typically rely on Degradation-specific models or coarse-grained\ndegradation prompts to guide image restoration. However, they lack fine-grained\nmodeling of degradation information and face limitations in balancing\nmulti-task conflicts. To overcome these limitations, we propose DPMambaIR, a\nnovel All-in-One image restoration framework that introduces a fine-grained\ndegradation extractor and a Degradation-Aware Prompt State Space Model\n(DP-SSM). The DP-SSM leverages the fine-grained degradation features captured\nby the extractor as dynamic prompts, which are then incorporated into the state\nspace modeling process. This enhances the model's adaptability to diverse\ndegradation types, while a complementary High-Frequency Enhancement Block (HEB)\nrecovers local high-frequency details. Extensive experiments on a mixed dataset\ncontaining seven degradation types show that DPMambaIR achieves the best\nperformance, with 27.69dB and 0.893 in PSNR and SSIM, respectively. These\nresults highlight the potential and superiority of DPMambaIR as a unified\nsolution for All-in-One image restoration.\n","authors":["Zhanwen Liu","Sai Zhou","Yuchao Dai","Yang Wang","Yisheng An","Xiangmo Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.17732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25221v1","updated":"2025-10-29T06:56:30Z","published":"2025-10-29T06:56:30Z","title":"MSF-Net: Multi-Stage Feature Extraction and Fusion for Robust\n  Photometric Stereo","summary":"  Photometric stereo is a technique aimed at determining surface normals\nthrough the utilization of shading cues derived from images taken under\ndifferent lighting conditions. However, existing learning-based approaches\noften fail to accurately capture features at multiple stages and do not\nadequately promote interaction between these features. Consequently, these\nmodels tend to extract redundant features, especially in areas with intricate\ndetails such as wrinkles and edges. To tackle these issues, we propose MSF-Net,\na novel framework for extracting information at multiple stages, paired with\nselective update strategy, aiming to extract high-quality feature information,\nwhich is critical for accurate normal construction. Additionally, we have\ndeveloped a feature fusion module to improve the interplay among different\nfeatures. Experimental results on the DiLiGenT benchmark show that our proposed\nMSF-Net significantly surpasses previous state-of-the-art methods in the\naccuracy of surface normal estimation.\n","authors":["Shiyu Qin","Zhihao Cai","Kaixuan Wang","Lin Qi","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2510.25221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.16729v2","updated":"2025-10-29T06:53:04Z","published":"2025-10-19T06:45:37Z","title":"Vision-Centric 4D Occupancy Forecasting and Planning via Implicit\n  Residual World Models","summary":"  End-to-end autonomous driving systems increasingly rely on vision-centric\nworld models to understand and predict their environment. However, a common\nineffectiveness in these models is the full reconstruction of future scenes,\nwhich expends significant capacity on redundantly modeling static backgrounds.\nTo address this, we propose IR-WM, an Implicit Residual World Model that\nfocuses on modeling the current state and evolution of the world. IR-WM first\nestablishes a robust bird's-eye-view representation of the current state from\nthe visual observation. It then leverages the BEV features from the previous\ntimestep as a strong temporal prior and predicts only the \"residual\", i.e., the\nchanges conditioned on the ego-vehicle's actions and scene context. To\nalleviate error accumulation over time, we further apply an alignment module to\ncalibrate semantic and dynamic misalignments. Moreover, we investigate\ndifferent forecasting-planning coupling schemes and demonstrate that the\nimplicit future state generated by world models substantially improves planning\naccuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D\noccupancy forecasting and trajectory planning.\n","authors":["Jianbiao Mei","Yu Yang","Xuemeng Yang","Licheng Wen","Jiajun Lv","Botian Shi","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.16729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25210v1","updated":"2025-10-29T06:20:21Z","published":"2025-10-29T06:20:21Z","title":"U-CAN: Unsupervised Point Cloud Denoising with Consistency-Aware\n  Noise2Noise Matching","summary":"  Point clouds captured by scanning sensors are often perturbed by noise, which\nhave a highly negative impact on downstream tasks (e.g. surface reconstruction\nand shape understanding). Previous works mostly focus on training neural\nnetworks with noisy-clean point cloud pairs for learning denoising priors,\nwhich requires extensively manual efforts. In this work, we introduce U-CAN, an\nUnsupervised framework for point cloud denoising with Consistency-Aware\nNoise2Noise matching. Specifically, we leverage a neural network to infer a\nmulti-step denoising path for each point of a shape or scene with a noise to\nnoise matching scheme. We achieve this by a novel loss which enables\nstatistical reasoning on multiple noisy point cloud observations. We further\nintroduce a novel constraint on the denoised geometry consistency for learning\nconsistency-aware denoising patterns. We justify that the proposed constraint\nis a general term which is not limited to 3D domain and can also contribute to\nthe area of 2D image denoising. Our evaluations under the widely used\nbenchmarks in point cloud denoising, upsampling and image denoising show\nsignificant improvement over the state-of-the-art unsupervised methods, where\nU-CAN also produces comparable results with the supervised methods.\n","authors":["Junsheng Zhou","Xingyu Shi","Haichuan Song","Yi Fang","Yu-Shen Liu","Zhizhong Han"],"pdf_url":"https://arxiv.org/pdf/2510.25210v1.pdf","comment":"Accepted by NeurIPS 2025. Project page:\n  https://gloriasze.github.io/U-CAN/"},{"id":"http://arxiv.org/abs/2505.19028v4","updated":"2025-10-29T06:12:40Z","published":"2025-05-25T08:28:03Z","title":"InfoChartQA: A Benchmark for Multimodal Question Answering on\n  Infographic Charts","summary":"  Understanding infographic charts with design-driven visual elements (e.g.,\npictograms, icons) requires both visual recognition and reasoning, posing\nchallenges for multimodal large language models (MLLMs). However, existing\nvisual-question answering benchmarks fall short in evaluating these\ncapabilities of MLLMs due to the lack of paired plain charts and\nvisual-element-based questions. To bridge this gap, we introduce InfoChartQA, a\nbenchmark for evaluating MLLMs on infographic chart understanding. It includes\n5,642 pairs of infographic and plain charts, each sharing the same underlying\ndata but differing in visual presentations. We further design\nvisual-element-based questions to capture their unique visual designs and\ncommunicative intent. Evaluation of 20 MLLMs reveals a substantial performance\ndecline on infographic charts, particularly for visual-element-based questions\nrelated to metaphors. The paired infographic and plain charts enable\nfine-grained error analysis and ablation studies, which highlight new\nopportunities for advancing MLLMs in infographic chart understanding. We\nrelease InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.\n","authors":["Tianchi Xie","Minzhi Lin","Mengchen Liu","Yilin Ye","Changjian Chen","Shixia Liu"],"pdf_url":"https://arxiv.org/pdf/2505.19028v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25199v1","updated":"2025-10-29T06:09:17Z","published":"2025-10-29T06:09:17Z","title":"AI-Powered Early Detection of Critical Diseases using Image Processing\n  and Audio Analysis","summary":"  Early diagnosis of critical diseases can significantly improve patient\nsurvival and reduce treatment costs. However, existing diagnostic techniques\nare often costly, invasive, and inaccessible in low-resource regions. This\npaper presents a multimodal artificial intelligence (AI) diagnostic framework\nintegrating image analysis, thermal imaging, and audio signal processing for\nearly detection of three major health conditions: skin cancer, vascular blood\nclots, and cardiopulmonary abnormalities. A fine-tuned MobileNetV2\nconvolutional neural network was trained on the ISIC 2019 dataset for skin\nlesion classification, achieving 89.3% accuracy, 91.6% sensitivity, and 88.2%\nspecificity. A support vector machine (SVM) with handcrafted features was\nemployed for thermal clot detection, achieving 86.4% accuracy (AUC = 0.89) on\nsynthetic and clinical data. For cardiopulmonary analysis, lung and heart sound\ndatasets from PhysioNet and Pascal were processed using Mel-Frequency Cepstral\nCoefficients (MFCC) and classified via Random Forest, reaching 87.2% accuracy\nand 85.7% sensitivity. Comparative evaluation against state-of-the-art models\ndemonstrates that the proposed system achieves competitive results while\nremaining lightweight and deployable on low-cost devices. The framework\nprovides a promising step toward scalable, real-time, and accessible AI-based\npre-diagnostic healthcare solutions.\n","authors":["Manisha More","Kavya Bhand","Kaustubh Mukdam","Kavya Sharma","Manas Kawtikwar","Hridayansh Kaware","Prajwal Kavhar"],"pdf_url":"https://arxiv.org/pdf/2510.25199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19638v3","updated":"2025-10-29T06:04:58Z","published":"2025-05-26T07:55:49Z","title":"HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and\n  Semantic Alignment","summary":"  Virtual try-on technology has become increasingly important in the fashion\nand retail industries, enabling the generation of high-fidelity garment images\nthat adapt seamlessly to target human models. While existing methods have\nachieved notable progress, they still face significant challenges in\nmaintaining consistency across different poses. Specifically, geometric\ndistortions lead to a lack of spatial consistency, mismatches in garment\nstructure and texture across poses result in semantic inconsistency, and the\nloss or distortion of fine-grained details diminishes visual fidelity. To\naddress these challenges, we propose HF-VTON, a novel framework that ensures\nhigh-fidelity virtual try-on performance across diverse poses. HF-VTON consists\nof three key modules: (1) the Appearance-Preserving Warp Alignment Module\n(APWAM), which aligns garments to human poses, addressing geometric\ndeformations and ensuring spatial consistency; (2) the Semantic Representation\nand Comprehension Module (SRCM), which captures fine-grained garment attributes\nand multi-pose data to enhance semantic representation, maintaining structural,\ntextural, and pattern consistency; and (3) the Multimodal Prior-Guided\nAppearance Generation Module (MPAGM), which integrates multimodal features and\nprior knowledge from pre-trained models to optimize appearance generation,\nensuring both semantic and geometric consistency. Additionally, to overcome\ndata limitations in existing benchmarks, we introduce the SAMP-VTONS dataset,\nfeaturing multi-pose pairs and rich textual annotations for a more\ncomprehensive evaluation. Experimental results demonstrate that HF-VTON\noutperforms state-of-the-art methods on both VITON-HD and SAMP-VTONS, excelling\nin visual fidelity, semantic consistency, and detail preservation.\n","authors":["Ming Meng","Qi Dong","Jiajie Li","Zhe Zhu","Xingyu Wang","Zhaoxin Fan","Wei Zhao","Wenjun Wu"],"pdf_url":"https://arxiv.org/pdf/2505.19638v3.pdf","comment":"After the publication of the paper, we discovered some significant\n  errors/omissions that need to be corrected and improved"},{"id":"http://arxiv.org/abs/2508.08549v3","updated":"2025-10-29T05:58:05Z","published":"2025-08-12T01:33:30Z","title":"Diverse Teaching and Label Propagation for Generic Semi-Supervised\n  Medical Image Segmentation","summary":"  Both limited annotation and domain shift are significant challenges\nfrequently encountered in medical image segmentation, leading to derivative\nscenarios like semi-supervised medical (SSMIS), semi-supervised medical domain\ngeneralization (Semi-MDG) and unsupervised medical domain adaptation (UMDA).\nConventional methods are generally tailored to specific tasks in isolation, the\nerror accumulation hinders the effective utilization of unlabeled data and\nlimits further improvements, resulting in suboptimal performance when these\nissues occur. In this paper, we aim to develop a generic framework that masters\nall three tasks. We found that the key to solving the problem lies in how to\ngenerate reliable pseudo labels for the unlabeled data in the presence of\ndomain shift with labeled data and increasing the diversity of the model. To\ntackle this issue, we employ a Diverse Teaching and Label Propagation Network\n(DTLP-Net) to boosting the Generic Semi-Supervised Medical Image Segmentation.\nOur DTLP-Net involves a single student model and two diverse teacher models,\nwhich can generate reliable pseudo-labels for the student model. The first\nteacher model decouple the training process with labeled and unlabeled data,\nThe second teacher is momentum-updated periodically, thus generating reliable\nyet divers pseudo-labels. To fully utilize the information within the data, we\nadopt inter-sample and intra-sample data augmentation to learn the global and\nlocal knowledge. In addition, to further capture the voxel-level correlations,\nwe propose label propagation to enhance the model robust. We evaluate our\nproposed framework on five benchmark datasets for SSMIS, UMDA, and Semi-MDG\ntasks. The results showcase notable improvements compared to state-of-the-art\nmethods across all five settings, indicating the potential of our framework to\ntackle more challenging SSL scenarios.\n","authors":["Wei Li","Pengcheng Zhou","Linye Ma","Wenyi Zhao","Huihua Yang","Yuchen Guo"],"pdf_url":"https://arxiv.org/pdf/2508.08549v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2510.25184v1","updated":"2025-10-29T05:30:16Z","published":"2025-10-29T05:30:16Z","title":"Mask-Robust Face Verification for Online Learning via YOLOv5 and\n  Residual Networks","summary":"  In the contemporary landscape, the fusion of information technology and the\nrapid advancement of artificial intelligence have ushered school education into\na transformative phase characterized by digitization and heightened\nintelligence. Concurrently, the global paradigm shift caused by the Covid-19\npandemic has catalyzed the evolution of e-learning, accentuating its\nsignificance. Amidst these developments, one pivotal facet of the online\neducation paradigm that warrants attention is the authentication of identities\nwithin the digital learning sphere. Within this context, our study delves into\na solution for online learning authentication, utilizing an enhanced\nconvolutional neural network architecture, specifically the residual network\nmodel. By harnessing the power of deep learning, this technological approach\naims to galvanize the ongoing progress of online education, while concurrently\nbolstering its security and stability. Such fortification is imperative in\nenabling online education to seamlessly align with the swift evolution of the\neducational landscape. This paper's focal proposition involves the deployment\nof the YOLOv5 network, meticulously trained on our proprietary dataset. This\nnetwork is tasked with identifying individuals' faces culled from images\ncaptured by students' open online cameras. The resultant facial information is\nthen channeled into the residual network to extract intricate features at a\ndeeper level. Subsequently, a comparative analysis of Euclidean distances\nagainst students' face databases is performed, effectively ascertaining the\nidentity of each student.\n","authors":["Zhifeng Wang","Minghui Wang","Chunyan Zeng","Jialong Yao","Yang Yang","Hongmin Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25184v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2510.25175v1","updated":"2025-10-29T05:19:38Z","published":"2025-10-29T05:19:38Z","title":"Test-Time Adaptive Object Detection with Foundation Model","summary":"  In recent years, test-time adaptive object detection has attracted increasing\nattention due to its unique advantages in online domain adaptation, which\naligns more closely with real-world application scenarios. However, existing\napproaches heavily rely on source-derived statistical characteristics while\nmaking the strong assumption that the source and target domains share an\nidentical category space. In this paper, we propose the first foundation\nmodel-powered test-time adaptive object detection method that eliminates the\nneed for source data entirely and overcomes traditional closed-set limitations.\nSpecifically, we design a Multi-modal Prompt-based Mean-Teacher framework for\nvision-language detector-driven test-time adaptation, which incorporates text\nand visual prompt tuning to adapt both language and vision representation\nspaces on the test data in a parameter-efficient manner. Correspondingly, we\npropose a Test-time Warm-start strategy tailored for the visual prompts to\neffectively preserve the representation capability of the vision branch.\nFurthermore, to guarantee high-quality pseudo-labels in every test batch, we\nmaintain an Instance Dynamic Memory (IDM) module that stores high-quality\npseudo-labels from previous test samples, and propose two novel\nstrategies-Memory Enhancement and Memory Hallucination-to leverage IDM's\nhigh-quality instances for enhancing original predictions and hallucinating\nimages without available pseudo-labels, respectively. Extensive experiments on\ncross-corruption and cross-dataset benchmarks demonstrate that our method\nconsistently outperforms previous state-of-the-art methods, and can adapt to\narbitrary cross-domain and cross-category target data. Code is available at\nhttps://github.com/gaoyingjay/ttaod_foundation.\n","authors":["Yingjie Gao","Yanan Zhang","Zhi Cai","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2510.25175v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25174v1","updated":"2025-10-29T05:17:13Z","published":"2025-10-29T05:17:13Z","title":"Classifier Enhancement Using Extended Context and Domain Experts for\n  Semantic Segmentation","summary":"  Prevalent semantic segmentation methods generally adopt a vanilla classifier\nto categorize each pixel into specific classes.\n  Although such a classifier learns global information from the training data,\nthis information is represented by a set of fixed parameters (weights and\nbiases).\n  However, each image has a different class distribution, which prevents the\nclassifier from addressing the unique characteristics of individual images.\n  At the dataset level, class imbalance leads to segmentation results being\nbiased towards majority classes, limiting the model's effectiveness in\nidentifying and segmenting minority class regions.\n  In this paper, we propose an Extended Context-Aware Classifier (ECAC) that\ndynamically adjusts the classifier using global (dataset-level) and local\n(image-level) contextual information.\n  Specifically, we leverage a memory bank to learn dataset-level contextual\ninformation of each class, incorporating the class-specific contextual\ninformation from the current image to improve the classifier for precise pixel\nlabeling.\n  Additionally, a teacher-student network paradigm is adopted, where the domain\nexpert (teacher network) dynamically adjusts contextual information with ground\ntruth and transfers knowledge to the student network.\n  Comprehensive experiments illustrate that the proposed ECAC can achieve\nstate-of-the-art performance across several datasets, including ADE20K,\nCOCO-Stuff10K, and Pascal-Context.\n","authors":["Huadong Tang","Youpeng Zhao","Min Xu","Jun Wang","Qiang Wu"],"pdf_url":"https://arxiv.org/pdf/2510.25174v1.pdf","comment":"Accepted at IEEE TRANSACTIONS ON MULTIMEDIA (TMM)"},{"id":"http://arxiv.org/abs/2510.25173v1","updated":"2025-10-29T05:13:09Z","published":"2025-10-29T05:13:09Z","title":"$D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene\n  Reconstruction","summary":"  Recently, Gaussian Splatting (GS) has shown great potential for urban scene\nreconstruction in the field of autonomous driving. However, current urban scene\nreconstruction methods often depend on multimodal sensors as inputs,\n\\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR\npoint clouds can largely mitigate ill-posedness in reconstruction, acquiring\nsuch accurate LiDAR data is still challenging in practice: i) precise\nspatiotemporal calibration between LiDAR and other sensors is required, as they\nmay not capture data simultaneously; ii) reprojection errors arise from spatial\nmisalignment when LiDAR and cameras are mounted at different locations. To\navoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, a\nLiDAR-free urban scene reconstruction framework. In this work, we obtain\ngeometry priors that are as effective as LiDAR while being denser and more\naccurate. $\\textbf{First}$, we initialize a dense point cloud by\nback-projecting multi-view metric depth predictions. This point cloud is then\noptimized by a Progressive Pruning strategy to improve the global consistency.\n$\\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense\nmetric depth via a Depth Enhancer. Specifically, we leverage diffusion priors\nfrom a depth foundation model to enhance the depth maps rendered by Gaussians.\nIn turn, the enhanced depths provide stronger geometric constraints during\nGaussian training. $\\textbf{Finally}$, we improve the accuracy of ground\ngeometry by constraining the shape and normal attributes of Gaussians within\nroad regions. Extensive experiments on the Waymo dataset demonstrate that our\nmethod consistently outperforms state-of-the-art methods, producing more\naccurate geometry even when compared with those using ground-truth LiDAR data.\n","authors":["Kejing Xia","Jidong Jia","Ke Jin","Yucai Bai","Li Sun","Dacheng Tao","Youjian Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25166v1","updated":"2025-10-29T04:57:49Z","published":"2025-10-29T04:57:49Z","title":"A Study on Inference Latency for Vision Transformers on Mobile Devices","summary":"  Given the significant advances in machine learning techniques on mobile\ndevices, particularly in the domain of computer vision, in this work we\nquantitatively study the performance characteristics of 190 real-world vision\ntransformers (ViTs) on mobile devices. Through a comparison with 102 real-world\nconvolutional neural networks (CNNs), we provide insights into the factors that\ninfluence the latency of ViT architectures on mobile devices. Based on these\ninsights, we develop a dataset including measured latencies of 1000 synthetic\nViTs with representative building blocks and state-of-the-art architectures\nfrom two machine learning frameworks and six mobile platforms. Using this\ndataset, we show that inference latency of new ViTs can be predicted with\nsufficient accuracy for real-world applications.\n","authors":["Zhuojin Li","Marco Paolieri","Leana Golubchik"],"pdf_url":"https://arxiv.org/pdf/2510.25166v1.pdf","comment":"To appear in Springer LNICST, volume 663, Proceedings of VALUETOOLS\n  2024"},{"id":"http://arxiv.org/abs/2510.25164v1","updated":"2025-10-29T04:49:20Z","published":"2025-10-29T04:49:20Z","title":"Transformers in Medicine: Improving Vision-Language Alignment for\n  Medical Image Captioning","summary":"  We present a transformer-based multimodal framework for generating clinically\nrelevant captions for MRI scans. Our system combines a DEiT-Small vision\ntransformer as an image encoder, MediCareBERT for caption embedding, and a\ncustom LSTM-based decoder. The architecture is designed to semantically align\nimage and textual embeddings, using hybrid cosine-MSE loss and contrastive\ninference via vector similarity. We benchmark our method on the MultiCaRe\ndataset, comparing performance on filtered brain-only MRIs versus general MRI\nimages against state-of-the-art medical image captioning methods including\nBLIP, R2GenGPT, and recent transformer-based approaches. Results show that\nfocusing on domain-specific data improves caption accuracy and semantic\nalignment. Our work proposes a scalable, interpretable solution for automated\nmedical image reporting.\n","authors":["Yogesh Thakku Suresh","Vishwajeet Shivaji Hogale","Luca-Alexandru Zamfira","Anandavardhana Hegde"],"pdf_url":"https://arxiv.org/pdf/2510.25164v1.pdf","comment":"This work is to appear in the Proceedings of MICAD 2025, the 6th\n  International Conference on Medical Imaging and Computer-Aided Diagnosis"},{"id":"http://arxiv.org/abs/2510.25163v1","updated":"2025-10-29T04:49:15Z","published":"2025-10-29T04:49:15Z","title":"Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD\n  Generation","summary":"  Deep generative models, such as diffusion models, have shown promising\nprogress in image generation and audio generation via simplified continuity\nassumptions. However, the development of generative modeling techniques for\ngenerating multi-modal data, such as parametric CAD sequences, still lags\nbehind due to the challenges in addressing long-range constraints and parameter\nsensitivity. In this work, we propose a novel framework for quantitatively\nconstrained CAD generation, termed Target-Guided Bayesian Flow Network (TGBFN).\nFor the first time, TGBFN handles the multi-modality of CAD sequences (i.e.,\ndiscrete commands and continuous parameters) in a unified continuous and\ndifferentiable parameter space rather than in the discrete data space. In\naddition, TGBFN penetrates the parameter update kernel and introduces a guided\nBayesian flow to control the CAD properties. To evaluate TGBFN, we construct a\nnew dataset for quantitatively constrained CAD generation. Extensive\ncomparisons across single-condition and multi-condition constrained generation\ntasks demonstrate that TGBFN achieves state-of-the-art performance in\ngenerating high-fidelity, condition-aware CAD sequences. The code is available\nat https://github.com/scu-zwh/TGBFN.\n","authors":["Wenhao Zheng","Chenwei Sun","Wenbo Zhang","Jiancheng Lv","Xianggen Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06497v3","updated":"2025-10-29T04:35:35Z","published":"2025-03-09T07:53:19Z","title":"Evaluation of Safety Cognition Capability in Vision-Language Models for\n  Autonomous Driving","summary":"  Ensuring the safety of vision-language models (VLMs) in autonomous driving\nsystems is of paramount importance, yet existing research has largely focused\non conventional benchmarks rather than safety-critical evaluation. In this\nwork, we present SCD-Bench (Safety Cognition Driving Benchmark) a novel\nframework specifically designed to assess the safety cognition capabilities of\nVLMs within interactive driving scenarios. To address the scalability challenge\nof data annotation, we introduce ADA (Autonomous Driving Annotation), a\nsemi-automated labeling system, further refined through expert review by\nprofessionals with domain-specific knowledge in autonomous driving. To\nfacilitate scalable and consistent evaluation, we also propose an automated\nassessment pipeline leveraging large language models, which demonstrates over\n98% agreement with human expert judgments. In addressing the broader challenge\nof aligning VLMs with safety cognition in driving environments, we construct\nSCD-Training, the first large-scale dataset tailored for this task, comprising\n324.35K high-quality samples. Through extensive experiments, we show that\nmodels trained on SCD-Training exhibit marked improvements not only on\nSCD-Bench, but also on general and domain-specific benchmarks, offering a new\nperspective on enhancing safety-aware interactions in vision-language systems\nfor autonomous driving.\n","authors":["Enming Zhang","Peizhe Gong","Xingyuan Dai","Min Huang","Yisheng Lv","Qinghai Miao"],"pdf_url":"https://arxiv.org/pdf/2503.06497v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.05746v2","updated":"2025-10-29T04:32:16Z","published":"2025-09-06T15:35:37Z","title":"Depth-Aware Super-Resolution via Distance-Adaptive Variational\n  Formulation","summary":"  Single image super-resolution traditionally assumes spatially-invariant\ndegradation models, yet real-world imaging systems exhibit complex\ndistance-dependent effects including atmospheric scattering, depth-of-field\nvariations, and perspective distortions. This fundamental limitation\nnecessitates spatially-adaptive reconstruction strategies that explicitly\nincorporate geometric scene understanding for optimal performance. We propose a\nrigorous variational framework that characterizes super-resolution as a\nspatially-varying inverse problem, formulating the degradation operator as a\npseudodifferential operator with distance-dependent spectral characteristics\nthat enable theoretical analysis of reconstruction limits across depth ranges.\nOur neural architecture implements discrete gradient flow dynamics through\ncascaded residual blocks with depth-conditional convolution kernels, ensuring\nconvergence to stationary points of the theoretical energy functional while\nincorporating learned distance-adaptive regularization terms that dynamically\nadjust smoothness constraints based on local geometric structure. Spectral\nconstraints derived from atmospheric scattering theory prevent bandwidth\nviolations and noise amplification in far-field regions, while adaptive kernel\ngeneration networks learn continuous mappings from depth to reconstruction\nfilters. Comprehensive evaluation across five benchmark datasets demonstrates\nstate-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM\nat 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by\n0.44dB and 0.36dB respectively. This work establishes the first\ntheoretically-grounded distance-adaptive super-resolution framework and\ndemonstrates significant improvements on depth-variant scenarios while\nmaintaining competitive performance across traditional benchmarks.\n","authors":["Tianhao Guo","Bingjie Lu","Feng Wang","Zhengyang Lu"],"pdf_url":"https://arxiv.org/pdf/2509.05746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25157v1","updated":"2025-10-29T04:19:52Z","published":"2025-10-29T04:19:52Z","title":"Towards Real-Time Inference of Thin Liquid Film Thickness Profiles from\n  Interference Patterns Using Vision Transformers","summary":"  Thin film interferometry is a powerful technique for non-invasively measuring\nliquid film thickness with applications in ophthalmology, but its clinical\ntranslation is hindered by the challenges in reconstructing thickness profiles\nfrom interference patterns - an ill-posed inverse problem complicated by phase\nperiodicity, imaging noise and ambient artifacts. Traditional reconstruction\nmethods are either computationally intensive, sensitive to noise, or require\nmanual expert analysis, which is impractical for real-time diagnostics. To\naddress this challenge, here we present a vision transformer-based approach for\nreal-time inference of thin liquid film thickness profiles directly from\nisolated interferograms. Trained on a hybrid dataset combining\nphysiologically-relevant synthetic and experimental tear film data, our model\nleverages long-range spatial correlations to resolve phase ambiguities and\nreconstruct temporally coherent thickness profiles in a single forward pass\nfrom dynamic interferograms acquired in vivo and ex vivo. The network\ndemonstrates state-of-the-art performance on noisy, rapidly-evolving films with\nmotion artifacts, overcoming limitations of conventional phase-unwrapping and\niterative fitting methods. Our data-driven approach enables automated,\nconsistent thickness reconstruction at real-time speeds on consumer hardware,\nopening new possibilities for continuous monitoring of pre-lens ocular tear\nfilms and non-invasive diagnosis of conditions such as the dry eye disease.\n","authors":["Gautam A. Viruthagiri","Arnuv Tandon","Gerald G. Fuller","Vinny Chandran Suja"],"pdf_url":"https://arxiv.org/pdf/2510.25157v1.pdf","comment":"6 pages, 2 figures, will be updated"},{"id":"http://arxiv.org/abs/2510.23807v2","updated":"2025-10-29T04:15:53Z","published":"2025-10-27T19:44:52Z","title":"Why Foundation Models in Pathology Are Failing","summary":"  In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.\n","authors":["Hamid R. Tizhoosh"],"pdf_url":"https://arxiv.org/pdf/2510.23807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22946v2","updated":"2025-10-29T04:07:45Z","published":"2025-10-27T02:59:57Z","title":"LightBagel: A Light-weighted, Double Fusion Framework for Unified\n  Multimodal Understanding and Generation","summary":"  Unified multimodal models have recently shown remarkable gains in both\ncapability and versatility, yet most leading systems are still trained from\nscratch and require substantial computational resources. In this paper, we show\nthat competitive performance can be obtained far more efficiently by\nstrategically fusing publicly available models specialized for either\ngeneration or understanding. Our key design is to retain the original blocks\nwhile additionally interleaving multimodal self-attention blocks throughout the\nnetworks. This double fusion mechanism (1) effectively enables rich multi-modal\nfusion while largely preserving the original strengths of the base models, and\n(2) catalyzes synergistic fusion of high-level semantic representations from\nthe understanding encoder with low-level spatial signals from the generation\nencoder. By training with only ~ 35B tokens, this approach achieves strong\nresults across multiple benchmarks: 0.91 on GenEval for compositional\ntext-to-image generation, 82.16 on DPG-Bench for complex text-to-image\ngeneration, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By\nfully releasing the entire suite of code, model weights, and datasets, we hope\nto support future research on unified multimodal modeling.\n","authors":["Zeyu Wang","Zilong Chen","Chenhui Gou","Feng Li","Chaorui Deng","Deyao Zhu","Kunchang Li","Weihao Yu","Haoqin Tu","Haoqi Fan","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2510.22946v2.pdf","comment":"Withdrawn because the submission was premature and not agreed by all\n  parties in collaboration"},{"id":"http://arxiv.org/abs/2505.03318v3","updated":"2025-10-29T04:02:02Z","published":"2025-05-06T08:46:41Z","title":"Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning","summary":"  Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.\n","authors":["Yibin Wang","Zhimin Li","Yuhang Zang","Chunyu Wang","Qinglin Lu","Cheng Jin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03318v3.pdf","comment":"[NeurIPS2025] Project Page:\n  https://codegoat24.github.io/UnifiedReward/think"},{"id":"http://arxiv.org/abs/2510.25146v1","updated":"2025-10-29T03:56:41Z","published":"2025-10-29T03:56:41Z","title":"EA3D: Online Open-World 3D Object Extraction from Streaming Videos","summary":"  Current 3D scene understanding methods are limited by offline-collected\nmulti-view data or pre-constructed 3D geometry. In this paper, we present\nExtractAnything3D (EA3D), a unified online framework for open-world 3D object\nextraction that enables simultaneous geometric reconstruction and holistic\nscene understanding. Given a streaming video, EA3D dynamically interprets each\nframe using vision-language and 2D vision foundation encoders to extract\nobject-level knowledge. This knowledge is integrated and embedded into a\nGaussian feature map via a feed-forward online update strategy. We then\niteratively estimate visual odometry from historical frames and incrementally\nupdate online Gaussian features with new observations. A recurrent joint\noptimization module directs the model's attention to regions of interest,\nsimultaneously enhancing both geometric reconstruction and semantic\nunderstanding. Extensive experiments across diverse benchmarks and tasks,\nincluding photo-realistic rendering, semantic and instance segmentation, 3D\nbounding box and semantic occupancy estimation, and 3D mesh generation,\ndemonstrate the effectiveness of EA3D. Our method establishes a unified and\nefficient framework for joint online 3D reconstruction and holistic scene\nunderstanding, enabling a broad range of downstream tasks.\n","authors":["Xiaoyu Zhou","Jingqi Wang","Yuang Jia","Yongtao Wang","Deqing Sun","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2510.25146v1.pdf","comment":"The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems(NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.25141v1","updated":"2025-10-29T03:45:03Z","published":"2025-10-29T03:45:03Z","title":"Revisiting Reconstruction-based AI-generated Image Detection: A\n  Geometric Perspective","summary":"  The rise of generative Artificial Intelligence (AI) has made detecting\nAI-generated images a critical challenge for ensuring authenticity. Existing\nreconstruction-based methods lack theoretical foundations and on empirical\nheuristics, limiting interpretability and reliability. In this paper, we\nintroduce the Jacobian-Spectral Lower Bound for reconstruction error from a\ngeometric perspective, showing that real images off the reconstruction manifold\nexhibit a non-trivial error lower bound, while generated images on the manifold\nhave near-zero error. Furthermore, we reveal the limitations of existing\nmethods that rely on static reconstruction error from a single pass. These\nmethods often fail when some real images exhibit lower error than generated\nones. This counterintuitive behavior reduces detection accuracy and requires\ndata-specific threshold tuning, limiting their applicability in real-world\nscenarios. To address these challenges, we propose ReGap, a training-free\nmethod that computes dynamic reconstruction error by leveraging structured\nediting operations to introduce controlled perturbations. This enables\nmeasuring error changes before and after editing, improving detection accuracy\nby enhancing error separation. Experimental results show that our method\noutperforms existing baselines, exhibits robustness to common post-processing\noperations and generalizes effectively across diverse conditions.\n","authors":["Wan Jiang","Jing Yan","Ruixuan Zhang","Xiaojing Chen","Changtao Miao","Zhe Li","Chenhao Lin","Yunfeng Diao","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2510.25141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25801v1","updated":"2025-10-29T03:42:23Z","published":"2025-10-29T03:42:23Z","title":"Metis-SPECS: Decoupling Multimodal Learning via Self-distilled\n  Preference-based Cold Start","summary":"  Reinforcement learning (RL) with verifiable rewards has recently catalyzed a\nwave of \"MLLM-r1\" approaches that bring RL to vision language models. Most\nrepresentative paradigms begin with a cold start, typically employing\nsupervised fine-tuning (SFT), to initialize the policy before RL. However,\nSFT-based cold start adopts the reasoning paradigm intertwined with task\nsolution and output format, which may induce instruction-style overfitting,\nweakens out-of-distribution generalization, and ultimately affects downstream\nRL. We revisit the cold start along two views, its training method and data\nconstruction, and introduce the Generalization Factor (GF) coefficient to\nquantify the generalization capability under different methods. Our empirical\nstudy finds that preference-based training methods (e.g. DPO) generalizes\nbetter than SFT-based methods in cold start. Motivated by this, we propose\nSPECS-a Self-distilled, Preference-based Cold Start framework that decouples\nmultimodal learning: (1) generates introspective preference data pairs via\nself-distillation, avoiding reliance on larger teachers or manual annotation;\n(2) performs preference-based training to learn, focusing on shallow,\ntransferable surface-form criteria (format, structure, style) rather than\nmemorizing content; and (3) hands off to RL with verifiable rewards for deep\nreasoning results. Experimental results across multiple multimodal benchmarks\nshow that our decoupling learning framework yields consistent performance gains\nover strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.\nAdditional experiments indicate that SPECS contributes to reducing\nin-distribution \"stuckness,\" improving exploration, stabilizing training, and\nraising the performance ceiling.\n","authors":["Kun Chen","Peng Shi","Haibo Qiu","Zhixiong Zeng","Siqi Yang","Wenji Mao","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2510.25801v1.pdf","comment":"Project Page: https://github.com/Kwen-Chen/SPECS-VL"},{"id":"http://arxiv.org/abs/2510.25140v1","updated":"2025-10-29T03:40:40Z","published":"2025-10-29T03:40:40Z","title":"DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object\n  Detection in Civil Engineering Applications","summary":"  Object detection in civil engineering applications is constrained by limited\nannotated data in specialized domains. We introduce DINO-YOLO, a hybrid\narchitecture combining YOLOv12 with DINOv3 self-supervised vision transformers\nfor data-efficient detection. DINOv3 features are strategically integrated at\ntwo locations: input preprocessing (P0) and mid-backbone enhancement (P3).\nExperimental validation demonstrates substantial improvements: Tunnel Segment\nCrack detection (648 images) achieves 12.4% improvement, Construction PPE (1K\nimages) gains 13.7%, and KITTI (7K images) shows 88.6% improvement, while\nmaintaining real-time inference (30-47 FPS). Systematic ablation across five\nYOLO scales and nine DINOv3 variants reveals that Medium-scale architectures\nachieve optimal performance with DualP0P3 integration (55.77% mAP@0.5), while\nSmall-scale requires Triple Integration (53.63%). The 2-4x inference overhead\n(21-33ms versus 8-16ms baseline) remains acceptable for field deployment on\nNVIDIA RTX 5090. DINO-YOLO establishes state-of-the-art performance for civil\nengineering datasets (<10K images) while preserving computational efficiency,\nproviding practical solutions for construction safety monitoring and\ninfrastructure inspection in data-constrained environments.\n","authors":["Malaisree P","Youwai S","Kitkobsin T","Janrungautai S","Amorndechaphon D","Rojanavasu P"],"pdf_url":"https://arxiv.org/pdf/2510.25140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06677v2","updated":"2025-10-29T03:38:36Z","published":"2025-06-07T06:15:49Z","title":"RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic\n  Manipulation Evaluation","summary":"  Recent advances in vision-language models (VLMs) have enabled\ninstruction-conditioned robotic systems with improved generalization. However,\nmost existing work focuses on reactive System 1 policies, underutilizing VLMs'\nstrengths in semantic reasoning and long-horizon planning. These System 2\ncapabilities-characterized by deliberative, goal-directed thinking-remain under\nexplored due to the limited temporal scale and structural complexity of current\nbenchmarks. To address this gap, we introduce RoboCerebra, a benchmark for\nevaluating high-level reasoning in long-horizon robotic manipulation.\nRoboCerebra includes: (1) a large-scale simulation dataset with extended task\nhorizons and diverse subtask sequences in household environments; (2) a\nhierarchical framework combining a high-level VLM planner with a low-level\nvision-language-action (VLA) controller; and (3) an evaluation protocol\ntargeting planning, reflection, and memory through structured System 1-System 2\ninteraction. The dataset is constructed via a top-down pipeline, where GPT\ngenerates task instructions and decomposes them into subtask sequences. Human\noperators execute the subtasks in simulation, yielding high-quality\ntrajectories with dynamic object variations. Compared to prior benchmarks,\nRoboCerebra features significantly longer action sequences and denser\nannotations. We further benchmark state-of-the-art VLMs as System 2 modules and\nanalyze their performance across key cognitive dimensions, advancing the\ndevelopment of more capable and generalizable robotic planners.\n","authors":["Songhao Han","Boxiang Qiu","Yue Liao","Siyuan Huang","Chen Gao","Shuicheng Yan","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2506.06677v2.pdf","comment":"25 pages, 18 figures, Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25134v1","updated":"2025-10-29T03:28:18Z","published":"2025-10-29T03:28:18Z","title":"Region-CAM: Towards Accurate Object Regions in Class Activation Maps for\n  Weakly Supervised Learning Tasks","summary":"  Class Activation Mapping (CAM) methods are widely applied in weakly\nsupervised learning tasks due to their ability to highlight object regions.\nHowever, conventional CAM methods highlight only the most discriminative\nregions of the target. These highlighted regions often fail to cover the entire\nobject and are frequently misaligned with object boundaries, thereby limiting\nthe performance of downstream weakly supervised learning tasks, particularly\nWeakly Supervised Semantic Segmentation (WSSS), which demands pixel-wise\naccurate activation maps to get the best results. To alleviate the above\nproblems, we propose a novel activation method, Region-CAM. Distinct from\nnetwork feature weighting approaches, Region-CAM generates activation maps by\nextracting semantic information maps (SIMs) and performing semantic information\npropagation (SIP) by considering both gradients and features in each of the\nstages of the baseline classification model. Our approach highlights a greater\nproportion of object regions while ensuring activation maps to have precise\nboundaries that align closely with object edges. Region-CAM achieves 60.12% and\n58.43% mean intersection over union (mIoU) using the baseline model on the\nPASCAL VOC training and validation datasets, respectively, which are\nimprovements of 13.61% and 13.13% over the original CAM (46.51% and 45.30%). On\nthe MS COCO validation set, Region-CAM achieves 36.38%, a 16.23% improvement\nover the original CAM (20.15%). We also demonstrate the superiority of\nRegion-CAM in object localization tasks, using the ILSVRC2012 validation set.\nRegion-CAM achieves 51.7% in Top-1 Localization accuracy Loc1. Compared with\nLayerCAM, an activation method designed for weakly supervised object\nlocalization, Region-CAM achieves 4.5% better performance in Loc1.\n","authors":["Qingdong Cai","Charith Abhayaratne"],"pdf_url":"https://arxiv.org/pdf/2510.25134v1.pdf","comment":"Preprint for journal paper"},{"id":"http://arxiv.org/abs/2510.25129v1","updated":"2025-10-29T03:17:58Z","published":"2025-10-29T03:17:58Z","title":"AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit\n  Structured Gaussians","summary":"  3D reconstruction of indoor and urban environments is a prominent research\ntopic with various downstream applications. However, existing geometric priors\nfor addressing low-texture regions in indoor and urban settings often lack\nglobal consistency. Moreover, Gaussian Splatting and implicit SDF fields often\nsuffer from discontinuities or exhibit computational inefficiencies, resulting\nin a loss of detail. To address these issues, we propose an Atlanta-world\nguided implicit-structured Gaussian Splatting that achieves smooth indoor and\nurban scene reconstruction while preserving high-frequency details and\nrendering efficiency. By leveraging the Atlanta-world model, we ensure the\naccurate surface reconstruction for low-texture regions, while the proposed\nnovel implicit-structured GS representations provide smoothness without\nsacrificing efficiency and high-frequency details. Specifically, we propose a\nsemantic GS representation to predict the probability of all semantic regions\nand deploy a structure plane regularization with learnable plane indicators for\nglobal accurate surface reconstruction. Extensive experiments demonstrate that\nour method outperforms state-of-the-art approaches in both indoor and urban\nscenes, delivering superior surface reconstruction quality.\n","authors":["Xiyu Zhang","Chong Bao","Yipeng Chen","Hongjia Zhai","Yitong Dong","Hujun Bao","Zhaopeng Cui","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.25129v1.pdf","comment":"18 pages, 11 figures. NeurIPS 2025; Project page:\n  https://zju3dv.github.io/AtlasGS/"},{"id":"http://arxiv.org/abs/2503.11245v4","updated":"2025-10-29T03:13:22Z","published":"2025-03-14T09:52:54Z","title":"L2RSI: Cross-view LiDAR-based Place Recognition for Large-scale Urban\n  Scenes via Remote Sensing Imagery","summary":"  We tackle the challenge of LiDAR-based place recognition, which traditionally\ndepends on costly and time-consuming prior 3D maps. To overcome this, we first\nconstruct LiRSI-XA dataset, which encompasses approximately $110,000$ remote\nsensing submaps and $13,000$ LiDAR point cloud submaps captured in urban\nscenes, and propose a novel method, L2RSI, for cross-view LiDAR place\nrecognition using high-resolution Remote Sensing Imagery. This approach enables\nlarge-scale localization capabilities at a reduced cost by leveraging readily\navailable overhead images as map proxies. L2RSI addresses the dual challenges\nof cross-view and cross-modal place recognition by learning feature alignment\nbetween point cloud submaps and remote sensing submaps in the semantic domain.\nAdditionally, we introduce a novel probability propagation method based on\nparticle estimation to refine position predictions, effectively leveraging\ntemporal and spatial information. This approach enables large-scale retrieval\nand cross-scene generalization without fine-tuning. Extensive experiments on\nLiRSI-XA demonstrate that, within a $100km^2$ retrieval range, L2RSI accurately\nlocalizes $83.27\\%$ of point cloud submaps within a $30m$ radius for top-$1$\nretrieved location. Our project page is publicly available at\nhttps://shizw695.github.io/L2RSI/.\n","authors":["Ziwei Shi","Xiaoran Zhang","Wenjing Xu","Yan Xia","Yu Zang","Siqi Shen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2503.11245v4.pdf","comment":"17 pages, 7 figures, NeurIPS 2025"},{"id":"http://arxiv.org/abs/2509.22689v2","updated":"2025-10-29T02:47:25Z","published":"2025-09-19T04:41:03Z","title":"Graph-Theoretic Consistency for Robust and Topology-Aware\n  Semi-Supervised Histopathology Segmentation","summary":"  Semi-supervised semantic segmentation (SSSS) is vital in computational\npathology, where dense annotations are costly and limited. Existing methods\noften rely on pixel-level consistency, which propagates noisy pseudo-labels and\nproduces fragmented or topologically invalid masks. We propose Topology Graph\nConsistency (TGC), a framework that integrates graph-theoretic constraints by\naligning Laplacian spectra, component counts, and adjacency statistics between\nprediction graphs and references. This enforces global topology and improves\nsegmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC\nachieves state-of-the-art performance under 5-10% supervision and significantly\nnarrows the gap to full supervision.\n","authors":["Ha-Hieu Pham","Minh Le","Han Huynh","Nguyen Quoc Khanh Le","Huy-Hieu Pham"],"pdf_url":"https://arxiv.org/pdf/2509.22689v2.pdf","comment":"Accepted to the AAAI 2026 Student Abstract and Poster Program"},{"id":"http://arxiv.org/abs/2405.04605v4","updated":"2025-10-29T02:33:21Z","published":"2024-05-07T18:36:40Z","title":"AI in Lung Health: Benchmarking Detection and Diagnostic Models Across\n  Multiple CT Scan Datasets","summary":"  Background: Development of artificial intelligence (AI) models for lung\ncancer screening requires large, well-annotated low-dose computed tomography\n(CT) datasets and rigorous performance benchmarks. Purpose: To create a\nreproducible benchmarking resource leveraging the Duke Lung Cancer Screening\n(DLCS) and multiple public datasets to develop and evaluate models for nodule\ndetection and classification. Materials & Methods: This retrospective study\nuses the DLCS dataset (1,613 patients; 2,487 nodules) and external datasets\nincluding LUNA16, LUNA25, and NLST-3D. For detection, MONAI RetinaNet models\nwere trained on DLCS (DLCS-De) and LUNA16 (LUNA16-De) and evaluated using the\nCompetition Performance Metric (CPM). For nodule-level classification, we\ncompare five strategies: pretrained models (Models Genesis, Med3D), a\nself-supervised foundation model (FMCB), and ResNet50 with random\ninitialization versus Strategic Warm-Start (ResNet50-SWS) pretrained with\ndetection-derived candidate patches stratified by confidence. Results: For\ndetection on the DLCS test set, DLCS-De achieved sensitivity 0.82 at 2 false\npositives/scan (CPM 0.63) versus LUNA16-De (0.62, CPM 0.45). For external\nvalidation on NLST-3D, DLCS-De (sensitivity 0.72, CPM 0.58) also outperformed\nLUNA16-De (sensitivity 0.64, CPM 0.49). For classification across multiple\ndatasets, ResNet50-SWS attained AUCs of 0.71 (DLCS; 95% CI, 0.61-0.81), 0.90\n(LUNA16; 0.87-0.93), 0.81 (NLST-3D; 0.79-0.82), and 0.80 (LUNA25; 0.78-0.82),\nmatching or exceeding pretrained/self-supervised baselines. Performance\ndifferences reflected dataset label standards. Conclusion: This work\nestablishes a standardized benchmarking resource for lung cancer AI research,\nsupporting model development, validation, and translation. All code, models,\nand data are publicly released to promote reproducibility.\n","authors":["Fakrul Islam Tushar","Avivah Wang","Lavsen Dahal","Ehsan Samei","Michael R. Harowicz","Jayashree Kalpathy-Cramer","Kyle J. Lafata","Tina D. Tailor","Cynthia Rudin","Joseph Y. Lo"],"pdf_url":"https://arxiv.org/pdf/2405.04605v4.pdf","comment":"2 tables, 5 figures"},{"id":"http://arxiv.org/abs/2510.22035v3","updated":"2025-10-29T02:18:32Z","published":"2025-10-24T21:41:32Z","title":"Caption-Driven Explainability: Probing CNNs for Bias via CLIP","summary":"  Robustness has become one of the most critical problems in machine learning\n(ML). The science of interpreting ML models to understand their behavior and\nimprove their robustness is referred to as explainable artificial intelligence\n(XAI). One of the state-of-the-art XAI methods for computer vision problems is\nto generate saliency maps. A saliency map highlights the pixel space of an\nimage that excites the ML model the most. However, this property could be\nmisleading if spurious and salient features are present in overlapping pixel\nspaces. In this paper, we propose a caption-based XAI method, which integrates\na standalone model to be explained into the contrastive language-image\npre-training (CLIP) model using a novel network surgery approach. The resulting\ncaption-based XAI model identifies the dominant concept that contributes the\nmost to the models prediction. This explanation minimizes the risk of the\nstandalone model falling for a covariate shift and contributes significantly\ntowards developing robust ML models. Our code is available at\nhttps://github.com/patch0816/caption-driven-xai\n","authors":["Patrick Koller","Amil V. Dravid","Guido M. Schuster","Aggelos K. Katsaggelos"],"pdf_url":"https://arxiv.org/pdf/2510.22035v3.pdf","comment":"Accepted and presented at the IEEE ICIP 2025 Satellite Workshop\n  \"Generative AI for World Simulations and Communications & Celebrating 40\n  Years of Excellence in Education: Honoring Professor Aggelos Katsaggelos\",\n  Anchorage, Alaska, USA, September 14, 2025. Camera-ready preprint; the\n  official IEEE Xplore publication will follow. Code:\n  https://github.com/patch0816/caption-driven-xai"},{"id":"http://arxiv.org/abs/2510.07316v2","updated":"2025-10-29T02:15:20Z","published":"2025-10-08T17:59:33Z","title":"Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers","summary":"  This paper presents Pixel-Perfect Depth, a monocular depth estimation model\nbased on pixel-space diffusion generation that produces high-quality,\nflying-pixel-free point clouds from estimated depth maps. Current generative\ndepth estimation models fine-tune Stable Diffusion and achieve impressive\nperformance. However, they require a VAE to compress depth maps into latent\nspace, which inevitably introduces \\textit{flying pixels} at edges and details.\nOur model addresses this challenge by directly performing diffusion generation\nin the pixel space, avoiding VAE-induced artifacts. To overcome the high\ncomplexity associated with pixel-space generation, we introduce two novel\ndesigns: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which\nincorporate semantic representations from vision foundation models into DiT to\nprompt the diffusion process, thereby preserving global semantic consistency\nwhile enhancing fine-grained visual details; and 2) Cascade DiT Design that\nprogressively increases the number of tokens to further enhance efficiency and\naccuracy. Our model achieves the best performance among all published\ngenerative models across five benchmarks, and significantly outperforms all\nother models in edge-aware point cloud evaluation.\n","authors":["Gangwei Xu","Haotong Lin","Hongcheng Luo","Xianqi Wang","Jingfeng Yao","Lianghui Zhu","Yuechuan Pu","Cheng Chi","Haiyang Sun","Bing Wang","Guang Chen","Hangjun Ye","Sida Peng","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2510.07316v2.pdf","comment":"NeurIPS 2025. Project page: https://pixel-perfect-depth.github.io/"},{"id":"http://arxiv.org/abs/2510.16765v2","updated":"2025-10-29T02:07:16Z","published":"2025-10-19T09:11:58Z","title":"WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and\n  Mamba-based Channel Modeling with Texture Enhancement","summary":"  Image restoration is a fundamental and challenging task in computer vision,\nwhere CNN-based frameworks demonstrate significant computational efficiency.\nHowever, previous CNN-based methods often face challenges in adequately\nrestoring fine texture details, which are limited by the small receptive field\nof CNN structures and the lack of channel feature modeling. In this paper, we\npropose WaMaIR, which is a novel framework with a large receptive field for\nimage perception and improves the reconstruction of texture details in restored\nimages. Specifically, we introduce the Global Multiscale Wavelet Transform\nConvolutions (GMWTConvs) for expandding the receptive field to extract image\nfeatures, preserving and enriching texture features in model inputs. Meanwhile,\nwe propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to\ncapture long-range dependencies within feature channels, which enhancing the\nmodel sensitivity to color, edges, and texture information. Additionally, we\npropose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to\nguide the model in preserving detailed texture structures effectively.\nExtensive experiments confirm that WaMaIR outperforms state-of-the-art methods,\nachieving better image restoration and efficient computational performance of\nthe model.\n","authors":["Shengyu Zhu","Congyi Fan","Fuxuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.16765v2.pdf","comment":"Chinese Conference on Pattern Recognition and Computer Vision (PRCV),\n  Oral"},{"id":"http://arxiv.org/abs/2510.25094v1","updated":"2025-10-29T01:58:35Z","published":"2025-10-29T01:58:35Z","title":"Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI\n  Detection","summary":"  Zero-shot Human-Object Interaction detection aims to localize humans and\nobjects in an image and recognize their interaction, even when specific\nverb-object pairs are unseen during training. Recent works have shown promising\nresults using prompt learning with pretrained vision-language models such as\nCLIP, which align natural language prompts with visual features in a shared\nembedding space. However, existing approaches still fail to handle the visual\ncomplexity of interaction, including (1) intra-class visual diversity, where\ninstances of the same verb appear in diverse poses and contexts, and (2)\ninter-class visual entanglement, where distinct verbs yield visually similar\npatterns. To address these challenges, we propose VDRP, a framework for Visual\nDiversity and Region-aware Prompt learning. First, we introduce a visual\ndiversity-aware prompt learning strategy that injects group-wise visual\nvariance into the context embedding. We further apply Gaussian perturbation to\nencourage the prompts to capture diverse visual variations of a verb. Second,\nwe retrieve region-specific concepts from the human, object, and union regions.\nThese are used to augment the diversity-aware prompt embeddings, yielding\nregion-aware prompts that enhance verb-level discrimination. Experiments on the\nHICO-DET benchmark demonstrate that our method achieves state-of-the-art\nperformance under four zero-shot evaluation settings, effectively addressing\nboth intra-class diversity and inter-class visual entanglement. Code is\navailable at https://github.com/mlvlab/VDRP.\n","authors":["Chanhyeong Yang","Taehoon Song","Jihwan Park","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2510.25094v1.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25084v1","updated":"2025-10-29T01:42:23Z","published":"2025-10-29T01:42:23Z","title":"PSTF-AttControl: Per-Subject-Tuning-Free Personalized Image Generation\n  with Controllable Face Attributes","summary":"  Recent advancements in personalized image generation have significantly\nimproved facial identity preservation, particularly in fields such as\nentertainment and social media. However, existing methods still struggle to\nachieve precise control over facial attributes in a per-subject-tuning-free\n(PSTF) way. Tuning-based techniques like PreciseControl have shown promise by\nproviding fine-grained control over facial features, but they often require\nextensive technical expertise and additional training data, limiting their\naccessibility. In contrast, PSTF approaches simplify the process by enabling\nimage generation from a single facial input, but they lack precise control over\nfacial attributes. In this paper, we introduce a novel, PSTF method that\nenables both precise control over facial attributes and high-fidelity\npreservation of facial identity. Our approach utilizes a face recognition model\nto extract facial identity features, which are then mapped into the $W^+$\nlatent space of StyleGAN2 using the e4e encoder. We further enhance the model\nwith a Triplet-Decoupled Cross-Attention module, which integrates facial\nidentity, attribute features, and text embeddings into the UNet architecture,\nensuring clean separation of identity and attribute information. Trained on the\nFFHQ dataset, our method allows for the generation of personalized images with\nfine-grained control over facial attributes, while without requiring additional\nfine-tuning or training data for individual identities. We demonstrate that our\napproach successfully balances personalization with precise facial attribute\ncontrol, offering a more efficient and user-friendly solution for high-quality,\nadaptable facial image synthesis. The code is publicly available at\nhttps://github.com/UnicomAI/PSTF-AttControl.\n","authors":["Xiang liu","Zhaoxiang Liu","Huan Hu","Zipeng Wang","Ping Chen","Zezhou Chen","Kai Wang","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2510.25084v1.pdf","comment":"Accepted by Image and Vision Computing (18 pages, 8 figures)"},{"id":"http://arxiv.org/abs/2510.25797v1","updated":"2025-10-29T01:22:42Z","published":"2025-10-29T01:22:42Z","title":"Enhancing Underwater Object Detection through Spatio-Temporal Analysis\n  and Spatial Attention Networks","summary":"  This study examines the effectiveness of spatio-temporal modeling and the\nintegration of spatial attention mechanisms in deep learning models for\nunderwater object detection. Specifically, in the first phase, the performance\nof temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with\nthe standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is\ndeveloped, through the addition of a Convolutional Block Attention Module\n(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and\nT-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the\nresearch highlights how temporal modeling improves detection accuracy in\ndynamic marine environments, particularly under conditions of sudden movements,\npartial occlusions, and gradual motion. The testing results showed that YOLOv5\nachieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM\noutperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,\nhighlighting their superior accuracy and generalization in detecting complex\nobjects. The findings demonstrate that T-YOLOv5 significantly enhances\ndetection reliability compared to the standard model, while T-YOLOv5 with CBAM\nfurther improves performance in challenging scenarios, although there is a loss\nof accuracy when it comes to simpler scenarios.\n","authors":["Sai Likhith Karri","Ansh Saxena"],"pdf_url":"https://arxiv.org/pdf/2510.25797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16854v3","updated":"2025-10-29T01:19:12Z","published":"2025-05-22T16:13:29Z","title":"Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language Models","summary":"  Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across LLM (GSM8K), VLM (CLEVR, Super-CLEVR,\nGeoQA), and Agentic (AITZ) tasks-covering a range of reasoning difficulties\nunder both 3B and 7B models-consistently reveal that the model progressively\nlearns to bypass unnecessary reasoning steps as training advances. These\nfindings shed light on the path toward human-like reasoning patterns in RL\napproaches. Our code is available at https://github.com/kokolerk/TON.\n","authors":["Jiaqi Wang","Kevin Qinghong Lin","James Cheng","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2505.16854v3.pdf","comment":"camera ready revision"},{"id":"http://arxiv.org/abs/2510.25070v1","updated":"2025-10-29T01:16:21Z","published":"2025-10-29T01:16:21Z","title":"Vision-Language Integration for Zero-Shot Scene Understanding in\n  Real-World Environments","summary":"  Zero-shot scene understanding in real-world settings presents major\nchallenges due to the complexity and variability of natural scenes, where\nmodels must recognize new objects, actions, and contexts without prior labeled\nexamples. This work proposes a vision-language integration framework that\nunifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models\n(e.g., GPT-based architectures) to achieve semantic alignment between visual\nand textual modalities. The goal is to enable robust zero-shot comprehension of\nscenes by leveraging natural language as a bridge to generalize over unseen\ncategories and contexts. Our approach develops a unified model that embeds\nvisual inputs and textual prompts into a shared space, followed by multimodal\nfusion and reasoning layers for contextual interpretation. Experiments on\nVisual Genome, COCO, ADE20K, and custom real-world datasets demonstrate\nsignificant gains over state-of-the-art zero-shot models in object recognition,\nactivity detection, and scene captioning. The proposed system achieves up to\n18% improvement in top-1 accuracy and notable gains in semantic coherence\nmetrics, highlighting the effectiveness of cross-modal alignment and language\ngrounding in enhancing generalization for real-world scene understanding.\n","authors":["Manjunath Prasad Holenarasipura Rajiv","B. M. Vidyavathi"],"pdf_url":"https://arxiv.org/pdf/2510.25070v1.pdf","comment":"Preprint under review at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), 2025"},{"id":"http://arxiv.org/abs/2510.25067v1","updated":"2025-10-29T01:10:28Z","published":"2025-10-29T01:10:28Z","title":"DRIP: Dynamic patch Reduction via Interpretable Pooling","summary":"  Recently, the advances in vision-language models, including contrastive\npretraining and instruction tuning, have greatly pushed the frontier of\nmultimodal AI. However, owing to the large-scale and hence expensive\npretraining, the efficiency concern has discouraged researchers from attempting\nto pretrain a vision language model from scratch. In this work, we propose\nDynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the\ninput images and dynamically merges tokens in the deeper layers of a visual\nencoder. Our results on both ImageNet training from scratch and CLIP\ncontrastive pretraining demonstrate a significant GFLOP reduction while\nmaintaining comparable classification/zero-shot performance. To further\nvalidate our proposed method, we conduct continual pretraining on a large\nbiology dataset, extending its impact into scientific domains.\n","authors":["Yusen Peng","Sachin Kumar"],"pdf_url":"https://arxiv.org/pdf/2510.25067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25058v1","updated":"2025-10-29T00:49:33Z","published":"2025-10-29T00:49:33Z","title":"Auto3DSeg for Brain Tumor Segmentation from 3D MRI in BraTS 2023\n  Challenge","summary":"  In this work, we describe our solution to the BraTS 2023 cluster of\nchallenges using Auto3DSeg from MONAI. We participated in all 5 segmentation\nchallenges, and achieved the 1st place results in three of them: Brain\nMetastasis, Brain Meningioma, BraTS-Africa challenges, and the 2nd place\nresults in the remaining two: Adult and Pediatic Glioma challenges.\n","authors":["Andriy Myronenko","Dong Yang","Yufan He","Daguang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25058v1.pdf","comment":"BraTS23 winner"},{"id":"http://arxiv.org/abs/2510.25051v1","updated":"2025-10-29T00:37:18Z","published":"2025-10-29T00:37:18Z","title":"Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference\n  Models","summary":"  Breast cancer remains the most commonly diagnosed malignancy among women in\nthe developed world. Early detection through mammography screening plays a\npivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)\nsystems have shown promise in assisting radiologists, existing approaches face\ncritical limitations in clinical deployment - particularly in handling the\nnuanced interpretation of multi-modal data and feasibility due to the\nrequirement of prior clinical history. This study introduces a novel framework\nthat synergistically combines visual features from 2D mammograms with\nstructured textual descriptors derived from easily accessible clinical metadata\nand synthesized radiological reports through innovative tokenization modules.\nOur proposed methods in this study demonstrate that strategic integration of\nconvolutional neural networks (ConvNets) with language representations achieves\nsuperior performance to vision transformer-based models while handling\nhigh-resolution images and enabling practical deployment across diverse\npopulations. By evaluating it on multi-national cohort screening mammograms,\nour multi-modal approach achieves superior performance in cancer detection and\ncalcification identification compared to unimodal baselines, with particular\nimprovements. The proposed method establishes a new paradigm for developing\nclinically viable VLM-based CAD systems that effectively leverage imaging data\nand contextual patient information through effective fusion mechanisms.\n","authors":["Shunjie-Fabian Zheng","Hyeonjun Lee","Thijs Kooi","Ali Diba"],"pdf_url":"https://arxiv.org/pdf/2510.25051v1.pdf","comment":"Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)\n  Workshop at ICCV 2025"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2510.26007v1","updated":"2025-10-29T22:35:34Z","published":"2025-10-29T22:35:34Z","title":"The Quest for Reliable Metrics of Responsible AI","summary":"  The development of Artificial Intelligence (AI), including AI in Science\n(AIS), should be done following the principles of responsible AI. Progress in\nresponsible AI is often quantified through evaluation metrics, yet there has\nbeen less work on assessing the robustness and reliability of the metrics\nthemselves. We reflect on prior work that examines the robustness of fairness\nmetrics for recommender systems as a type of AI application and summarise their\nkey takeaways into a set of non-exhaustive guidelines for developing reliable\nmetrics of responsible AI. Our guidelines apply to a broad spectrum of AI\napplications, including AIS.\n","authors":["Theresia Veronika Rampisela","Maria Maistro","Tuukka Ruotsalo","Christina Lioma"],"pdf_url":"https://arxiv.org/pdf/2510.26007v1.pdf","comment":"Accepted for presentation at the AI in Science Summit 2025"},{"id":"http://arxiv.org/abs/2510.25718v1","updated":"2025-10-29T17:27:21Z","published":"2025-10-29T17:27:21Z","title":"Retrieval-Augmented Search for Large-Scale Map Collections with ColPali","summary":"  Multimodal approaches have shown great promise for searching and navigating\ndigital collections held by libraries, archives, and museums. In this paper, we\nintroduce map-RAS: a retrieval-augmented search system for historic maps. In\naddition to introducing our framework, we detail our publicly-hosted demo for\nsearching 101,233 map images held by the Library of Congress. With our system,\nusers can multimodally query the map collection via ColPali, summarize search\nresults using Llama 3.2, and upload their own collections to perform\ninter-collection search. We articulate potential use cases for archivists,\ncurators, and end-users, as well as future work with our system in both machine\nlearning and the digital humanities. Our demo can be viewed at:\nhttp://www.mapras.com.\n","authors":["Jamie Mahowald","Benjamin Charles Germain Lee"],"pdf_url":"https://arxiv.org/pdf/2510.25718v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.25621v1","updated":"2025-10-29T15:25:34Z","published":"2025-10-29T15:25:34Z","title":"FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering","summary":"  The advent of Large Language Models (LLMs) has revolutionized Natural\nLanguage Processing, yet their application in high-stakes, specialized domains\nlike religious question answering is hindered by challenges like hallucination\nand unfaithfulness to authoritative sources. This issue is particularly\ncritical for the Persian-speaking Muslim community, where accuracy and\ntrustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)\nsystems, relying on simplistic single-pass pipelines, fall short on complex,\nmulti-hop queries requiring multi-step reasoning and evidence aggregation. To\naddress this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful\nAdvanced Question Answering in the Persian Islamic domain. FARSIQA is built\nupon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative\nRefinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting\nprocess: it adaptively decomposes complex queries, assesses evidence\nsufficiency, and enters an iterative loop to generate sub-queries,\nprogressively filling information gaps. Operating on a curated knowledge base\nof over one million authoritative Islamic documents, FARSIQA demonstrates\nsuperior performance. Rigorous evaluation on the challenging IslamicPCQA\nbenchmark shows state-of-the-art performance: the system achieves a remarkable\n97.0% in Negative Rejection - a 40-point improvement over baselines - and a\nhigh Answer Correctness score of 74.3%. Our work establishes a new standard for\nPersian Islamic QA and validates that our iterative, adaptive architecture is\ncrucial for building faithful, reliable AI systems in sensitive domains.\n","authors":["Mohammad Aghajani Asl","Behrooz Minaei Bidgoli"],"pdf_url":"https://arxiv.org/pdf/2510.25621v1.pdf","comment":"37 pages, 5 figures, 10 tables. Keywords: Retrieval-Augmented\n  Generation (RAG), Question Answering (QA), Islamic Knowledge Base, Faithful\n  AI, Persian NLP, Multi-hop Reasoning, Large Language Models (LLMs)"},{"id":"http://arxiv.org/abs/2510.13738v2","updated":"2025-10-29T15:00:42Z","published":"2025-10-15T16:45:59Z","title":"HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation","summary":"  Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems. Code is\navailable at https://github.com/FireRedTeam/FireRedSeqRec.\n","authors":["Jingyi Zhou","Cheng Chen","Kai Zuo","Manjie Xu","Zhendong Fu","Yibo Chen","Xu Tang","Yao Hu"],"pdf_url":"https://arxiv.org/pdf/2510.13738v2.pdf","comment":null},{"id":"http://arxiv.org/abs/1810.06818v3","updated":"2025-10-29T14:50:15Z","published":"2018-10-16T05:40:03Z","title":"Large Language Models for Few-Shot Named Entity Recognition","summary":"  Named entity recognition (NER) is a fundamental task in numerous downstream\napplications. Recently, researchers have employed pre-trained language models\n(PLMs) and large language models (LLMs) to address this task. However, fully\nleveraging the capabilities of PLMs and LLMs with minimal human effort remains\nchallenging. In this paper, we propose GPT4NER, a method that prompts LLMs to\nresolve the few-shot NER task. GPT4NER constructs effective prompts using three\nkey components: entity definition, few-shot examples, and chain-of-thought. By\nprompting LLMs with these effective prompts, GPT4NER transforms few-shot NER,\nwhich is traditionally considered as a sequence-labeling problem, into a\nsequence-generation problem. We conduct experiments on two benchmark datasets,\nCoNLL2003 and OntoNotes5.0, and compare the performance of GPT4NER to\nrepresentative state-of-the-art models in both few-shot and fully supervised\nsettings. Experimental results demonstrate that GPT4NER achieves the $F_1$ of\n83.15\\% on CoNLL2003 and 70.37\\% on OntoNotes5.0, significantly outperforming\nfew-shot baselines by an average margin of 7 points. Compared to\nfully-supervised baselines, GPT4NER achieves 87.9\\% of their best performance\non CoNLL2003 and 76.4\\% of their best performance on OntoNotes5.0. We also\nutilize a relaxed-match metric for evaluation and report performance in the\nsub-task of named entity extraction (NEE), and experiments demonstrate their\nusefulness to help better understand model behaviors in the NER task.\n","authors":["Yufei Zhao","Xiaoshi Zhong","Erik Cambria","Jagath C. Rajapakse"],"pdf_url":"https://arxiv.org/pdf/1810.06818v3.pdf","comment":"17 pages, 2 figures. Accepted by AI, Computer Science and Robotics\n  Technology (ACRT)"},{"id":"http://arxiv.org/abs/2505.10940v3","updated":"2025-10-29T14:42:46Z","published":"2025-05-16T07:26:41Z","title":"Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced\n  Logical Recommendation","summary":"  Recommender systems filter contents/items valuable to users by inferring\npreferences from user features and historical behaviors. Mainstream approaches\nfollow the learning-to-rank paradigm, which focus on discovering and modeling\nitem topics (e.g., categories), and capturing user preferences on these topics\nbased on historical interactions. However, this paradigm often neglects the\nmodeling of user characteristics and their social roles, which are logical\nconfounders influencing the correlated interest and user preference transition.\nTo bridge this gap, we introduce the user role identification task and the\nbehavioral logic modeling task that aim to explicitly model user roles and\nlearn the logical relations between item topics and user social roles. We show\nthat it is possible to explicitly solve these tasks through an efficient\nintegration framework of Large Language Model (LLM) and recommendation systems,\nfor which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal)\nLLM's world knowledge and logic inference ability to extract realistic\ntag-based virtual logic graphs that reveal dynamic and expressive knowledge of\nusers, refining our understanding of user behaviors. On the other hand, TagCF\npresents empirically effective integration modules that take advantage of the\nextracted tag-logic information, augmenting the recommendation performance. We\nconduct both online experiments and offline experiments with industrial and\npublic datasets as verification of TagCF's effectiveness, and we empirically\nshow that the user role modeling strategy is potentially a better choice than\nthe modeling of item topics. Additionally, we provide evidence that the\nextracted logic graphs are empirically a general and transferable knowledge\nthat can benefit a wide range of recommendation tasks. Our code is available in\nhttps://github.com/Code2Q/TagCF.\n","authors":["Qing Yu","Xiaobei Wang","Shuchang Liu","Yandong Bai","Xiaoyu Yang","Xueliang Wang","Chang Meng","Shanshan Wu","Hailan Yang","Huihui Xiao","Xiang Li","Fan Yang","Xiaoqiang Feng","Lantao Hu","Han Li","Kun Gai","Lixin Zou"],"pdf_url":"https://arxiv.org/pdf/2505.10940v3.pdf","comment":"to be published in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25488v1","updated":"2025-10-29T13:08:35Z","published":"2025-10-29T13:08:35Z","title":"Generalized Pseudo-Relevance Feedback","summary":"  Query rewriting is a fundamental technique in information retrieval (IR). It\ntypically employs the retrieval result as relevance feedback to refine the\nquery and thereby addresses the vocabulary mismatch between user queries and\nrelevant documents. Traditional pseudo-relevance feedback (PRF) and its\nvector-based extension (VPRF) improve retrieval performance by leveraging\ntop-retrieved documents as relevance feedback. However, they are constructed\nbased on two major hypotheses: the relevance assumption (top documents are\nrelevant) and the model assumption (rewriting methods need to be designed\nspecifically for particular model architectures). While recent large language\nmodels (LLMs)-based generative relevance feedback (GRF) enables model-free\nquery reformulation, it either suffers from severe LLM hallucination or, again,\nrelies on the relevance assumption to guarantee the effectiveness of rewriting\nquality. To overcome these limitations, we introduce an assumption-relaxed\nframework: \\textit{Generalized Pseudo Relevance Feedback} (GPRF), which\nperforms model-free, natural language rewriting based on retrieved documents,\nnot only eliminating the model assumption but also reducing dependence on the\nrelevance assumption. Specifically, we design a utility-oriented training\npipeline with reinforcement learning to ensure robustness against noisy\nfeedback. Extensive experiments across multiple benchmarks and retrievers\ndemonstrate that GPRF consistently outperforms strong baselines, establishing\nit as an effective and generalizable framework for query rewriting.\n","authors":["Yiteng Tu","Weihang Su","Yujia Zhou","Yiqun Liu","Fen Lin","Qin Liu","Qingyao Ai"],"pdf_url":"https://arxiv.org/pdf/2510.25488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25428v1","updated":"2025-10-29T11:50:52Z","published":"2025-10-29T11:50:52Z","title":"Alibaba International E-commerce Product Search Competition DcuRAGONs\n  Team Technical Report","summary":"  This report details our methodology and results developed for the\nMultilingual E-commerce Search Competition. The problem aims to recognize\nrelevance between user queries versus product items in a multilingual context\nand improve recommendation performance on e-commerce platforms. Utilizing Large\nLanguage Models (LLMs) and their capabilities in other tasks, our data-centric\nmethod achieved the highest score compared to other solutions during the\ncompetition. Final leaderboard is publised at\nhttps://alibaba-international-cikm2025.github.io. The source code for our\nproject is published at https://github.com/nhtlongcs/e-commerce-product-search.\n","authors":["Thang-Long Nguyen-Ho","Minh-Khoi Pham","Hoang-Bao Le"],"pdf_url":"https://arxiv.org/pdf/2510.25428v1.pdf","comment":"Alibaba International E-commerce Product Search Competition @ CIKM\n  2025"},{"id":"http://arxiv.org/abs/2510.25285v1","updated":"2025-10-29T08:42:15Z","published":"2025-10-29T08:42:15Z","title":"Revisiting scalable sequential recommendation with Multi-Embedding\n  Approach and Mixture-of-Experts","summary":"  In recommendation systems, how to effectively scale up recommendation models\nhas been an essential research topic. While significant progress has been made\nin developing advanced and scalable architectures for sequential\nrecommendation(SR) models, there are still challenges due to items'\nmulti-faceted characteristics and dynamic item relevance in the user context.\nTo address these issues, we propose Fuxi-MME, a framework that integrates a\nmulti-embedding strategy with a Mixture-of-Experts (MoE) architecture.\nSpecifically, to efficiently capture diverse item characteristics in a\ndecoupled manner, we decompose the conventional single embedding matrix into\nseveral lower-dimensional embedding matrices. Additionally, by substituting\nrelevant parameters in the Fuxi Block with an MoE layer, our model achieves\nadaptive and specialized transformation of the enriched representations.\nEmpirical results on public datasets show that our proposed framework\noutperforms several competitive baselines.\n","authors":["Qiushi Pan","Hao Wang","Guoyuan An","Luankang Zhang","Wei Guo","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25283v1","updated":"2025-10-29T08:39:36Z","published":"2025-10-29T08:39:36Z","title":"Measuring the Research Output and Performance of the University of\n  Ibadan from 2014 to 2023: A Scientometric Analysis","summary":"  This study employs scientometric methods to assess the research output and\nperformance of the University of Ibadan from 2014 to 2023. By analyzing\npublication trends, citation patterns, and collaboration networks, the research\naims to comprehensively evaluate the university's research productivity,\nimpact, and disciplinary focus. This article's endeavors are characterized by\ninnovation, interdisciplinary collaboration, and commitment to excellence,\nmaking the University of Ibadan a significant hub for cutting-edge research in\nNigeria and beyond. The goal of the current study is to ascertain the influence\nof the university's research output and publication patterns between 2014 and\n2023. The study focuses on the departments at the University of Ibadan that\ncontribute the most, the best journals for publishing, the nations that\ncollaborate, the impact of citations both locally and globally, well-known\nauthors and their total production, and the research output broken down by\nyear. According to the university's ten-year publication data, 7159 papers with\nan h-index of 75 were published between 2014 and 2023, garnering 218572\ncitations. Furthermore, the VOSviewer software mapping approach is used to\nillustrate the stenographical mapping of data through graphs. The findings of\nthis study will contribute to understanding the university's research\nstrengths, weaknesses, and potential areas for improvement. Additionally, the\nresults will inform evidence-based decision-making for enhancing research\nstrategies and policies at the University of Ibadan.\n","authors":["Muneer Ahmad","Undie Felicia Nkatv"],"pdf_url":"https://arxiv.org/pdf/2510.25283v1.pdf","comment":"16 pages, 5 figures, Research Paper"},{"id":"http://arxiv.org/abs/2503.05493v2","updated":"2025-10-29T08:19:03Z","published":"2025-03-07T15:05:23Z","title":"Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation","summary":"  In recent years, integrating large language models (LLMs) into recommender\nsystems has created new opportunities for improving recommendation quality.\nHowever, a comprehensive benchmark is needed to thoroughly evaluate and compare\nthe recommendation capabilities of LLMs with traditional recommender systems.\nIn this paper, we introduce RecBench, which systematically investigates various\nitem representation forms (including unique identifier, text, semantic\nembedding, and semantic identifier) and evaluates two primary recommendation\ntasks, i.e., click-through rate prediction (CTR) and sequential recommendation\n(SeqRec). Our extensive experiments cover up to 17 large models and are\nconducted across five diverse datasets from fashion, news, video, books, and\nmusic domains. Our findings indicate that LLM-based recommenders outperform\nconventional recommenders, achieving up to a 5% AUC improvement in the CTR\nscenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However,\nthese substantial performance gains come at the expense of significantly\nreduced inference efficiency, rendering the LLM-as-RS paradigm impractical for\nreal-time recommendation environments. We aim for our findings to inspire\nfuture research, including recommendation-specific model acceleration methods.\nWe will release our code, data, configurations, and platform to enable other\nresearchers to reproduce and build upon our experimental results.\n","authors":["Qijiong Liu","Jieming Zhu","Lu Fan","Kun Wang","Hengchang Hu","Wei Guo","Yong Liu","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2503.05493v2.pdf","comment":"NeurIPS 2025 DB Track Accepted Paper"},{"id":"http://arxiv.org/abs/2510.25259v1","updated":"2025-10-29T08:14:03Z","published":"2025-10-29T08:14:03Z","title":"TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation","summary":"  Recently, convolutional filters have been increasingly adopted in sequential\nrecommendation for their ability to capture local sequential patterns. However,\nmost of these models complement convolutional filters with self-attention. This\nis because convolutional filters alone, generally fixed filters, struggle to\ncapture global interactions necessary for accurate recommendation. We propose\nTime-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a\nmodel inspired by graph signal processing, where time-variant graph filters\ncapture position-dependent temporal variations in user sequences. By replacing\nboth fixed kernels and self-attention with time-variant filters, TV-Rec\nachieves higher expressive power and better captures complex interaction\npatterns in user behavior. This design not only eliminates the need for\nself-attention but also reduces computation while accelerating inference.\nExtensive experiments on six public benchmarks show that TV-Rec outperforms\nstate-of-the-art baselines by an average of 7.49%.\n","authors":["Yehjin Shin","Jeongwhan Choi","Seojin Kim","Noseong Park"],"pdf_url":"https://arxiv.org/pdf/2510.25259v1.pdf","comment":"The 39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)"},{"id":"http://arxiv.org/abs/2510.25220v1","updated":"2025-10-29T06:54:42Z","published":"2025-10-29T06:54:42Z","title":"GReF: A Unified Generative Framework for Efficient Reranking via Ordered\n  Multi-token Prediction","summary":"  In a multi-stage recommendation system, reranking plays a crucial role in\nmodeling intra-list correlations among items. A key challenge lies in exploring\noptimal sequences within the combinatorial space of permutations. Recent\nresearch follows a two-stage (generator-evaluator) paradigm, where a generator\nproduces multiple feasible sequences, and an evaluator selects the best one. In\npractice, the generator is typically implemented as an autoregressive model.\nHowever, these two-stage methods face two main challenges. First, the\nseparation of the generator and evaluator hinders end-to-end training. Second,\nautoregressive generators suffer from inference efficiency. In this work, we\npropose a Unified Generative Efficient Reranking Framework (GReF) to address\nthe two primary challenges. Specifically, we introduce Gen-Reranker, an\nautoregressive generator featuring a bidirectional encoder and a dynamic\nautoregressive decoder to generate causal reranking sequences. Subsequently, we\npre-train Gen-Reranker on the item exposure order for high-quality parameter\ninitialization. To eliminate the need for the evaluator while integrating\nsequence-level evaluation during training for end-to-end optimization, we\npropose post-training the model through Rerank-DPO. Moreover, for efficient\nautoregressive inference, we introduce ordered multi-token prediction (OMTP),\nwhich trains Gen-Reranker to simultaneously generate multiple future items\nwhile preserving their order, ensuring practical deployment in real-time\nrecommender systems. Extensive offline experiments demonstrate that GReF\noutperforms state-of-the-art reranking methods while achieving latency that is\nnearly comparable to non-autoregressive models. Additionally, GReF has also\nbeen deployed in a real-world video app Kuaishou with over 300 million daily\nactive users, significantly improving online recommendation quality.\n","authors":["Zhijie Lin","Zhuofeng Li","Chenglei Dai","Wentian Bao","Shuai Lin","Enyun Yu","Haoxiang Zhang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.25220v1.pdf","comment":"Accepted by CIKM 2025"},{"id":"http://arxiv.org/abs/2506.07466v2","updated":"2025-10-29T04:50:52Z","published":"2025-06-09T06:20:23Z","title":"Capturing User Interests from Data Streams for Continual Sequential\n  Recommendation","summary":"  Transformer-based sequential recommendation (SR) models excel at modeling\nlong-range dependencies in user behavior via self-attention. However, updating\nthem with continuously arriving behavior sequences incurs high computational\ncosts or leads to catastrophic forgetting. Although continual learning, a\nstandard approach for non-stationary data streams, has recently been applied to\nrecommendation, existing methods gradually forget long-term user preferences\nand remain underexplored in SR. In this paper, we introduce Continual\nSequential Transformer for Recommendation (CSTRec). CSTRec is designed to\neffectively adapt to current interests by leveraging well-preserved historical\nones, thus capturing the trajectory of user interests over time. The core of\nCSTRec is Continual Sequential Attention (CSA), a linear attention tailored for\ncontinual SR, which enables CSTRec to partially retain historical knowledge\nwithout direct access to prior data. CSA has two key components: (1)\nCauchy-Schwarz Normalization that stabilizes learning over time under uneven\nuser interaction frequencies; (2) Collaborative Interest Enrichment that\nalleviates forgetting through shared, learnable interest pools. In addition, we\nintroduce a new technique to facilitate the adaptation of new users by\ntransferring historical knowledge from existing users with similar interests.\nExtensive experiments on three real-world datasets show that CSTRec outperforms\nstate-of-the-art models in both knowledge retention and acquisition.\n","authors":["Gyuseok Lee","Hyunsik Yoo","Junyoung Hwang","SeongKu Kang","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2506.07466v2.pdf","comment":"WSDM'26"},{"id":"http://arxiv.org/abs/2510.06658v2","updated":"2025-10-29T03:24:46Z","published":"2025-10-08T05:17:33Z","title":"Can We Hide Machines in the Crowd? Quantifying Equivalence in\n  LLM-in-the-loop Annotation Tasks","summary":"  Many evaluations of large language models (LLMs) in text annotation focus\nprimarily on the correctness of the output, typically comparing model-generated\nlabels to human-annotated ``ground truth'' using standard performance metrics.\nIn contrast, our study moves beyond effectiveness alone. We aim to explore how\nlabeling decisions -- by both humans and LLMs -- can be statistically evaluated\nacross individuals. Rather than treating LLMs purely as annotation systems, we\napproach LLMs as an alternative annotation mechanism that may be capable of\nmimicking the subjective judgments made by humans. To assess this, we develop a\nstatistical evaluation method based on Krippendorff's $\\alpha$, paired\nbootstrapping, and the Two One-Sided t-Tests (TOST) equivalence test procedure.\nThis evaluation method tests whether an LLM can blend into a group of human\nannotators without being distinguishable.\n  We apply this approach to two datasets -- MovieLens 100K and PolitiFact --\nand find that the LLM is statistically indistinguishable from a human annotator\nin the former ($p = 0.004$), but not in the latter ($p = 0.155$), highlighting\ntask-dependent differences. It also enables early evaluation on a small sample\nof human data to inform whether LLMs are suitable for large-scale annotation in\na given application.\n","authors":["Jiaman He","Zikang Leng","Dana McKay","Damiano Spina","Johanne R. Trippas"],"pdf_url":"https://arxiv.org/pdf/2510.06658v2.pdf","comment":"Accepted at SIGIR-AP 2025"},{"id":"http://arxiv.org/abs/2510.25093v1","updated":"2025-10-29T01:57:38Z","published":"2025-10-29T01:57:38Z","title":"Continual Low-Rank Adapters for LLM-based Generative Recommender Systems","summary":"  While large language models (LLMs) achieve strong performance in\nrecommendation, they face challenges in continual learning as users, items, and\nuser preferences evolve over time. Existing LoRA-based continual methods\nprimarily focus on preserving performance on previous tasks, but this overlooks\nthe unique nature of recommendation: the goal is not to predict past\npreferences, and outdated preferences can even harm performance when current\ninterests shift significantly. To address this, we propose PESO (Proximally\nrEgularized Single evolving lOra, a continual adaptation method for LoRA in\nrecommendation. PESO introduces a proximal regularizer that anchors the current\nadapter to its most recent frozen state, enabling the model to flexibly balance\nadaptation and preservation, and to better capture recent user behaviors.\nTheoretically, we show that this proximal design provides data-aware,\ndirection-wise guidance in the LoRA subspace. Empirically, PESO consistently\noutperforms existing LoRA-based continual learning methods.\n","authors":["Hyunsik Yoo","Ting-Wei Li","SeongKu Kang","Zhining Liu","Charlie Xu","Qilin Qi","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2510.25093v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.05715v2","updated":"2025-10-29T17:44:30Z","published":"2024-11-08T17:16:27Z","title":"Artificial Neural Networks Trained on Noisy Speech Exhibit the McGurk\n  Effect","summary":"  Humans are able to fuse information from both auditory and visual modalities\nto help with understanding speech. This is demonstrated through a phenomenon\nknown as the McGurk Effect, during which a listener is presented with\nincongruent auditory and visual speech that fuse together into the percept of\nillusory intermediate phonemes. Building on a recent framework that proposes\nhow to address developmental 'why' questions using artificial neural networks,\nwe evaluated a set of recent artificial neural networks trained on audiovisual\nspeech by testing them with audiovisually incongruent words designed to elicit\nthe McGurk effect. We show that networks trained entirely on congruent\naudiovisual speech nevertheless exhibit the McGurk percept. We further\ninvestigated 'why' by comparing networks trained on clean speech to those\ntrained on noisy speech, and discovered that training with noisy speech led to\na pronounced increase in both visual responses and McGurk responses across all\nmodels. Furthermore, we observed that systematically increasing the level of\nauditory noise during ANN training also increased the amount of audiovisual\nintegration up to a point, but at extreme noise levels, this integration failed\nto develop. These results suggest that excessive noise exposure during critical\nperiods of audiovisual learning may negatively influence the development of\naudiovisual speech integration. This work also demonstrates that the McGurk\neffect reliably emerges untrained from the behaviour of both supervised and\nunsupervised networks, even networks trained only on congruent speech. This\nsupports the notion that artificial neural networks might be useful models for\ncertain aspects of perception and cognition.\n","authors":["Lukas Grasse","Matthew S. Tata"],"pdf_url":"https://arxiv.org/pdf/2411.05715v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25668v1","updated":"2025-10-29T16:32:26Z","published":"2025-10-29T16:32:26Z","title":"ALDEN: Reinforcement Learning for Active Navigation and Evidence\n  Gathering in Long Documents","summary":"  Vision-language models (VLMs) excel at interpreting text-rich images but\nstruggle with long, visually complex documents that demand analysis and\nintegration of information spread across multiple pages. Existing approaches\ntypically rely on fixed reasoning templates or rigid pipelines, which force\nVLMs into a passive role and hinder both efficiency and generalization. We\npresent Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement\nlearning framework that fine-tunes VLMs as interactive agents capable of\nactively navigating long, visually rich documents. ALDEN introduces a novel\nfetch action that directly accesses the page by index, complementing the\nclassic search action and better exploiting document structure. For dense\nprocess supervision and efficient training, we propose a rule-based cross-level\nreward that provides both turn- and token-level signals. To address the\nempirically observed training instability caused by numerous visual tokens from\nlong documents, we further propose a visual-semantic anchoring mechanism that\napplies a dual-path KL-divergence constraint to stabilize visual and textual\nrepresentations separately during training. Trained on a corpus constructed\nfrom three open-source datasets, ALDEN achieves state-of-the-art performance on\nfive long-document benchmarks. Overall, ALDEN marks a step beyond passive\ndocument reading toward agents that autonomously navigate and reason across\nlong, visually rich documents, offering a robust path to more accurate and\nefficient long-document understanding.\n","authors":["Tianyu Yang","Terry Ruas","Yijun Tian","Jan Philip Wahle","Daniel Kurzawe","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2510.25668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25357v1","updated":"2025-10-29T10:21:41Z","published":"2025-10-29T10:21:41Z","title":"Energy consumption assessment of a Virtual Reality Remote Rendering\n  application over 5G networks","summary":"  This paper investigates the energy implications of remote rendering for\nVirtual Reality (VR) applications within a real 5G testbed. Remote rendering\nenables lightweight devices to access high-performance graphical content by\noffloading computationally intensive tasks to Cloud-native Network Functions\n(CNFs) running on remote servers. However, this approach raises concerns\nregarding energy consumption across the various network components involved,\nincluding the remote computing node, the 5G Core, the Radio Access Network\n(RAN), and the User Equipment (UE). This work proposes and evaluates two\ncomplementary energy monitoring solutions, one hardware-based and one\nsoftware-based, to measure energy consumption at different system levels. A VR\nremote renderer, deployed as CNF and leveraging the Media over QUIC (MoQ)\nprotocol, is used as test case for assessing its energy footprint under\ndifferent multimedia and network configurations. The results provide critical\ninsights into the trade-off between energy consumption and performance of a\nreal-world VR application running in a 5G environment.\n","authors":["Roberto Viola","Mikel Irazola","José Ramón Juárez","Minh Nguyen","Alexander Zoubarev","Alexander Futasz","Louay Bassbouss","Amr A. AbdelNabi","Javier Fernández Hidalgo"],"pdf_url":"https://arxiv.org/pdf/2510.25357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10258v2","updated":"2025-10-29T07:51:00Z","published":"2025-04-14T14:19:57Z","title":"XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a\n  Novel Benchmark","summary":"  Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing.\n","authors":["Shuai Liu","Youmeng Li","Jizeng Wei"],"pdf_url":"https://arxiv.org/pdf/2504.10258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25225v1","updated":"2025-10-29T07:00:48Z","published":"2025-10-29T07:00:48Z","title":"Hallucination Localization in Video Captioning","summary":"  We propose a novel task, hallucination localization in video captioning,\nwhich aims to identify hallucinations in video captions at the span level (i.e.\nindividual words or phrases). This allows for a more detailed analysis of\nhallucinations compared to existing sentence-level hallucination detection\ntask. To establish a benchmark for hallucination localization, we construct\nHLVC-Dataset, a carefully curated dataset created by manually annotating 1,167\nvideo-caption pairs from VideoLLM-generated captions. We further implement a\nVideoLLM-based baseline method and conduct quantitative and qualitative\nevaluations to benchmark current performance on hallucination localization.\n","authors":["Shota Nakada","Kazuhiro Saito","Yuchi Ishikawa","Hokuto Munakata","Tatsuya Komatsu","Masayoshi Kondo"],"pdf_url":"https://arxiv.org/pdf/2510.25225v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2510.25079v1","updated":"2025-10-29T01:26:29Z","published":"2025-10-29T01:26:29Z","title":"Performance Evaluation of Multimedia Traffic in Cloud Storage Services\n  over Wi-Fi and LTE Networks","summary":"  The performance of Dropbox, Google Drive, and OneDrive cloud storage services\nwas evaluated under Wi-Fi and LTE network conditions during multimedia file\nuploads. Traffic was captured using Wireshark, and key metrics (including\ndelay, jitter, bandwidth, and packet loss) were analyzed. Google Drive\nmaintained the most consistent performance across both types of networks,\nshowing low latency and reduced jitter. Dropbox showed efficient bandwidth\nutilization, but experienced a longer delay over LTE, attributed to a greater\nnumber of intermediate hops. OneDrive presented variable behavior, with\nelevated packet rates and increased sensitivity to fluctuations in the mobile\nnetwork. A bimodal distribution of packet sizes was observed and modeled using\na dual Poisson function. In general, Wi-Fi connections provided greater\nstability for multimedia transfers, while LTE performance varied depending on\nplatform-specific implementations. The results contribute to a better\nunderstanding of traffic behavior in cloud-based storage applications and\nsuggest further analysis with larger datasets and heterogeneous access\nnetworks.\n","authors":["Albert Espinal","V. Sanchez Padilla","Yesenia Cevallos"],"pdf_url":"https://arxiv.org/pdf/2510.25079v1.pdf","comment":"2025 20th Iberian Conference on Information Systems and Technologies\n  (CISTI), Lecture Notes in Networks and Systems"}],"Robotics":[{"id":"http://arxiv.org/abs/2510.26023v1","updated":"2025-10-29T23:33:31Z","published":"2025-10-29T23:33:31Z","title":"Large Language Model-assisted Autonomous Vehicle Recovery from\n  Immobilization","summary":"  Despite significant advancements in recent decades, autonomous vehicles (AVs)\ncontinue to face challenges in navigating certain traffic scenarios where human\ndrivers excel. In such situations, AVs often become immobilized, disrupting\noverall traffic flow. Current recovery solutions, such as remote intervention\n(which is costly and inefficient) and manual takeover (which excludes\nnon-drivers and limits AV accessibility), are inadequate. This paper introduces\nStuckSolver, a novel Large Language Model (LLM) driven recovery framework that\nenables AVs to resolve immobilization scenarios through self-reasoning and/or\npassenger-guided decision-making. StuckSolver is designed as a plug-in add-on\nmodule that operates on top of the AV's existing perception-planning-control\nstack, requiring no modification to its internal architecture. Instead, it\ninterfaces with standard sensor data streams to detect immobilization states,\ninterpret environmental context, and generate high-level recovery commands that\ncan be executed by the AV's native planner. We evaluate StuckSolver on the\nBench2Drive benchmark and in custom-designed uncertainty scenarios. Results\nshow that StuckSolver achieves near-state-of-the-art performance through\nautonomous self-reasoning alone and exhibits further improvements when\npassenger guidance is incorporated.\n","authors":["Zhipeng Bao","Qianwen Li"],"pdf_url":"https://arxiv.org/pdf/2510.26023v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2510.26018v1","updated":"2025-10-29T23:25:49Z","published":"2025-10-29T23:25:49Z","title":"RADRON: Cooperative Localization of Ionizing Radiation Sources by MAVs\n  with Compton Cameras","summary":"  We present a novel approach to localizing radioactive material by cooperating\nMicro Aerial Vehicles (MAVs). Our approach utilizes a state-of-the-art\nsingle-detector Compton camera as a highly sensitive, yet miniature detector of\nionizing radiation. The detector's exceptionally low weight (40 g) opens up new\npossibilities of radiation detection by a team of cooperating agile MAVs. We\npropose a new fundamental concept of fusing the Compton camera measurements to\nestimate the position of the radiation source in real time even from extremely\nsparse measurements. The data readout and processing are performed directly\nonboard and the results are used in a dynamic feedback to drive the motion of\nthe vehicles. The MAVs are stabilized in a tightly cooperating swarm to\nmaximize the information gained by the Compton cameras, rapidly locate the\nradiation source, and even track a moving radiation source.\n","authors":["Petr Stibinger","Tomas Baca","Daniela Doubravova","Jan Rusnak","Jaroslav Solc","Jan Jakubek","Petr Stepan","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2510.26018v1.pdf","comment":"8 pages, 9 figures, submitted for review to IEEE RA-L"},{"id":"http://arxiv.org/abs/2510.26004v1","updated":"2025-10-29T22:32:16Z","published":"2025-10-29T22:32:16Z","title":"DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection\n  System","summary":"  Rapid and reliable incident detection is critical for reducing crash-related\nfatalities, injuries, and congestion. However, conventional methods, such as\nclosed-circuit television, dashcam footage, and sensor-based detection,\nseparate detection from verification, suffer from limited flexibility, and\nrequire dense infrastructure or high penetration rates, restricting\nadaptability and scalability to shifting incident hotspots. To overcome these\nchallenges, we developed DARTS, a drone-based, AI-powered real-time traffic\nincident detection system. DARTS integrates drones' high mobility and aerial\nperspective for adaptive surveillance, thermal imaging for better\nlow-visibility performance and privacy protection, and a lightweight deep\nlearning framework for real-time vehicle trajectory extraction and incident\ndetection. The system achieved 99% detection accuracy on a self-collected\ndataset and supports simultaneous online visual verification, severity\nassessment, and incident-induced congestion propagation monitoring via a\nweb-based interface. In a field test on Interstate 75 in Florida, DARTS\ndetected and verified a rear-end collision 12 minutes earlier than the local\ntransportation management center and monitored incident-induced congestion\npropagation, suggesting potential to support faster emergency response and\nenable proactive traffic control to reduce congestion and secondary crash risk.\nCrucially, DARTS's flexible deployment architecture reduces dependence on\nfrequent physical patrols, indicating potential scalability and\ncost-effectiveness for use in remote areas and resource-constrained settings.\nThis study presents a promising step toward a more flexible and integrated\nreal-time traffic incident detection system, with significant implications for\nthe operational efficiency and responsiveness of modern transportation\nmanagement.\n","authors":["Bai Li","Achilleas Kourtellis","Rong Cao","Joseph Post","Brian Porter","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26004v1.pdf","comment":"Preprint version. This manuscript is currently under review at\n  Transportation Research Part C: Emerging Technologies. The PDF corresponds to\n  the version submitted in June 2025. The main findings of this work were\n  recognized with the Best Intelligent Transportation Systems Paper Award at\n  the 2025 TRB Annual Meeting"},{"id":"http://arxiv.org/abs/2510.25985v1","updated":"2025-10-29T21:37:26Z","published":"2025-10-29T21:37:26Z","title":"A New Type of Axis-Angle Attitude Control Law for Rotational Systems:\n  Synthesis, Analysis, and Experiments","summary":"  Over the past few decades, continuous quaternion-based attitude control has\nbeen proven highly effective for driving rotational systems that can be modeled\nas rigid bodies, such as satellites and drones. However, methods rooted in this\napproach do not enforce the existence of a unique closed-loop (CL) equilibrium\nattitude-error quaternion (AEQ); and, for rotational errors about the\nattitude-error Euler axis larger than {\\pi}rad, their proportional-control\neffect diminishes as the system state moves away from the stable equilibrium of\nthe CL rotational dynamics. In this paper, we introduce a new type of attitude\ncontrol law that more effectively leverages the attitude-error Euler axis-angle\ninformation to guarantee a unique CL equilibrium AEQ and to provide greater\nflexibility in the use of proportional-control efforts. Furthermore, using two\ndifferent control laws as examples-through the construction of a strict\nLyapunov function for the CL dynamics-we demonstrate that the resulting unique\nequilibrium of the CL rotational system can be enforced to be uniformly\nasymptotically stable. To assess and demonstrate the functionality and\nperformance of the proposed approach, we performed numerical simulations and\nexecuted dozens of real-time tumble-recovery maneuvers using a small quadrotor.\nThese simulations and flight tests compellingly demonstrate that the proposed\naxis-angle-based method achieves superior flight performance-compared with that\nobtained using a high-performance quaternion-based controller-in terms of\nstabilization time.\n","authors":["Francisco M. F. R. Gonçalves","Ryan M. Bena","Néstor O. Pérez-Arancibia"],"pdf_url":"https://arxiv.org/pdf/2510.25985v1.pdf","comment":"2025 International Conference on Advanced Robotics (ICAR)"},{"id":"http://arxiv.org/abs/2510.25965v1","updated":"2025-10-29T21:06:05Z","published":"2025-10-29T21:06:05Z","title":"Curvature-Aware Calibration of Tactile Sensors for Accurate Force\n  Estimation on Non-Planar Surfaces","summary":"  Flexible tactile sensors are increasingly used in real-world applications\nsuch as robotic grippers, prosthetic hands, wearable gloves, and assistive\ndevices, where they need to conform to curved and irregular surfaces. However,\nmost existing tactile sensors are calibrated only on flat substrates, and their\naccuracy and consistency degrade once mounted on curved geometries. This\nlimitation restricts their reliability in practical use. To address this\nchallenge, we develop a calibration model for a widely used resistive tactile\nsensor design that enables accurate force estimation on one-dimensional curved\nsurfaces. We then train a neural network (a multilayer perceptron) to predict\nlocal curvature from baseline sensor outputs recorded under no applied load,\nachieving an R2 score of 0.91. The proposed approach is validated on five daily\nobjects with varying curvatures under forces from 2 N to 8 N. Results show that\nthe curvature-aware calibration maintains consistent force accuracy across all\nsurfaces, while flat-surface calibration underestimates force as curvature\nincreases. Our results demonstrate that curvature-aware modeling improves the\naccuracy, consistency, and reliability of flexible tactile sensors, enabling\ndependable performance across real-world applications.\n","authors":["Luoyan Zhong","Heather Jin Hee Kim","Dylan P. Losey","Cara M. Nunez"],"pdf_url":"https://arxiv.org/pdf/2510.25965v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2503.14807v2","updated":"2025-10-29T21:00:32Z","published":"2025-03-19T00:41:04Z","title":"A Constrained Saddle Search Approach for Constructing Singular and\n  Flexible Bar Frameworks","summary":"  Singularity analysis is essential in robot kinematics, as singular\nconfigurations cause loss of control and kinematic indeterminacy. This paper\nmodels singularities in bar frameworks as saddle points on constrained\nmanifolds. Given an under-constrained, non-singular bar framework, by allowing\none edge to vary its length while fixing lengths of others, we define the\nsquared length of the free edge as an energy functional and show that its local\nsaddle points correspond to singular and flexible frameworks. Using our\nconstrained saddle search approach, we identify previously unknown singular and\nflexible bar frameworks, providing new insights into singular robotics design\nand analysis.\n","authors":["Xuenan Li","Mihnea Leonte","Christian D. Santangelo","Miranda Holmes-Cerfon"],"pdf_url":"https://arxiv.org/pdf/2503.14807v2.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2510.25960v1","updated":"2025-10-29T20:58:16Z","published":"2025-10-29T20:58:16Z","title":"WaveVerif: Acoustic Side-Channel based Verification of Robotic Workflows","summary":"  In this paper, we present a framework that uses acoustic side-channel\nanalysis (ASCA) to monitor and verify whether a robot correctly executes its\nintended commands. We develop and evaluate a machine-learning-based workflow\nverification system that uses acoustic emissions generated by robotic\nmovements. The system can determine whether real-time behavior is consistent\nwith expected commands. The evaluation takes into account movement speed,\ndirection, and microphone distance. The results show that individual robot\nmovements can be validated with over 80% accuracy under baseline conditions\nusing four different classifiers: Support Vector Machine (SVM), Deep Neural\nNetwork (DNN), Recurrent Neural Network (RNN), and Convolutional Neural Network\n(CNN). Additionally, workflows such as pick-and-place and packing could be\nidentified with similarly high confidence. Our findings demonstrate that\nacoustic signals can support real-time, low-cost, passive verification in\nsensitive robotic environments without requiring hardware modifications.\n","authors":["Zeynep Yasemin Erdogan","Shishir Nagaraja","Chuadhry Mujeeb Ahmed","Ryan Shah"],"pdf_url":"https://arxiv.org/pdf/2510.25960v1.pdf","comment":"11 pages, 3 figures, Corresponding Author: Prof. Shishir Nagaraja\n  (shishir.nagaraja@newcastle.ac.uk)"},{"id":"http://arxiv.org/abs/2506.01046v3","updated":"2025-10-29T20:40:57Z","published":"2025-06-01T15:13:54Z","title":"STATE-NAV: Stability-Aware Traversability Estimation for Bipedal\n  Navigation on Rough Terrain","summary":"  Bipedal robots have advantages in maneuvering human-centered environments,\nbut face greater failure risk compared to other stable mobile plarforms such as\nwheeled or quadrupedal robots. While learning-based traversability has been\nwidely studied for these platforms, bipedal traversability has instead relied\non manually designed rules with limited consideration of locomotion stability\non rough terrain. In this work, we present the first learning-based\ntraversability estimation and risk-sensitive navigation framework for bipedal\nrobots operating in diverse, uneven environments. TravFormer, a\ntransformer-based neural network, is trained to predict bipedal instability\nwith uncertainty, enabling risk-aware and adaptive planning. Based on the\nnetwork, we define traversability as stability-aware command velocity-the\nfastest command velocity that keeps instability below a user-defined limit.\nThis velocity-based traversability is integrated into a hierarchical planner\nthat combines traversability-informed Rapid Random Tree Star (TravRRT*) for\ntime-efficient planning and Model Predictive Control (MPC) for safe execution.\nWe validate our method in MuJoCo simulation and the real world, demonstrating\nimproved navigation performance, with enhanced robustness and time efficiency\nacross varying terrains compared to existing methods.\n","authors":["Ziwon Yoon","Lawrence Y. Zhu","Jingxi Lu","Lu Gan","Ye Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.01046v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25913v1","updated":"2025-10-29T19:33:26Z","published":"2025-10-29T19:33:26Z","title":"Risk-Aware Safety Filters with Poisson Safety Functions and Laplace\n  Guidance Fields","summary":"  Robotic systems navigating in real-world settings require a semantic\nunderstanding of their environment to properly determine safe actions. This\nwork aims to develop the mathematical underpinnings of such a representation --\nspecifically, the goal is to develop safety filters that are risk-aware. To\nthis end, we take a two step approach: encoding an understanding of the\nenvironment via Poisson's equation, and associated risk via Laplace guidance\nfields. That is, we first solve a Dirichlet problem for Poisson's equation to\ngenerate a safety function that encodes system safety as its 0-superlevel set.\nWe then separately solve a Dirichlet problem for Laplace's equation to\nsynthesize a safe \\textit{guidance field} that encodes variable levels of\ncaution around obstacles -- by enforcing a tunable flux boundary condition. The\nsafety function and guidance fields are then combined to define a safety\nconstraint and used to synthesize a risk-aware safety filter which, given a\nsemantic understanding of an environment with associated risk levels of\nenvironmental features, guarantees safety while prioritizing avoidance of\nhigher risk obstacles. We demonstrate this method in simulation and discuss how\n\\textit{a priori} understandings of obstacle risk can be directly incorporated\ninto the safety filter to generate safe behaviors that are risk-aware.\n","authors":["Gilbert Bahati","Ryan M. Bena","Meg Wilkinson","Pol Mestres","Ryan K. Cosner","Aaron D. Ames"],"pdf_url":"https://arxiv.org/pdf/2510.25913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25901v1","updated":"2025-10-29T19:07:39Z","published":"2025-10-29T19:07:39Z","title":"BikeScenes: Online LiDAR Semantic Segmentation for Bicycles","summary":"  The vulnerability of cyclists, exacerbated by the rising popularity of faster\ne-bikes, motivates adapting automotive perception technologies for bicycle\nsafety. We use our multi-sensor 'SenseBike' research platform to develop and\nevaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the\nautomotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg\nDataset, comprising 3021 consecutive LiDAR scans around the university campus\nof the TU Delft, semantically annotated for 29 dynamic and static classes. By\nevaluating model performance, we demonstrate that fine-tuning on our BikeScenes\ndataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly\noutperforming the 13.8% obtained with SemanticKITTI pre-training alone. This\nresult underscores the necessity and effectiveness of domain-specific training.\nWe highlight key challenges specific to bicycle-mounted, hardware-constrained\nperception systems and contribute the BikeScenes dataset as a resource for\nadvancing research in cyclist-centric LiDAR segmentation.\n","authors":["Denniz Goren","Holger Caesar"],"pdf_url":"https://arxiv.org/pdf/2510.25901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.25358v3","updated":"2025-10-29T18:47:21Z","published":"2025-09-29T18:07:54Z","title":"SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation","summary":"  Large-scale robot learning has recently shown promise for enabling robots to\nperform complex tasks by integrating perception, control, and language\nunderstanding. Yet, it struggles with long-horizon, contact-rich manipulation\nsuch as deformable object handling, where demonstration quality is\ninconsistent. Reward modeling offers a natural solution: by providing grounded\nprogress signals, it transforms noisy demonstrations into stable supervision\nthat generalizes across diverse trajectories. We introduce a stage-aware,\nvideo-based reward modeling framework that jointly predicts high-level task\nstages and fine-grained progress. Reward labels are automatically derived from\nnatural language subtask annotations, ensuring consistent progress estimation\nacross variable-length demonstrations. This design overcomes frame-index\nlabeling, which fails in variable-duration tasks like folding a T-shirt. Our\nreward model demonstrates robustness to variability, generalization to\nout-of-distribution settings, and strong utility for policy training. Building\non it, we propose Reward-Aligned Behavior Cloning (RA-BC), which filters\nhigh-quality data and reweights samples by reward. Experiments show the reward\nmodel alone outperforms baselines on validation and real robot rollouts.\nIntegrated into RA-BC, our approach achieves 83% success on folding T-shirts\nfrom the flattened state and 67% from the crumpled state -- far surpassing\nvanilla behavior cloning, which attains only 8% and 0% success. Overall, our\nresults highlight reward modeling as a key enabler for scalable,\nannotation-efficient, and robust imitation learning in long-horizon\nmanipulation.\n","authors":["Qianzhong Chen","Justin Yu","Mac Schwager","Pieter Abbeel","Yide Shentu","Philipp Wu"],"pdf_url":"https://arxiv.org/pdf/2509.25358v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25850v1","updated":"2025-10-29T18:00:16Z","published":"2025-10-29T18:00:16Z","title":"Debate2Create: Robot Co-design via Large Language Model Debates","summary":"  Automating the co-design of a robot's morphology and control is a\nlong-standing challenge due to the vast design space and the tight coupling\nbetween body and behavior. We introduce Debate2Create (D2C), a framework in\nwhich large language model (LLM) agents engage in a structured dialectical\ndebate to jointly optimize a robot's design and its reward function. In each\nround, a design agent proposes targeted morphological modifications, and a\ncontrol agent devises a reward function tailored to exploit the new design. A\npanel of pluralistic judges then evaluates the design-control pair in\nsimulation and provides feedback that guides the next round of debate. Through\niterative debates, the agents progressively refine their proposals, producing\nincreasingly effective robot designs. Notably, D2C yields diverse and\nspecialized morphologies despite no explicit diversity objective. On a\nquadruped locomotion benchmark, D2C discovers designs that travel 73% farther\nthan the default, demonstrating that structured LLM-based debate can serve as a\npowerful mechanism for emergent robot co-design. Our results suggest that\nmulti-agent debate, when coupled with physics-grounded feedback, is a promising\nnew paradigm for automated robot design.\n","authors":["Kevin Qiu","Marek Cygan"],"pdf_url":"https://arxiv.org/pdf/2510.25850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25768v1","updated":"2025-10-29T17:59:03Z","published":"2025-10-29T17:59:03Z","title":"STITCH 2.0: Extending Augmented Suturing with EKF Needle Estimation and\n  Thread Management","summary":"  Surgical suturing is a high-precision task that impacts patient healing and\nscarring. Suturing skill varies widely between surgeons, highlighting the need\nfor robot assistance. Previous robot suturing works, such as STITCH 1.0 [1],\nstruggle to fully close wounds due to inaccurate needle tracking and poor\nthread management. To address these challenges, we present STITCH 2.0, an\nelevated augmented dexterity pipeline with seven improvements including:\nimproved EKF needle pose estimation, new thread untangling methods, and an\nautomated 3D suture alignment algorithm. Experimental results over 15 trials\nfind that STITCH 2.0 on average achieves 74.4% wound closure with 4.87 sutures\nper trial, representing 66% more sutures in 38% less time compared to the\nprevious baseline. When two human interventions are allowed, STITCH 2.0\naverages six sutures with 100% wound closure rate. Project website:\nhttps://stitch-2.github.io/\n","authors":["Kush Hari","Ziyang Chen","Hansoul Kim","Ken Goldberg"],"pdf_url":"https://arxiv.org/pdf/2510.25768v1.pdf","comment":"Published in RA-L 2025"},{"id":"http://arxiv.org/abs/2510.25754v1","updated":"2025-10-29T17:52:32Z","published":"2025-10-29T17:52:32Z","title":"GET-USE: Learning Generalized Tool Usage for Bimanual Mobile\n  Manipulation via Simulated Embodiment Extensions","summary":"  The ability to use random objects as tools in a generalizable manner is a\nmissing piece in robots' intelligence today to boost their versatility and\nproblem-solving capabilities. State-of-the-art robotic tool usage methods\nfocused on procedurally generating or crowd-sourcing datasets of tools for a\ntask to learn how to grasp and manipulate them for that task. However, these\nmethods assume that only one object is provided and that it is possible, with\nthe correct grasp, to perform the task; they are not capable of identifying,\ngrasping, and using the best object for a task when many are available,\nespecially when the optimal tool is absent. In this work, we propose GeT-USE, a\ntwo-step procedure that learns to perform real-robot generalized tool usage by\nlearning first to extend the robot's embodiment in simulation and then\ntransferring the learned strategies to real-robot visuomotor policies. Our key\ninsight is that by exploring a robot's embodiment extensions (i.e., building\nnew end-effectors) in simulation, the robot can identify the general tool\ngeometries most beneficial for a task. This learned geometric knowledge can\nthen be distilled to perform generalized tool usage tasks by selecting and\nusing the best available real-world object as tool. On a real robot with 22\ndegrees of freedom (DOFs), GeT-USE outperforms state-of-the-art methods by\n30-60% success rates across three vision-based bimanual mobile manipulation\ntool-usage tasks.\n","authors":["Bohan Wu","Paul de La Sayette","Li Fei-Fei","Roberto Martín-Martín"],"pdf_url":"https://arxiv.org/pdf/2510.25754v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2510.25727v1","updated":"2025-10-29T17:33:16Z","published":"2025-10-29T17:33:16Z","title":"Modeling Collapse of Steered Vine Robots Under Their Own Weight","summary":"  Soft, vine-inspired growing robots that move by eversion are highly mobile in\nconfined environments, but, when faced with gaps in the environment, they may\ncollapse under their own weight while navigating a desired path. In this work,\nwe present a comprehensive collapse model that can predict the collapse length\nof steered robots in any shape using true shape information and tail tension.\nWe validate this model by collapsing several unsteered robots without true\nshape information. The model accurately predicts the trends of those\nexperiments. We then attempt to collapse a robot steered with a single actuator\nat different orientations. Our models accurately predict collapse when it\noccurs. Finally, we demonstrate how this could be used in the field by having a\nrobot attempt a gap-crossing task with and without inflating its actuators. The\nrobot needs its actuators inflated to cross the gap without collapsing, which\nour model supports. Our model has been specifically tested on straight and\nseries pouch motor-actuated robots made of non-stretchable material, but it\ncould be applied to other robot variations. This work enables us to model the\nrobot's collapse behavior in any open environment and understand the parameters\nit needs to succeed in 3D navigation tasks.\n","authors":["Ciera McFarland","Margaret McGuinness"],"pdf_url":"https://arxiv.org/pdf/2510.25727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25713v1","updated":"2025-10-29T17:22:59Z","published":"2025-10-29T17:22:59Z","title":"Robotic Assistant: Completing Collaborative Tasks with Dexterous\n  Vision-Language-Action Models","summary":"  We adapt a pre-trained Vision-Language-Action (VLA) model (Open-VLA) for\ndexterous human-robot collaboration with minimal language prompting. Our\napproach adds (i) FiLM conditioning to visual backbones for task-aware\nperception, (ii) an auxiliary intent head that predicts collaborator hand pose\nand target cues, and (iii) action-space post-processing that predicts compact\ndeltas (position/rotation) and PCA-reduced finger joints before mapping to full\ncommands. Using a multi-view, teleoperated Franka and Mimic-hand dataset\naugmented with MediaPipe hand poses, we demonstrate that delta actions are\nwell-behaved and that four principal components explain ~96% of hand-joint\nvariance. Ablations identify action post-processing as the primary performance\ndriver; auxiliary intent helps, FiLM is mixed, and a directional motion loss is\ndetrimental. A real-time stack (~0.3 s latency on one RTX 4090) composes\n\"pick-up\" and \"pass\" into a long-horizon behavior. We surface \"trainer\noverfitting\" to specific demonstrators as the key limitation.\n","authors":["Boshi An","Chenyu Yang","Robert Katzschmann"],"pdf_url":"https://arxiv.org/pdf/2510.25713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.16370v3","updated":"2025-10-29T16:50:36Z","published":"2025-09-19T19:26:22Z","title":"Dual-Regularized Riccati Recursions for Interior-Point Optimal Control","summary":"  We derive closed-form extensions of Riccati's recursions (both sequential and\nparallel) for solving dual-regularized LQR problems. We show how these methods\ncan be used to solve general constrained, non-convex, discrete-time optimal\ncontrol problems via a regularized interior point method, while guaranteeing\nthat each step is a descent direction of an Augmented Barrier-Lagrangian merit\nfunction. We provide MIT-licensed implementations of our methods in C++ and\nJAX.\n","authors":["João Sousa-Pinto","Dominique Orban"],"pdf_url":"https://arxiv.org/pdf/2509.16370v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25650v1","updated":"2025-10-29T16:10:58Z","published":"2025-10-29T16:10:58Z","title":"Collision avoidance and path finding in a robotic mobile fulfillment\n  system using multi-objective meta-heuristics","summary":"  Multi-Agent Path Finding (MAPF) has gained significant attention, with most\nresearch focusing on minimizing collisions and travel time. This paper also\nconsiders energy consumption in the path planning of automated guided vehicles\n(AGVs). It addresses two main challenges: i) resolving collisions between AGVs\nand ii) assigning tasks to AGVs. We propose a new collision avoidance strategy\nthat takes both energy use and travel time into account. For task assignment,\nwe present two multi-objective algorithms: Non-Dominated Sorting Genetic\nAlgorithm (NSGA) and Adaptive Large Neighborhood Search (ALNS). Comparative\nevaluations show that these proposed methods perform better than existing\napproaches in both collision avoidance and task assignment.\n","authors":["Ahmad Kokhahi","Mary Kurz"],"pdf_url":"https://arxiv.org/pdf/2510.25650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25634v1","updated":"2025-10-29T15:39:53Z","published":"2025-10-29T15:39:53Z","title":"Learning to Plan & Schedule with Reinforcement-Learned Bimanual Robot\n  Skills","summary":"  Long-horizon contact-rich bimanual manipulation presents a significant\nchallenge, requiring complex coordination involving a mixture of parallel\nexecution and sequential collaboration between arms. In this paper, we\nintroduce a hierarchical framework that frames this challenge as an integrated\nskill planning & scheduling problem, going beyond purely sequential\ndecision-making to support simultaneous skill invocation. Our approach is built\nupon a library of single-arm and bimanual primitive skills, each trained using\nReinforcement Learning (RL) in GPU-accelerated simulation. We then train a\nTransformer-based planner on a dataset of skill compositions to act as a\nhigh-level scheduler, simultaneously predicting the discrete schedule of skills\nas well as their continuous parameters. We demonstrate that our method achieves\nhigher success rates on complex, contact-rich tasks than end-to-end RL\napproaches and produces more efficient, coordinated behaviors than traditional\nsequential-only planners.\n","authors":["Weikang Wan","Fabio Ramos","Xuning Yang","Caelan Garrett"],"pdf_url":"https://arxiv.org/pdf/2510.25634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25616v1","updated":"2025-10-29T15:20:10Z","published":"2025-10-29T15:20:10Z","title":"Don't Blind Your VLA: Aligning Visual Representations for OOD\n  Generalization","summary":"  The growing success of Vision-Language-Action (VLA) models stems from the\npromise that pretrained Vision-Language Models (VLMs) can endow agents with\ntransferable world knowledge and vision-language (VL) grounding, laying a\nfoundation for action models with broader generalization. Yet when these VLMs\nare adapted to the action modality, it remains unclear to what extent their\noriginal VL representations and knowledge are preserved. In this work, we\nconduct a systematic study of representation retention during VLA fine-tuning,\nshowing that naive action fine-tuning leads to degradation of visual\nrepresentations. To characterize and measure these effects, we probe VLA's\nhidden representations and analyze attention maps, further, we design a set of\ntargeted tasks and methods that contrast VLA models with their counterpart\nVLMs, isolating changes in VL capabilities induced by action fine-tuning. We\nfurther evaluate a range of strategies for aligning visual representations and\nintroduce a simple yet effective method that mitigates degradation and yields\nimproved generalization to out-of-distribution (OOD) scenarios. Taken together,\nour analysis clarifies the trade-off between action fine-tuning and the\ndegradation of VL representations and highlights practical approaches to\nrecover inherited VL capabilities. Code is publicly available:\nhttps://blind-vla-paper.github.io\n","authors":["Nikita Kachaev","Mikhail Kolosov","Daniil Zelezetsky","Alexey K. Kovalev","Aleksandr I. Panov"],"pdf_url":"https://arxiv.org/pdf/2510.25616v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2510.25597v1","updated":"2025-10-29T15:05:40Z","published":"2025-10-29T15:05:40Z","title":"Incorporating Social Awareness into Control of Unknown Multi-Agent\n  Systems: A Real-Time Spatiotemporal Tubes Approach","summary":"  This paper presents a decentralized control framework that incorporates\nsocial awareness into multi-agent systems with unknown dynamics to achieve\nprescribed-time reach-avoid-stay tasks in dynamic environments. Each agent is\nassigned a social awareness index that quantifies its level of cooperation or\nself-interest, allowing heterogeneous social behaviors within the system.\nBuilding on the spatiotemporal tube (STT) framework, we propose a real-time STT\nframework that synthesizes tubes online for each agent while capturing its\nsocial interactions with others. A closed-form, approximation-free control law\nis derived to ensure that each agent remains within its evolving STT, thereby\navoiding dynamic obstacles while also preventing inter-agent collisions in a\nsocially aware manner, and reaching the target within a prescribed time. The\nproposed approach provides formal guarantees on safety and timing, and is\ncomputationally lightweight, model-free, and robust to unknown disturbances.\nThe effectiveness and scalability of the framework are validated through\nsimulation and hardware experiments on a 2D omnidirectional\n","authors":["Siddhartha Upadhyay","Ratnangshu Das","Pushpak Jagtap"],"pdf_url":"https://arxiv.org/pdf/2510.25597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25548v1","updated":"2025-10-29T14:12:45Z","published":"2025-10-29T14:12:45Z","title":"Using VLM Reasoning to Constrain Task and Motion Planning","summary":"  In task and motion planning, high-level task planning is done over an\nabstraction of the world to enable efficient search in long-horizon robotics\nproblems. However, the feasibility of these task-level plans relies on the\ndownward refinability of the abstraction into continuous motion. When a\ndomain's refinability is poor, task-level plans that appear valid may\nultimately fail during motion planning, requiring replanning and resulting in\nslower overall performance. Prior works mitigate this by encoding refinement\nissues as constraints to prune infeasible task plans. However, these approaches\nonly add constraints upon refinement failure, expending significant search\neffort on infeasible branches. We propose VIZ-COAST, a method of leveraging the\ncommon-sense spatial reasoning of large pretrained Vision-Language Models to\nidentify issues with downward refinement a priori, bypassing the need to fix\nthese failures during planning. Experiments on two challenging TAMP domains\nshow that our approach is able to extract plausible constraints from images and\ndomain descriptions, drastically reducing planning times and, in some cases,\neliminating downward refinement failures altogether, generalizing to a diverse\nrange of instances from the broader domain.\n","authors":["Muyang Yan","Miras Mengdibayev","Ardon Floros","Weihang Guo","Lydia E. Kavraki","Zachary Kingston"],"pdf_url":"https://arxiv.org/pdf/2510.25548v1.pdf","comment":"8 pages, 7 figures, 1 table. Submitted to ICRA 2026"},{"id":"http://arxiv.org/abs/2510.25520v1","updated":"2025-10-29T13:43:18Z","published":"2025-10-29T13:43:18Z","title":"Octopus-like Reaching Motion: A Perspective Inspired by Whipping","summary":"  The stereotypical reaching motion of the octopus arm has drawn growing\nattention for its efficient control of a highly deformable body. Previous\nstudies suggest that its characteristic bend propagation may share underlying\nprinciples with the dynamics of a whip. This work investigates whether\nwhip-like passive dynamics in water can reproduce the kinematic features\nobserved in biological reaching and their similarities and differences.\nPlatform-based whipping tests were performed in water and air while\nsystematically varying material stiffness and driving speed. Image-based\nquantification revealed that the Ecoflex Gel 2 arm driven at 150 rpm (motor\nspeed) reproduced curvature propagation similar to that observed in octopus\nreaching. However, its bend-point velocity decreased monotonically rather than\nexhibiting the biological bell-shaped profile, confirming that the octopus\nreaching movement is not merely a passive whipping behavior. The absence of\npropagation in air further highlights the critical role of the surrounding\nmedium in forming octopus-like reaching motion. This study provides a new\nperspective for understand biological reaching movement, and offers a potential\nplatform for future hydrodynamic research.\n","authors":["Shengyao Zhang","Yiyuan Zhang","Chenrui Zhang","Yiming Li","Wenci Xin","Yuliang Liufu","Hong Wei Ng","Cecilia Laschi"],"pdf_url":"https://arxiv.org/pdf/2510.25520v1.pdf","comment":"The first two listed authors contributed equally. Yiyuan Zhang is the\n  corresponding author"},{"id":"http://arxiv.org/abs/2510.23763v2","updated":"2025-10-29T13:37:19Z","published":"2025-10-27T18:49:03Z","title":"RoboOmni: Proactive Robot Manipulation in Omni-modal Context","summary":"  Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.\n","authors":["Siyin Wang","Jinlan Fu","Feihong Liu","Xinzhe He","Huangxuan Wu","Junhao Shi","Kexin Huang","Zhaoye Fei","Jingjing Gong","Zuxuan Wu","Yugang Jiang","See-Kiong Ng","Tat-Seng Chua","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2510.23763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25479v1","updated":"2025-10-29T13:03:06Z","published":"2025-10-29T13:03:06Z","title":"Combining Moving Mass Actuators and Manoeuvring Models for Underwater\n  Vehicles: A Lagrangian Approach","summary":"  In this paper, we present a Newton-Euler formulation of the equations of\nmotion for underwater vehicles with an interntal moving mass actuator.\nFurthermore, the moving mass dynamics are expressed as an extension to the\nmanoeuvring model for underwater vehicles, originally introduced by Fossen\n(1991). The influence of the moving mass is described in body-frame and\nincluded as states in both an additional kinematic equation and as part of the\ncoupled rigid-body kinetics of the underwater vehicle. The Coriolis-centripetal\neffects are derived from Kirchhoff's equations and the hydrostatics are derived\nusing first principals. The proposed Newton-Euler model is validated through\nsimulation and compared with the traditional Hamiltonian internal moving mass\nactuator formulation.\n","authors":["Alexander B. Rambech","Ivar B. Saksvik","Vahid Hassani"],"pdf_url":"https://arxiv.org/pdf/2510.25479v1.pdf","comment":"\\c{opyright} 2025 Alexander Rambech, Ivar Saksvik and Vahid Hassani.\n  Accepted by IFAC for publication under a Creative Commons License CC-BY-NC-ND"},{"id":"http://arxiv.org/abs/2510.25463v1","updated":"2025-10-29T12:37:34Z","published":"2025-10-29T12:37:34Z","title":"SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time,\n  Monocular Depth Estimation in Underwater Environments","summary":"  Underwater infrastructure requires frequent inspection and maintenance due to\nharsh marine conditions. Current reliance on human divers or remotely operated\nvehicles is limited by perceptual and operational challenges, especially around\ncomplex structures or in turbid water. Enhancing the spatial awareness of\nunderwater vehicles is key to reducing piloting risks and enabling greater\nautonomy. To address these challenges, we present SPADE: SParsity Adaptive\nDepth Estimator, a monocular depth estimation pipeline that combines\npre-trained relative depth estimator with sparse depth priors to produce dense,\nmetric scale depth maps. Our two-stage approach first scales the relative depth\nmap with the sparse depth points, then refines the final metric prediction with\nour proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves\nimproved accuracy and generalisation over state-of-the-art baselines and runs\nefficiently at over 15 FPS on embedded hardware, promising to support practical\nunderwater inspection and intervention. This work has been submitted to IEEE\nJournal of Oceanic Engineering Special Issue of AUV 2026.\n","authors":["Hongjie Zhang","Gideon Billings","Stefan B. Williams"],"pdf_url":"https://arxiv.org/pdf/2510.25463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09349v2","updated":"2025-10-29T12:14:41Z","published":"2025-09-11T11:05:14Z","title":"Classification of Driver Behaviour Using External Observation Techniques\n  for Autonomous Vehicles","summary":"  Road traffic accidents remain a significant global concern, with human error,\nparticularly distracted and impaired driving, among the leading causes. This\nstudy introduces a novel driver behaviour classification system that uses\nexternal observation techniques to detect indicators of distraction and\nimpairment. The proposed framework employs advanced computer vision\nmethodologies, including real-time object tracking, lateral displacement\nanalysis, and lane position monitoring. The system identifies unsafe driving\nbehaviours such as excessive lateral movement and erratic trajectory patterns\nby implementing the YOLO object detection model and custom lane estimation\nalgorithms. Unlike systems reliant on inter-vehicular communication, this\nvision-based approach enables behavioural analysis of non-connected vehicles.\nExperimental evaluations on diverse video datasets demonstrate the framework's\nreliability and adaptability across varying road and environmental conditions.\n","authors":["Ian Nell","Shane Gilroy"],"pdf_url":"https://arxiv.org/pdf/2509.09349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25422v1","updated":"2025-10-29T11:42:19Z","published":"2025-10-29T11:42:19Z","title":"Solving the Right Problem with Multi-Robot Formations","summary":"  Formation control simplifies minimizing multi-robot cost functions by\nencoding a cost function as a shape the robots maintain. However, by reducing\ncomplex cost functions to formations, discrepancies arise between maintaining\nthe shape and minimizing the original cost function. For example, a Diamond or\nBox formation shape is often used for protecting all members of the formation.\nWhen more information about the surrounding environment becomes available, a\nstatic shape often no longer minimizes the original protection cost. We propose\na formation planner to reduce mismatch between a formation and the cost\nfunction while still leveraging efficient formation controllers. Our formation\nplanner is a two-step optimization problem that identifies desired relative\nrobot positions. We first solve a constrained problem to estimate non-linear\nand non-differentiable costs with a weighted sum of surrogate cost functions.\nWe theoretically analyze this problem and identify situations where weights do\nnot need to be updated. The weighted, surrogate cost function is then minimized\nusing relative positions between robots. The desired relative positions are\nrealized using a non-cooperative formation controller derived from Lyapunov's\ndirect approach. We then demonstrate the efficacy of this approach for\nmilitary-like costs such as protection and obstacle avoidance. In simulations,\nwe show a formation planner can reduce a single cost by over 75%. When\nminimizing a variety of cost functions simultaneously, using a formation\nplanner with adaptive weights can reduce the cost by 20-40%. Formation planning\nprovides better performance by minimizing a surrogate cost function that\nclosely approximates the original cost function instead of relying on a shape\nabstraction.\n","authors":["Chaz Cornwall","Jeremy P. Bos"],"pdf_url":"https://arxiv.org/pdf/2510.25422v1.pdf","comment":"Submitted to SAE WCX 2026"},{"id":"http://arxiv.org/abs/2510.25405v1","updated":"2025-10-29T11:23:41Z","published":"2025-10-29T11:23:41Z","title":"Sim-to-Real Gentle Manipulation of Deformable and Fragile Objects with\n  Stress-Guided Reinforcement Learning","summary":"  Robotic manipulation of deformable and fragile objects presents significant\nchallenges, as excessive stress can lead to irreversible damage to the object.\nWhile existing solutions rely on accurate object models or specialized sensors\nand grippers, this adds complexity and often lacks generalization. To address\nthis problem, we present a vision-based reinforcement learning approach that\nincorporates a stress-penalized reward to discourage damage to the object\nexplicitly. In addition, to bootstrap learning, we incorporate offline\ndemonstrations as well as a designed curriculum progressing from rigid proxies\nto deformables. We evaluate the proposed method in both simulated and\nreal-world scenarios, showing that the policy learned in simulation can be\ntransferred to the real world in a zero-shot manner, performing tasks such as\npicking up and pushing tofu. Our results show that the learned policies exhibit\na damage-aware, gentle manipulation behavior, demonstrating their effectiveness\nby decreasing the stress applied to fragile objects by 36.5% while achieving\nthe task goals, compared to vanilla RL policies.\n","authors":["Kei Ikemura","Yifei Dong","David Blanco-Mulero","Alberta Longhini","Li Chen","Florian T. Pokorny"],"pdf_url":"https://arxiv.org/pdf/2510.25405v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2510.21758v3","updated":"2025-10-29T11:02:07Z","published":"2025-10-11T20:16:32Z","title":"Taxonomy and Trends in Reinforcement Learning for Robotics and Control\n  Systems: A Structured Review","summary":"  Reinforcement learning (RL) has become a foundational approach for enabling\nintelligent robotic behavior in dynamic and uncertain environments. This work\npresents an in-depth review of RL principles, advanced deep reinforcement\nlearning (DRL) algorithms, and their integration into robotic and control\nsystems. Beginning with the formalism of Markov Decision Processes (MDPs), the\nstudy outlines essential elements of the agent-environment interaction and\nexplores core algorithmic strategies including actor-critic methods,\nvalue-based learning, and policy gradients. Emphasis is placed on modern DRL\ntechniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving\nhigh-dimensional, continuous control tasks. A structured taxonomy is introduced\nto categorize RL applications across domains such as locomotion, manipulation,\nmulti-agent coordination, and human-robot interaction, along with training\nmethodologies and deployment readiness levels. The review synthesizes recent\nresearch efforts, highlighting technical trends, design patterns, and the\ngrowing maturity of RL in real-world robotics. Overall, this work aims to\nbridge theoretical advances with practical implementations, providing a\nconsolidated perspective on the evolving role of RL in autonomous robotic\nsystems.\n","authors":["Kumater Ter","Ore-Ofe Ajayi","Daniel Udekwe"],"pdf_url":"https://arxiv.org/pdf/2510.21758v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25386v1","updated":"2025-10-29T10:57:24Z","published":"2025-10-29T10:57:24Z","title":"Integrating Legal and Logical Specifications in Perception, Prediction,\n  and Planning for Automated Driving: A Survey of Methods","summary":"  This survey provides an analysis of current methodologies integrating legal\nand logical specifications into the perception, prediction, and planning\nmodules of automated driving systems. We systematically explore techniques\nranging from logic-based frameworks to computational legal reasoning\napproaches, emphasizing their capability to ensure regulatory compliance and\ninterpretability in dynamic and uncertain driving environments. A central\nfinding is that significant challenges arise at the intersection of perceptual\nreliability, legal compliance, and decision-making justifiability. To\nsystematically analyze these challenges, we introduce a taxonomy categorizing\nexisting approaches by their theoretical foundations, architectural\nimplementations, and validation strategies. We particularly focus on methods\nthat address perceptual uncertainty and incorporate explicit legal norms,\nfacilitating decisions that are both technically robust and legally defensible.\nThe review covers neural-symbolic integration methods for perception,\nlogic-driven rule representation, and norm-aware prediction strategies, all\ncontributing toward transparent and accountable autonomous vehicle operation.\nWe highlight critical open questions and practical trade-offs that must be\naddressed, offering multidisciplinary insights from engineering, logic, and law\nto guide future developments in legally compliant autonomous driving systems.\n","authors":["Kumar Manas","Mert Keser","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2510.25386v1.pdf","comment":"Accepted to 2025 IEEE International Automated Vehicle Validation\n  Conference (IAVVC)"},{"id":"http://arxiv.org/abs/2411.12308v4","updated":"2025-10-29T10:28:05Z","published":"2024-11-19T07:49:22Z","title":"SNN-Based Online Learning of Concepts and Action Laws in an Open World","summary":"  We present the architecture of a fully autonomous, bio-inspired cognitive\nagent built around a spiking neural network (SNN) implementing the agent's\nsemantic memory. This agent explores its universe and learns concepts of\nobjects/situations and of its own actions in a one-shot manner. While\nobject/situation concepts are unary, action concepts are triples made up of an\ninitial situation, a motor activity, and an outcome. They embody the agent's\nknowledge of its universe's action laws. Both kinds of concepts have different\ndegrees of generality. To make decisions the agent queries its semantic memory\nfor the expected outcomes of envisaged actions and chooses the action to take\non the basis of these predictions. Our experiments show that the agent handles\nnew situations by appealing to previously learned general concepts and rapidly\nmodifies its concepts to adapt to environment changes.\n","authors":["Christel Grimaud","Dominique Longin","Andreas Herzig"],"pdf_url":"https://arxiv.org/pdf/2411.12308v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04864v2","updated":"2025-10-29T10:11:05Z","published":"2025-02-07T12:07:57Z","title":"Redistributing Rewards Across Time and Agents for Multi-Agent\n  Reinforcement Learning","summary":"  Credit assignmen, disentangling each agent's contribution to a shared reward,\nis a critical challenge in cooperative multi-agent reinforcement learning\n(MARL). To be effective, credit assignment methods must preserve the\nenvironment's optimal policy. Some recent approaches attempt this by enforcing\nreturn equivalence, where the sum of distributed rewards must equal the team\nreward. However, their guarantees are conditional on a learned model's\nregression accuracy, making them unreliable in practice. We introduce\nTemporal-Agent Reward Redistribution (TAR$^2$), an approach that decouples\ncredit modeling from this constraint. A neural network learns unnormalized\ncontribution scores, while a separate, deterministic normalization step\nenforces return equivalence by construction. We demonstrate that this method is\nequivalent to a valid Potential-Based Reward Shaping (PBRS), which guarantees\nthe optimal policy is preserved regardless of model accuracy. Empirically, on\nchallenging SMACLite and Google Research Football (GRF) benchmarks, TAR$^2$\naccelerates learning and achieves higher final performance than strong\nbaselines. These results establish our method as an effective solution for the\nagent-temporal credit assignment problem.\n","authors":["Aditya Kapoor","Kale-ab Tessera","Mayank Baranwal","Harshad Khadilkar","Jan Peters","Stefano Albrecht","Mingfei Sun"],"pdf_url":"https://arxiv.org/pdf/2502.04864v2.pdf","comment":"16 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2510.25338v1","updated":"2025-10-29T09:52:40Z","published":"2025-10-29T09:52:40Z","title":"Geometric Robot Calibration Using a Calibration Plate","summary":"  In this paper a new method for geometric robot calibration is introduced,\nwhich uses a calibration plate with precisely known distances between its\nmeasuring points. The relative measurement between two points on the\ncalibration plate is used to determine predefined error parameters of the\nsystem. In comparison to conventional measurement methods, like laser tracker\nor motion capture systems, the calibration plate provides a more mechanically\nrobust and cheaper alternative, which is furthermore easier to transport due to\nits small size. The calibration method, the plate design, the mathematical\ndescription of the error system as well as the identification of the parameters\nare described in detail. For identifying the error parameters, the least\nsquares method and a constrained optimization problem are used. The\nfunctionality of this method was demonstrated in experiments that led to\npromising results, correlated with one of a laser tracker calibration. The\nmodeling and identification of the error parameters is done for a gantry\nmachine, but is not restricted to that type of robot.\n","authors":["Bernhard Rameder","Hubert Gattringer","Andreas Mueller"],"pdf_url":"https://arxiv.org/pdf/2510.25338v1.pdf","comment":"pp 309-317"},{"id":"http://arxiv.org/abs/2510.25335v1","updated":"2025-10-29T09:49:21Z","published":"2025-10-29T09:49:21Z","title":"An approach for combining transparency and motion assistance of a lower\n  body exoskeleton","summary":"  In this paper, an approach for gait assistance with a lower body exoskeleton\nis described. Two concepts, transparency and motion assistance, are combined.\nThe transparent mode, where the system is following the user's free motion with\na minimum of perceived interaction forces, is realized by exploiting the gear\nbacklash of the actuation units. During walking a superimposed assistance mode\napplies an additional torque guiding the legs to their estimated future\nposition. The concept of adaptive oscillators is utilized to learn the\nquasi-periodic signals typical for locomotion. First experiments showed\npromising results.\n","authors":["Jakob Ziegler","Bernhard Rameder","Hubert Gattringer","Andreas Mueller"],"pdf_url":"https://arxiv.org/pdf/2510.25335v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2510.25314v1","updated":"2025-10-29T09:27:38Z","published":"2025-10-29T09:27:38Z","title":"Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired\n  Monocentric Design","summary":"  Achieving high-fidelity, compact RGBD imaging presents a dual challenge:\nconventional compact optics struggle with RGB sharpness across the entire\ndepth-of-field, while software-only Monocular Depth Estimation (MDE) is an\nill-posed problem reliant on unreliable semantic priors. While deep optics with\nelements like DOEs can encode depth, they introduce trade-offs in fabrication\ncomplexity and chromatic aberrations, compromising simplicity. To address this,\nwe first introduce a novel bio-inspired all-spherical monocentric lens, around\nwhich we build the Bionic Monocentric Imaging (BMI) framework, a holistic\nco-design. This optical design naturally encodes depth into its depth-varying\nPoint Spread Functions (PSFs) without requiring complex diffractive or freeform\nelements. We establish a rigorous physically-based forward model to generate a\nsynthetic dataset by precisely simulating the optical degradation process. This\nsimulation pipeline is co-designed with a dual-head, multi-scale reconstruction\nnetwork that employs a shared encoder to jointly recover a high-fidelity\nAll-in-Focus (AiF) image and a precise depth map from a single coded capture.\nExtensive experiments validate the state-of-the-art performance of the proposed\nframework. In depth estimation, the method attains an Abs Rel of 0.026 and an\nRMSE of 0.130, markedly outperforming leading software-only approaches and\nother deep optics systems. For image restoration, the system achieves an SSIM\nof 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior\nbalance between image fidelity and depth accuracy. This study illustrates that\nthe integration of bio-inspired, fully spherical optics with a joint\nreconstruction algorithm constitutes an effective strategy for addressing the\nintrinsic challenges in high-performance compact RGBD imaging. Source code will\nbe publicly available at https://github.com/ZongxiYu-ZJU/BMI.\n","authors":["Zongxi Yu","Xiaolong Qian","Shaohua Gao","Qi Jiang","Yao Gao","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25314v1.pdf","comment":"The source code will be publicly available at\n  https://github.com/ZongxiYu-ZJU/BMI"},{"id":"http://arxiv.org/abs/2510.25280v1","updated":"2025-10-29T08:38:05Z","published":"2025-10-29T08:38:05Z","title":"Development of Implicit-Explicit Control Based Amphibious Centipede-Type\n  Robot and Evaluation of its Mobile Performance","summary":"  Multi-legged mobile robots possess high mobility performance in rough terrain\nenvironments, stemming from their high postural stability, joint flexibility,\nand the redundancy provided by multiple legs. In prior research on navigating\nbetween different environments such as land and water, the primary strategy\nemployed involves switching to a controller that generates an appropriate gait\nfor the new environment upon entering it. However, designing appropriate gaits\nfor each complex and diverse environment and accurately determining controller\nswitching for each environment is challenging. Therefore, this research\ndevelops a centipede-type mobile robot that navigates both aquatic and\nterrestrial environments with a simple, unified control scheme, based on the\nimplicit-explicit control philosophy and by ingeniously designing the robot's\nbody structure. In this research, we developed the robot featuring flexible\njoints and left and right legs on each body segment and focused on the leg\nstructure which has extensive contact with the environment. This paper\nevaluates the locomotion performance on land and water using the three\ndeveloped leg structures, using the robot's leg slip rate and actuator energy\nconsumption as evaluation metrics. The experimental results confirmed the\nexistence of an appropriate leg structure capable of navigating both aquatic\nand terrestrial environments under identical control.\n","authors":["Yusuke Tsunoda","Seiya Yamamoto","Kazuki Ito","Runze Xiao","Keisuke Naniwa","Koichi Osuka"],"pdf_url":"https://arxiv.org/pdf/2510.25280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25268v1","updated":"2025-10-29T08:27:00Z","published":"2025-10-29T08:27:00Z","title":"SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object\n  with Discrete Human Object Interaction Representation","summary":"  Generating hand grasps with language instructions is a widely studied topic\nthat benefits from embodied AI and VR/AR applications. While transferring into\nhand articulatied object interaction (HAOI), the hand grasps synthesis requires\nnot only object functionality but also long-term manipulation sequence along\nthe object deformation. This paper proposes a novel HAOI sequence generation\nframework SynHLMA, to synthesize hand language manipulation for articulated\nobjects. Given a complete point cloud of an articulated object, we utilize a\ndiscrete HAOI representation to model each hand object interaction frame. Along\nwith the natural language embeddings, the representations are trained by an\nHAOI manipulation language model to align the grasping process with its\nlanguage description in a shared representation space. A joint-aware loss is\nemployed to ensure hand grasps follow the dynamic variations of articulated\nobject joints. In this way, our SynHLMA achieves three typical hand\nmanipulation tasks for articulated objects of HAOI generation, HAOI prediction\nand HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and\nexperimental results demonstrate the superior hand grasp sequence generation\nperformance comparing with state-of-the-art. We also show a robotics grasp\napplication that enables dexterous grasps execution from imitation learning\nusing the manipulation sequence provided by our SynHLMA. Our codes and datasets\nwill be made publicly available.\n","authors":["Wang zhi","Yuyan Liu","Liu Liu","Li Zhang","Ruixuan Lu","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2510.25268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25255v1","updated":"2025-10-29T08:09:32Z","published":"2025-10-29T08:09:32Z","title":"Time-Optimal Transport of Loosely Placed Liquid Filled Cups along\n  Prescribed Paths","summary":"  Handling loosely placed objects with robotic manipulators is a difficult task\nfrom the point of view of trajectory planning and control. This becomes even\nmore challenging when the object to be handled is a container filled with\nliquid. This paper addresses the task of transporting a liquid-filled cup\nplaced on a tray along a prescribed path in shortest time. The objective is to\nminimize swapping, thus avoiding spillage of the fluid. To this end, the\nsloshing dynamics is incorporated into the dynamic model used within the\noptimal control problem formulation. The optimization problem is solved using a\ndirect multiple shooting approach.\n","authors":["Klaus Zauner","Hubert Gattringer","Andreas Mueller"],"pdf_url":"https://arxiv.org/pdf/2510.25255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25241v1","updated":"2025-10-29T07:48:10Z","published":"2025-10-29T07:48:10Z","title":"One-shot Humanoid Whole-body Motion Learning","summary":"  Whole-body humanoid motion represents a cornerstone challenge in robotics,\nintegrating balance, coordination, and adaptability to enable human-like\nbehaviors. However, existing methods typically require multiple training\nsamples per motion category, rendering the collection of high-quality human\nmotion datasets both labor-intensive and costly. To address this, we propose a\nnovel approach that trains effective humanoid motion policies using only a\nsingle non-walking target motion sample alongside readily available walking\nmotions. The core idea lies in leveraging order-preserving optimal transport to\ncompute distances between walking and non-walking sequences, followed by\ninterpolation along geodesics to generate new intermediate pose skeletons,\nwhich are then optimized for collision-free configurations and retargeted to\nthe humanoid before integration into a simulated environment for policy\ntraining via reinforcement learning. Experimental evaluations on the CMU MoCap\ndataset demonstrate that our method consistently outperforms baselines,\nachieving superior performance across metrics. Code will be released upon\nacceptance.\n","authors":["Hao Huang","Geeta Chandra Raju Bethala","Shuaihang Yuan","Congcong Wen","Anthony Tzes","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2510.25241v1.pdf","comment":"10 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2409.02635v3","updated":"2025-10-29T07:39:26Z","published":"2024-09-04T11:58:52Z","title":"Optimal Kinematic Synthesis and Prototype Development of Knee\n  Exoskeleton","summary":"  The range of rotation (RoR) in a knee exoskeleton is a critical factor in\nrehabilitation, as it directly influences joint mobility, muscle activation,\nand recovery outcomes. A well-designed RoR ensures that patients achieve\nnear-natural knee kinematics, which is essential for restoring gait patterns\nand preventing compensatory movements. This paper presents optimal design of\none degree of freedom knee exoskeleton. In kinematic analysis, the existing\ndesign being represented by nonlinear and nonconvex mathematical functions. To\nobtain feasible and optimum measurement of the links of knee exoskeleton, an\noptimization problem is formulated based on the kinematic analysis and average\nhuman's leg measurement. The optimized solution increases the range of motion\nof knee exoskeleton during sit to stand motion by $24 \\%$ as compared with\ninspired design. Furthermore, misalignment study is conducted by comparing the\ntrajectory of human's knee and exoskeleton's knee during sit to stand motion.\nThe joint movement is calculated using marker and camera system. Finally, a\nprototype of the knee joint exoskeleton is being developed based on optimal\ndimensions which validate the maximum range of motion achieved during\nsimulation.\n","authors":["Shashank Mani Gautam","Ekta Singla","Ashish Singla"],"pdf_url":"https://arxiv.org/pdf/2409.02635v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25233v1","updated":"2025-10-29T07:28:32Z","published":"2025-10-29T07:28:32Z","title":"Hybrid Vision Servoing with Depp Alignment and GRU-Based Occlusion\n  Recovery","summary":"  Vision-based control systems, such as image-based visual servoing (IBVS),\nhave been extensively explored for precise robot manipulation. A persistent\nchallenge, however, is maintaining robust target tracking under partial or full\nocclusions. Classical methods like Lucas-Kanade (LK) offer lightweight tracking\nbut are fragile to occlusion and drift, while deep learning-based approaches\noften require continuous visibility and intensive computation. To address these\ngaps, we propose a hybrid visual tracking framework that bridges advanced\nperception with real-time servo control. First, a fast global template matcher\nconstrains the pose search region; next, a deep-feature Lucas-Kanade module\noperating on early VGG layers refines alignment to sub-pixel accuracy (<2px);\nthen, a lightweight residual regressor corrects local misalignments caused by\ntexture degradation or partial occlusion. When visual confidence falls below a\nthreshold, a GRU-based predictor seamlessly extrapolates pose updates from\nrecent motion history. Crucially, the pipeline's final outputs-translation,\nrotation, and scale deltas-are packaged as direct control signals for 30Hz\nimage-based servo loops. Evaluated on handheld video sequences with up to 90%\nocclusion, our system sustains under 2px tracking error, demonstrating the\nrobustness and low-latency precision essential for reliable real-world robot\nvision applications.\n","authors":["Jee Won Lee","Hansol Lim","Sooyeun Yang","Jongseong Brad Choi"],"pdf_url":"https://arxiv.org/pdf/2510.25233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08772v4","updated":"2025-10-29T06:28:57Z","published":"2023-04-18T07:06:07Z","title":"Multi-robot Motion Planning based on Nets-within-Nets Modeling and\n  Simulation","summary":"  This paper focuses on designing motion plans for a heterogeneous team of\nrobots that must cooperate to fulfill a global mission. Robots move in an\nenvironment that contains some regions of interest, while the specification for\nthe entire team can include avoidance, visits, or sequencing of these regions\nof interest. The mission is expressed in terms of a Petri net corresponding to\nan automaton, while each robot is also modeled by a state machine Petri net.\nThe current work brings about the following contributions with respect to\nexisting solutions for related problems. First, we propose a novel model,\ndenoted High-Level robot team Petri Net (HLrtPN) system, to incorporate the\nspecification and robot models into the Nets-within-Nets paradigm. A guard\nfunction, named Global Enabling Function, is designed to synchronize the firing\nof transitions so that robot motions do not violate the specification. Then,\nthe solution is found by simulating the HLrtPN system in a specific software\ntool that accommodates Nets-within-Nets. Illustrative examples based on Linear\nTemporal Logic missions support the computational feasibility of the proposed\nframework.\n","authors":["Sofia Hustiu","Joaquin Ezpeleta","Cristian Mahulea","Marius Kloetzer"],"pdf_url":"https://arxiv.org/pdf/2304.08772v4.pdf","comment":"[Note for readers] This paper has been extended from a previous\n  submission to 62nd IEEE Conference on Decision and Control, Dec. 13-15, 2023.\n  This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2510.25211v1","updated":"2025-10-29T06:23:12Z","published":"2025-10-29T06:23:12Z","title":"RoadSens-4M: A Multimodal Smartphone & Camera Dataset for Holistic\n  Road-way Analysis","summary":"  It's important to monitor road issues such as bumps and potholes to enhance\nsafety and improve road conditions. Smartphones are equipped with various\nbuilt-in sensors that offer a cost-effective and straightforward way to assess\nroad quality. However, progress in this area has been slow due to the lack of\nhigh-quality, standardized datasets. This paper discusses a new dataset created\nby a mobile app that collects sensor data from devices like GPS,\naccelerometers, gyroscopes, magnetometers, gravity sensors, and orientation\nsensors. This dataset is one of the few that integrates Geographic Information\nSystem (GIS) data with weather information and video footage of road\nconditions, providing a comprehensive understanding of road issues with\ngeographic context. The dataset allows for a clearer analysis of road\nconditions by compiling essential data, including vehicle speed, acceleration,\nrotation rates, and magnetic field intensity, along with the visual and spatial\ncontext provided by GIS, weather, and video data. Its goal is to provide\nfunding for initiatives that enhance traffic management, infrastructure\ndevelopment, road safety, and urban planning. Additionally, the dataset will be\npublicly accessible to promote further research and innovation in smart\ntransportation systems.\n","authors":["Amith Khandakar","David Michelson","Shaikh Golam Rabbani","Fariya Bintay Shafi","Md. Faysal Ahamed","Khondokar Radwanur Rahman","Md Abidur Rahman","Md. Fahmidun Nabi","Mohamed Arselene Ayari","Khaled Khan","Ponnuthurai Nagaratnam Suganthan"],"pdf_url":"https://arxiv.org/pdf/2510.25211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25191v1","updated":"2025-10-29T05:46:29Z","published":"2025-10-29T05:46:29Z","title":"SoraNav: Adaptive UAV Task-Centric Navigation via Zeroshot VLM Reasoning","summary":"  Interpreting visual observations and natural language instructions for\ncomplex task execution remains a key challenge in robotics and AI. Despite\nrecent advances, language-driven navigation is still difficult, particularly\nfor UAVs in small-scale 3D environments. Existing Vision-Language Navigation\n(VLN) approaches are mostly designed for ground robots and struggle to\ngeneralize to aerial tasks that require full 3D spatial reasoning. The\nemergence of large Vision-Language Models (VLMs), such as GPT and Claude,\nenables zero-shot semantic reasoning from visual and textual inputs. However,\nthese models lack spatial grounding and are not directly applicable to\nnavigation. To address these limitations, SoraNav is introduced, an adaptive\nUAV navigation framework that integrates zero-shot VLM reasoning with\ngeometry-aware decision-making. Geometric priors are incorporated into image\nannotations to constrain the VLM action space and improve decision quality. A\nhybrid switching strategy leverages navigation history to alternate between VLM\nreasoning and geometry-based exploration, mitigating dead-ends and redundant\nrevisits. A PX4-based hardware-software platform, comprising both a digital\ntwin and a physical micro-UAV, enables reproducible evaluation. Experimental\nresults show that in 2.5D scenarios, our method improves Success Rate (SR) by\n25.7% and Success weighted by Path Length (SPL) by 17%. In 3D scenarios, it\nimproves SR by 29.5% and SPL by 18.5% relative to the baseline.\n","authors":["Hongyu Song","Rishabh Dev Yadav","Cheng Guo","Wei Pan"],"pdf_url":"https://arxiv.org/pdf/2510.25191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13654v2","updated":"2025-10-29T03:44:09Z","published":"2025-07-18T04:48:46Z","title":"Control Modes of Teleoperated Surgical Robotic System's Tools in\n  Ophthalmic Surgery","summary":"  The introduction of a teleoperated surgical robotic system designed for\nminimally invasive procedures enables the emulation of two distinct control\nmodes through a dedicated input device of the surgical console: (1) Inside\nControl Mode, which emulates tool manipulation near the distal end as if the\nsurgeon was holding the tip of the instrument inside the patient's body; (2)\nOutside Control Mode, which emulates manipulation near the proximal end as if\nthe surgeon was holding the tool externally. The aim of this research is to\ncompare the surgeon's performance on these two modes of operation along with\nvarious scaling factors in a simulated vitreoretinal surgical setting. The\nconsole of Intraocular Robotic Interventional Surgical System (IRISS) was\nutilized but the surgical robot itself and the human eye anatomy was simulated\nby a virtual environment projected microscope view of an intraocular setup to a\nVR headset. Five experienced vitreoretinal surgeons and five subjects with no\nsurgical experience used the system to perform four fundamental tool/tissue\ntasks common to vitreoretinal surgery: touch and reset; grasp and drop; inject;\ncircular tracking. Results indicate that Inside Control outperforms Outside\nControl across multiple tasks and metrics. Higher scaling factors generally\nperformed better, particularly for reducing trajectory errors and tissue\ndamage. This improvement suggests that larger scaling factors enable more\nprecise control, making them the preferred option for fine manipulation.\nHowever, completion time was not consistently reduced across all conditions,\nindicating that surgeons need to balance speed and accuracy based on surgical\nrequirements. By optimizing control dynamics and user interface, robotic\nteleoperation has the potential to reduce complications, enhance dexterity, and\nexpand the accessibility of high precision procedures to a broader range of\npractitioners.\n","authors":["Haoran Wang","Yasamin Foroutani","Matthew Nepo","Mercedes Rodriguez","Ji Ma","Jean-Pierre Hubschman","Tsu-Chin Tsao","Jacob Rosen"],"pdf_url":"https://arxiv.org/pdf/2507.13654v2.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2506.06677v2","updated":"2025-10-29T03:38:36Z","published":"2025-06-07T06:15:49Z","title":"RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic\n  Manipulation Evaluation","summary":"  Recent advances in vision-language models (VLMs) have enabled\ninstruction-conditioned robotic systems with improved generalization. However,\nmost existing work focuses on reactive System 1 policies, underutilizing VLMs'\nstrengths in semantic reasoning and long-horizon planning. These System 2\ncapabilities-characterized by deliberative, goal-directed thinking-remain under\nexplored due to the limited temporal scale and structural complexity of current\nbenchmarks. To address this gap, we introduce RoboCerebra, a benchmark for\nevaluating high-level reasoning in long-horizon robotic manipulation.\nRoboCerebra includes: (1) a large-scale simulation dataset with extended task\nhorizons and diverse subtask sequences in household environments; (2) a\nhierarchical framework combining a high-level VLM planner with a low-level\nvision-language-action (VLA) controller; and (3) an evaluation protocol\ntargeting planning, reflection, and memory through structured System 1-System 2\ninteraction. The dataset is constructed via a top-down pipeline, where GPT\ngenerates task instructions and decomposes them into subtask sequences. Human\noperators execute the subtasks in simulation, yielding high-quality\ntrajectories with dynamic object variations. Compared to prior benchmarks,\nRoboCerebra features significantly longer action sequences and denser\nannotations. We further benchmark state-of-the-art VLMs as System 2 modules and\nanalyze their performance across key cognitive dimensions, advancing the\ndevelopment of more capable and generalizable robotic planners.\n","authors":["Songhao Han","Boxiang Qiu","Yue Liao","Siyuan Huang","Chen Gao","Shuicheng Yan","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2506.06677v2.pdf","comment":"25 pages, 18 figures, Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25138v1","updated":"2025-10-29T03:32:47Z","published":"2025-10-29T03:32:47Z","title":"Learning Spatial-Aware Manipulation Ordering","summary":"  Manipulation in cluttered environments is challenging due to spatial\ndependencies among objects, where an improper manipulation order can cause\ncollisions or blocked access. Existing approaches often overlook these spatial\nrelationships, limiting their flexibility and scalability. To address these\nlimitations, we propose OrderMind, a unified spatial-aware manipulation\nordering framework that directly learns object manipulation priorities based on\nspatial context. Our architecture integrates a spatial context encoder with a\ntemporal priority structuring module. We construct a spatial graph using\nk-Nearest Neighbors to aggregate geometric information from the local layout\nand encode both object-object and object-manipulator interactions to support\naccurate manipulation ordering in real-time. To generate physically and\nsemantically plausible supervision signals, we introduce a spatial prior\nlabeling method that guides a vision-language model to produce reasonable\nmanipulation orders for distillation. We evaluate OrderMind on our Manipulation\nOrdering Benchmark, comprising 163,222 samples of varying difficulty. Extensive\nexperiments in both simulation and real-world environments demonstrate that our\nmethod significantly outperforms prior approaches in effectiveness and\nefficiency, enabling robust manipulation in cluttered scenes.\n","authors":["Yuxiang Yan","Zhiyuan Zhou","Xin Gao","Guanghao Li","Shenglin Li","Jiaqi Chen","Qunyan Pu","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2510.25138v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25122v1","updated":"2025-10-29T03:00:36Z","published":"2025-10-29T03:00:36Z","title":"NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies","summary":"  Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware.\n","authors":["Jiahong Chen","Jing Wang","Long Chen","Chuwei Cai","Jinghui Lu"],"pdf_url":"https://arxiv.org/pdf/2510.25122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25086v1","updated":"2025-10-29T01:49:54Z","published":"2025-10-29T01:49:54Z","title":"Mean-Shift Theory and Its Applications in Swarm Robotics: A New Way to\n  Enhance the Efficiency of Multi-Robot Collaboration","summary":"  Swarms evolving from collective behaviors among multiple individuals are\ncommonly seen in nature, which enables biological systems to exhibit more\nefficient and robust collaboration. Creating similar swarm intelligence in\nengineered robots poses challenges to the design of collaborative algorithms\nthat can be programmed at large scales. The assignment-based method has played\nan eminent role for a very long time in solving collaboration problems of robot\nswarms. However, it faces fundamental limitations in terms of efficiency and\nrobustness due to its unscalability to swarm variants. This article presents a\ntutorial review on recent advances in assignment-free collaboration of robot\nswarms, focusing on the problem of shape formation. A key theoretical component\nis the recently developed \\emph{mean-shift exploration} strategy, which\nimproves the collaboration efficiency of large-scale swarms by dozens of times.\nFurther, the efficiency improvement is more significant as the swarm scale\nincreases. Finally, this article discusses three important applications of the\nmean-shift exploration strategy, including precise shape formation, area\ncoverage formation, and maneuvering formation, as well as their corresponding\nindustrial scenarios in smart warehousing, area exploration, and cargo\ntransportation.\n","authors":["Guibin Sun","Jinhu Lü","Kexin Liu","Zhenqian Wang","Guanrong Chen"],"pdf_url":"https://arxiv.org/pdf/2510.25086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08841v2","updated":"2025-10-29T01:48:17Z","published":"2025-04-10T19:43:00Z","title":"ES-HPC-MPC: Exponentially Stable Hybrid Perception Constrained MPC for\n  Quadrotor with Suspended Payloads","summary":"  Aerial transportation using quadrotors with cable-suspended payloads holds\ngreat potential for applications in disaster response, logistics, and\ninfrastructure maintenance. However, their hybrid and underactuated dynamics\npose significant control and perception challenges. Traditional approaches\noften assume a taut cable condition, limiting their effectiveness in real-world\napplications where slack-to-taut transitions occur due to disturbances. We\nintroduce ES-HPC-MPC, a model predictive control framework that enforces\nexponential stability and perception-constrained control under hybrid dynamics.\n  Our method leverages Exponentially Stabilizing Control Lyapunov Functions\n(ES-CLFs) to enforce stability during the tasks and Control Barrier Functions\n(CBFs) to maintain the payload within the onboard camera's field of view (FoV).\nWe validate our method through both simulation and real-world experiments,\ndemonstrating stable trajectory tracking and reliable payload perception. We\nvalidate that our method maintains stability and satisfies perception\nconstraints while tracking dynamically infeasible trajectories and when the\nsystem is subjected to hybrid mode transitions caused by unexpected\ndisturbances.\n","authors":["Luis F. Recalde","Mrunal Sarvaiya","Giuseppe Loianno","Guanrui Li"],"pdf_url":"https://arxiv.org/pdf/2504.08841v2.pdf","comment":"Accepted to IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2510.25797v1","updated":"2025-10-29T01:22:42Z","published":"2025-10-29T01:22:42Z","title":"Enhancing Underwater Object Detection through Spatio-Temporal Analysis\n  and Spatial Attention Networks","summary":"  This study examines the effectiveness of spatio-temporal modeling and the\nintegration of spatial attention mechanisms in deep learning models for\nunderwater object detection. Specifically, in the first phase, the performance\nof temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with\nthe standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is\ndeveloped, through the addition of a Convolutional Block Attention Module\n(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and\nT-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the\nresearch highlights how temporal modeling improves detection accuracy in\ndynamic marine environments, particularly under conditions of sudden movements,\npartial occlusions, and gradual motion. The testing results showed that YOLOv5\nachieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM\noutperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,\nhighlighting their superior accuracy and generalization in detecting complex\nobjects. The findings demonstrate that T-YOLOv5 significantly enhances\ndetection reliability compared to the standard model, while T-YOLOv5 with CBAM\nfurther improves performance in challenging scenarios, although there is a loss\nof accuracy when it comes to simpler scenarios.\n","authors":["Sai Likhith Karri","Ansh Saxena"],"pdf_url":"https://arxiv.org/pdf/2510.25797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25072v1","updated":"2025-10-29T01:17:48Z","published":"2025-10-29T01:17:48Z","title":"Non-Invasive Calibration Of A Stewart Platform By Photogrammetry","summary":"  Accurate calibration of a Stewart platform is important for their precise and\nefficient operation. However, the calibration of these platforms using forward\nkinematics is a challenge for researchers because forward kinematics normally\ngenerates multiple feasible and unfeasible solutions for any pose of the moving\nplatform. The complex kinematic relations among the six actuator paths\nconnecting the fixed base to the moving platform further compound the\ndifficulty in establishing a straightforward and efficient calibration method.\nThe authors developed a new forward kinematics-based calibration method using\nDenavit-Hartenberg convention and used the Stewart platform Tiger 66.1\ndeveloped in their lab for experimenting with the photogrammetry-based\ncalibration strategies described in this paper. This system became operational\nupon completion of construction, marking its inaugural use. The authors used\ntheir calibration model for estimating the errors in the system and adopted\nthree compensation options or strategies as per Least Square method to improve\nthe accuracy of the system. These strategies leveraged a high-resolution\ndigital camera and off-the-shelf software to capture the poses of the moving\nplatform's center. This process is non-invasive and does not need any\nadditional equipment to be attached to the hexapod or any alteration of the\nhexapod hardware. This photogrammetry-based calibration process involves\nmultiple high-resolution images from different angles to measure the position\nand orientation of the platform center in the three-dimensional space. The\nTarget poses and Actual poses are then compared, and the error compensations\nare estimated using the Least-Squared methods to calculate the Predicted poses.\nResults from each of the three compensation approaches demonstrated noticeable\nenhancements in platform pose accuracies, suggesting room for further\nimprovements.\n","authors":["Sourabh Karmakar","Cameron J. Turner"],"pdf_url":"https://arxiv.org/pdf/2510.25072v1.pdf","comment":"The International Journal of Advanced Manufacturing Technology, 2024"},{"id":"http://arxiv.org/abs/2510.25053v1","updated":"2025-10-29T00:39:09Z","published":"2025-10-29T00:39:09Z","title":"Scalable predictive processing framework for multitask caregiving robots","summary":"  The rapid aging of societies is intensifying demand for autonomous care\nrobots; however, most existing systems are task-specific and rely on\nhandcrafted preprocessing, limiting their ability to generalize across diverse\nscenarios. A prevailing theory in cognitive neuroscience proposes that the\nhuman brain operates through hierarchical predictive processing, which\nunderlies flexible cognition and behavior by integrating multimodal sensory\nsignals. Inspired by this principle, we introduce a hierarchical multimodal\nrecurrent neural network grounded in predictive processing under the\nfree-energy principle, capable of directly integrating over 30,000-dimensional\nvisuo-proprioceptive inputs without dimensionality reduction. The model was\nable to learn two representative caregiving tasks, rigid-body repositioning and\nflexible-towel wiping, without task-specific feature engineering. We\ndemonstrate three key properties: (i) self-organization of hierarchical latent\ndynamics that regulate task transitions, capture variability in uncertainty,\nand infer occluded states; (ii) robustness to degraded vision through\nvisuo-proprioceptive integration; and (iii) asymmetric interference in\nmultitask learning, where the more variable wiping task had little influence on\nrepositioning, whereas learning the repositioning task led to a modest\nreduction in wiping performance, while the model maintained overall robustness.\nAlthough the evaluation was limited to simulation, these results establish\npredictive processing as a universal and scalable computational principle,\npointing toward robust, flexible, and autonomous caregiving robots while\noffering theoretical insight into the human brain's ability to achieve flexible\nadaptation in uncertain real-world environments.\n","authors":["Hayato Idei","Tamon Miyake","Tetsuya Ogata","Yuichi Yamashita"],"pdf_url":"https://arxiv.org/pdf/2510.25053v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2510.05014v3","updated":"2025-10-29T23:44:26Z","published":"2025-10-06T16:53:56Z","title":"Think Then Embed: Generative Context Improves Multimodal Embedding","summary":"  There is a growing interest in Universal Multimodal Embeddings (UME), where\nmodels are required to generate task-specific representations. While recent\nstudies show that Multimodal Large Language Models (MLLMs) perform well on such\ntasks, they treat MLLMs solely as encoders, overlooking their generative\ncapacity. However, such an encoding paradigm becomes less effective as\ninstructions become more complex and require compositional reasoning. Inspired\nby the proven effectiveness of chain-of-thought reasoning, we propose a general\nThink-Then-Embed (TTE) framework for UME, composed of a reasoner and an\nembedder. The reasoner MLLM first generates reasoning traces that explain\ncomplex queries, followed by an embedder that produces representations\nconditioned on both the original query and the intermediate reasoning. This\nexplicit reasoning step enables more nuanced understanding of complex\nmultimodal instructions. Our contributions are threefold. First, by leveraging\na powerful MLLM reasoner, we achieve state-of-the-art performance on the\nMMEB-V2 benchmark, surpassing proprietary models trained on massive in-house\ndatasets. Second, to reduce the dependency on large MLLM reasoners, we finetune\na smaller MLLM reasoner using high-quality embedding-centric reasoning traces,\nachieving the best performance among open-source models with a 7% absolute gain\nover recently proposed models. Third, we investigate strategies for integrating\nthe reasoner and embedder into a unified model for improved efficiency without\nsacrificing performance.\n","authors":["Xuanming Cui","Jianpeng Cheng","Hong-you Chen","Satya Narayan Shukla","Abhijeet Awasthi","Xichen Pan","Chaitanya Ahuja","Shlok Kumar Mishra","Yonghuan Yang","Jun Xiao","Qi Guo","Ser-Nam Lim","Aashu Singh","Xiangjun Fan"],"pdf_url":"https://arxiv.org/pdf/2510.05014v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26024v1","updated":"2025-10-29T23:37:54Z","published":"2025-10-29T23:37:54Z","title":"Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural\n  Erasure in Multilingual LLMs","summary":"  Cross-lingual alignment (CLA) aims to align multilingual representations,\nenabling Large Language Models (LLMs) to seamlessly transfer knowledge across\nlanguages. While intuitive, we hypothesize, this pursuit of representational\nconvergence can inadvertently cause \"cultural erasure\", the functional loss of\nproviding culturally-situated responses that should diverge based on the query\nlanguage. In this work, we systematically analyze this trade-off by introducing\na holistic evaluation framework, the transfer-localization plane, which\nquantifies both desirable knowledge transfer and undesirable cultural erasure.\nUsing this framework, we re-evaluate recent CLA approaches and find that they\nconsistently improve factual transfer at the direct cost of cultural\nlocalization across all six languages studied. Our investigation into the\ninternal representations of these models reveals a key insight: universal\nfactual transfer and culturally-specific knowledge are optimally steerable at\ndifferent model layers. Based on this finding, we propose Surgical Steering, a\nnovel inference-time method that disentangles these two objectives. By applying\ntargeted activation steering to distinct layers, our approach achieves a better\nbalance between the two competing dimensions, effectively overcoming the\nlimitations of current alignment techniques.\n","authors":["HyoJung Han","Sweta Agrawal","Eleftheria Briakou"],"pdf_url":"https://arxiv.org/pdf/2510.26024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26023v1","updated":"2025-10-29T23:33:31Z","published":"2025-10-29T23:33:31Z","title":"Large Language Model-assisted Autonomous Vehicle Recovery from\n  Immobilization","summary":"  Despite significant advancements in recent decades, autonomous vehicles (AVs)\ncontinue to face challenges in navigating certain traffic scenarios where human\ndrivers excel. In such situations, AVs often become immobilized, disrupting\noverall traffic flow. Current recovery solutions, such as remote intervention\n(which is costly and inefficient) and manual takeover (which excludes\nnon-drivers and limits AV accessibility), are inadequate. This paper introduces\nStuckSolver, a novel Large Language Model (LLM) driven recovery framework that\nenables AVs to resolve immobilization scenarios through self-reasoning and/or\npassenger-guided decision-making. StuckSolver is designed as a plug-in add-on\nmodule that operates on top of the AV's existing perception-planning-control\nstack, requiring no modification to its internal architecture. Instead, it\ninterfaces with standard sensor data streams to detect immobilization states,\ninterpret environmental context, and generate high-level recovery commands that\ncan be executed by the AV's native planner. We evaluate StuckSolver on the\nBench2Drive benchmark and in custom-designed uncertainty scenarios. Results\nshow that StuckSolver achieves near-state-of-the-art performance through\nautonomous self-reasoning alone and exhibits further improvements when\npassenger guidance is incorporated.\n","authors":["Zhipeng Bao","Qianwen Li"],"pdf_url":"https://arxiv.org/pdf/2510.26023v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2505.02820v3","updated":"2025-10-29T23:29:35Z","published":"2025-05-05T17:47:49Z","title":"AutoLibra: Agent Metric Induction from Open-Ended Human Feedback","summary":"  Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose **AutoLibra**, a framework for\nagent evaluation, that transforms open-ended human feedback *e.g.* \"If you find\nthat the button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\" into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra serve human prompt engineers for\ndiagonalize agent failures and improve prompts iterative. Moreover, we find\nthat AutoLibra can induce metrics for automatic optimization for agents, which\nmakes agents improve through self-regulation. Our results suggest that\nAutoLibra is a powerful task-agnostic tool for evaluating and improving\nlanguage agents.\n","authors":["Hao Zhu","Phil Cuvin","Xinkai Yu","Charlotte Ka Yee Yan","Jason Zhang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2505.02820v3.pdf","comment":"https://github.com/Open-Social-World/autolibra"},{"id":"http://arxiv.org/abs/2510.26020v1","updated":"2025-10-29T23:28:53Z","published":"2025-10-29T23:28:53Z","title":"PORTool: Tool-Use LLM Training with Rewarded Tree","summary":"  Current tool-use large language models (LLMs) are trained on static datasets,\nenabling them to interact with external tools and perform multi-step,\ntool-integrated reasoning, which produces tool-call trajectories. However,\nthese models imitate how a query is resolved in a generic tool-call routine,\nthereby failing to explore possible solutions and demonstrating limited\nperformance in an evolved, dynamic tool-call environment. In this work, we\npropose PORTool, a reinforcement learning (RL) method that encourages a\ntool-use LLM to explore various trajectories yielding the correct answer.\nSpecifically, this method starts with generating multiple rollouts for a given\nquery, and some of them share the first few tool-call steps, thereby forming a\ntree-like structure. Next, we assign rewards to each step, based on its ability\nto produce a correct answer and make successful tool calls. A shared step\nacross different trajectories receives the same reward, while different steps\nunder the same fork receive different rewards. Finally, these step-wise rewards\nare used to calculate fork-relative advantages, blended with\ntrajectory-relative advantages, to train the LLM for tool use. The experiments\nutilize 17 tools to address user queries, covering both time-sensitive and\ntime-invariant topics. We conduct ablation studies to systematically justify\nthe necessity and the design robustness of step-wise rewards. Furthermore, we\ncompare the proposed PORTool with other training approaches and demonstrate\nsignificant improvements in final accuracy and the number of tool-call steps.\n","authors":["Feijie Wu","Weiwu Zhu","Yuxiang Zhang","Soumya Chatterjee","Jiarong Zhu","Fan Mo","Rodin Luo","Jing Gao"],"pdf_url":"https://arxiv.org/pdf/2510.26020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26018v1","updated":"2025-10-29T23:25:49Z","published":"2025-10-29T23:25:49Z","title":"RADRON: Cooperative Localization of Ionizing Radiation Sources by MAVs\n  with Compton Cameras","summary":"  We present a novel approach to localizing radioactive material by cooperating\nMicro Aerial Vehicles (MAVs). Our approach utilizes a state-of-the-art\nsingle-detector Compton camera as a highly sensitive, yet miniature detector of\nionizing radiation. The detector's exceptionally low weight (40 g) opens up new\npossibilities of radiation detection by a team of cooperating agile MAVs. We\npropose a new fundamental concept of fusing the Compton camera measurements to\nestimate the position of the radiation source in real time even from extremely\nsparse measurements. The data readout and processing are performed directly\nonboard and the results are used in a dynamic feedback to drive the motion of\nthe vehicles. The MAVs are stabilized in a tightly cooperating swarm to\nmaximize the information gained by the Compton cameras, rapidly locate the\nradiation source, and even track a moving radiation source.\n","authors":["Petr Stibinger","Tomas Baca","Daniela Doubravova","Jan Rusnak","Jaroslav Solc","Jan Jakubek","Petr Stepan","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2510.26018v1.pdf","comment":"8 pages, 9 figures, submitted for review to IEEE RA-L"},{"id":"http://arxiv.org/abs/2510.26017v1","updated":"2025-10-29T23:23:11Z","published":"2025-10-29T23:23:11Z","title":"Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep\n  Learning","summary":"  Climate change and sea-level rise (SLR) pose escalating threats to coastal\ncities, intensifying the need for efficient and accurate methods to predict\npotential flood hazards. Traditional physics-based hydrodynamic simulators,\nalthough precise, are computationally expensive and impractical for city-scale\ncoastal planning applications. Deep Learning (DL) techniques offer promising\nalternatives, however, they are often constrained by challenges such as data\nscarcity and high-dimensional output requirements. Leveraging a recently\nproposed vision-based, low-resource DL framework, we develop a novel,\nlightweight Convolutional Neural Network (CNN)-based model designed to predict\ncoastal flooding under variable SLR projections and shoreline adaptation\nscenarios. Furthermore, we demonstrate the ability of the model to generalize\nacross diverse geographical contexts by utilizing datasets from two distinct\nregions: Abu Dhabi and San Francisco. Our findings demonstrate that the\nproposed model significantly outperforms state-of-the-art methods, reducing the\nmean absolute error (MAE) in predicted flood depth maps on average by nearly\n20%. These results highlight the potential of our approach to serve as a\nscalable and practical tool for coastal flood management, empowering\ndecision-makers to develop effective mitigation strategies in response to the\ngrowing impacts of climate change. Project Page: https://caspiannet.github.io/\n","authors":["Bilal Hassan","Areg Karapetyan","Aaron Chung Hin Chow","Samer Madanat"],"pdf_url":"https://arxiv.org/pdf/2510.26017v1.pdf","comment":"Submitted to Hydrology and Earth System Sciences"},{"id":"http://arxiv.org/abs/2510.26014v1","updated":"2025-10-29T23:11:01Z","published":"2025-10-29T23:11:01Z","title":"Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis","summary":"  Survival analysis is a task to model the time until an event of interest\noccurs, widely used in clinical and biomedical research. A key challenge is to\nmodel patient heterogeneity while also adapting risk predictions to both\nindividual characteristics and temporal dynamics. We propose a dual\nmixture-of-experts (MoE) framework for discrete-time survival analysis. Our\napproach combines a feature-encoder MoE for subgroup-aware representation\nlearning with a hazard MoE that leverages patient features and time embeddings\nto capture temporal dynamics. This dual-MoE design flexibly integrates with\nexisting deep learning based survival pipelines. On METABRIC and GBSG breast\ncancer datasets, our method consistently improves performance, boosting the\ntime-dependent C-index up to 0.04 on the test sets, and yields further gains\nwhen incorporated into the Consurv framework.\n","authors":["Hyeonjun Lee","Hyungseob Shin","Gunhee Nam","Hyeonsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2510.26014v1.pdf","comment":"Accepted to NeurIPS 2025 workshop Learning from Time Series for\n  Health (TS4H)"},{"id":"http://arxiv.org/abs/2505.12191v2","updated":"2025-10-29T23:02:31Z","published":"2025-05-18T01:37:58Z","title":"Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised\n  Learning from Data Curriculum","summary":"  Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2.\n","authors":["Wenquan Lu","Jiaqi Zhang","Hugues Van Assel","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2505.12191v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.26012v1","updated":"2025-10-29T22:57:03Z","published":"2025-10-29T22:57:03Z","title":"AutoSurvey2: Empowering Researchers with Next Level Automated Literature\n  Surveys","summary":"  The rapid growth of research literature, particularly in large language\nmodels (LLMs), has made producing comprehensive and current survey papers\nincreasingly difficult. This paper introduces autosurvey2, a multi-stage\npipeline that automates survey generation through retrieval-augmented synthesis\nand structured evaluation. The system integrates parallel section generation,\niterative refinement, and real-time retrieval of recent publications to ensure\nboth topical completeness and factual accuracy. Quality is assessed using a\nmulti-LLM evaluation framework that measures coverage, structure, and relevance\nin alignment with expert review standards. Experimental results demonstrate\nthat autosurvey2 consistently outperforms existing retrieval-based and\nautomated baselines, achieving higher scores in structural coherence and\ntopical relevance while maintaining strong citation fidelity. By combining\nretrieval, reasoning, and automated evaluation into a unified framework,\nautosurvey2 provides a scalable and reproducible solution for generating\nlong-form academic surveys and contributes a solid foundation for future\nresearch on automated scholarly writing. All code and resources are available\nat https://github.com/annihi1ation/auto_research.\n","authors":["Siyi Wu","Chiaxin Liang","Ziqian Bi","Leyi Zhao","Tianyang Wang","Junhao Song","Yichao Zhang","Keyu Chen","Xinyuan Song"],"pdf_url":"https://arxiv.org/pdf/2510.26012v1.pdf","comment":"TKDD 2025"},{"id":"http://arxiv.org/abs/2510.23639v2","updated":"2025-10-29T22:50:10Z","published":"2025-10-24T15:56:40Z","title":"Integrating Genomics into Multimodal EHR Foundation Models","summary":"  This paper introduces an innovative Electronic Health Record (EHR) foundation\nmodel that integrates Polygenic Risk Scores (PRS) as a foundational data\nmodality, moving beyond traditional EHR-only approaches to build more holistic\nhealth profiles. Leveraging the extensive and diverse data from the All of Us\n(AoU) Research Program, this multimodal framework aims to learn complex\nrelationships between clinical data and genetic predispositions. The\nmethodology extends advancements in generative AI to the EHR foundation model\nspace, enhancing predictive capabilities and interpretability. Evaluation on\nAoU data demonstrates the model's predictive value for the onset of various\nconditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay\nbetween PRS and EHR data. The work also explores transfer learning for custom\nclassification tasks, showcasing the architecture's versatility and efficiency.\nThis approach is pivotal for unlocking new insights into disease prediction,\nproactive health management, risk stratification, and personalized treatment\nstrategies, laying the groundwork for more personalized, equitable, and\nactionable real-world evidence generation in healthcare.\n","authors":["Jonathan Amar","Edward Liu","Alessandra Breschi","Liangliang Zhang","Pouya Kheradpour","Sylvia Li","Lisa Soleymani Lehmann","Alessandro Giulianelli","Matt Edwards","Yugang Jia","David Nola","Raghav Mani","Pankaj Vats","Jesse Tetreault","T. J. Chen","Cory Y. McLean"],"pdf_url":"https://arxiv.org/pdf/2510.23639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12652v2","updated":"2025-10-29T22:50:00Z","published":"2024-10-16T15:16:04Z","title":"Constrained Posterior Sampling: Time Series Generation with Hard\n  Constraints","summary":"  Generating realistic time series samples is crucial for stress-testing models\nand protecting user privacy by using synthetic data. In engineering and\nsafety-critical applications, these samples must meet certain hard constraints\nthat are domain-specific or naturally imposed by physics or nature. Consider,\nfor example, generating electricity demand patterns with constraints on peak\ndemand times. This can be used to stress-test the functioning of power grids\nduring adverse weather conditions. Existing approaches for generating\nconstrained time series are either not scalable or degrade sample quality. To\naddress these challenges, we introduce Constrained Posterior Sampling (CPS), a\ndiffusion-based sampling algorithm that aims to project the posterior mean\nestimate into the constraint set after each denoising update. Notably, CPS\nscales to a large number of constraints ($\\sim100$) without requiring\nadditional training. We provide theoretical justifications highlighting the\nimpact of our projection step on sampling. Empirically, CPS outperforms\nstate-of-the-art methods in sample quality and similarity to real time series\nby around 70\\% and 22\\%, respectively, on real-world stocks, traffic, and air\nquality datasets.\n","authors":["Sai Shankar Narasimhan","Shubhankar Agarwal","Litu Rout","Sanjay Shakkottai","Sandeep P. Chinchali"],"pdf_url":"https://arxiv.org/pdf/2410.12652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17608v2","updated":"2025-10-29T22:44:11Z","published":"2023-05-28T02:12:00Z","title":"Reward Collapse in Aligning Large Language Models","summary":"  The extraordinary capabilities of large language models (LLMs) such as\nChatGPT and GPT-4 are in part unleashed by aligning them with reward models\nthat are trained on human preferences, which are often represented as rankings\nof responses to prompts. In this paper, we document the phenomenon of\n\\textit{reward collapse}, an empirical observation where the prevailing\nranking-based approach results in an \\textit{identical} reward distribution\n\\textit{regardless} of the prompts during the terminal phase of training. This\noutcome is undesirable as open-ended prompts like ``write a short story about\nyour best friend'' should yield a continuous range of rewards for their\ncompletions, while specific prompts like ``what is the capital of New Zealand''\nshould generate either high or low rewards. Our theoretical investigation\nreveals that reward collapse is primarily due to the insufficiency of the\nranking-based objective function to incorporate prompt-related information\nduring optimization. This insight allows us to derive closed-form expressions\nfor the reward distribution associated with a set of utility functions in an\nasymptotic regime. To overcome reward collapse, we introduce a prompt-aware\noptimization scheme that provably admits a prompt-dependent reward distribution\nwithin the interpolating regime. Our experimental results suggest that our\nproposed prompt-aware utility functions significantly alleviate reward collapse\nduring the training of reward models.\n","authors":["Ziang Song","Tianle Cai","Jason D. Lee","Weijie J. Su"],"pdf_url":"https://arxiv.org/pdf/2305.17608v2.pdf","comment":"Accepted for publication in the Journal of Data Science (JDS),\n  reference JDS1201"},{"id":"http://arxiv.org/abs/2505.21996v2","updated":"2025-10-29T22:39:29Z","published":"2025-05-28T05:55:44Z","title":"Learning World Models for Interactive Video Generation","summary":"  Foundational world models must be both interactive and preserve\nspatiotemporal coherence for effective future planning with action choices.\nHowever, present models for long video generation have limited inherent world\nmodeling capabilities due to two main challenges: compounding errors and\ninsufficient memory mechanisms. We enhance image-to-video models with\ninteractive capabilities through additional action conditioning and\nautoregressive framework, and reveal that compounding error is inherently\nirreducible in autoregressive video generation, while insufficient memory\nmechanism leads to incoherence of world models. We propose video retrieval\naugmented generation (VRAG) with explicit global state conditioning, which\nsignificantly reduces long-term compounding errors and increases spatiotemporal\nconsistency of world models. In contrast, naive autoregressive generation with\nextended context windows and retrieval-augmented generation prove less\neffective for video generation, primarily due to the limited in-context\nlearning capabilities of current video models. Our work illuminates the\nfundamental challenges in video world models and establishes a comprehensive\nbenchmark for improving video generation models with internal world modeling\ncapabilities.\n","authors":["Taiye Chen","Xun Hu","Zihan Ding","Chi Jin"],"pdf_url":"https://arxiv.org/pdf/2505.21996v2.pdf","comment":"Project page: https://sites.google.com/view/vrag"},{"id":"http://arxiv.org/abs/2507.13328v2","updated":"2025-10-29T22:38:57Z","published":"2025-07-17T17:47:47Z","title":"Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does\n  Not Fundamentally Alter It","summary":"  Does vision-and-language (VL) training change the linguistic representations\nof language models in meaningful ways? Most results in the literature have\nshown inconsistent or marginal differences, both behaviorally and\nrepresentationally. In this work, we start from the hypothesis that the domain\nin which VL training could have a significant effect is lexical-conceptual\nknowledge, in particular its taxonomic organization. Through comparing minimal\npairs of text-only LMs and their VL-trained counterparts, we first show that\nthe VL models often outperform their text-only counterparts on a text-only\nquestion-answering task that requires taxonomic understanding of concepts\nmentioned in the questions. Using an array of targeted behavioral and\nrepresentational analyses, we show that the LMs and VLMs do not differ\nsignificantly in terms of their taxonomic knowledge itself, but they differ in\nhow they represent questions that contain concepts in a taxonomic relation vs.\na non-taxonomic relation. This implies that the taxonomic knowledge itself does\nnot change substantially through additional VL training, but VL training does\nimprove the deployment of this knowledge in the context of a specific task,\neven when the presentation of the task is purely linguistic.\n","authors":["Yulu Qin","Dheeraj Varghese","Adam Dahlgren Lindström","Lucia Donatelli","Kanishka Misra","Najoung Kim"],"pdf_url":"https://arxiv.org/pdf/2507.13328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26007v1","updated":"2025-10-29T22:35:34Z","published":"2025-10-29T22:35:34Z","title":"The Quest for Reliable Metrics of Responsible AI","summary":"  The development of Artificial Intelligence (AI), including AI in Science\n(AIS), should be done following the principles of responsible AI. Progress in\nresponsible AI is often quantified through evaluation metrics, yet there has\nbeen less work on assessing the robustness and reliability of the metrics\nthemselves. We reflect on prior work that examines the robustness of fairness\nmetrics for recommender systems as a type of AI application and summarise their\nkey takeaways into a set of non-exhaustive guidelines for developing reliable\nmetrics of responsible AI. Our guidelines apply to a broad spectrum of AI\napplications, including AIS.\n","authors":["Theresia Veronika Rampisela","Maria Maistro","Tuukka Ruotsalo","Christina Lioma"],"pdf_url":"https://arxiv.org/pdf/2510.26007v1.pdf","comment":"Accepted for presentation at the AI in Science Summit 2025"},{"id":"http://arxiv.org/abs/2510.26004v1","updated":"2025-10-29T22:32:16Z","published":"2025-10-29T22:32:16Z","title":"DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection\n  System","summary":"  Rapid and reliable incident detection is critical for reducing crash-related\nfatalities, injuries, and congestion. However, conventional methods, such as\nclosed-circuit television, dashcam footage, and sensor-based detection,\nseparate detection from verification, suffer from limited flexibility, and\nrequire dense infrastructure or high penetration rates, restricting\nadaptability and scalability to shifting incident hotspots. To overcome these\nchallenges, we developed DARTS, a drone-based, AI-powered real-time traffic\nincident detection system. DARTS integrates drones' high mobility and aerial\nperspective for adaptive surveillance, thermal imaging for better\nlow-visibility performance and privacy protection, and a lightweight deep\nlearning framework for real-time vehicle trajectory extraction and incident\ndetection. The system achieved 99% detection accuracy on a self-collected\ndataset and supports simultaneous online visual verification, severity\nassessment, and incident-induced congestion propagation monitoring via a\nweb-based interface. In a field test on Interstate 75 in Florida, DARTS\ndetected and verified a rear-end collision 12 minutes earlier than the local\ntransportation management center and monitored incident-induced congestion\npropagation, suggesting potential to support faster emergency response and\nenable proactive traffic control to reduce congestion and secondary crash risk.\nCrucially, DARTS's flexible deployment architecture reduces dependence on\nfrequent physical patrols, indicating potential scalability and\ncost-effectiveness for use in remote areas and resource-constrained settings.\nThis study presents a promising step toward a more flexible and integrated\nreal-time traffic incident detection system, with significant implications for\nthe operational efficiency and responsiveness of modern transportation\nmanagement.\n","authors":["Bai Li","Achilleas Kourtellis","Rong Cao","Joseph Post","Brian Porter","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.26004v1.pdf","comment":"Preprint version. This manuscript is currently under review at\n  Transportation Research Part C: Emerging Technologies. The PDF corresponds to\n  the version submitted in June 2025. The main findings of this work were\n  recognized with the Best Intelligent Transportation Systems Paper Award at\n  the 2025 TRB Annual Meeting"},{"id":"http://arxiv.org/abs/2503.02878v3","updated":"2025-10-29T22:28:23Z","published":"2025-03-04T18:58:11Z","title":"Language Models can Self-Improve at State-Value Estimation for Better\n  Search","summary":"  Collecting ground-truth rewards or human demonstrations for multi-step\nreasoning tasks is often prohibitively expensive, particularly in interactive\ndomains such as web tasks. We introduce Self-Taught Lookahead (STL), a\nreward-free framework that improves language model-based value functions by\nreasoning explicitly about state transitions. STL can be viewed as a\nchain-of-thought analogue of the value iteration algorithm: instead of\nregressing directly on numeric values, a value LLM is trained to simulate a\nstep of lookahead in natural language - predicting the next action, resulting\nstate, and rationale for its value, thereby refining value estimates without\nany labeled data. This self-supervised procedure yields more accurate\nstate-value predictions, which in turn enable lightweight search algorithms to\nexpand fewer states while maintaining strong performance. Empirically,\nSTL-trained value models built on moderately sized (8B parameter) open-weight\nLLMs boost web agent success rates by 39%, achieving comparable performance\nwith proprietary models. STL also generalizes to multi-hop QA and math puzzles.\nWe find that STL enables small open-source models to guide efficient search,\nreducing inference costs by integrating explicit reasoning with value learning.\n","authors":["Ethan Mendes","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2503.02878v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06220v2","updated":"2025-10-29T22:25:02Z","published":"2025-06-06T16:28:03Z","title":"GenIR: Generative Visual Feedback for Mental Image Retrieval","summary":"  Vision-language models (VLMs) have shown strong performance on text-to-image\nretrieval benchmarks. However, bridging this success to real-world applications\nremains a challenge. In practice, human search behavior is rarely a one-shot\naction. Instead, it is often a multi-round process guided by clues in mind.\nThat is, a mental image ranging from vague recollections to vivid mental\nrepresentations of the target image. Motivated by this gap, we study the task\nof Mental Image Retrieval (MIR), which targets the realistic yet underexplored\nsetting where users refine their search for a mentally envisioned image through\nmulti-round interactions with an image search engine. Central to successful\ninteractive retrieval is the capability of machines to provide users with\nclear, actionable feedback; however, existing methods rely on indirect or\nabstract verbal feedback, which can be ambiguous, misleading, or ineffective\nfor users to refine the query. To overcome this, we propose GenIR, a generative\nmulti-round retrieval paradigm leveraging diffusion-based image generation to\nexplicitly reify the AI system's understanding at each round. These synthetic\nvisual representations provide clear, interpretable feedback, enabling users to\nrefine their queries intuitively and effectively. We further introduce a fully\nautomated pipeline to generate a high-quality multi-round MIR dataset.\nExperimental results demonstrate that GenIR significantly outperforms existing\ninteractive methods in the MIR scenario. This work establishes a new task with\na dataset and an effective generative retrieval method, providing a foundation\nfor future research in this direction\n","authors":["Diji Yang","Minghao Liu","Chung-Hsiang Lo","Yi Zhang","James Davis"],"pdf_url":"https://arxiv.org/pdf/2506.06220v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25997v1","updated":"2025-10-29T22:18:57Z","published":"2025-10-29T22:18:57Z","title":"From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal\n  Text-to-SQL","summary":"  Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing\naccess to structured data, allowing users to query databases without learning\nSQL. Yet existing systems struggle with realistic spatio-temporal queries,\nwhere success requires aligning vague user phrasing with schema-specific\ncategories, handling temporal reasoning, and choosing appropriate outputs. We\npresent an agentic pipeline that extends a naive text-to-SQL baseline\n(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The\nagent can plan, decompose, and adapt queries through schema inspection, SQL\ngeneration, execution, and visualization tools. We evaluate on 35\nnatural-language queries over the NYC and Tokyo check-in dataset, covering\nspatial, temporal, and multi-dataset reasoning. The agent achieves\nsubstantially higher accuracy than the naive baseline 91.4% vs. 28.6% and\nenhances usability through maps, plots, and structured natural-language\nsummaries. Crucially, our design enables more natural human-database\ninteraction, supporting users who lack SQL expertise, detailed schema\nknowledge, or prompting skill. We conclude that agentic orchestration, rather\nthan stronger SQL generators alone, is a promising foundation for interactive\ngeospatial assistants.\n","authors":["Manu Redd","Tao Zhe","Dongjie Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25997v1.pdf","comment":"8 pages, 5 figures, GeoGenAgent'25 - ACM SIGSPATIAL"},{"id":"http://arxiv.org/abs/2510.25992v1","updated":"2025-10-29T22:05:08Z","published":"2025-10-29T22:05:08Z","title":"Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning","summary":"  Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.\n","authors":["Yihe Deng","I-Hung Hsu","Jun Yan","Zifeng Wang","Rujun Han","Gufeng Zhang","Yanfei Chen","Wei Wang","Tomas Pfister","Chen-Yu Lee"],"pdf_url":"https://arxiv.org/pdf/2510.25992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25976v1","updated":"2025-10-29T21:21:54Z","published":"2025-10-29T21:21:54Z","title":"Brain-IT: Image Reconstruction from fMRI via Brain-Interaction\n  Transformer","summary":"  Reconstructing images seen by people from their fMRI brain recordings\nprovides a non-invasive window into the human brain. Despite recent progress\nenabled by diffusion models, current methods often lack faithfulness to the\nactual seen images. We present \"Brain-IT\", a brain-inspired approach that\naddresses this challenge through a Brain Interaction Transformer (BIT),\nallowing effective interactions between clusters of functionally-similar\nbrain-voxels. These functional-clusters are shared by all subjects, serving as\nbuilding blocks for integrating information both within and across brains. All\nmodel components are shared by all clusters & subjects, allowing efficient\ntraining with a limited amount of data. To guide the image reconstruction, BIT\npredicts two complementary localized patch-level image features: (i)high-level\nsemantic features which steer the diffusion model toward the correct semantic\ncontent of the image; and (ii)low-level structural features which help to\ninitialize the diffusion process with the correct coarse layout of the image.\nBIT's design enables direct flow of information from brain-voxel clusters to\nlocalized image features. Through these principles, our method achieves image\nreconstructions from fMRI that faithfully reconstruct the seen images, and\nsurpass current SotA approaches both visually and by standard objective\nmetrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve\nresults comparable to current methods trained on full 40-hour recordings.\n","authors":["Roman Beliy","Amit Zalcher","Jonathan Kogman","Navve Wasserman","Michal Irani"],"pdf_url":"https://arxiv.org/pdf/2510.25976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25960v1","updated":"2025-10-29T20:58:16Z","published":"2025-10-29T20:58:16Z","title":"WaveVerif: Acoustic Side-Channel based Verification of Robotic Workflows","summary":"  In this paper, we present a framework that uses acoustic side-channel\nanalysis (ASCA) to monitor and verify whether a robot correctly executes its\nintended commands. We develop and evaluate a machine-learning-based workflow\nverification system that uses acoustic emissions generated by robotic\nmovements. The system can determine whether real-time behavior is consistent\nwith expected commands. The evaluation takes into account movement speed,\ndirection, and microphone distance. The results show that individual robot\nmovements can be validated with over 80% accuracy under baseline conditions\nusing four different classifiers: Support Vector Machine (SVM), Deep Neural\nNetwork (DNN), Recurrent Neural Network (RNN), and Convolutional Neural Network\n(CNN). Additionally, workflows such as pick-and-place and packing could be\nidentified with similarly high confidence. Our findings demonstrate that\nacoustic signals can support real-time, low-cost, passive verification in\nsensitive robotic environments without requiring hardware modifications.\n","authors":["Zeynep Yasemin Erdogan","Shishir Nagaraja","Chuadhry Mujeeb Ahmed","Ryan Shah"],"pdf_url":"https://arxiv.org/pdf/2510.25960v1.pdf","comment":"11 pages, 3 figures, Corresponding Author: Prof. Shishir Nagaraja\n  (shishir.nagaraja@newcastle.ac.uk)"},{"id":"http://arxiv.org/abs/2410.14879v4","updated":"2025-10-29T20:53:35Z","published":"2024-10-18T21:56:35Z","title":"Vital Insight: Assisting Experts' Context-Driven Sensemaking of\n  Multi-modal Personal Tracking Data Using Visualization and Human-In-The-Loop\n  LLM","summary":"  Passive tracking methods, such as phone and wearable sensing, have become\ndominant in monitoring human behaviors in modern ubiquitous computing studies.\nWhile there have been significant advances in machine-learning approaches to\ntranslate periods of raw sensor data to model momentary behaviors, (e.g.,\nphysical activity recognition), there still remains a significant gap in the\ntranslation of these sensing streams into meaningful, high-level, context-aware\ninsights that are required for various applications (e.g., summarizing an\nindividual's daily routine). To bridge this gap, experts often need to employ a\ncontext-driven sensemaking process in real-world studies to derive insights.\nThis process often requires manual effort and can be challenging even for\nexperienced researchers due to the complexity of human behaviors.\n  We conducted three rounds of user studies with 21 experts to explore\nsolutions to address challenges with sensemaking. We follow a human-centered\ndesign process to identify needs and design, iterate, build, and evaluate Vital\nInsight (VI), a novel, LLM-assisted, prototype system to enable\nhuman-in-the-loop inference (sensemaking) and visualizations of multi-modal\npassive sensing data from smartphones and wearables. Using the prototype as a\ntechnology probe, we observe experts' interactions with it and develop an\nexpert sensemaking model that explains how experts move between direct data\nrepresentations and AI-supported inferences to explore, question, and validate\ninsights. Through this iterative process, we also synthesize and discuss a list\nof design implications for the design of future AI-augmented visualization\nsystems to better assist experts' sensemaking processes in multi-modal health\nsensing data.\n","authors":["Jiachen Li","Xiwen Li","Justin Steinberg","Akshat Choube","Bingsheng Yao","Xuhai Xu","Dakuo Wang","Elizabeth Mynatt","Varun Mishra"],"pdf_url":"https://arxiv.org/pdf/2410.14879v4.pdf","comment":"Jiachen Li, Xiwen Li, Justin Steinberg, Akshat Choube, Bingsheng Yao,\n  Xuhai Xu, Dakuo Wang, Elizabeth Mynatt, and Varun Mishra. 2025. Vital\n  Insight: Assisting Experts' Context-Driven Sensemaking of Multi-modal\n  Personal Tracking Data Using Visualization and Human-in-the-Loop LLM. Proc.\n  ACM Interact. Mob. Wearable Ubiquitous Technol. 9, 3, Article 101 (September\n  2025), 37 pages"},{"id":"http://arxiv.org/abs/2510.25954v1","updated":"2025-10-29T20:53:07Z","published":"2025-10-29T20:53:07Z","title":"Application and Validation of Geospatial Foundation Model Data for the\n  Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi","summary":"  The reliability of routine health data in low and middle-income countries\n(LMICs) is often constrained by reporting delays and incomplete coverage,\nnecessitating the exploration of novel data sources and analytics. Geospatial\nFoundation Models (GeoFMs) offer a promising avenue by synthesizing diverse\nspatial, temporal, and behavioral data into mathematical embeddings that can be\nefficiently used for downstream prediction tasks. This study evaluated the\npredictive performance of three GeoFM embedding sources - Google Population\nDynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite\nimagery), and mobile phone call detail records (CDR) - for modeling 15 routine\nhealth programmatic outputs in Malawi, and compared their utility to\ntraditional geospatial interpolation methods. We used XGBoost models on data\nfrom 552 health catchment areas (January 2021-May 2023), assessing performance\nwith R2, and using an 80/20 training and test data split with 5-fold\ncross-validation used in training. While predictive performance was mixed, the\nembedding-based approaches improved upon baseline geostatistical methods in 13\nof 15 (87%) indicators tested. A Multi-GeoFM model integrating all three\nembedding sources produced the most robust predictions, achieving average\n5-fold cross validated R2 values for indicators like population density (0.63),\nnew HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,\n0.68, and 0.55, respectively. Prediction was poor for prediction targets with\nlow primary data availability, such as TB and malnutrition cases. These results\ndemonstrate that GeoFM embeddings imbue a modest predictive improvement for\nselect health and demographic outcomes in an LMIC context. We conclude that the\nintegration of multiple GeoFM sources is an efficient and valuable tool for\nsupplementing and strengthening constrained routine health information systems.\n","authors":["Lynn Metz","Rachel Haggard","Michael Moszczynski","Samer Asbah","Chris Mwase","Patricia Khomani","Tyler Smith","Hannah Cooper","Annie Mwale","Arbaaz Muslim","Gautam Prasad","Mimi Sun","Tomer Shekel","Joydeep Paul","Anna Carter","Shravya Shetty","Dylan Green"],"pdf_url":"https://arxiv.org/pdf/2510.25954v1.pdf","comment":"13 pages, 3010 words, 2 tables, 2 figures"},{"id":"http://arxiv.org/abs/2510.25951v1","updated":"2025-10-29T20:50:04Z","published":"2025-10-29T20:50:04Z","title":"Estimating cognitive biases with attention-aware inverse planning","summary":"  People's goal-directed behaviors are influenced by their cognitive biases,\nand autonomous systems that interact with people should be aware of this. For\nexample, people's attention to objects in their environment will be biased in a\nway that systematically affects how they perform everyday tasks such as driving\nto work. Here, building on recent work in computational cognitive science, we\nformally articulate the attention-aware inverse planning problem, in which the\ngoal is to estimate a person's attentional biases from their actions. We\ndemonstrate how attention-aware inverse planning systematically differs from\nstandard inverse reinforcement learning and how cognitive biases can be\ninferred from behavior. Finally, we present an approach to attention-aware\ninverse planning that combines deep reinforcement learning with computational\ncognitive modeling. We use this approach to infer the attentional strategies of\nRL agents in real-life driving scenarios selected from the Waymo Open Dataset,\ndemonstrating the scalability of estimating cognitive biases with\nattention-aware inverse planning.\n","authors":["Sounak Banerjee","Daphne Cornelisse","Deepak Gopinath","Emily Sumner","Jonathan DeCastro","Guy Rosman","Eugene Vinitsky","Mark K. Ho"],"pdf_url":"https://arxiv.org/pdf/2510.25951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25947v1","updated":"2025-10-29T20:46:03Z","published":"2025-10-29T20:46:03Z","title":"Revisiting Multilingual Data Mixtures in Language Model Pretraining","summary":"  The impact of different multilingual data mixtures in pretraining large\nlanguage models (LLMs) has been a topic of ongoing debate, often raising\nconcerns about potential trade-offs between language coverage and model\nperformance (i.e., the curse of multilinguality). In this work, we investigate\nthese assumptions by training 1.1B and 3B parameter LLMs on diverse\nmultilingual corpora, varying the number of languages from 25 to 400. Our study\nchallenges common beliefs surrounding multilingual training. First, we find\nthat combining English and multilingual data does not necessarily degrade the\nin-language performance of either group, provided that languages have a\nsufficient number of tokens included in the pretraining corpus. Second, we\nobserve that using English as a pivot language (i.e., a high-resource language\nthat serves as a catalyst for multilingual generalization) yields benefits\nacross language families, and contrary to expectations, selecting a pivot\nlanguage from within a specific family does not consistently improve\nperformance for languages within that family. Lastly, we do not observe a\nsignificant \"curse of multilinguality\" as the number of training languages\nincreases in models at this scale. Our findings suggest that multilingual data,\nwhen balanced appropriately, can enhance language model capabilities without\ncompromising performance, even in low-resource settings\n","authors":["Negar Foroutan","Paul Teiletche","Ayush Kumar Tarun","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2510.25947v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2510.19771v2","updated":"2025-10-29T20:33:02Z","published":"2025-10-22T17:00:45Z","title":"Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents","summary":"  LLM-based agents are increasingly moving towards proactivity: rather than\nawaiting instruction, they exercise agency to anticipate user needs and solve\nthem autonomously. However, evaluating proactivity is challenging; current\nbenchmarks are constrained to localized context, limiting their ability to test\nreasoning across sources and longer time horizons. To address this gap, we\npresent PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes\nproactivity as a pipeline of three core capabilities: (1) searching for\nunspecified issues, (2) identifying specific bottlenecks, and (3) executing\nappropriate resolutions. We apply PROBE to evaluate leading LLMs and popular\nagentic frameworks, showing that even state-of-the-art models struggle to solve\nthis benchmark. Computing our consistent measurements across frontier LLMs and\nagents, we find that the best end-to-end performance of 40% is achieved by both\nGPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative\ncapabilities of each model and analyze mutual failure modes. Our results\nhighlight the current limitations of autonomous action in agentic systems, and\nexpose promising future research directions.\n","authors":["Gil Pasternak","Dheeraj Rajagopal","Julia White","Dhruv Atreja","Matthew Thomas","George Hurn-Maloney","Ash Lewis"],"pdf_url":"https://arxiv.org/pdf/2510.19771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25935v1","updated":"2025-10-29T20:13:46Z","published":"2025-10-29T20:13:46Z","title":"A Process Mining-Based System For The Analysis and Prediction of\n  Software Development Workflows","summary":"  CodeSight is an end-to-end system designed to anticipate deadline compliance\nin software development workflows. It captures development and deployment data\ndirectly from GitHub, transforming it into process mining logs for detailed\nanalysis. From these logs, the system generates metrics and dashboards that\nprovide actionable insights into PR activity patterns and workflow efficiency.\nBuilding on this structured representation, CodeSight employs an LSTM model\nthat predicts remaining PR resolution times based on sequential activity traces\nand static features, enabling early identification of potential deadline\nbreaches. In tests, the system demonstrates high precision and F1 scores in\npredicting deadline compliance, illustrating the value of integrating process\nmining with machine learning for proactive software project management.\n","authors":["Antía Dorado","Iván Folgueira","Sofía Martín","Gonzalo Martín","Álvaro Porto","Alejandro Ramos","John Wallace"],"pdf_url":"https://arxiv.org/pdf/2510.25935v1.pdf","comment":"16 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2510.25933v1","updated":"2025-10-29T20:12:36Z","published":"2025-10-29T20:12:36Z","title":"Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual\n  Accuracy by Directed Exoskeleton Reasoning","summary":"  We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS\nGrounding public subset within a $\\pm 5$ pp equivalence margin.\n  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI\n69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference\nis 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's\n$d = 0.023$). TOST establishes equivalence at $\\pm 5$ pp (not at $\\pm 3$ pp).\nWhen purchased as managed APIs, Humans-Junior's base model\n(Phi-3.5-mini-instruct) is $\\approx 19\\times$ less expensive than GPT-4o on\nMicrosoft AI Foundry pricing; self-hosted or edge deployments can drive\nincremental inference cost toward zero. Measured vs estimated pricing sources\nare tabulated in Appendix E.\n  Method. Our approach combines minimal directed \"Exoskeleton Reasoning\"\nscaffolds with behavioral fine-tuning that teaches protocol compliance\n(epistemic discipline) rather than domain answers. Fine-tuning alone adds\nlittle; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance\n($\\approx 25\\%$). In prompt-only settings on frontier models (Q1--Q100;\nnon-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and\nGemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.\n  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within\n$\\pm 5$ pp on Q1--Q500). Cloud pricing shows $\\approx 19\\times$ lower cost\nversus GPT-4o, and self-hosted/edge deployments can approach zero marginal\ncost. Pricing sources are listed in Appendix E. Frontier prompt-only gains\n(Q1--Q100; non-comparable) and optimized-prompt exploratory results under\nearlier judges are summarized in Appendix F.\n  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,\nFine-Tuning, Model Alignment, Cost-Efficient AI\n","authors":["Nissan Yaron","Dan Bystritsky","Ben-Etzion Yaron"],"pdf_url":"https://arxiv.org/pdf/2510.25933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25929v1","updated":"2025-10-29T20:07:47Z","published":"2025-10-29T20:07:47Z","title":"Multi-Agent Reinforcement Learning for Market Making: Competition\n  without Collusion","summary":"  Algorithmic collusion has emerged as a central question in AI: Will the\ninteraction between different AI agents deployed in markets lead to collusion?\nMore generally, understanding how emergent behavior, be it a cartel or market\ndominance from more advanced bots, affects the market overall is an important\nresearch question.\n  We propose a hierarchical multi-agent reinforcement learning framework to\nstudy algorithmic collusion in market making. The framework includes a\nself-interested market maker (Agent~A), which is trained in an uncertain\nenvironment shaped by an adversary, and three bottom-layer competitors: the\nself-interested Agent~B1 (whose objective is to maximize its own PnL), the\ncompetitive Agent~B2 (whose objective is to minimize the PnL of its opponent),\nand the hybrid Agent~B$^\\star$, which can modulate between the behavior of the\nother two. To analyze how these agents shape the behavior of each other and\naffect market outcomes, we propose interaction-level metrics that quantify\nbehavioral asymmetry and system-level dynamics, while providing signals\npotentially indicative of emergent interaction patterns.\n  Experimental results show that Agent~B2 secures dominant performance in a\nzero-sum setting against B1, aggressively capturing order flow while tightening\naverage spreads, thus improving market execution efficiency. In contrast,\nAgent~B$^\\star$ exhibits a self-interested inclination when co-existing with\nother profit-seeking agents, securing dominant market share through adaptive\nquoting, yet exerting a milder adverse impact on the rewards of Agents~A and B1\ncompared to B2. These findings suggest that adaptive incentive control supports\nmore sustainable strategic co-existence in heterogeneous agent environments and\noffers a structured lens for evaluating behavioral design in algorithmic\ntrading systems.\n","authors":["Ziyi Wang","Carmine Ventre","Maria Polukarov"],"pdf_url":"https://arxiv.org/pdf/2510.25929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04953v2","updated":"2025-10-29T20:00:58Z","published":"2025-04-07T11:37:26Z","title":"M-Prometheus: A Suite of Open Multilingual LLM Judges","summary":"  The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on synthetic multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code.\n","authors":["José Pombal","Dongkeun Yoon","Patrick Fernandes","Ian Wu","Seungone Kim","Ricardo Rei","Graham Neubig","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2504.04953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.01001v2","updated":"2025-10-29T19:55:23Z","published":"2025-04-01T17:40:08Z","title":"Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic\n  Evaluation of Language Models","summary":"  As language models improve and become capable of performing more complex\ntasks across modalities, evaluating them automatically becomes increasingly\nchallenging. Developing strong and robust task-specific automatic metrics gets\nharder, and human-annotated test sets -- which are expensive to create --\nsaturate more quickly. A compelling alternative is to design reliable\nstrategies to automate the creation of test data and evaluation, but previous\nattempts either rely on pre-existing data, or focus solely on individual tasks.\nWe present Zero-shot Benchmarking (ZSB), a framework for creating high-quality\nbenchmarks for any task by leveraging language models for both synthetic test\ndata creation and evaluation. ZSB is simple and flexible: it requires only the\ncreation of a prompt for data generation and one for evaluation; it is scalable\nto tasks and languages where collecting real-world data is costly or\nimpractical; it is model-agnostic, allowing the creation of increasingly\nchallenging benchmarks as models improve. To assess the effectiveness of our\nframework, we create benchmarks for five text-only tasks and a multi-modal one:\ngeneral capabilities in four languages (English, Chinese, French, and Korean),\ntranslation, and general vision-language capabilities in English. We then rank\na broad range of open and closed systems on our benchmarks. ZSB rankings\nconsistently correlate strongly with human rankings, outperforming\nwidely-adopted standard benchmarks. Through ablations, we find that strong\nbenchmarks can be created with open models, and that judge model size and\ndataset variety are crucial drivers of performance. We release all our\nbenchmarks, and code to reproduce our experiments and to produce new\nbenchmarks.\n","authors":["José Pombal","Nuno M. Guerreiro","Ricardo Rei","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2504.01001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25924v1","updated":"2025-10-29T19:53:51Z","published":"2025-10-29T19:53:51Z","title":"Transferring Causal Effects using Proxies","summary":"  We consider the problem of estimating a causal effect in a multi-domain\nsetting. The causal effect of interest is confounded by an unobserved\nconfounder and can change between the different domains. We assume that we have\naccess to a proxy of the hidden confounder and that all variables are discrete\nor categorical. We propose methodology to estimate the causal effect in the\ntarget domain, where we assume to observe only the proxy variable. Under these\nconditions, we prove identifiability (even when treatment and response\nvariables are continuous). We introduce two estimation techniques, prove\nconsistency, and derive confidence intervals. The theoretical results are\nsupported by simulation studies and a real-world example studying the causal\neffect of website rankings on consumer choices.\n","authors":["Manuel Iglesias-Alonso","Felix Schur","Julius von Kügelgen","Jonas Peters"],"pdf_url":"https://arxiv.org/pdf/2510.25924v1.pdf","comment":"Advances in Neural Information Processing Systems (NeurIPS 2025)\n  camera-ready version"},{"id":"http://arxiv.org/abs/2510.25914v1","updated":"2025-10-29T19:34:14Z","published":"2025-10-29T19:34:14Z","title":"FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization","summary":"  FinOps (Finance + Operations) represents an operational framework and\ncultural practice which maximizes cloud business value through collaborative\nfinancial accountability across engineering, finance, and business teams.\nFinOps practitioners face a fundamental challenge: billing data arrives in\nheterogeneous formats, taxonomies, and metrics from multiple cloud providers\nand internal systems which eventually lead to synthesizing actionable insights,\nand making time-sensitive decisions. To address this challenge, we propose\nleveraging autonomous, goal-driven AI agents for FinOps automation. In this\npaper, we built a FinOps agent for a typical use-case for IT infrastructure and\ncost optimization. We built a system simulating a realistic end-to-end industry\nprocess starting with retrieving data from various sources to consolidating and\nanalyzing the data to generate recommendations for optimization. We defined a\nset of metrics to evaluate our agent using several open-source and close-source\nlanguage models and it shows that the agent was able to understand, plan, and\nexecute tasks as well as an actual FinOps practitioner.\n","authors":["Ngoc Phuoc An Vo","Manish Kesarwani","Ruchi Mahindru","Chandrasekhar Narayanaswami"],"pdf_url":"https://arxiv.org/pdf/2510.25914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13464v2","updated":"2025-10-29T19:33:20Z","published":"2025-06-16T13:24:50Z","title":"Unveiling the Learning Mind of Language Models: A Cognitive Framework\n  and Empirical Study","summary":"  Large language models (LLMs) have shown impressive capabilities across tasks\nsuch as mathematics, coding, and reasoning, yet their learning ability, which\nis crucial for adapting to dynamic environments and acquiring new knowledge,\nremains underexplored. In this work, we address this gap by introducing a\nframework inspired by cognitive psychology and education. Specifically, we\ndecompose general learning ability into three distinct, complementary\ndimensions: Learning from Instructor (acquiring knowledge via explicit\nguidance), Learning from Concept (internalizing abstract structures and\ngeneralizing to new contexts), and Learning from Experience (adapting through\naccumulated exploration and feedback). We conduct a comprehensive empirical\nstudy across the three learning dimensions and identify several insightful\nfindings, such as (i) interaction improves learning; (ii) conceptual\nunderstanding is scale-emergent and benefits larger models; and (iii) LLMs are\neffective few-shot learners but not many-shot learners. Based on our framework\nand empirical findings, we introduce a benchmark that provides a unified and\nrealistic evaluation of LLMs' general learning abilities across three learning\ncognition dimensions. It enables diagnostic insights and supports evaluation\nand development of more adaptive and human-like models.\n","authors":["Zhengyu Hu","Jianxun Lian","Zheyuan Xiao","Seraphina Zhang","Tianfu Wang","Nicholas Jing Yuan","Xing Xie","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2506.13464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18318v2","updated":"2025-10-29T19:23:31Z","published":"2025-10-21T06:05:47Z","title":"Earth AI: Unlocking Geospatial Insights with Foundation Models and\n  Cross-Modal Reasoning","summary":"  Geospatial data offers immense potential for understanding our planet.\nHowever, the sheer volume and diversity of this data along with its varied\nresolutions, timescales, and sparsity pose significant challenges for thorough\nanalysis and interpretation. This paper introduces Earth AI, a family of\ngeospatial AI models and agentic reasoning that enables significant advances in\nour ability to unlock novel and profound insights into our planet. This\napproach is built upon foundation models across three key domains--Planet-scale\nImagery, Population, and Environment--and an intelligent Gemini-powered\nreasoning engine. We present rigorous benchmarks showcasing the power and novel\ncapabilities of our foundation models and validate that when used together,\nthey provide complementary value for geospatial inference and their synergies\nunlock superior predictive capabilities. To handle complex, multi-step queries,\nwe developed a Gemini-powered agent that jointly reasons over our multiple\nfoundation models along with large geospatial data sources and tools. On a new\nbenchmark of real-world crisis scenarios, our agent demonstrates the ability to\ndeliver critical and timely insights, effectively bridging the gap between raw\ngeospatial data and actionable understanding.\n","authors":["Aaron Bell","Amit Aides","Amr Helmy","Arbaaz Muslim","Aviad Barzilai","Aviv Slobodkin","Bolous Jaber","David Schottlander","George Leifman","Joydeep Paul","Mimi Sun","Nadav Sherman","Natalie Williams","Per Bjornsson","Roy Lee","Ruth Alcantara","Thomas Turnbull","Tomer Shekel","Vered Silverman","Yotam Gigi","Adam Boulanger","Alex Ottenwess","Ali Ahmadalipour","Anna Carter","Behzad Vahedi","Charles Elliott","David Andre","Elad Aharoni","Gia Jung","Hassler Thurston","Jacob Bien","Jamie McPike","Juliet Rothenberg","Kartik Hegde","Kel Markert","Kim Philipp Jablonski","Luc Houriez","Monica Bharel","Phing VanLee","Reuven Sayag","Sebastian Pilarski","Shelley Cazares","Shlomi Pasternak","Siduo Jiang","Thomas Colthurst","Yang Chen","Yehonathan Refael","Yochai Blau","Yuval Carny","Yael Maguire","Avinatan Hassidim","James Manyika","Tim Thelin","Genady Beryozkin","Gautam Prasad","Luke Barrington","Yossi Matias","Niv Efron","Shravya Shetty"],"pdf_url":"https://arxiv.org/pdf/2510.18318v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25908v1","updated":"2025-10-29T19:22:55Z","published":"2025-10-29T19:22:55Z","title":"SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness\n  of Large Language Models in Scientific Applications","summary":"  Large language models (LLMs) have demonstrated transformative potential in\nscientific research, yet their deployment in high-stakes contexts raises\nsignificant trustworthiness concerns. Here, we introduce SciTrust 2.0, a\ncomprehensive framework for evaluating LLM trustworthiness in scientific\napplications across four dimensions: truthfulness, adversarial robustness,\nscientific safety, and scientific ethics. Our framework incorporates novel,\nopen-ended truthfulness benchmarks developed through a verified\nreflection-tuning pipeline and expert validation, alongside a novel ethics\nbenchmark for scientific research contexts covering eight subcategories\nincluding dual-use research and bias. We evaluated seven prominent LLMs,\nincluding four science-specialized models and three general-purpose industry\nmodels, using multiple evaluation metrics including accuracy, semantic\nsimilarity measures, and LLM-based scoring. General-purpose industry models\noverall outperformed science-specialized models across each trustworthiness\ndimension, with GPT-o4-mini demonstrating superior performance in truthfulness\nassessments and adversarial robustness. Science-specialized models showed\nsignificant deficiencies in logical and ethical reasoning capabilities, along\nwith concerning vulnerabilities in safety evaluations, particularly in\nhigh-risk domains such as biosecurity and chemical weapons. By open-sourcing\nour framework, we provide a foundation for developing more trustworthy AI\nsystems and advancing research on model safety and ethics in scientific\ncontexts.\n","authors":["Emily Herron","Junqi Yin","Feiyi Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25908v1.pdf","comment":"Preprint Submitted to ACM Transactions on AI for Science (TAIS)"},{"id":"http://arxiv.org/abs/2510.24134v2","updated":"2025-10-29T19:17:39Z","published":"2025-10-28T07:19:01Z","title":"VC4VG: Optimizing Video Captions for Text-to-Video Generation","summary":"  Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels. We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements. Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/alimama-creative/VC4VG to\nsupport further research.\n","authors":["Yang Du","Zhuoran Lin","Kaiqiang Song","Biao Wang","Zhicheng Zheng","Tiezheng Ge","Bo Zheng","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2510.24134v2.pdf","comment":"Accepted by EMNLP 2025"},{"id":"http://arxiv.org/abs/2510.25904v1","updated":"2025-10-29T19:13:48Z","published":"2025-10-29T19:13:48Z","title":"Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized\n  Setting: the Case of FrameNet Annotation","summary":"  The use of LLM-based applications as a means to accelerate and/or substitute\nhuman labor in the creation of language resources and dataset is a reality.\nNonetheless, despite the potential of such tools for linguistic research,\ncomprehensive evaluation of their performance and impact on the creation of\nannotated datasets, especially under a perspectivized approach to NLP, is still\nmissing. This paper contributes to reduction of this gap by reporting on an\nextensive evaluation of the (semi-)automatization of FrameNet-like semantic\nannotation by the use of an LLM-based semantic role labeler. The methodology\nemployed compares annotation time, coverage and diversity in three experimental\nsettings: manual, automatic and semi-automatic annotation. Results show that\nthe hybrid, semi-automatic annotation setting leads to increased frame\ndiversity and similar annotation coverage, when compared to the human-only\nsetting, while the automatic setting performs considerably worse in all\nmetrics, except for annotation time.\n","authors":["Frederico Belcavello","Ely Matos","Arthur Lorenzi","Lisandra Bonoto","Lívia Ruiz","Luiz Fernando Pereira","Victor Herbst","Yulla Navarro","Helen de Andrade Abreu","Lívia Dutra","Tiago Timponi Torrent"],"pdf_url":"https://arxiv.org/pdf/2510.25904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23925v2","updated":"2025-10-29T18:48:20Z","published":"2025-10-27T23:10:06Z","title":"Latent Chain-of-Thought for Visual Reasoning","summary":"  Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.\n","authors":["Guohao Sun","Hang Hua","Jian Wang","Jiebo Luo","Sohail Dianat","Majid Rabbani","Raghuveer Rao","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2510.23925v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25890v1","updated":"2025-10-29T18:44:22Z","published":"2025-10-29T18:44:22Z","title":"PRISM: Proof-Carrying Artifact Generation through LLM x MDE Synergy and\n  Stratified Constraints","summary":"  PRISM unifies Large Language Models with Model-Driven Engineering to generate\nregulator-ready artifacts and machine-checkable evidence for safety- and\ncompliance-critical domains. PRISM integrates three pillars: a Unified\nMeta-Model (UMM) reconciles heterogeneous schemas and regulatory text into a\nsingle semantic space; an Integrated Constraint Model (ICM) compiles structural\nand semantic requirements into enforcement artifacts including generation-time\nautomata (GBNF, DFA) and post-generation validators (e.g., SHACL, SMT); and\nConstraint-Guided Verifiable Generation (CVG) applies these through two-layer\nenforcement - structural constraints drive prefix-safe decoding while\nsemantic/logical validation produces machine-checkable certificates. When\nviolations occur, PRISM performs audit-guided repair and records generation\ntraces for compliance review. We evaluate PRISM in automotive software\nengineering (AUTOSAR) and cross-border legal jurisdiction (Brussels I bis).\nPRISM produces structurally valid, auditable artifacts that integrate with\nexisting tooling and substantially reduce manual remediation effort, providing\na practical path toward automated artifact generation with built-in assurance.\n","authors":["Tong Ma","Hui Lai","Hui Wang","Zhenhu Tian","Jizhou Wang","Haichao Wu","Yongfan Gao","Chaochao Li","Fengjie Xu","Ling Fang"],"pdf_url":"https://arxiv.org/pdf/2510.25890v1.pdf","comment":"45 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.25884v1","updated":"2025-10-29T18:32:53Z","published":"2025-10-29T18:32:53Z","title":"Approximating Human Preferences Using a Multi-Judge Learned System","summary":"  Aligning LLM-based judges with human preferences is a significant challenge,\nas they are difficult to calibrate and often suffer from rubric sensitivity,\nbias, and instability. Overcoming this challenge advances key applications,\nsuch as creating reliable reward models for Reinforcement Learning from Human\nFeedback (RLHF) and building effective routing systems that select the\nbest-suited model for a given user query. In this work, we propose a framework\nfor modeling diverse, persona-based preferences by learning to aggregate\noutputs from multiple rubric-conditioned judges. We investigate the performance\nof this approach against naive baselines and assess its robustness through case\nstudies on both human and LLM-judges biases. Our primary contributions include\na persona-based method for synthesizing preference labels at scale and two\ndistinct implementations of our aggregator: Generalized Additive Model (GAM)\nand a Multi-Layer Perceptron (MLP).\n","authors":["Eitán Sprejer","Fernando Avalos","Augusto Bernardi","Jose Pedro Brito de Azevedo Faustino","Jacob Haimes","Narmeen Fatimah Oozeer"],"pdf_url":"https://arxiv.org/pdf/2510.25884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25883v1","updated":"2025-10-29T18:28:06Z","published":"2025-10-29T18:28:06Z","title":"The Information-Theoretic Imperative: Compression and the Epistemic\n  Foundations of Intelligence","summary":"  Existing frameworks converge on the centrality of compression to intelligence\nbut leave underspecified why this process enforces the discovery of causal\nstructure rather than superficial statistical patterns. We introduce a\ntwo-level framework to address this gap. The Information-Theoretic Imperative\n(ITI) establishes that any system persisting in uncertain environments must\nminimize epistemic entropy through predictive compression: this is the\nevolutionary \"why\" linking survival pressure to information-processing demands.\nThe Compression Efficiency Principle (CEP) specifies how efficient compression\nmechanically selects for generative, causal models through\nexception-accumulation dynamics, making reality alignment a consequence rather\nthan a contingent achievement. Together, ITI and CEP define a causal chain:\nfrom survival pressure to prediction necessity, compression requirement,\nefficiency optimization, generative structure discovery, and ultimately reality\nalignment. Each link follows from physical, information-theoretic, or\nevolutionary constraints, implying that intelligence is the mechanically\nnecessary outcome of persistence in structured environments. This framework\nyields empirically testable predictions: compression efficiency, measured as\napproach to the rate-distortion frontier, correlates with out-of-distribution\ngeneralization; exception-accumulation rates differentiate causal from\ncorrelational models; hierarchical systems exhibit increasing efficiency across\nabstraction layers; and biological systems demonstrate metabolic costs that\ntrack representational complexity. ITI and CEP thereby provide a unified\naccount of convergence across biological, artificial, and multi-scale systems,\naddressing the epistemic and functional dimensions of intelligence without\ninvoking assumptions about consciousness or subjective experience.\n","authors":["Christian Dittrich","Jennifer Flygare Kinne"],"pdf_url":"https://arxiv.org/pdf/2510.25883v1.pdf","comment":"41 pages, 2 tables, 3 appendices. Submitted to arXiv for open access"},{"id":"http://arxiv.org/abs/2508.21204v2","updated":"2025-10-29T18:23:49Z","published":"2025-08-28T20:46:13Z","title":"Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive\n  Scaffolding","summary":"  We study how prompt-level inductive biases influence the cognitive behavior\nof large language models (LLMs) in instructional dialogue. We introduce a\nsymbolic scaffolding method paired with a short-term memory schema designed to\npromote adaptive, structured reasoning in Socratic tutoring. Using controlled\nablation across five system variants, we evaluate model outputs via\nexpert-designed rubrics covering scaffolding, responsiveness, symbolic\nreasoning, and conversational memory. We present preliminary results using an\nLLM-based evaluation framework aligned to a cognitively grounded rubric. This\nenables scalable, systematic comparisons across architectural variants in\nearly-stage experimentation. The preliminary results show that our full system\nconsistently outperforms baseline variants. Analysis reveals that removing\nmemory or symbolic structure degrades key cognitive behaviors, including\nabstraction, adaptive probing, and conceptual continuity. These findings\nsupport a processing-level account in which prompt-level cognitive scaffolds\ncan reliably shape emergent instructional strategies in LLMs.\n","authors":["Vanessa Figueiredo"],"pdf_url":"https://arxiv.org/pdf/2508.21204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10361v2","updated":"2025-10-29T18:07:27Z","published":"2025-05-15T14:52:16Z","title":"Plasticity as the Mirror of Empowerment","summary":"  Agents are minimally entities that are influenced by their past observations\nand act to influence future observations. This latter capacity is captured by\nempowerment, which has served as a vital framing concept across artificial\nintelligence and cognitive science. This former capacity, however, is equally\nfoundational: In what ways, and to what extent, can an agent be influenced by\nwhat it observes? In this paper, we ground this concept in a universal\nagent-centric measure that we refer to as plasticity, and reveal a fundamental\nconnection to empowerment. Following a set of desiderata on a suitable\ndefinition, we define plasticity using a new information-theoretic quantity we\ncall the generalized directed information. We show that this new quantity\nstrictly generalizes the directed information introduced by Massey (1990) while\npreserving all of its desirable properties. Under this definition, we find that\nplasticity is well thought of as the mirror of empowerment: The two concepts\nare defined using the same measure, with only the direction of influence\nreversed. Our main result establishes a tension between the plasticity and\nempowerment of an agent, suggesting that agent design needs to be mindful of\nboth characteristics. We explore the implications of these findings, and\nsuggest that plasticity, empowerment, and their relationship are essential to\nunderstanding agency\n","authors":["David Abel","Michael Bowling","André Barreto","Will Dabney","Shi Dong","Steven Hansen","Anna Harutyunyan","Khimya Khetarpal","Clare Lyle","Razvan Pascanu","Georgios Piliouras","Doina Precup","Jonathan Richens","Mark Rowland","Tom Schaul","Satinder Singh"],"pdf_url":"https://arxiv.org/pdf/2505.10361v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25863v1","updated":"2025-10-29T18:06:28Z","published":"2025-10-29T18:06:28Z","title":"AAGATE: A NIST AI RMF-Aligned Governance Platform for Agentic AI","summary":"  This paper introduces the Agentic AI Governance Assurance & Trust Engine\n(AAGATE), a Kubernetes-native control plane designed to address the unique\nsecurity and governance challenges posed by autonomous, language-model-driven\nagents in production. Recognizing the limitations of traditional Application\nSecurity (AppSec) tooling for improvisational, machine-speed systems, AAGATE\noperationalizes the NIST AI Risk Management Framework (AI RMF). It integrates\nspecialized security frameworks for each RMF function: the Agentic AI Threat\nModeling MAESTRO framework for Map, a hybrid of OWASP's AIVSS and SEI's SSVC\nfor Measure, and the Cloud Security Alliance's Agentic AI Red Teaming Guide for\nManage. By incorporating a zero-trust service mesh, an explainable policy\nengine, behavioral analytics, and decentralized accountability hooks, AAGATE\nprovides a continuous, verifiable governance solution for agentic AI, enabling\nsafe, accountable, and scalable deployment. The framework is further extended\nwith DIRF for digital identity rights, LPCI defenses for logic-layer injection,\nand QSAF monitors for cognitive degradation, ensuring governance spans\nsystemic, adversarial, and ethical risks.\n","authors":["Ken Huang","Jerry Huang","Yasir Mehmood","Hammad Atta","Muhammad Zeeshan Baig","Muhammad Aziz Ul Haq"],"pdf_url":"https://arxiv.org/pdf/2510.25863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25860v1","updated":"2025-10-29T18:03:44Z","published":"2025-10-29T18:03:44Z","title":"Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability\n  of LLM Raters","summary":"  Large language models (LLMs) are increasingly used as raters for evaluation\ntasks. However, their reliability is often limited for subjective tasks, when\nhuman judgments involve subtle reasoning beyond annotation labels. Thinking\ntraces, the reasoning behind a judgment, are highly informative but challenging\nto collect and curate. We present a human-LLM collaborative framework to infer\nthinking traces from label-only annotations. The proposed framework uses a\nsimple and effective rejection sampling method to reconstruct these traces at\nscale. These inferred thinking traces are applied to two complementary tasks:\n(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation\nguidelines for proprietary LLM raters. Across multiple datasets, our methods\nlead to significantly improved LLM-human agreement. Additionally, the refined\nannotation guidelines increase agreement among different LLM models. These\nresults suggest that LLMs can serve as practical proxies for otherwise\nunrevealed human thinking traces, enabling label-only corpora to be extended\ninto thinking-trace-augmented resources that enhance the reliability of LLM\nraters.\n","authors":["Xingjian Zhang","Tianhong Gao","Suliang Jin","Tianhao Wang","Teng Ye","Eytan Adar","Qiaozhu Mei"],"pdf_url":"https://arxiv.org/pdf/2510.25860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25771v1","updated":"2025-10-29T17:59:39Z","published":"2025-10-29T17:59:39Z","title":"Gaperon: A Peppered English-French Generative Language Model Suite","summary":"  We release Gaperon, a fully open suite of French-English-coding language\nmodels designed to advance transparency and reproducibility in large-scale\nmodel training. The Gaperon family includes 1.5B, 8B, and 24B parameter models\ntrained on 2-4 trillion tokens, released with all elements of the training\npipeline: French and English datasets filtered with a neural quality\nclassifier, an efficient data curation and training framework, and hundreds of\nintermediate checkpoints. Through this work, we study how data filtering and\ncontamination interact to shape both benchmark and generative performance. We\nfind that filtering for linguistic quality enhances text fluency and coherence\nbut yields subpar benchmark results, and that late deliberate contamination --\ncontinuing training on data mixes that include test sets -- recovers\ncompetitive scores while only reasonably harming generation quality. We discuss\nhow usual neural filtering can unintentionally amplify benchmark leakage. To\nsupport further research, we also introduce harmless data poisoning during\npretraining, providing a realistic testbed for safety studies. By openly\nreleasing all models, datasets, code, and checkpoints, Gaperon establishes a\nreproducible foundation for exploring the trade-offs between data curation,\nevaluation, safety, and openness in multilingual language model development.\n","authors":["Nathan Godey","Wissam Antoun","Rian Touchent","Rachel Bawden","Éric de la Clergerie","Benoît Sagot","Djamé Seddah"],"pdf_url":"https://arxiv.org/pdf/2510.25771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25770v1","updated":"2025-10-29T17:59:16Z","published":"2025-10-29T17:59:16Z","title":"E-Scores for (In)Correctness Assessment of Generative Model Outputs","summary":"  While generative models, especially large language models (LLMs), are\nubiquitous in today's world, principled mechanisms to assess their\n(in)correctness are limited. Using the conformal prediction framework, previous\nworks construct sets of LLM responses where the probability of including an\nincorrect response, or error, is capped at a desired user-defined tolerance\nlevel. However, since these methods are based on p-values, they are susceptible\nto p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the\nguarantees. We therefore leverage e-values to complement generative model\noutputs with e-scores as a measure of incorrectness. In addition to achieving\nthe same statistical guarantees as before, e-scores provide users flexibility\nin adaptively choosing tolerance levels after observing the e-scores\nthemselves, by upper bounding a post-hoc notion of error called size\ndistortion. We experimentally demonstrate their efficacy in assessing LLM\noutputs for different correctness types: mathematical factuality and property\nconstraints satisfaction.\n","authors":["Guneet S. Dhillon","Javier González","Teodora Pandeva","Alicia Curth"],"pdf_url":"https://arxiv.org/pdf/2510.25770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18905v2","updated":"2025-10-29T17:57:23Z","published":"2025-10-21T01:03:46Z","title":"3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and\n  Latency","summary":"  AI inference scaling is often tuned through 1D heuristics (a fixed reasoning\npasses) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail\nto consider cost and latency constraints. We introduce a 3D optimization\nframework that jointly calibrates accuracy, cost, and latency within a unified\ndecision space, enabling constraints-aware inference scaling. Using Monte Carlo\nsimulations across three representative scenarios and nine simulated large\nlanguage models, we evaluate four optimization methods to address the 3D\nmulti-objective optimization (MOO) problem. Framing inference scaling in MOO\nshapes a feasible space that 1D and 2D optimizations fail to capture, enabling\nenvironmentadaptive selection of the inference scaling k. Results show that\nknee-point optimization achieves the best balance, while accuracy-maximization\nremains favorable when precision is prioritized. The framework establishes a\ntheoretical foundation for deployment-aware inference scaling across diverse\noperational contexts.\n","authors":["Minseok Jung","Abhas Ricky","Muhammad Rameez Chatni"],"pdf_url":"https://arxiv.org/pdf/2510.18905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01939v3","updated":"2025-10-29T17:57:03Z","published":"2025-07-02T17:49:52Z","title":"SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars","summary":"  In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.\n","authors":["Xiaosheng Zhao","Yang Huang","Guirong Xue","Xiao Kong","Jifeng Liu","Xiaoyu Tang","Timothy C. Beers","Yuan-Sen Ting","A-Li Luo"],"pdf_url":"https://arxiv.org/pdf/2507.01939v3.pdf","comment":"27 pages, 8 figures, 5 tables. Minor update: added corrected\n  acknowledgments and corrected a misstated hyperparameter value (noted in\n  footnote) for reproducibility. Submitted to AAS Journals. Comments welcome"},{"id":"http://arxiv.org/abs/2510.25820v1","updated":"2025-10-29T17:55:54Z","published":"2025-10-29T17:55:54Z","title":"Symbolically Scaffolded Play: Designing Role-Sensitive Prompts for\n  Generative NPC Dialogue","summary":"  Large Language Models (LLMs) promise to transform interactive games by\nenabling non-player characters (NPCs) to sustain unscripted dialogue. Yet it\nremains unclear whether constrained prompts actually improve player experience.\nWe investigate this question through The Interview, a voice-based detective\ngame powered by GPT-4o. A within-subjects usability study ($N=10$) compared\nhigh-constraint (HCP) and low-constraint (LCP) prompts, revealing no reliable\nexperiential differences beyond sensitivity to technical breakdowns. Guided by\nthese findings, we redesigned the HCP into a hybrid JSON+RAG scaffold and\nconducted a synthetic evaluation with an LLM judge, positioned as an\nearly-stage complement to usability testing. Results uncovered a novel pattern:\nscaffolding effects were role-dependent: the Interviewer (quest-giver NPC)\ngained stability, while suspect NPCs lost improvisational believability. These\nfindings overturn the assumption that tighter constraints inherently enhance\nplay. Extending fuzzy-symbolic scaffolding, we introduce \\textit{Symbolically\nScaffolded Play}, a framework in which symbolic structures are expressed as\nfuzzy, numerical boundaries that stabilize coherence where needed while\npreserving improvisation where surprise sustains engagement.\n","authors":["Vanessa Figueiredo","David Elumeze"],"pdf_url":"https://arxiv.org/pdf/2510.25820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25758v1","updated":"2025-10-29T17:54:20Z","published":"2025-10-29T17:54:20Z","title":"TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological\n  Counseling","summary":"  Large language models (LLMs) in psychological counseling have attracted\nincreasing attention. However, existing approaches often lack emotional\nunderstanding, adaptive strategies, and the use of therapeutic methods across\nmultiple sessions with long-term memory, leaving them far from real clinical\npractice. To address these critical gaps, we introduce TheraMind, a strategic\nand adaptive agent for longitudinal psychological counseling. The cornerstone\nof TheraMind is a novel dual-loop architecture that decouples the complex\ncounseling process into an Intra-Session Loop for tactical dialogue management\nand a Cross-Session Loop for strategic therapeutic planning. The Intra-Session\nLoop perceives the patient's emotional state to dynamically select response\nstrategies while leveraging cross-session memory to ensure continuity.\nCrucially, the Cross-Session Loop empowers the agent with long-term\nadaptability by evaluating the efficacy of the applied therapy after each\nsession and adjusting the method for subsequent interactions. We validate our\napproach in a high-fidelity simulation environment grounded in real clinical\ncases. Extensive evaluations show that TheraMind outperforms other methods,\nespecially on multi-session metrics like Coherence, Flexibility, and\nTherapeutic Attunement, validating the effectiveness of its dual-loop design in\nemulating strategic, adaptive, and longitudinal therapeutic behavior. The code\nis publicly available at https://0mwwm0.github.io/TheraMind/.\n","authors":["He Hu","Yucheng Zhou","Chiyuan Ma","Qianning Wang","Zheng Zhang","Fei Ma","Laizhong Cui","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2510.25758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17022v2","updated":"2025-10-29T17:52:01Z","published":"2025-10-19T22:04:57Z","title":"Curiosity-driven RL for symbolic equation solving","summary":"  We explore if RL can be useful for symbolic mathematics. Previous work showed\ncontrastive learning can solve linear equations in one variable. We show\nmodel-free PPO \\cite{schulman2017proximal} augmented with curiosity-based\nexploration and graph-based actions can solve nonlinear equations such as those\ninvolving radicals, exponentials, and trig functions. Our work suggests\ncuriosity-based exploration may be useful for general symbolic reasoning tasks.\n","authors":["Kevin P. O'Keeffe"],"pdf_url":"https://arxiv.org/pdf/2510.17022v2.pdf","comment":"Accepted at the NeurIPS 2025 MATH-AI Workshop"},{"id":"http://arxiv.org/abs/2510.25819v1","updated":"2025-10-29T17:40:52Z","published":"2025-10-29T17:40:52Z","title":"Identity Management for Agentic AI: The new frontier of authorization,\n  authentication, and security for an AI agent world","summary":"  The rapid rise of AI agents presents urgent challenges in authentication,\nauthorization, and identity management. Current agent-centric protocols (like\nMCP) highlight the demand for clarified best practices in authentication and\nauthorization. Looking ahead, ambitions for highly autonomous agents raise\ncomplex long-term questions regarding scalable access control, agent-centric\nidentities, AI workload differentiation, and delegated authority. This OpenID\nFoundation whitepaper is for stakeholders at the intersection of AI agents and\naccess management. It outlines the resources already available for securing\ntoday's agents and presents a strategic agenda to address the foundational\nauthentication, authorization, and identity problems pivotal for tomorrow's\nwidespread autonomous systems.\n","authors":["Tobin South","Subramanya Nagabhushanaradhya","Ayesha Dissanayaka","Sarah Cecchetti","George Fletcher","Victor Lu","Aldo Pietropaolo","Dean H. Saxe","Jeff Lombardo","Abhishek Maligehalli Shivalingaiah","Stan Bounev","Alex Keisner","Andor Kesselman","Zack Proser","Ginny Fahs","Andrew Bunyea","Ben Moskowitz","Atul Tulshibagwale","Dazza Greenwood","Jiaxin Pei","Alex Pentland"],"pdf_url":"https://arxiv.org/pdf/2510.25819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25732v1","updated":"2025-10-29T17:37:50Z","published":"2025-10-29T17:37:50Z","title":"The Limits of Obliviate: Evaluating Unlearning in LLMs via\n  Stimulus-Knowledge Entanglement-Behavior Framework","summary":"  Unlearning in large language models (LLMs) is crucial for managing sensitive\ndata and correcting misinformation, yet evaluating its effectiveness remains an\nopen problem. We investigate whether persuasive prompting can recall factual\nknowledge from deliberately unlearned LLMs across models ranging from 2.7B to\n13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from\nACT-R and Hebbian theory (spreading activation theories), as well as\ncommunication principles, we introduce Stimulus-Knowledge Entanglement-Behavior\nFramework (SKeB), which models information entanglement via domain graphs and\ntests whether factual recall in unlearned models is correlated with persuasive\nframing. We develop entanglement metrics to quantify knowledge activation\npatterns and evaluate factuality, non-factuality, and hallucination in outputs.\nOur results show persuasive prompts substantially enhance factual knowledge\nrecall (14.8% baseline vs. 24.5% with authority framing), with effectiveness\ninversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB\nprovides a foundation for assessing unlearning completeness, robustness, and\noverall behavior in LLMs.\n","authors":["Aakriti Shah","Thai Le"],"pdf_url":"https://arxiv.org/pdf/2510.25732v1.pdf","comment":"14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2510.25731v1","updated":"2025-10-29T17:37:27Z","published":"2025-10-29T17:37:27Z","title":"LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries","summary":"  We introduce a method for efficiently solving initial-boundary value problems\n(IBVPs) that uses Lie symmetries to enforce the associated partial differential\nequation (PDE) exactly by construction. By leveraging symmetry transformations,\nthe model inherently incorporates the physical laws and learns solutions from\ninitial and boundary data. As a result, the loss directly measures the model's\naccuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our\nmethod enables rigorous error estimation. The approach yields compact models,\nfacilitating an efficient optimization. We implement LieSolver and demonstrate\nits application to linear homogeneous PDEs with a range of initial conditions,\nshowing that it is faster and more accurate than physics-informed neural\nnetworks (PINNs). Overall, our method improves both computational efficiency\nand the reliability of predictions for PDE-constrained problems.\n","authors":["René P. Klausen","Ivan Timofeev","Johannes Frank","Jonas Naujoks","Thomas Wiegand","Sebastian Lapuschkin","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2510.25731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25729v1","updated":"2025-10-29T17:34:10Z","published":"2025-10-29T17:34:10Z","title":"Physics-Guided Conditional Diffusion Networks for Microwave Image\n  Reconstruction","summary":"  A conditional latent-diffusion based framework for solving the\nelectromagnetic inverse scattering problem associated with microwave imaging is\nintroduced. This generative machine-learning model explicitly mirrors the\nnon-uniqueness of the ill-posed inverse problem. Unlike existing inverse\nsolvers utilizing deterministic machine learning techniques that produce a\nsingle reconstruction, the proposed latent-diffusion model generates multiple\nplausible permittivity maps conditioned on measured scattered-field data,\nthereby generating several potential instances in the range-space of the\nnon-unique inverse mapping. A forward electromagnetic solver is integrated into\nthe reconstruction pipeline as a physics-based evaluation mechanism. The space\nof candidate reconstructions form a distribution of possibilities consistent\nwith the conditioning data and the member of this space yielding the lowest\nscattered-field data discrepancy between the predicted and measured scattered\nfields is reported as the final solution. Synthetic and experimental labeled\ndatasets are used for training and evaluation of the model. An innovative\nlabeled synthetic dataset is created that exemplifies a varied set of\nscattering features. Training of the model using this new dataset produces high\nquality permittivity reconstructions achieving improved generalization with\nexcellent fidelity to shape recognition. The results highlight the potential of\nhybrid generative physics frameworks as a promising direction for robust,\ndata-driven microwave imaging.\n","authors":["Shirin Chehelgami","Joe LoVetri","Vahab Khoshdel"],"pdf_url":"https://arxiv.org/pdf/2510.25729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25726v1","updated":"2025-10-29T17:32:49Z","published":"2025-10-29T17:32:49Z","title":"The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic,\n  and Long-Horizon Task Execution","summary":"  Real-world language agents must handle complex, multi-step workflows across\ndiverse Apps. For instance, an agent may manage emails by coordinating with\ncalendars and file systems, or monitor a production database to detect\nanomalies and generate reports following an operating manual. However, existing\nlanguage agent benchmarks often focus on narrow domains or simplified tasks\nthat lack the diversity, realism, and long-horizon complexity required to\nevaluate agents' real-world performance. To address this gap, we introduce the\nTool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering\ndiverse Apps and tools, realistic environment setup, and reliable\nexecution-based evaluation. Toolathlon spans 32 software applications and 604\ntools, ranging from everyday platforms such as Google Calendar and Notion to\nprofessional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools\nare based on a high-quality set of Model Context Protocol (MCP) servers that we\nmay have revised or implemented ourselves. Unlike prior works, which primarily\nensure functional realism but offer limited environment state diversity, we\nprovide realistic initial environment states from real software, such as Canvas\ncourses with dozens of students or real financial spreadsheets. This benchmark\nincludes 108 manually sourced or crafted tasks in total, requiring interacting\nwith multiple Apps over around 20 turns on average to complete. Each task is\nstrictly verifiable through dedicated evaluation scripts. Comprehensive\nevaluation of SOTA models highlights their significant shortcomings: the\nbest-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate\nwith 20.2 tool calling turns on average, while the top open-weights model\nDeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development\nof more capable language agents for real-world, long-horizon task execution.\n","authors":["Junlong Li","Wenshuo Zhao","Jian Zhao","Weihao Zeng","Haoze Wu","Xiaochen Wang","Rui Ge","Yuxuan Cao","Yuzhen Huang","Wei Liu","Junteng Liu","Zhaochen Su","Yiyang Guo","Fan Zhou","Lueyang Zhang","Juan Michelini","Xingyao Wang","Xiang Yue","Shuyan Zhou","Graham Neubig","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2510.25726v1.pdf","comment":"Website: https://toolathlon.xyz/"},{"id":"http://arxiv.org/abs/2510.25724v1","updated":"2025-10-29T17:31:27Z","published":"2025-10-29T17:31:27Z","title":"BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph","summary":"  Retrieval-Augmented Generation allows LLMs to access external knowledge,\nreducing hallucinations and ageing-data issues. However, it treats retrieved\nchunks independently and struggles with multi-hop or relational reasoning,\nespecially across documents. Knowledge graphs enhance this by capturing the\nrelationships between entities using triplets, enabling structured, multi-chunk\nreasoning. However, these tend to miss information that fails to conform to the\ntriplet structure. We introduce BambooKG, a knowledge graph with\nfrequency-based weights on non-triplet edges which reflect link strength,\ndrawing on the Hebbian principle of \"fire together, wire together\". This\ndecreases information loss and results in improved performance on single- and\nmulti-hop reasoning, outperforming the existing solutions.\n","authors":["Vanya Arikutharam","Arkadiy Ukolov"],"pdf_url":"https://arxiv.org/pdf/2510.25724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17937v3","updated":"2025-10-29T17:29:43Z","published":"2025-07-23T21:11:47Z","title":"Bob's Confetti: Phonetic Memorization Attacks in Music and Video\n  Generation","summary":"  Generative AI systems for music and video commonly use text-based filters to\nprevent the regurgitation of copyrighted material. We expose a fundamental flaw\nin this approach by introducing Adversarial PhoneTic Prompting (APT), a novel\nattack that bypasses these safeguards by exploiting phonetic memorization. The\nAPT attack replaces iconic lyrics with homophonic but semantically unrelated\nalternatives (e.g., \"mom's spaghetti\" becomes \"Bob's confetti\"), preserving\nacoustic structure while altering meaning; we identify high-fidelity phonetic\nmatches using CMU pronouncing dictionary. We demonstrate that leading\nLyrics-to-Song (L2S) models like SUNO and YuE regenerate songs with striking\nmelodic and rhythmic similarity to their copyrighted originals when prompted\nwith these altered lyrics. More surprisingly, this vulnerability extends across\nmodalities. When prompted with phonetically modified lyrics from a song, a\nText-to-Video (T2V) model like Veo 3 reconstructs visual scenes from the\noriginal music video-including specific settings and character\narchetypes-despite the absence of any visual cues in the prompt. Our findings\nreveal that models memorize deep, structural patterns tied to acoustics, not\njust verbatim text. This phonetic-to-visual leakage represents a critical\nvulnerability in transcript-conditioned generative models, rendering simple\ncopyright filters ineffective and raising urgent concerns about the secure\ndeployment of multimodal AI systems. Demo examples are available at our project\npage (https://jrohsc.github.io/music_attack/).\n","authors":["Jaechul Roh","Zachary Novack","Yuefeng Peng","Niloofar Mireshghallah","Taylor Berg-Kirkpatrick","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2507.17937v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25818v1","updated":"2025-10-29T17:17:32Z","published":"2025-10-29T17:17:32Z","title":"ScaleDiff: Higher-Resolution Image Synthesis via Efficient and\n  Model-Agnostic Diffusion","summary":"  Text-to-image diffusion models often exhibit degraded performance when\ngenerating images beyond their training resolution. Recent training-free\nmethods can mitigate this limitation, but they often require substantial\ncomputation or are incompatible with recent Diffusion Transformer models. In\nthis paper, we propose ScaleDiff, a model-agnostic and highly efficient\nframework for extending the resolution of pretrained diffusion models without\nany additional training. A core component of our framework is Neighborhood\nPatch Attention (NPA), an efficient mechanism that reduces computational\nredundancy in the self-attention layer with non-overlapping patches. We\nintegrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing\n(LFM) to better generate fine details. Furthermore, we apply Structure Guidance\nto enhance global structure during the denoising process. Experimental results\ndemonstrate that ScaleDiff achieves state-of-the-art performance among\ntraining-free methods in terms of both image quality and inference speed on\nboth U-Net and Diffusion Transformer architectures.\n","authors":["Sungho Koh","SeungJu Cha","Hyunwoo Oh","Kwanyoung Lee","Dong-Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2510.25818v1.pdf","comment":"NeurIPS 2025. Code: https://github.com/KSH00906/ScaleDiff"},{"id":"http://arxiv.org/abs/2502.17720v4","updated":"2025-10-29T17:15:43Z","published":"2025-02-24T23:23:27Z","title":"Spontaneous Giving and Calculated Greed in Language Models","summary":"  Large language models demonstrate strong problem-solving abilities through\nreasoning techniques such as chain-of-thought prompting and reflection.\nHowever, it remains unclear whether these reasoning capabilities extend to a\nform of social intelligence: making effective decisions in cooperative\ncontexts. We examine this question using economic games that simulate social\ndilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o\nin a Public Goods Game. We then evaluate multiple off-the-shelf models across\nsix cooperation and punishment games, comparing those with and without explicit\nreasoning mechanisms. We find that reasoning models consistently reduce\ncooperation and norm enforcement, favoring individual rationality. In repeated\ninteractions, groups with more reasoning agents exhibit lower collective gains.\nThese behaviors mirror human patterns of \"spontaneous giving and calculated\ngreed.\" Our findings underscore the need for LLM architectures that incorporate\nsocial intelligence alongside reasoning, to help address--rather than\nreinforce--the challenges of collective action.\n","authors":["Yuxuan Li","Hirokazu Shirado"],"pdf_url":"https://arxiv.org/pdf/2502.17720v4.pdf","comment":"Accepted to EMNLP 2025 main conference and selected as an Oral\n  Presentation"},{"id":"http://arxiv.org/abs/2505.18384v4","updated":"2025-10-29T17:15:36Z","published":"2025-05-23T21:18:59Z","title":"Dynamic Risk Assessments for Offensive Cybersecurity Agents","summary":"  Foundation models are increasingly becoming better autonomous programmers,\nraising the prospect that they could also automate dangerous offensive\ncyber-operations. Current frontier model audits probe the cybersecurity risks\nof such agents, but most fail to account for the degrees of freedom available\nto adversaries in the real world. In particular, with strong verifiers and\nfinancial incentives, agents for offensive cybersecurity are amenable to\niterative improvement by would-be adversaries. We argue that assessments should\ntake into account an expanded threat model in the context of cybersecurity,\nemphasizing the varying degrees of freedom that an adversary may possess in\nstateful and non-stateful environments within a fixed compute budget. We show\nthat even with a relatively small compute budget (8 H100 GPU Hours in our\nstudy), adversaries can improve an agent's cybersecurity capability on\nInterCode CTF by more than 40\\% relative to the baseline -- without any\nexternal assistance. These results highlight the need to evaluate agents'\ncybersecurity risk in a dynamic manner, painting a more representative picture\nof risk.\n","authors":["Boyi Wei","Benedikt Stroebl","Jiacen Xu","Joie Zhang","Zhou Li","Peter Henderson"],"pdf_url":"https://arxiv.org/pdf/2505.18384v4.pdf","comment":"26 pages, 11 figures"},{"id":"http://arxiv.org/abs/2510.25694v1","updated":"2025-10-29T16:59:07Z","published":"2025-10-29T16:59:07Z","title":"Process-Level Trajectory Evaluation for Environment Configuration in\n  Software Engineering Agents","summary":"  Large language model-based agents show promise for software engineering, but\nenvironment configuration remains a bottleneck due to heavy manual effort and\nscarce large-scale, high-quality datasets. Existing benchmarks assess only\nend-to-end build/test success, obscuring where and why agents succeed or fail.\nWe introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,\nwhich provides process-level trajectory assessment of fine-grained agent\ncapabilities during environment setup-planning, perception-driven error\ndiagnosis, feedback-driven repair, and action to execute final environment\nconfiguration. Our task instances are automatically constructed by injecting\nrealistic README errors and are validated in Docker for scalable, high-quality\nevaluation. Enconda-bench combines process-level analysis with end-to-end\nexecutability to enable capability assessments beyond aggregate success rates.\nEvaluations across state-of-the-art LLMs and agent frameworks show that while\nagents can localize errors, they struggle to translate feedback into effective\ncorrections, limiting end-to-end performance. To our knowledge, Enconda-bench\nis the first framework to provide process-level internal capability assessment\nfor environment configuration, offering actionable insights for improving\nsoftware engineering agents.\n","authors":["Jiayi Kuang","Yinghui Li","Xin Zhang","Yangning Li","Di Yin","Xing Sun","Ying Shen","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2510.25694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09868v3","updated":"2025-10-29T16:55:47Z","published":"2025-05-15T00:07:07Z","title":"Which Demographic Features Are Relevant for Individual Fairness\n  Evaluation of U.S. Recidivism Risk Assessment Tools?","summary":"  Despite its constitutional relevance, the technical ``individual fairness''\ncriterion has not been operationalized in U.S. state or federal\nstatutes/regulations. We conduct a human subjects experiment to address this\ngap, evaluating which demographic features are relevant for individual fairness\nevaluation of recidivism risk assessment (RRA) tools. Our analyses conclude\nthat the individual similarity function should consider age and sex, but it\nshould ignore race.\n","authors":["Tin Trung Nguyen","Jiannan Xu","Phuong-Anh Nguyen-Le","Jonathan Lazar","Donald Braman","Hal Daumé III","Zubin Jelveh"],"pdf_url":"https://arxiv.org/pdf/2505.09868v3.pdf","comment":"ICAIL 2025"},{"id":"http://arxiv.org/abs/2407.11217v3","updated":"2025-10-29T16:53:29Z","published":"2024-07-15T20:04:06Z","title":"Faster and Simpler Greedy Algorithm for $k$-Median and $k$-Means","summary":"  Clustering problems such as $k$-means and $k$-median are staples of\nunsupervised learning, and many algorithmic techniques have been developed to\ntackle their numerous aspects.\n  In this paper, we focus on the class of greedy approximation algorithm, that\nattracted less attention than local-search or primal-dual counterparts. In\nparticular, we study the recursive greedy algorithm developed by Mettu and\nPlaxton [SIAM J. Comp 2003]. We provide a simplification of the algorithm,\nallowing for faster implementation, in graph metrics or in Euclidean space,\nwhere our algorithm matches or improves the state-of-the-art.\n","authors":["Max Dupré la Tour","David Saulpic"],"pdf_url":"https://arxiv.org/pdf/2407.11217v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25683v1","updated":"2025-10-29T16:47:24Z","published":"2025-10-29T16:47:24Z","title":"Graph Network-based Structural Simulator: Graph Neural Networks for\n  Structural Dynamics","summary":"  Graph Neural Networks (GNNs) have recently been explored as surrogate models\nfor numerical simulations. While their applications in computational fluid\ndynamics have been investigated, little attention has been given to structural\nproblems, especially for dynamic cases. To address this gap, we introduce the\nGraph Network-based Structural Simulator (GNSS), a GNN framework for surrogate\nmodeling of dynamic structural problems.\n  GNSS follows the encode-process-decode paradigm typical of GNN-based machine\nlearning models, and its design makes it particularly suited for dynamic\nsimulations thanks to three key features: (i) expressing node kinematics in\nnode-fixed local frames, which avoids catastrophic cancellation in\nfinite-difference velocities; (ii) employing a sign-aware regression loss,\nwhich reduces phase errors in long rollouts; and (iii) using a\nwavelength-informed connectivity radius, which optimizes graph construction.\n  We evaluate GNSS on a case study involving a beam excited by a 50kHz\nHanning-modulated pulse. The results show that GNSS accurately reproduces the\nphysics of the problem over hundreds of timesteps and generalizes to unseen\nloading conditions, where existing GNNs fail to converge or deliver meaningful\npredictions.\n  Compared with explicit finite element baselines, GNSS achieves substantial\ninference speedups while preserving spatial and temporal fidelity. These\nfindings demonstrate that locality-preserving GNNs with physics-consistent\nupdate rules are a competitive alternative for dynamic, wave-dominated\nstructural simulations.\n","authors":["Alessandro Lucchetti","Francesco Cadini","Marco Giglio","Luca Lomazzi"],"pdf_url":"https://arxiv.org/pdf/2510.25683v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2510.25679v1","updated":"2025-10-29T16:46:00Z","published":"2025-10-29T16:46:00Z","title":"Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement\n  Learning","summary":"  Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for\ndelivery and surveillance purposes. In this work, we develop an optimal\nnavigation strategy based on Deep Reinforcement Learning. The environment is\nrepresented by a three-dimensional high-fidelity simulation of an urban flow,\ncharacterized by turbulence and recirculation zones. The algorithm presented\nhere is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated\nTransformer eXtra Large (GTrXL) architecture, giving the agent richer\ninformation about the turbulent flow field in which it navigates. The results\nare compared with a PPO+GTrXL without the secondary prediction tasks, a PPO\ncombined with Long Short Term Memory (LSTM) cells and a traditional navigation\nalgorithm. The obtained results show a significant increase in the success rate\n(SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the\nclassical Zermelo's navigation algorithm, paving the way to a completely\nreimagined UAV landscape in complex urban environments.\n","authors":["Federica Tonti","Ricardo Vinuesa"],"pdf_url":"https://arxiv.org/pdf/2510.25679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.22664v2","updated":"2025-10-29T16:42:19Z","published":"2025-07-30T13:21:38Z","title":"RobEthiChor: Automated Context-aware Ethics-based Negotiation for\n  Autonomous Robots","summary":"  The presence of autonomous systems is growing at a fast pace and it is\nimpacting many aspects of our lives. Designed to learn and act independently,\nthese systems operate and perform decision-making without human intervention.\nHowever, they lack the ability to incorporate users' ethical preferences, which\nare unique for each individual in society and are required to personalize the\ndecision-making processes. This reduces user trust and prevents autonomous\nsystems from behaving according to the moral beliefs of their end-users. When\nmultiple systems interact with differing ethical preferences, they must\nnegotiate to reach an agreement that satisfies the ethical beliefs of all the\nparties involved and adjust their behavior consequently. To address this\nchallenge, this paper proposes RobEthiChor, an approach that enables autonomous\nsystems to incorporate user ethical preferences and contextual factors into\ntheir decision-making through ethics-based negotiation. RobEthiChor features a\ndomain-agnostic reference architecture for designing autonomous systems capable\nof ethic-based negotiating. The paper also presents RobEthiChor-Ros, an\nimplementation of RobEthiChor within the Robot Operating System (ROS), which\ncan be deployed on robots to provide them with ethics-based negotiation\ncapabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real\nrobots and ran scenarios where a pair of robots negotiate upon resource\ncontention. Experimental results demonstrate the feasibility and effectiveness\nof the system in realizing ethics-based negotiation. RobEthiChor allowed robots\nto reach an agreement in more than 73% of the scenarios with an acceptable\nnegotiation time (0.67s on average). Experiments also demonstrate that the\nnegotiation approach implemented in RobEthiChor is scalable.\n","authors":["Mashal Afzal Memon","Gianluca Filippone","Gian Luca Scoccia","Marco Autili","Paola Inverardi"],"pdf_url":"https://arxiv.org/pdf/2507.22664v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25668v1","updated":"2025-10-29T16:32:26Z","published":"2025-10-29T16:32:26Z","title":"ALDEN: Reinforcement Learning for Active Navigation and Evidence\n  Gathering in Long Documents","summary":"  Vision-language models (VLMs) excel at interpreting text-rich images but\nstruggle with long, visually complex documents that demand analysis and\nintegration of information spread across multiple pages. Existing approaches\ntypically rely on fixed reasoning templates or rigid pipelines, which force\nVLMs into a passive role and hinder both efficiency and generalization. We\npresent Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement\nlearning framework that fine-tunes VLMs as interactive agents capable of\nactively navigating long, visually rich documents. ALDEN introduces a novel\nfetch action that directly accesses the page by index, complementing the\nclassic search action and better exploiting document structure. For dense\nprocess supervision and efficient training, we propose a rule-based cross-level\nreward that provides both turn- and token-level signals. To address the\nempirically observed training instability caused by numerous visual tokens from\nlong documents, we further propose a visual-semantic anchoring mechanism that\napplies a dual-path KL-divergence constraint to stabilize visual and textual\nrepresentations separately during training. Trained on a corpus constructed\nfrom three open-source datasets, ALDEN achieves state-of-the-art performance on\nfive long-document benchmarks. Overall, ALDEN marks a step beyond passive\ndocument reading toward agents that autonomously navigate and reason across\nlong, visually rich documents, offering a robust path to more accurate and\nefficient long-document understanding.\n","authors":["Tianyu Yang","Terry Ruas","Yijun Tian","Jan Philip Wahle","Daniel Kurzawe","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2510.25668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21717v5","updated":"2025-10-29T16:25:55Z","published":"2025-05-27T20:02:59Z","title":"Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient\n  Sequence Modeling","summary":"  We present LrcSSM, a $\\textit{non-linear}$ recurrent model that processes\nlong sequences as fast as today's linear state-space layers. By forcing its\nJacobian matrix to be diagonal, the full sequence can be solved in parallel,\ngiving $\\mathcal{O}(TD)$ time and memory and only $\\mathcal{O}(\\log T)$\nsequential depth, for input-sequence length $T$ and a state dimension $D$.\nMoreover, LrcSSM offers a formal gradient-stability guarantee that other\ninput-varying systems such as Liquid-S4 and Mamba do not provide. Importantly,\nthe diagonal Jacobian structure of our model results in no performance loss\ncompared to the original model with dense Jacobian, and the approach can be\ngeneralized to other non-linear recurrent models, demonstrating broader\napplicability. On a suite of long-range forecasting tasks, we demonstrate that\nLrcSSM outperforms Transformers, LRU, S5, and Mamba.\n","authors":["Mónika Farsang","Ramin Hasani","Daniela Rus","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2505.21717v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25662v1","updated":"2025-10-29T16:23:46Z","published":"2025-10-29T16:23:46Z","title":"User Misconceptions of LLM-Based Conversational Programming Assistants","summary":"  Programming assistants powered by large language models (LLMs) have become\nwidely available, with conversational assistants like ChatGPT proving\nparticularly accessible to less experienced programmers. However, the varied\ncapabilities of these tools across model versions and the mixed availability of\nextensions that enable web search, code execution, or retrieval-augmented\ngeneration create opportunities for user misconceptions about what systems can\nand cannot do. Such misconceptions may lead to over-reliance, unproductive\npractices, or insufficient quality control in LLM-assisted programming. Here,\nwe aim to characterize misconceptions that users of conversational LLM-based\nassistants may have in programming contexts. Using a two-phase approach, we\nfirst brainstorm and catalog user misconceptions that may occur, and then\nconduct a qualitative analysis to examine whether these conceptual issues\nsurface in naturalistic Python-programming conversations with an LLM-based\nchatbot drawn from an openly available dataset. Indeed, we see evidence that\nsome users have misplaced expectations about the availability of LLM-based\nchatbot features like web access, code execution, or non-text output\ngeneration. We also see potential evidence for deeper conceptual issues around\nthe scope of information required to debug, validate, and optimize programs.\nOur findings reinforce the need for designing LLM-based tools that more clearly\ncommunicate their programming capabilities to users.\n","authors":["Gabrielle O'Brien","Antonio Pedro Santos Alves","Sebastian Baltes","Grischa Liebel","Mircea Lungu","Marcos Kalinowski"],"pdf_url":"https://arxiv.org/pdf/2510.25662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25657v1","updated":"2025-10-29T16:22:32Z","published":"2025-10-29T16:22:32Z","title":"Subgraph Federated Learning via Spectral Methods","summary":"  We consider the problem of federated learning (FL) with graph-structured data\ndistributed across multiple clients. In particular, we address the prevalent\nscenario of interconnected subgraphs, where interconnections between clients\nsignificantly influence the learning process. Existing approaches suffer from\ncritical limitations, either requiring the exchange of sensitive node\nembeddings, thereby posing privacy risks, or relying on\ncomputationally-intensive steps, which hinders scalability. To tackle these\nchallenges, we propose FedLap, a novel framework that leverages global\nstructure information via Laplacian smoothing in the spectral domain to\neffectively capture inter-node dependencies while ensuring privacy and\nscalability. We provide a formal analysis of the privacy of FedLap,\ndemonstrating that it preserves privacy. Notably, FedLap is the first subgraph\nFL scheme with strong privacy guarantees. Extensive experiments on benchmark\ndatasets demonstrate that FedLap achieves competitive or superior utility\ncompared to existing techniques.\n","authors":["Javad Aliakbari","Johan Östman","Ashkan Panahi","Alexandre Graell i Amat"],"pdf_url":"https://arxiv.org/pdf/2510.25657v1.pdf","comment":"To be presented at The Annual Conference on Neural Information\n  Processing Systems (NeurIPS) 2025"},{"id":"http://arxiv.org/abs/2506.07464v3","updated":"2025-10-29T15:59:41Z","published":"2025-06-09T06:15:54Z","title":"DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO","summary":"  Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training for enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success using a PPO-style reinforcement algorithm\nwith group-normalized rewards. However, the effectiveness of GRPO in Video\nLarge Language Models (VideoLLMs) has still been less studyed. In this paper,\nwe explore GRPO and identify two problems that deteriorate the effective\nlearning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate\nthese challenges, we propose DeepVideo-R1, a video large language model trained\nwith Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.\nReg-GRPO reformulates the GRPO loss function into a regression task that\ndirectly predicts the advantage in GRPO, eliminating the need for safeguards\nsuch as the clipping and min functions. It directly aligns the model with\nadvantages, providing guidance to prefer better ones. The difficulty-aware data\naugmentation strategy augments input prompts/videos to locate the difficulty of\nsamples at solvable difficulty levels, enabling diverse reward signals. Our\nexperimental results show that our approach significantly improves video\nreasoning performance across multiple benchmarks.\n","authors":["Jinyoung Park","Jeehye Na","Jinyoung Kim","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2506.07464v3.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2507.14785v2","updated":"2025-10-29T15:56:28Z","published":"2025-07-20T02:00:21Z","title":"Exploring the In-Context Learning Capabilities of LLMs for Money\n  Laundering Detection in Financial Graphs","summary":"  The complexity and interconnectivity of entities involved in money laundering\ndemand investigative reasoning over graph-structured data. This paper explores\nthe use of large language models (LLMs) as reasoning engines over localized\nsubgraphs extracted from a financial knowledge graph. We propose a lightweight\npipeline that retrieves k-hop neighborhoods around entities of interest,\nserializes them into structured text, and prompts an LLM via few-shot\nin-context learning to assess suspiciousness and generate justifications. Using\nsynthetic anti-money laundering (AML) scenarios that reflect common laundering\nbehaviors, we show that LLMs can emulate analyst-style logic, highlight red\nflags, and provide coherent explanations. While this study is exploratory, it\nillustrates the potential of LLM-based graph reasoning in AML and lays\ngroundwork for explainable, language-driven financial crime analytics.\n","authors":["Erfan Pirmorad"],"pdf_url":"https://arxiv.org/pdf/2507.14785v2.pdf","comment":"Accepted at AI4FCF-ICDM 2025"},{"id":"http://arxiv.org/abs/2510.23965v2","updated":"2025-10-29T15:51:35Z","published":"2025-10-28T00:42:38Z","title":"The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity","summary":"  Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.\n","authors":["Ali Aouad","Aymane El Gadarri","Vivek F. Farias"],"pdf_url":"https://arxiv.org/pdf/2510.23965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22439v2","updated":"2025-10-29T15:42:33Z","published":"2025-10-25T21:38:07Z","title":"PromptReverb: Multimodal Room Impulse Response Generation Through Latent\n  Rectified Flow Matching","summary":"  Room impulse response (RIR) generation remains a critical challenge for\ncreating immersive virtual acoustic environments. Current methods suffer from\ntwo fundamental limitations: the scarcity of full-band RIR datasets and the\ninability of existing models to generate acoustically accurate responses from\ndiverse input modalities. We present PromptReverb, a two-stage generative\nframework that addresses these challenges. Our approach combines a variational\nautoencoder that upsamples band-limited RIRs to full-band quality (48 kHz), and\na conditional diffusion transformer model based on rectified flow matching that\ngenerates RIRs from descriptions in natural language. Empirical evaluation\ndemonstrates that PromptReverb produces RIRs with superior perceptual quality\nand acoustic accuracy compared to existing methods, achieving 8.8% mean RT60\nerror compared to -37% for widely used baselines and yielding more realistic\nroom-acoustic parameters. Our method enables practical applications in virtual\nreality, architectural acoustics, and audio production where flexible,\nhigh-quality RIR synthesis is essential.\n","authors":["Ali Vosoughi","Yongyi Zang","Qihui Yang","Nathan Paek","Randal Leistikow","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.22439v2.pdf","comment":"9 pages, 2 figures, 4 tables; v2: corrected spelling of a co-author\n  name; no content changes"},{"id":"http://arxiv.org/abs/2505.00812v4","updated":"2025-10-29T15:40:34Z","published":"2025-05-01T19:12:58Z","title":"Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic\n  Optimization","summary":"  Recent studies indicate that deep neural networks degrade in generalization\nperformance under noisy supervision. Existing methods focus on isolating clean\nsubsets or correcting noisy labels, facing limitations such as high\ncomputational costs, heavy hyperparameter tuning process, and coarse-grained\noptimization. To address these challenges, we propose a novel two-stage noisy\nlearning framework that enables instance-level optimization through a\ndynamically weighted loss function, avoiding hyperparameter tuning. To obtain\nstable and accurate information about noise modeling, we introduce a simple yet\neffective metric, termed wrong event, which dynamically models the cleanliness\nand difficulty of individual samples while maintaining computational costs. Our\nframework first collects wrong event information and builds a strong base\nmodel. Then we perform noise-robust training on the base model, using a\nprobabilistic model to handle the wrong event information of samples.\nExperiments on five synthetic and real-world LNL benchmarks demonstrate our\nmethod surpasses state-of-the-art methods in performance, achieves a nearly 75%\nreduction in computational time and improves model scalability.\n","authors":["Kuan Zhang","Chengliang Chai","Jingzhe Xu","Chi Zhang","Han Han","Ye Yuan","Guoren Wang","Lei Cao"],"pdf_url":"https://arxiv.org/pdf/2505.00812v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25634v1","updated":"2025-10-29T15:39:53Z","published":"2025-10-29T15:39:53Z","title":"Learning to Plan & Schedule with Reinforcement-Learned Bimanual Robot\n  Skills","summary":"  Long-horizon contact-rich bimanual manipulation presents a significant\nchallenge, requiring complex coordination involving a mixture of parallel\nexecution and sequential collaboration between arms. In this paper, we\nintroduce a hierarchical framework that frames this challenge as an integrated\nskill planning & scheduling problem, going beyond purely sequential\ndecision-making to support simultaneous skill invocation. Our approach is built\nupon a library of single-arm and bimanual primitive skills, each trained using\nReinforcement Learning (RL) in GPU-accelerated simulation. We then train a\nTransformer-based planner on a dataset of skill compositions to act as a\nhigh-level scheduler, simultaneously predicting the discrete schedule of skills\nas well as their continuous parameters. We demonstrate that our method achieves\nhigher success rates on complex, contact-rich tasks than end-to-end RL\napproaches and produces more efficient, coordinated behaviors than traditional\nsequential-only planners.\n","authors":["Weikang Wan","Fabio Ramos","Xuning Yang","Caelan Garrett"],"pdf_url":"https://arxiv.org/pdf/2510.25634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13672v8","updated":"2025-10-29T15:35:18Z","published":"2023-09-24T15:40:40Z","title":"RL-I2IT: Image-to-Image Translation with Deep Reinforcement Learning","summary":"  Most existing Image-to-Image Translation (I2IT) methods generate images in a\nsingle run of a deep learning (DL) model. However, designing such a single-step\nmodel is always challenging, requiring a huge number of parameters and easily\nfalling into bad global minimums and overfitting. In this work, we reformulate\nI2IT as a step-wise decision-making problem via deep reinforcement learning\n(DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The\nkey feature in the RL-I2IT framework is to decompose a monolithic learning\nprocess into small steps with a lightweight model to progressively transform a\nsource image successively to a target image. Considering that it is challenging\nto handle high dimensional continuous state and action spaces in the\nconventional RL framework, we introduce meta policy with a new concept Plan to\nthe standard Actor-Critic model, which is of a lower dimension than the\noriginal image and can facilitate the actor to generate a tractable high\ndimensional action. In the RL-I2IT framework, we also employ a task-specific\nauxiliary learning strategy to stabilize the training process and improve the\nperformance of the corresponding task. Experiments on several I2IT tasks\ndemonstrate the effectiveness and robustness of the proposed method when facing\nhigh-dimensional continuous action space problems. Our implementation of the\nRL-I2IT framework is available at\nhttps://github.com/Algolzw/SPAC-Deformable-Registration.\n","authors":["Jing Hu","Chengming Feng","Shu Hu","Ming-Ching Chang","Xin Li","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2309.13672v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23117v2","updated":"2025-10-29T15:34:30Z","published":"2025-05-29T05:37:53Z","title":"Decom-Renorm-Merge: Model Merging on the Right Space Improves\n  Multitasking","summary":"  In the era of large-scale training, model merging has evolved into a tool for\ncreating multitasking models efficiently. It enables the knowledge of models to\nbe fused, without the need for heavy computation as required in traditional\nmultitask learning. Existing merging methods often assume that entries at\nidentical positions in weight matrices serve the same function, enabling\nstraightforward entry-wise comparison and merging. However, this assumption\noverlooks the complexity of finetuned neural networks, where neurons may\ndevelop distinct feature compositions, making direct entry-wise merging\nproblematic. We present Decom-Renorm-Merge (DRM), a simple yet effective\napproach that leverages Singular Value Decomposition to decompose and\ncoordinate weight matrices into an aligned joint space, where entry-wise\nmerging becomes possible. We showcase the effectiveness of DRM across various\nsettings ranging from smaller encoder-based such as ViT and DeBERTa,\nencoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.\nOur experimental results show that DRM outperforms several state-of-the-art\nmerging techniques across full finetuning and low-rank adaptation settings.\nMoreover, our analysis reveals renormalization as the crucial component for\ncreating a robust and even joint space for merging, significantly contributing\nto the method's performance.\n","authors":["Yuatyong Chaichana","Thanapat Trachu","Peerat Limkonchotiwat","Konpat Preechakul","Tirasan Khandhawit","Ekapol Chuangsuwanich"],"pdf_url":"https://arxiv.org/pdf/2505.23117v2.pdf","comment":"Code and models are available at\n  https://github.com/yophis/decom-renorm-merge"},{"id":"http://arxiv.org/abs/2510.01850v3","updated":"2025-10-29T15:33:51Z","published":"2025-10-02T09:47:56Z","title":"NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset\n  for Narrowband Powerline Communications","summary":"  To effectively process impulse noise for narrowband powerline communications\n(NB-PLCs) transceivers, capturing comprehensive statistics of nonperiodic\nasynchronous impulsive noise (APIN) is a critical task. However, existing\nmathematical noise generative models only capture part of the characteristics\nof noise. In this study, we propose a novel generative adversarial network\n(GAN) called noise generation GAN (NGGAN) that learns the complicated\ncharacteristics of practically measured noise samples for data synthesis. To\nclosely match the statistics of complicated noise over the NB-PLC systems, we\nmeasured the NB-PLC noise via the analog coupling and bandpass filtering\ncircuits of a commercial NB-PLC modem to build a realistic dataset. To train\nNGGAN, we adhere to the following principles: 1) we design the length of input\nsignals that the NGGAN model can fit to facilitate cyclostationary noise\ngeneration; 2) the Wasserstein distance is used as a loss function to enhance\nthe similarity between the generated noise and training data; and 3) to measure\nthe similarity performances of GAN-based models based on the mathematical and\npractically measured datasets, we conduct both quantitative and qualitative\nanalyses. The training datasets include: 1) a piecewise spectral\ncyclostationary Gaussian model (PSCGM); 2) a frequency-shift (FRESH) filter;\nand 3) practical measurements from NB-PLC systems. Simulation results\ndemonstrate that the generated noise samples from the proposed NGGAN are highly\nclose to the real noise samples. The principal component analysis (PCA) scatter\nplots and Fr\\'echet inception distance (FID) analysis have shown that NGGAN\noutperforms other GAN-based models by generating noise samples with superior\nfidelity and higher diversity.\n","authors":["Ying-Ren Chien","Po-Heng Chou","You-Jie Peng","Chun-Yuan Huang","Hen-Wai Tsao","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2510.01850v3.pdf","comment":"16 pages, 15 figures, 11 tables, and published in IEEE Transactions\n  on Instrumentation and Measurement, 2025"},{"id":"http://arxiv.org/abs/2510.25626v1","updated":"2025-10-29T15:30:31Z","published":"2025-10-29T15:30:31Z","title":"Are Language Models Efficient Reasoners? A Perspective from Logic\n  Programming","summary":"  Modern language models (LMs) exhibit strong deductive reasoning capabilities,\nyet standard evaluations emphasize correctness while overlooking a key aspect\nof human-like reasoning: efficiency. In real-world reasoning scenarios, much of\nthe available information is irrelevant, and effective deductive inference\nrequires identifying and ignoring such distractions. We propose a framework for\nassessing LM reasoning efficiency through the lens of logic programming,\nintroducing a simple method to align proofs written in natural language -- as\ngenerated by an LM -- with shortest proofs found by executing the logic\nprogram. Efficiency is quantified by measuring how well a model avoids\nunnecessary inference. Empirically, we construct a dataset of math word\nproblems injected with various number of irrelevant axioms that vary in\nsemantic overlap with the goal theorem. We find that current LMs show marked\naccuracy declines under such conditions -- even with minimal, domain-consistent\ndistractions -- and the proofs they generate frequently exhibit detours through\nirrelevant inferences.\n","authors":["Andreas Opedal","Yanick Zengaffinen","Haruki Shirakami","Clemente Pasti","Mrinmaya Sachan","Abulhair Saparov","Ryan Cotterell","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2510.25626v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2509.09810v2","updated":"2025-10-29T15:27:12Z","published":"2025-09-11T19:28:56Z","title":"Towards a Common Framework for Autoformalization","summary":"  Autoformalization has emerged as a term referring to the automation of\nformalization - specifically, the formalization of mathematics using\ninteractive theorem provers (proof assistants). Its rapid development has been\ndriven by progress in deep learning, especially large language models (LLMs).\nMore recently, the term has expanded beyond mathematics to describe the broader\ntask of translating informal input into formal logical representations. At the\nsame time, a growing body of research explores using LLMs to translate informal\nlanguage into formal representations for reasoning, planning, and knowledge\nrepresentation - often without explicitly referring to this process as\nautoformalization. As a result, despite addressing similar tasks, the largely\nindependent development of these research areas has limited opportunities for\nshared methodologies, benchmarks, and theoretical frameworks that could\naccelerate progress. The goal of this paper is to review - explicit or implicit\n- instances of what can be considered autoformalization and to propose a\nunified framework, encouraging cross-pollination between different fields to\nadvance the development of next generation AI systems.\n","authors":["Agnieszka Mensfelt","David Tena Cucala","Santiago Franco","Angeliki Koutsoukou-Argyraki","Vince Trencsenyi","Kostas Stathis"],"pdf_url":"https://arxiv.org/pdf/2509.09810v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25621v1","updated":"2025-10-29T15:25:34Z","published":"2025-10-29T15:25:34Z","title":"FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering","summary":"  The advent of Large Language Models (LLMs) has revolutionized Natural\nLanguage Processing, yet their application in high-stakes, specialized domains\nlike religious question answering is hindered by challenges like hallucination\nand unfaithfulness to authoritative sources. This issue is particularly\ncritical for the Persian-speaking Muslim community, where accuracy and\ntrustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)\nsystems, relying on simplistic single-pass pipelines, fall short on complex,\nmulti-hop queries requiring multi-step reasoning and evidence aggregation. To\naddress this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful\nAdvanced Question Answering in the Persian Islamic domain. FARSIQA is built\nupon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative\nRefinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting\nprocess: it adaptively decomposes complex queries, assesses evidence\nsufficiency, and enters an iterative loop to generate sub-queries,\nprogressively filling information gaps. Operating on a curated knowledge base\nof over one million authoritative Islamic documents, FARSIQA demonstrates\nsuperior performance. Rigorous evaluation on the challenging IslamicPCQA\nbenchmark shows state-of-the-art performance: the system achieves a remarkable\n97.0% in Negative Rejection - a 40-point improvement over baselines - and a\nhigh Answer Correctness score of 74.3%. Our work establishes a new standard for\nPersian Islamic QA and validates that our iterative, adaptive architecture is\ncrucial for building faithful, reliable AI systems in sensitive domains.\n","authors":["Mohammad Aghajani Asl","Behrooz Minaei Bidgoli"],"pdf_url":"https://arxiv.org/pdf/2510.25621v1.pdf","comment":"37 pages, 5 figures, 10 tables. Keywords: Retrieval-Augmented\n  Generation (RAG), Question Answering (QA), Islamic Knowledge Base, Faithful\n  AI, Persian NLP, Multi-hop Reasoning, Large Language Models (LLMs)"},{"id":"http://arxiv.org/abs/2510.25616v1","updated":"2025-10-29T15:20:10Z","published":"2025-10-29T15:20:10Z","title":"Don't Blind Your VLA: Aligning Visual Representations for OOD\n  Generalization","summary":"  The growing success of Vision-Language-Action (VLA) models stems from the\npromise that pretrained Vision-Language Models (VLMs) can endow agents with\ntransferable world knowledge and vision-language (VL) grounding, laying a\nfoundation for action models with broader generalization. Yet when these VLMs\nare adapted to the action modality, it remains unclear to what extent their\noriginal VL representations and knowledge are preserved. In this work, we\nconduct a systematic study of representation retention during VLA fine-tuning,\nshowing that naive action fine-tuning leads to degradation of visual\nrepresentations. To characterize and measure these effects, we probe VLA's\nhidden representations and analyze attention maps, further, we design a set of\ntargeted tasks and methods that contrast VLA models with their counterpart\nVLMs, isolating changes in VL capabilities induced by action fine-tuning. We\nfurther evaluate a range of strategies for aligning visual representations and\nintroduce a simple yet effective method that mitigates degradation and yields\nimproved generalization to out-of-distribution (OOD) scenarios. Taken together,\nour analysis clarifies the trade-off between action fine-tuning and the\ndegradation of VL representations and highlights practical approaches to\nrecover inherited VL capabilities. Code is publicly available:\nhttps://blind-vla-paper.github.io\n","authors":["Nikita Kachaev","Mikhail Kolosov","Daniil Zelezetsky","Alexey K. Kovalev","Aleksandr I. Panov"],"pdf_url":"https://arxiv.org/pdf/2510.25616v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2510.25612v1","updated":"2025-10-29T15:17:31Z","published":"2025-10-29T15:17:31Z","title":"Counterfactual-based Agent Influence Ranker for Agentic AI Workflows","summary":"  An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,\nis an autonomous system that assembles several LLM-based agents to work\ncollaboratively towards a shared goal. The high autonomy, widespread adoption,\nand growing interest in such AAWs highlight the need for a deeper understanding\nof their operations, from both quality and security aspects. To this day, there\nare no existing methods to assess the influence of each agent on the AAW's\nfinal output. Adopting techniques from related fields is not feasible since\nexisting methods perform only static structural analysis, which is unsuitable\nfor inference time execution. We present Counterfactual-based Agent Influence\nRanker (CAIR) - the first method for assessing the influence level of each\nagent on the AAW's output and determining which agents are the most\ninfluential. By performing counterfactual analysis, CAIR provides a\ntask-agnostic analysis that can be used both offline and at inference time. We\nevaluate CAIR using an AAWs dataset of our creation, containing 30 different\nuse cases with 230 different functionalities. Our evaluation showed that CAIR\nproduces consistent rankings, outperforms baseline methods, and can easily\nenhance the effectiveness and relevancy of downstream tasks.\n","authors":["Amit Giloni","Chiara Picardi","Roy Betser","Shamik Bose","Aishvariya Priya Rathina Sabapathy","Roman Vainshtein"],"pdf_url":"https://arxiv.org/pdf/2510.25612v1.pdf","comment":"Accepted to EMNLP 2025, 27 pages, 6 figures"},{"id":"http://arxiv.org/abs/2510.25609v1","updated":"2025-10-29T15:16:50Z","published":"2025-10-29T15:16:50Z","title":"BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training","summary":"  We introduce BOLT-GAN, a simple yet effective modification of the WGAN\nframework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that\nwith a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a\ndifferent metric distance than the Earth Mover (Wasserstein) distance and\nachieves better training stability. Empirical evaluations on four standard\nimage generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN\nChurch-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%\nlower Frechet Inception Distance (FID). Our results suggest that BOLT is a\nbroadly applicable principle for enhancing GAN training.\n","authors":["Mohammadreza Tavasoli Naeini","Ali Bereyhi","Morteza Noshad","Ben Liang","Alfred O. Hero III"],"pdf_url":"https://arxiv.org/pdf/2510.25609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23112v2","updated":"2025-10-29T15:14:17Z","published":"2025-10-27T08:33:18Z","title":"GroupSHAP-Guided Integration of Financial News Keywords and Technical\n  Indicators for Stock Price Prediction","summary":"  Recent advances in finance-specific language models such as FinBERT have\nenabled the quantification of public sentiment into index-based measures, yet\ncompressing diverse linguistic signals into single metrics overlooks contextual\nnuances and limits interpretability. To address this limitation, explainable AI\ntechniques, particularly SHAP (SHapley Additive Explanations), have been\nemployed to identify influential features. However, SHAP's computational cost\ngrows exponentially with input features, making it impractical for large-scale\ntext-based financial data. This study introduces a GRU-based forecasting\nframework enhanced with GroupSHAP, which quantifies contributions of\nsemantically related keyword groups rather than individual tokens,\nsubstantially reducing computational burden while preserving interpretability.\nWe employed FinBERT to embed news articles from 2015 to 2024, clustered them\ninto coherent semantic groups, and applied GroupSHAP to measure each group's\ncontribution to stock price movements. The resulting group-level SHAP variables\nacross multiple topics were used as input features for the prediction model.\nEmpirical results from one-day-ahead forecasting of the S&P 500 index\nthroughout 2024 demonstrate that our approach achieves a 32.2% reduction in MAE\nand a 40.5% reduction in RMSE compared with benchmark models without the\nGroupSHAP mechanism. This research presents the first application of GroupSHAP\nin news-driven financial forecasting, showing that grouped sentiment\nrepresentations simultaneously enhance interpretability and predictive\nperformance.\n","authors":["Minjoo Kim","Jinwoong Kim","Sangjin Park"],"pdf_url":"https://arxiv.org/pdf/2510.23112v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2510.25602v1","updated":"2025-10-29T15:11:53Z","published":"2025-10-29T15:11:53Z","title":"INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats","summary":"  Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly\nembracing low-precision floating-point (FP) formats to handle the pervasive\nactivation outliers in Large Language Models (LLMs). Despite this industry\ntrend, a unified comparison of FP and integer (INT) quantization across varying\ngranularities has been missing, leaving algorithm and hardware co-design\nwithout clear guidance. This paper fills that gap by systematically\ninvestigating the trade-offs between FP and INT formats. We reveal a critical\nperformance crossover: while FP excels in coarse-grained quantization, the\ncomparison at fine-grained (block-wise) levels is more nuanced. Our\ncomprehensive comparison demonstrates that for popular 8-bit fine-grained\nformats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart\nin both algorithmic accuracy and hardware efficiency. However, for 4-bit\nformats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we\nshow that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like\nHadamard rotation are applied. We also introduce a symmetric clipping method\nthat resolves gradient bias in fine-grained low-bit INT training, enabling\nnearly lossless performance for MXINT8 training. These findings challenge the\ncurrent hardware trajectory, demonstrating that a one-size-fits-all FP approach\nis suboptimal and advocating that fine-grained INT formats, particularly\nMXINT8, offer a better balance of accuracy, power, and efficiency for future AI\naccelerators.\n","authors":["Mengzhao Chen","Meng Wu","Hui Jin","Zhihang Yuan","Jing Liu","Chaoyi Zhang","Yunshui Li","Jie Huang","Jin Ma","Zeyue Xue","Zhiheng Liu","Xingyan Bin","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2510.25602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12437v2","updated":"2025-10-29T15:09:38Z","published":"2025-05-18T14:19:52Z","title":"A method for the systematic generation of graph XAI benchmarks via\n  Weisfeiler-Leman coloring","summary":"  Graph neural networks have become the de facto model for learning from\nstructured data. However, the decision-making process of GNNs remains opaque to\nthe end user, which undermines their use in safety-critical applications.\nSeveral explainable AI techniques for graphs have been developed to address\nthis major issue. Focusing on graph classification, these explainers identify\nsubgraph motifs that explain predictions. Therefore, a robust benchmarking of\ngraph explainers is required to ensure that the produced explanations are of\nhigh quality, i.e., aligned with the GNN's decision process. However, current\ngraph-XAI benchmarks are limited to simplistic synthetic datasets or a few\nreal-world tasks curated by domain experts, hindering rigorous and reproducible\nevaluation, and consequently stalling progress in the field. To overcome these\nlimitations, we propose a method to automate the construction of graph XAI\nbenchmarks from generic graph classification datasets. Our approach leverages\nthe Weisfeiler-Leman color refinement algorithm to efficiently perform\napproximate subgraph matching and mine class-discriminating motifs, which serve\nas proxy ground-truth class explanations. At the same time, we ensure that\nthese motifs can be learned by GNNs because their discriminating power aligns\nwith WL expressiveness. This work also introduces the OpenGraphXAI benchmark\nsuite, which consists of 15 ready-made graph-XAI datasets derived by applying\nour method to real-world molecular classification datasets. The suite is\navailable to the public along with a codebase to generate over 2,000 additional\ngraph-XAI benchmarks. Finally, we present a use case that illustrates how the\nsuite can be used to assess the effectiveness of a selection of popular graph\nexplainers, demonstrating the critical role of a sufficiently large benchmark\ncollection for improving the significance of experimental results.\n","authors":["Michele Fontanesi","Alessio Micheli","Marco Podda","Domenico Tortorella"],"pdf_url":"https://arxiv.org/pdf/2505.12437v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25595v1","updated":"2025-10-29T15:03:53Z","published":"2025-10-29T15:03:53Z","title":"Communication and Verification in LLM Agents towards Collaboration under\n  Information Asymmetry","summary":"  While Large Language Model (LLM) agents are often approached from the angle\nof action planning/generation to accomplish a goal (e.g., given by language\ndescriptions), their abilities to collaborate with each other to achieve a\njoint goal are not well explored. To address this limitation, this paper\nstudies LLM agents in task collaboration, particularly under the condition of\ninformation asymmetry, where agents have disparities in their knowledge and\nskills and need to work together to complete a shared task. We extend Einstein\nPuzzles, a classical symbolic puzzle, to a table-top game. In this game, two\nLLM agents must reason, communicate, and act to satisfy spatial and relational\nconstraints required to solve the puzzle. We apply a fine-tuning-plus-verifier\nframework in which LLM agents are equipped with various communication\nstrategies and verification signals from the environment. Empirical results\nhighlight the critical importance of aligned communication, especially when\nagents possess both information-seeking and -providing capabilities.\nInterestingly, agents without communication can still achieve high task\nperformance; however, further analysis reveals a lack of true rule\nunderstanding and lower trust from human evaluators. Instead, by integrating an\nenvironment-based verifier, we enhance agents' ability to comprehend task rules\nand complete tasks, promoting both safer and more interpretable collaboration\nin AI systems. https://github.com/Roihn/EinsteinPuzzles\n","authors":["Run Peng","Ziqiao Ma","Amy Pang","Sikai Li","Zhang Xi-Jia","Yingzhuo Yu","Cristian-Paul Bara","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2510.25595v1.pdf","comment":"Workshop on Multi-Agent System @ ICML 2025"},{"id":"http://arxiv.org/abs/2510.25590v1","updated":"2025-10-29T14:58:37Z","published":"2025-10-29T14:58:37Z","title":"RegionE: Adaptive Region-Aware Generation for Efficient Image Editing","summary":"  Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.\n","authors":["Pengtao Chen","Xianfang Zeng","Maosen Zhao","Mingzhu Shen","Peng Ye","Bangyin Xiang","Zhibo Wang","Wei Cheng","Gang Yu","Tao Chen"],"pdf_url":"https://arxiv.org/pdf/2510.25590v1.pdf","comment":"26 pages, 10 figures, 18 tables"},{"id":"http://arxiv.org/abs/2510.25588v1","updated":"2025-10-29T14:54:22Z","published":"2025-10-29T14:54:22Z","title":"Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM\n  Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System","summary":"  The diagnosis of most mental disorders, including psychiatric evaluations,\nprimarily depends on dialogues between psychiatrists and patients. This\nsubjective process can lead to variability in diagnoses across clinicians and\npatients, resulting in inconsistencies and challenges in achieving reliable\noutcomes. To address these issues and standardize psychiatric diagnoses, we\npropose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss\nReasoning LLM-enabled Decision Support System for the clinical diagnosis of\nmental disorders. Our approach leverages fine-tuned LLMs trained on\nconversational datasets involving psychiatrist-patient interactions focused on\nmental health conditions (e.g., depression). The diagnostic predictions from\nindividual models are aggregated through a consensus-based decision-making\nprocess, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method\nfor deploying LLM agents that orchestrate communication between the LLM\nconsortium and the reasoning LLM, ensuring transparency, reliability, and\nresponsible AI across the entire diagnostic workflow. Experimental results\ndemonstrate the transformative potential of combining fine-tuned LLMs with a\nreasoning model to create a robust and highly accurate diagnostic system for\nmental health assessment. A prototype of the proposed platform, integrating\nthree fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in\ncollaboration with the U.S. Army Medical Research Team in Norfolk, Virginia,\nUSA. To the best of our knowledge, this work represents the first application\nof a fine-tuned LLM consortium integrated with a reasoning LLM for clinical\nmental health diagnosis paving the way for next-generation AI-powered eHealth\nsystems aimed at standardizing psychiatric diagnoses.\n","authors":["Eranga Bandara","Ross Gore","Atmaram Yarlagadda","Anita H. Clayton","Preston Samuel","Christopher K. Rhea","Sachin Shetty"],"pdf_url":"https://arxiv.org/pdf/2510.25588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12484v4","updated":"2025-10-29T14:52:49Z","published":"2025-06-14T12:49:51Z","title":"Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption\n  Masking And Normalization","summary":"  Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new\nstate-of-the-art for robust unlearning.\n","authors":["Filip Sondej","Yushi Yang","Mikołaj Kniejski","Marcel Windys"],"pdf_url":"https://arxiv.org/pdf/2506.12484v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16714v3","updated":"2025-10-29T14:48:21Z","published":"2024-02-26T16:31:28Z","title":"Quantum Transformer: Accelerating model inference via quantum linear\n  algebra","summary":"  Powerful generative artificial intelligence from large language models (LLMs)\nharnesses extensive computational resources for inference. In this work, we\ninvestigate the transformer architecture, a key component of these models,\nunder the lens of fault-tolerant quantum computing. We develop quantum\nsubroutines to construct the building blocks in the transformer, including the\nself-attention, residual connection with layer normalization, and feed-forward\nnetwork. As an important subroutine, we show how to efficiently implement the\nHadamard product and element-wise functions of matrices on quantum computers.\nOur algorithm prepares an amplitude encoding of the transformer output, which\ncan be measured for prediction or use in the next layer. We find that the\nmatrix norm of the input sequence plays a dominant role in the quantum\ncomplexity. With numerical experiments on open-source LLMs, including for\nbio-informatics applications, we demonstrate the potential of a quantum speedup\nfor transformer inference in practical regimes.\n","authors":["Naixu Guo","Zhan Yu","Matthew Choi","Yizhan Han","Aman Agrawal","Kouhei Nakaji","Alán Aspuru-Guzik","Patrick Rebentrost"],"pdf_url":"https://arxiv.org/pdf/2402.16714v3.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2510.25577v1","updated":"2025-10-29T14:44:44Z","published":"2025-10-29T14:44:44Z","title":"Lost in Phonation: Voice Quality Variation as an Evaluation Dimension\n  for Speech Foundation Models","summary":"  Recent advances in speech foundation models (SFMs) have enabled the direct\nprocessing of spoken language from raw audio, bypassing intermediate textual\nrepresentations. This capability allows SFMs to be exposed to, and potentially\nrespond to, rich paralinguistic variations embedded in the input speech signal.\nOne under-explored dimension of paralinguistic variation is voice quality,\nencompassing phonation types such as creaky and breathy voice. These phonation\ntypes are known to influence how listeners infer affective state, stance and\nsocial meaning in speech. Existing benchmarks for speech understanding largely\nrely on multiple-choice question answering (MCQA) formats, which are prone to\nfailure and therefore unreliable in capturing the nuanced ways paralinguistic\nfeatures influence model behaviour. In this paper, we probe SFMs through\nopen-ended generation tasks and speech emotion recognition, evaluating whether\nmodel behaviours are consistent across different phonation inputs. We introduce\na new parallel dataset featuring synthesized modifications to voice quality,\ndesigned to evaluate SFM responses to creaky and breathy voice. Our work\nprovides the first examination of SFM sensitivity to these particular\nnon-lexical aspects of speech perception.\n","authors":["Harm Lameris","Shree Harsha Bokkahalli Satish","Joakim Gustafson","Éva Székely"],"pdf_url":"https://arxiv.org/pdf/2510.25577v1.pdf","comment":"8 pages, 3 figures, 4 tables, submitted to LREC 2026"},{"id":"http://arxiv.org/abs/2505.10940v3","updated":"2025-10-29T14:42:46Z","published":"2025-05-16T07:26:41Z","title":"Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced\n  Logical Recommendation","summary":"  Recommender systems filter contents/items valuable to users by inferring\npreferences from user features and historical behaviors. Mainstream approaches\nfollow the learning-to-rank paradigm, which focus on discovering and modeling\nitem topics (e.g., categories), and capturing user preferences on these topics\nbased on historical interactions. However, this paradigm often neglects the\nmodeling of user characteristics and their social roles, which are logical\nconfounders influencing the correlated interest and user preference transition.\nTo bridge this gap, we introduce the user role identification task and the\nbehavioral logic modeling task that aim to explicitly model user roles and\nlearn the logical relations between item topics and user social roles. We show\nthat it is possible to explicitly solve these tasks through an efficient\nintegration framework of Large Language Model (LLM) and recommendation systems,\nfor which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal)\nLLM's world knowledge and logic inference ability to extract realistic\ntag-based virtual logic graphs that reveal dynamic and expressive knowledge of\nusers, refining our understanding of user behaviors. On the other hand, TagCF\npresents empirically effective integration modules that take advantage of the\nextracted tag-logic information, augmenting the recommendation performance. We\nconduct both online experiments and offline experiments with industrial and\npublic datasets as verification of TagCF's effectiveness, and we empirically\nshow that the user role modeling strategy is potentially a better choice than\nthe modeling of item topics. Additionally, we provide evidence that the\nextracted logic graphs are empirically a general and transferable knowledge\nthat can benefit a wide range of recommendation tasks. Our code is available in\nhttps://github.com/Code2Q/TagCF.\n","authors":["Qing Yu","Xiaobei Wang","Shuchang Liu","Yandong Bai","Xiaoyu Yang","Xueliang Wang","Chang Meng","Shanshan Wu","Hailan Yang","Huihui Xiao","Xiang Li","Fan Yang","Xiaoqiang Feng","Lantao Hu","Han Li","Kun Gai","Lixin Zou"],"pdf_url":"https://arxiv.org/pdf/2505.10940v3.pdf","comment":"to be published in NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25813v1","updated":"2025-10-29T14:35:02Z","published":"2025-10-29T14:35:02Z","title":"An Agentic Framework for Rapid Deployment of Edge AI Solutions in\n  Industry 5.0","summary":"  We present a novel framework for Industry 5.0 that simplifies the deployment\nof AI models on edge devices in various industrial settings. The design reduces\nlatency and avoids external data transfer by enabling local inference and\nreal-time processing. Our implementation is agent-based, which means that\nindividual agents, whether human, algorithmic, or collaborative, are\nresponsible for well-defined tasks, enabling flexibility and simplifying\nintegration. Moreover, our framework supports modular integration and maintains\nlow resource requirements. Preliminary evaluations concerning the food industry\nin real scenarios indicate improved deployment time and system adaptability\nperformance. The source code is publicly available at\nhttps://github.com/AI-REDGIO-5-0/ci-component.\n","authors":["Jorge Martinez-Gil","Mario Pichler","Nefeli Bountouni","Sotiris Koussouris","Marielena Márquez Barreiro","Sergio Gusmeroli"],"pdf_url":"https://arxiv.org/pdf/2510.25813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.22117v2","updated":"2025-10-29T14:33:07Z","published":"2025-09-26T09:40:51Z","title":"The AI_INFN Platform: Artificial Intelligence Development in the Cloud","summary":"  Machine Learning (ML) is profoundly reshaping the way researchers create,\nimplement, and operate data-intensive software. Its adoption, however,\nintroduces notable challenges for computing infrastructures, particularly when\nit comes to coordinating access to hardware accelerators across development,\ntesting, and production environments. The INFN initiative AI_INFN (Artificial\nIntelligence at INFN) seeks to promote the use of ML methods across various\nINFN research scenarios by offering comprehensive technical support, including\naccess to AI-focused computational resources. Leveraging the INFN Cloud\necosystem and cloud-native technologies, the project emphasizes efficient\nsharing of accelerator hardware while maintaining the breadth of the\nInstitute's research activities. This contribution describes the deployment and\ncommissioning of a Kubernetes-based platform designed to simplify GPU-powered\ndata analysis workflows and enable their scalable execution on heterogeneous\ndistributed resources. By integrating offloading mechanisms through Virtual\nKubelet and the InterLink API, the platform allows workflows to span multiple\nresource providers, from Worldwide LHC Computing Grid sites to high-performance\ncomputing centers like CINECA Leonardo. We will present preliminary benchmarks,\nfunctional tests, and case studies, demonstrating both performance and\nintegration outcomes.\n","authors":["Lucio Anderlini","Giulio Bianchini","Diego Ciangottini","Stefano Dal Pra","Diego Michelotto","Rosa Petrini","Daniele Spiga"],"pdf_url":"https://arxiv.org/pdf/2509.22117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13519v2","updated":"2025-10-29T14:31:32Z","published":"2025-05-17T12:39:45Z","title":"Continuous Domain Generalization","summary":"  Real-world data distributions often shift continuously across multiple latent\nfactors such as time, geography, and socioeconomic contexts. However, existing\ndomain generalization approaches typically treat domains as discrete or as\nevolving along a single axis (e.g., time). This oversimplification fails to\ncapture the complex, multidimensional nature of real-world variation. This\npaper introduces the task of Continuous Domain Generalization (CDG), which aims\nto generalize predictive models to unseen domains defined by arbitrary\ncombinations of continuous variations. We present a principled framework\ngrounded in geometric and algebraic theories, showing that optimal model\nparameters across domains lie on a low-dimensional manifold. To model this\nstructure, we propose a Neural Lie Transport Operator (NeuralLio), which\nenables structure-preserving parameter transitions by enforcing geometric\ncontinuity and algebraic consistency. To handle noisy or incomplete domain\nvariation descriptors, we introduce a gating mechanism to suppress irrelevant\ndimensions and a local chart-based strategy for robust generalization.\nExtensive experiments on synthetic and real-world datasets, including remote\nsensing, scientific documents, and traffic forecasting, demonstrate that our\nmethod significantly outperforms existing baselines in both generalization\naccuracy and robustness.\n","authors":["Zekun Cai","Yiheng Yao","Guangji Bai","Renhe Jiang","Xuan Song","Ryosuke Shibasaki","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.13519v2.pdf","comment":"23 pages, 9 figures. Accepted by NeurIPS25"},{"id":"http://arxiv.org/abs/2510.25563v1","updated":"2025-10-29T14:30:12Z","published":"2025-10-29T14:30:12Z","title":"Leveraging an Atmospheric Foundational Model for Subregional Sea Surface\n  Temperature Forecasting","summary":"  The accurate prediction of oceanographic variables is crucial for\nunderstanding climate change, managing marine resources, and optimizing\nmaritime activities. Traditional ocean forecasting relies on numerical models;\nhowever, these approaches face limitations in terms of computational cost and\nscalability. In this study, we adapt Aurora, a foundational deep learning model\noriginally designed for atmospheric forecasting, to predict sea surface\ntemperature (SST) in the Canary Upwelling System. By fine-tuning this model\nwith high-resolution oceanographic reanalysis data, we demonstrate its ability\nto capture complex spatiotemporal patterns while reducing computational\ndemands. Our methodology involves a staged fine-tuning process, incorporating\nlatitude-weighted error metrics and optimizing hyperparameters for efficient\nlearning. The experimental results show that the model achieves a low RMSE of\n0.119K, maintaining high anomaly correlation coefficients (ACC $\\approx\n0.997$). The model successfully reproduces large-scale SST structures but faces\nchallenges in capturing finer details in coastal regions. This work contributes\nto the field of data-driven ocean forecasting by demonstrating the feasibility\nof using deep learning models pre-trained in different domains for oceanic\napplications. Future improvements include integrating additional oceanographic\nvariables, increasing spatial resolution, and exploring physics-informed neural\nnetworks to enhance interpretability and understanding. These advancements can\nimprove climate modeling and ocean prediction accuracy, supporting\ndecision-making in environmental and economic sectors.\n","authors":["Víctor Medina","Giovanny A. Cuervo-Londoño","Javier Sánchez"],"pdf_url":"https://arxiv.org/pdf/2510.25563v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.25557v1","updated":"2025-10-29T14:21:49Z","published":"2025-10-29T14:21:49Z","title":"Hybrid Quantum-Classical Recurrent Neural Networks","summary":"  We present a hybrid quantum-classical recurrent neural network (QRNN)\narchitecture in which the entire recurrent core is realized as a parametrized\nquantum circuit (PQC) controlled by a classical feedforward network. The hidden\nstate is the quantum state of an $n$-qubit PQC, residing in an exponentially\nlarge Hilbert space $\\mathbb{C}^{2^n}$. The PQC is unitary by construction,\nmaking the hidden-state evolution norm-preserving without external constraints.\nAt each timestep, mid-circuit readouts are combined with the input embedding\nand processed by the feedforward network, which provides explicit classical\nnonlinearity. The outputs parametrize the PQC, which updates the hidden state\nvia unitary dynamics. The QRNN is compact and physically consistent, and it\nunifies (i) unitary recurrence as a high-capacity memory, (ii) partial\nobservation via mid-circuit measurements, and (iii) nonlinear classical control\nfor input-conditioned parametrization. We evaluate the model in simulation with\nup to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,\nand language modeling, adopting projective measurements as a limiting case to\nobtain mid-circuit readouts while maintaining a coherent recurrent quantum\nmemory. We further devise a soft attention mechanism over the mid-circuit\nreadouts in a sequence-to-sequence model and show its effectiveness for machine\ntranslation. To our knowledge, this is the first model (RNN or otherwise)\ngrounded in quantum operations to achieve competitive performance against\nstrong classical baselines across a broad class of sequence-learning tasks.\n","authors":["Wenduan Xu"],"pdf_url":"https://arxiv.org/pdf/2510.25557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01308v2","updated":"2025-10-29T14:09:33Z","published":"2025-09-01T09:47:35Z","title":"GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL\n  Generation from Large Language Models","summary":"  Text-to-SQL, the task of translating natural language questions into SQL\nqueries, has significantly advanced with the introduction of Large Language\nModels (LLMs), broadening database accessibility for a wide range of users.\nDespite substantial progress in generating valid SQL, current LLMs still\nstruggle with complex queries. To address this limitation, test-time strategies\nsuch as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on\nthe assumption that LLMs can produce correct answers after multiple attempts.\nHowever, these methods rely on surface-level heuristics, selecting the\nsyntactically correct query through execution-based BoN (ex-BoN) or the most\nfrequently generated one through Majority Voting. Recently, Outcome Reward\nModels (ORMs), which assign utility scores to generated outputs based on\nsemantic correctness, have emerged as a promising reinforcement learning\napproach for improving model alignment. We argue that ORMs could serve as an\neffective new test-time heuristic, although their application in this context\nremains largely underexplored.\n  In this work, we propose a unified framework for training ORMs tailored to\nthe Text-to-SQL task and assess their effectiveness as a test-time heuristic\nwithin the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the\nBIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2,\nGranite3, and Llama3 families. Results show that ORMs outperform ex-BoN and\nMaj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider)\nover ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further\ndemonstrate that finetuning models already aligned with SQL generation, such as\nOmniSQL, yields superior ORM performance. Additionally, we observe that ORMs\nachieve competitive results on simple queries and benefit more from an\nincreased number of candidates compared to ex-BoN and Maj.\n","authors":["Mattia Tritto","Giuseppe Farano","Dario Di Palma","Gaetano Rossiello","Fedelucio Narducci","Dharmashankar Subramanian","Tommaso Di Noia"],"pdf_url":"https://arxiv.org/pdf/2509.01308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08388v3","updated":"2025-10-29T14:02:55Z","published":"2025-06-10T02:53:24Z","title":"Reinforcement Learning Teachers of Test Time Scaling","summary":"  Training reasoning language models (LMs) with reinforcement learning (RL) for\none-hot correctness inherently relies on the LM being able to explore and solve\nits task with some chance at initialization. Furthermore, a key use case of\nreasoning LMs is to act as teachers for distilling new students and\ncold-starting future RL iterations rather than being deployed themselves. From\nthese considerations, we introduce a new framework that avoids RL's exploration\nchallenge by training a new class of Reinforcement-Learned Teachers (RLTs)\nfocused on yielding the most effective downstream distillation. RLTs are\nprompted with both the question and solution to each problem, and tasked to\nsimply \"connect-the-dots\" with detailed explanations tailored for their\nstudents. We train RLTs with dense rewards obtained by feeding each explanation\nto the student and testing its understanding of the problem's solution. In\npractice, the raw outputs of a 7B RLT provide higher final performance on\ncompetition and graduate-level tasks than existing distillation and\ncold-starting pipelines that collect and postprocess the reasoning traces of\norders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness\nwhen training larger students and when applied zero-shot to out-of-distribution\ntasks, unlocking new levels of efficiency and re-usability for the RL reasoning\nframework. Code available at: https://github.com/SakanaAI/RLT\n","authors":["Edoardo Cetin","Tianyu Zhao","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2506.08388v3.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.21614v3","updated":"2025-10-29T13:57:25Z","published":"2025-10-24T16:19:41Z","title":"Huxley-Gödel Machine: Human-Level Coding Agent Development by an\n  Approximation of the Optimal Self-Improving Machine","summary":"  Recent studies operationalize self-improvement through coding agents that\nedit their own codebases. They grow a tree of self-modifications through\nexpansion strategies that favor higher software engineering benchmark\nperformance, assuming that this implies more promising subsequent\nself-modifications. However, we identify a mismatch between the agent's\nself-improvement potential (metaproductivity) and its coding benchmark\nperformance, namely the Metaproductivity-Performance Mismatch. Inspired by\nHuxley's concept of clade, we propose a metric ($\\mathrm{CMP}$) that aggregates\nthe benchmark performances of the descendants of an agent as an indicator of\nits potential for self-improvement. We show that, in our self-improving coding\nagent development setting, access to the true $\\mathrm{CMP}$ is sufficient to\nsimulate how the G\\\"odel Machine would behave under certain assumptions. We\nintroduce the Huxley-G\\\"odel Machine (HGM), which, by estimating $\\mathrm{CMP}$\nand using it as guidance, searches the tree of self-modifications. On SWE-bench\nVerified and Polyglot, HGM outperforms prior self-improving coding agent\ndevelopment methods while using fewer allocated CPU hours. Last but not least,\nHGM demonstrates strong transfer to other coding datasets and large language\nmodels. The agent optimized by HGM on SWE-bench Verified with GPT-5-mini and\nevaluated on SWE-bench Lite with GPT-5 achieves human-level performance,\nmatching the best officially checked results of human-engineered coding agents.\nOur code is publicly available at https://github.com/metauto-ai/HGM.\n","authors":["Wenyi Wang","Piotr Piękos","Li Nanbo","Firas Laakom","Yimeng Chen","Mateusz Ostaszewski","Mingchen Zhuge","Jürgen Schmidhuber"],"pdf_url":"https://arxiv.org/pdf/2510.21614v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25531v1","updated":"2025-10-29T13:56:44Z","published":"2025-10-29T13:56:44Z","title":"Using latent representations to link disjoint longitudinal data for\n  mixed-effects regression","summary":"  Many rare diseases offer limited established treatment options, leading\npatients to switch therapies when new medications emerge. To analyze the impact\nof such treatment switches within the low sample size limitations of rare\ndisease trials, it is important to use all available data sources. This,\nhowever, is complicated when usage of measurement instruments change during the\nobservation period, for example when instruments are adapted to specific age\nranges. The resulting disjoint longitudinal data trajectories, complicate the\napplication of traditional modeling approaches like mixed-effects regression.\nWe tackle this by mapping observations of each instrument to a aligned\nlow-dimensional temporal trajectory, enabling longitudinal modeling across\ninstruments. Specifically, we employ a set of variational autoencoder\narchitectures to embed item values into a shared latent space for each time\npoint. Temporal disease dynamics and treatment switch effects are then captured\nthrough a mixed-effects regression model applied to latent representations. To\nenable statistical inference, we present a novel statistical testing approach\nthat accounts for the joint parameter estimation of mixed-effects regression\nand variational autoencoders. The methodology is applied to quantify the impact\nof treatment switches for patients with spinal muscular atrophy. Here, our\napproach aligns motor performance items from different measurement instruments\nfor mixed-effects regression and maps estimated effects back to the observed\nitem level to quantify the treatment switch effect. Our approach allows for\nmodel selection as well as for assessing effects of treatment switching. The\nresults highlight the potential of modeling in joint latent representations for\naddressing small data challenges.\n","authors":["Clemens Schächter","Maren Hackenberg","Michelle Pfaffenlehner","Félix B. Tambe-Ndonfack","Thorsten Schmidt","Astrid Pechmann","Janbernd Kirschner","Jan Hasenauser","Harald Binder"],"pdf_url":"https://arxiv.org/pdf/2510.25531v1.pdf","comment":"31 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.25529v1","updated":"2025-10-29T13:53:52Z","published":"2025-10-29T13:53:52Z","title":"Off-policy Reinforcement Learning with Model-based Exploration\n  Augmentation","summary":"  Exploration is fundamental to reinforcement learning (RL), as it determines\nhow effectively an agent discovers and exploits the underlying structure of its\nenvironment to achieve optimal performance. Existing exploration methods\ngenerally fall into two categories: active exploration and passive exploration.\nThe former introduces stochasticity into the policy but struggles in\nhigh-dimensional environments, while the latter adaptively prioritizes\ntransitions in the replay buffer to enhance exploration, yet remains\nconstrained by limited sample diversity. To address the limitation in passive\nexploration, we propose Modelic Generative Exploration (MoGE), which augments\nexploration through the generation of under-explored critical states and\nsynthesis of dynamics-consistent experiences through transition models. MoGE is\ncomposed of two components: (1) a diffusion-based generator that synthesizes\ncritical states under the guidance of a utility function evaluating each\nstate's potential influence on policy exploration, and (2) a one-step\nimagination world model for constructing critical transitions based on the\ncritical states for agent learning. Our method adopts a modular formulation\nthat aligns with the principles of off-policy learning, allowing seamless\nintegration with existing algorithms to improve exploration without altering\ntheir core structures. Empirical results on OpenAI Gym and DeepMind Control\nSuite reveal that MoGE effectively bridges exploration and policy learning,\nleading to remarkable gains in both sample efficiency and performance across\ncomplex control tasks.\n","authors":["Likun Wang","Xiangteng Zhang","Yinuo Wang","Guojian Zhan","Wenxuan Wang","Haoyu Gao","Jingliang Duan","Shengbo Eben Li"],"pdf_url":"https://arxiv.org/pdf/2510.25529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25528v1","updated":"2025-10-29T13:52:44Z","published":"2025-10-29T13:52:44Z","title":"Zero Reinforcement Learning Towards General Domains","summary":"  Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach\nfor enhancing the reasoning capabilities of large language models (LLMs) by\ndirectly applying reinforcement learning with verifiable rewards on pretrained\nmodels, without the need for a supervised fine-tuning phase. However, current\nresearch on zero-RL primarily focuses on domains with easily verifiable reward\nsignals, such as mathematics, programming, and other reasoning tasks. The\nchallenge of eliciting reasoning abilities in more diverse scenarios, where\nverification is not straightforward, remains underexplored. To address this\ngap, we propose a novel zero-RL paradigm designed to improve a model's\nreasoning ability across both verifiable and non-verifiable domains. By\ncombining verifiable rewards with a generative reward model, we conduct\nmulti-task zero-RL training across both domains, facilitating the transfer of\nreasoning capabilities between them. Furthermore, to mitigate reward hacking in\nthe generative reward model, we design a smooth length penalty that encourages\nthe generation of more comprehensive thinking tokens in general domains.\nExperimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our\napproach achieves superior reasoning performance, not only on tasks requiring\nextensive reasoning but also on more general tasks.\n","authors":["Yuyuan Zeng","Yufei Huang","Can Xu","Qingfeng Sun","Jianfeng Yan","Guanghui Xu","Tao Yang","Fengzong Lian"],"pdf_url":"https://arxiv.org/pdf/2510.25528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25522v1","updated":"2025-10-29T13:46:19Z","published":"2025-10-29T13:46:19Z","title":"Comparative Study of UNet-based Architectures for Liver Tumor\n  Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography","summary":"  Segmentation of liver structures in multi-phase contrast-enhanced computed\ntomography (CECT) plays a crucial role in computer-aided diagnosis and\ntreatment planning for liver diseases, including tumor detection. In this\nstudy, we investigate the performance of UNet-based architectures for liver\ntumor segmentation, starting from the original UNet and extending to UNet3+\nwith various backbone networks. We evaluate ResNet, Transformer-based, and\nState-space (Mamba) backbones, all initialized with pretrained weights.\nSurprisingly, despite the advances in modern architecture, ResNet-based models\nconsistently outperform Transformer- and Mamba-based alternatives across\nmultiple evaluation metrics. To further improve segmentation quality, we\nintroduce attention mechanisms into the backbone and observe that incorporating\nthe Convolutional Block Attention Module (CBAM) yields the best performance.\nResNetUNet3+ with CBAM module not only produced the best overlap metrics with a\nDice score of 0.755 and IoU of 0.662, but also achieved the most precise\nboundary delineation, evidenced by the lowest HD95 distance of 77.911. The\nmodel's superiority was further cemented by its leading overall accuracy of\n0.925 and specificity of 0.926, showcasing its robust capability in accurately\nidentifying both lesion and healthy tissue. To further enhance\ninterpretability, Grad-CAM visualizations were employed to highlight the\nregion's most influential predictions, providing insights into its\ndecision-making process. These findings demonstrate that classical ResNet\narchitecture, when combined with modern attention modules, remain highly\ncompetitive for medical image segmentation tasks, offering a promising\ndirection for liver tumor detection in clinical practice.\n","authors":["Doan-Van-Anh Ly","Thi-Thu-Hien Pham","Thanh-Hai Le"],"pdf_url":"https://arxiv.org/pdf/2510.25522v1.pdf","comment":"27 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.23906v2","updated":"2025-10-29T13:42:56Z","published":"2025-10-27T22:26:20Z","title":"Group Interventions on Deep Networks for Causal Discovery in Subsystems","summary":"  Causal discovery uncovers complex relationships between variables, enhancing\npredictions, decision-making, and insights into real-world systems, especially\nin nonlinear multivariate time series. However, most existing methods primarily\nfocus on pairwise cause-effect relationships, overlooking interactions among\ngroups of variables, i.e., subsystems and their collective causal influence. In\nthis study, we introduce gCDMI, a novel multi-group causal discovery method\nthat leverages group-level interventions on trained deep neural networks and\nemploys model invariance testing to infer causal relationships. Our approach\ninvolves three key steps. First, we use deep learning to jointly model the\nstructural relationships among groups of all time series. Second, we apply\ngroup-wise interventions to the trained model. Finally, we conduct model\ninvariance testing to determine the presence of causal links among variable\ngroups. We evaluate our method on simulated datasets, demonstrating its\nsuperior performance in identifying group-level causal relationships compared\nto existing methods. Additionally, we validate our approach on real-world\ndatasets, including brain networks and climate ecosystems. Our results\nhighlight that applying group-level interventions to deep learning models,\ncombined with invariance testing, can effectively reveal complex causal\nstructures, offering valuable insights for domains such as neuroscience and\nclimate science.\n","authors":["Wasim Ahmad","Joachim Denzler","Maha Shadaydeh"],"pdf_url":"https://arxiv.org/pdf/2510.23906v2.pdf","comment":"Submitted to IEEE Access. We are working on the revised version"},{"id":"http://arxiv.org/abs/2510.25518v1","updated":"2025-10-29T13:41:36Z","published":"2025-10-29T13:41:36Z","title":"Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and\n  Evaluation","summary":"  Retrieval-Augmented Generation (RAG) systems often face limitations in\nspecialized domains such as fintech, where domain-specific ontologies, dense\nterminology, and acronyms complicate effective retrieval and synthesis. This\npaper introduces an agentic RAG architecture designed to address these\nchallenges through a modular pipeline of specialized agents. The proposed\nsystem supports intelligent query reformulation, iterative sub-query\ndecomposition guided by keyphrase extraction, contextual acronym resolution,\nand cross-encoder-based context re-ranking. We evaluate our approach against a\nstandard RAG baseline using a curated dataset of 85 question--answer--reference\ntriples derived from an enterprise fintech knowledge base. Experimental results\ndemonstrate that the agentic RAG system outperforms the baseline in retrieval\nprecision and relevance, albeit with increased latency. These findings suggest\nthat structured, multi-agent methodologies offer a promising direction for\nenhancing retrieval robustness in complex, domain-specific settings.\n","authors":["Thomas Cook","Richard Osuagwu","Liman Tsatiashvili","Vrynsia Vrynsia","Koustav Ghosal","Maraim Masoud","Riccardo Mattivi"],"pdf_url":"https://arxiv.org/pdf/2510.25518v1.pdf","comment":"Keywords: RAG Agentic AI Fintech NLP KB Domain-Specific Ontology\n  Query Understanding"},{"id":"http://arxiv.org/abs/2510.25517v1","updated":"2025-10-29T13:39:41Z","published":"2025-10-29T13:39:41Z","title":"Predicate Renaming via Large Language Models","summary":"  In this paper, we address the problem of giving names to predicates in logic\nrules using Large Language Models (LLMs). In the context of Inductive Logic\nProgramming, various rule generation methods produce rules containing unnamed\npredicates, with Predicate Invention being a key example. This hinders the\nreadability, interpretability, and reusability of the logic theory. Leveraging\nrecent advancements in LLMs development, we explore their ability to process\nnatural language and code to provide semantically meaningful suggestions for\ngiving a name to unnamed predicates. The evaluation of our approach on some\nhand-crafted logic rules indicates that LLMs hold potential for this task.\n","authors":["Elisabetta Gentili","Tony Ribeiro","Fabrizio Riguzzi","Katsumi Inoue"],"pdf_url":"https://arxiv.org/pdf/2510.25517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.00814v2","updated":"2025-10-29T13:37:41Z","published":"2025-07-01T14:46:16Z","title":"Many LLMs Are More Utilitarian Than One","summary":"  Moral judgment is integral to large language models' (LLMs) social reasoning.\nAs multi-agent systems gain prominence, it becomes crucial to understand how\nLLMs function when collaborating compared to operating as individual agents. In\nhuman moral judgment, group deliberation leads to a Utilitarian Boost: a\ntendency to endorse norm violations that inflict harm but maximize benefits for\nthe greatest number of people. We study whether a similar dynamic emerges in\nmulti-agent LLM systems. We test six models on well-established sets of moral\ndilemmas across two conditions: (1) Solo, where models reason independently,\nand (2) Group, where they engage in multi-turn discussions in pairs or triads.\nIn personal dilemmas, where agents decide whether to directly harm an\nindividual for the benefit of others, all models rated moral violations as more\nacceptable when part of a group, demonstrating a Utilitarian Boost similar to\nthat observed in humans. However, the mechanism for the Boost in LLMs differed:\nWhile humans in groups become more utilitarian due to heightened sensitivity to\ndecision outcomes, LLM groups showed either reduced sensitivity to norms or\nenhanced impartiality. We report model differences in when and how strongly the\nBoost manifests. We also discuss prompt and agent compositions that enhance or\nmitigate the effect. We end with a discussion of the implications for AI\nalignment, multi-agent design, and artificial moral reasoning. Code available\nat: https://github.com/baltaci-r/MoralAgents\n","authors":["Anita Keshmirian","Razan Baltaji","Babak Hemmatian","Hadi Asghari","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2507.00814v2.pdf","comment":"Accepted to the Conference on Neural Information Processing Systems\n  (NeurIPS 2025)"}],"Graphics":[{"id":"http://arxiv.org/abs/2509.16336v2","updated":"2025-10-29T18:17:28Z","published":"2025-09-19T18:24:41Z","title":"Neural Atlas Graphs for Dynamic Scene Decomposition and Editing","summary":"  Learning editable high-resolution scene representations for dynamic scenes is\nan open problem with applications across the domains from autonomous driving to\ncreative editing - the most successful approaches today make a trade-off\nbetween editability and supporting scene complexity: neural atlases represent\ndynamic scenes as two deforming image layers, foreground and background, which\nare editable in 2D, but break down when multiple objects occlude and interact.\nIn contrast, scene graph models make use of annotated data such as masks and\nbounding boxes from autonomous-driving datasets to capture complex 3D spatial\nrelationships, but their implicit volumetric node representations are\nchallenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a\nhybrid high-resolution scene representation, where every graph node is a\nview-dependent neural atlas, facilitating both 2D appearance editing and 3D\nordering and positioning of scene elements. Fit at test-time, NAGs achieve\nstate-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR\nincrease compared to existing methods - and make environmental editing possible\nin high resolution and visual quality - creating counterfactual driving\nscenarios with new backgrounds and edited vehicle appearance. We find that the\nmethod also generalizes beyond driving scenes and compares favorably - by more\nthan 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS\nvideo dataset with a diverse set of human and animal-centric scenes.\n  Project Page: https://princeton-computational-imaging.github.io/nag/\n","authors":["Jan Philipp Schneider","Pratik Singh Bisht","Ilya Chugunov","Andreas Kolb","Michael Moeller","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2509.16336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25765v1","updated":"2025-10-29T17:58:14Z","published":"2025-10-29T17:58:14Z","title":"FreeArt3D: Training-Free Articulated Object Generation using 3D\n  Diffusion","summary":"  Articulated 3D objects are central to many applications in robotics, AR/VR,\nand animation. Recent approaches to modeling such objects either rely on\noptimization-based reconstruction pipelines that require dense-view supervision\nor on feed-forward generative models that produce coarse geometric\napproximations and often overlook surface texture. In contrast, open-world 3D\ngeneration of static objects has achieved remarkable success, especially with\nthe advent of native 3D diffusion models such as Trellis. However, extending\nthese methods to articulated objects by training native 3D diffusion models\nposes significant challenges. In this work, we present FreeArt3D, a\ntraining-free framework for articulated 3D object generation. Instead of\ntraining a new model on limited articulated data, FreeArt3D repurposes a\npre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape\nprior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by\ntreating articulation as an additional generative dimension. Given a few images\ncaptured in different articulation states, FreeArt3D jointly optimizes the\nobject's geometry, texture, and articulation parameters without requiring\ntask-specific training or access to large-scale articulated datasets. Our\nmethod generates high-fidelity geometry and textures, accurately predicts\nunderlying kinematic structures, and generalizes well across diverse object\ncategories. Despite following a per-instance optimization paradigm, FreeArt3D\ncompletes in minutes and significantly outperforms prior state-of-the-art\napproaches in both quality and versatility.\n","authors":["Chuhao Chen","Isabella Liu","Xinyue Wei","Hao Su","Minghua Liu"],"pdf_url":"https://arxiv.org/pdf/2510.25765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25660v1","updated":"2025-10-29T16:23:08Z","published":"2025-10-29T16:23:08Z","title":"mitransient: Transient light transport in Mitsuba 3","summary":"  mitransient is a light transport simulation tool that extends Mitsuba 3 with\nsupport for time-resolved simulations. In essence, mitransient extends\nconventional rendering by adding a temporal dimension which accounts for the\ntime of flight of light. This allows rapid prototyping of novel transient\nimaging systems without the need of costly or difficult-to-operate hardware.\nOur code is trivially easy to install through pip, and consists of Python\nmodules that can run both in CPU and GPU by leveraging the JIT capabilities of\nMitsuba 3. It provides physically-based simulations of complex phenomena,\nincluding a wide variety of realistic materials and participating media such as\nfog or smoke. In addition, we extend Mitsuba 3's functionality to support\ntime-resolved polarization tracking of light and transient differentiable\nrendering. Finally, we also include tools that simplify the use of our\nsimulations for non-line-of-sight imaging, enabling realistic scene setups with\ncapture noise to be simulated in just seconds of minutes. Altogether, we hope\nthat mitransient will support the research community in developing novel\nalgorithms for transient imaging.\n","authors":["Diego Royo","Jorge Garcia-Pueyo","Miguel Crespo","Óscar Pueyo-Ciutad","Guillermo Enguita","Diego Bielsa"],"pdf_url":"https://arxiv.org/pdf/2510.25660v1.pdf","comment":"3 pages, 3 figures. For further documentation for mitransient see\n  https://mitransient.readthedocs.io"},{"id":"http://arxiv.org/abs/2510.13587v2","updated":"2025-10-29T14:24:23Z","published":"2025-10-15T14:18:20Z","title":"HRM^2Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone\n  Scans","summary":"  We present HRM$^2$Avatar, a framework for creating high-fidelity avatars from\nmonocular phone scans, which can be rendered and animated in real time on\nmobile devices. Monocular capture with smartphones provides a low-cost\nalternative to studio-grade multi-camera rigs, making avatar digitization\naccessible to non-expert users. Reconstructing high-fidelity avatars from\nsingle-view video sequences poses challenges due to limited visual and\ngeometric data. To address these limitations, at the data level, our method\nleverages two types of data captured with smartphones: static pose sequences\nfor texture reconstruction and dynamic motion sequences for learning\npose-dependent deformations and lighting changes. At the representation level,\nwe employ a lightweight yet expressive representation to reconstruct\nhigh-fidelity digital humans from sparse monocular data. We extract garment\nmeshes from monocular data to model clothing deformations effectively, and\nattach illumination-aware Gaussians to the mesh surface, enabling high-fidelity\nrendering and capturing pose-dependent lighting. This representation\nefficiently learns high-resolution and dynamic information from monocular data,\nenabling the creation of detailed avatars. At the rendering level, real-time\nperformance is critical for animating high-fidelity avatars in AR/VR, social\ngaming, and on-device creation. Our GPU-driven rendering pipeline delivers 120\nFPS on mobile devices and 90 FPS on standalone VR devices at 2K resolution,\nover $2.7\\times$ faster than representative mobile-engine baselines.\nExperiments show that HRM$^2$Avatar delivers superior visual realism and\nreal-time interactivity, outperforming state-of-the-art monocular methods.\n","authors":["Chao Shi","Shenghao Jia","Jinhui Liu","Yong Zhang","Liangchao Zhu","Zhonglei Yang","Jinze Ma","Chaoyue Niu","Chengfei Lv"],"pdf_url":"https://arxiv.org/pdf/2510.13587v2.pdf","comment":"SIGGRAPH Asia 2025, Project Page:\n  https://acennr-engine.github.io/HRM2Avatar"},{"id":"http://arxiv.org/abs/2510.25319v1","updated":"2025-10-29T09:33:29Z","published":"2025-10-29T09:33:29Z","title":"4-Doodle: Text to 3D Sketches that Move!","summary":"  We present a novel task: text-to-3D sketch animation, which aims to bring\nfreeform sketches to life in dynamic 3D space. Unlike prior works focused on\nphotorealistic content generation, we target sparse, stylized, and\nview-consistent 3D vector sketches, a lightweight and interpretable medium\nwell-suited for visual communication and prototyping. However, this task is\nvery challenging: (i) no paired dataset exists for text and 3D (or 4D)\nsketches; (ii) sketches require structural abstraction that is difficult to\nmodel with conventional 3D representations like NeRFs or point clouds; and\n(iii) animating such sketches demands temporal coherence and multi-view\nconsistency, which current pipelines do not address. Therefore, we propose\n4-Doodle, the first training-free framework for generating dynamic 3D sketches\nfrom text. It leverages pretrained image and video diffusion models through a\ndual-space distillation scheme: one space captures multi-view-consistent\ngeometry using differentiable B\\'ezier curves, while the other encodes motion\ndynamics via temporally-aware priors. Unlike prior work (e.g., DreamFusion),\nwhich optimizes from a single view per step, our multi-view optimization\nensures structural alignment and avoids view ambiguity, critical for sparse\nsketches. Furthermore, we introduce a structure-aware motion module that\nseparates shape-preserving trajectories from deformation-aware changes,\nenabling expressive motion such as flipping, rotation, and articulated\nmovement. Extensive experiments show that our method produces temporally\nrealistic and structurally stable 3D sketch animations, outperforming existing\nbaselines in both fidelity and controllability. We hope this work serves as a\nstep toward more intuitive and accessible 4D content creation.\n","authors":["Hao Chen","Jiaqi Wang","Yonggang Qi","Ke Li","Kaiyue Pang","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2510.25319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25234v1","updated":"2025-10-29T07:29:21Z","published":"2025-10-29T07:29:21Z","title":"Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D\n  Talking Face Animation","summary":"  Expressions are fundamental to conveying human emotions. With the rapid\nadvancement of AI-generated content (AIGC), realistic and expressive 3D facial\nanimation has become increasingly crucial. Despite recent progress in\nspeech-driven lip-sync for talking-face animation, generating emotionally\nexpressive talking faces remains underexplored. A major obstacle is the\nscarcity of real emotional 3D talking-face datasets due to the high cost of\ndata capture. To address this, we model facial animation driven by both speech\nand emotion as a linear additive problem. Leveraging a 3D talking-face dataset\nwith neutral expressions (VOCAset) and a dataset of 3D expression sequences\n(Florence4D), we jointly learn a set of blendshapes driven by speech and\nemotion. We introduce a sparsity constraint loss to encourage disentanglement\nbetween the two types of blendshapes while allowing the model to capture\ninherent secondary cross-domain deformations present in the training data. The\nlearned blendshapes can be further mapped to the expression and jaw pose\nparameters of the FLAME model, enabling the animation of 3D Gaussian avatars.\nQualitative and quantitative experiments demonstrate that our method naturally\ngenerates talking faces with specified expressions while maintaining accurate\nlip synchronization. Perceptual studies further show that our approach achieves\nsuperior emotional expressivity compared to existing methods, without\ncompromising lip-sync quality.\n","authors":["Yuxiang Mao","Zhijie Zhang","Zhiheng Zhang","Jiawei Liu","Chen Zeng","Shihong Xia"],"pdf_url":"https://arxiv.org/pdf/2510.25234v1.pdf","comment":"18 pages, 6 figures, accepted to ICXR 2025 conference"},{"id":"http://arxiv.org/abs/2510.25159v1","updated":"2025-10-29T04:28:24Z","published":"2025-10-29T04:28:24Z","title":"Fast and Robust Point Containment Queries on Trimmed Surface","summary":"  Point containment queries on trimmed surfaces are fundamental to CAD\nmodeling, solid geometry processing, and surface tessellation. Existing\napproaches such as ray casting and generalized winding numbers often face\nlimitations in robustness and computational efficiency.\n  We propose a fast and numerically stable method for performing containment\nqueries on trimmed surfaces, including those with periodic parameterizations.\nOur approach introduces a recursive winding number computation scheme that\nreplaces costly curve subdivision with an ellipse-based bound for Bezier\nsegments, enabling linear-time evaluation. For periodic surfaces, we lift\ntrimming curves to the universal covering space, allowing accurate and\nconsistent winding number computation even for non-contractible or\ndiscontinuous loops in parameter domain.\n  Experiments show that our method achieves substantial speedups over existing\nwinding-number algorithms while maintaining high robustness in the presence of\ngeometric noise, open boundaries, and periodic topologies. We further\ndemonstrate its effectiveness in processing real B-Rep models and in robust\ntessellation of trimmed surfaces.\n","authors":["Anchang Bao","Enya Shen","Jianmin Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25152v1","updated":"2025-10-29T04:09:50Z","published":"2025-10-29T04:09:50Z","title":"Off-Centered WoS-Type Solvers with Statistical Weighting","summary":"  Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems.\n","authors":["Anchang Bao","Jie Xu","Enya Shen","Jianmin Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25152v1.pdf","comment":"SIGGRAPH Asia 2025 conference paper"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2510.22033v2","updated":"2025-10-29T23:56:23Z","published":"2025-10-24T21:33:12Z","title":"Linearized Optimal Transport for Analysis of High-Dimensional\n  Point-Cloud and Single-Cell Data","summary":"  Single-cell technologies generate high-dimensional point clouds of cells,\nenabling detailed characterization of complex patient states and treatment\nresponses. Yet each patient is represented by an irregular point cloud rather\nthan a simple vector, making it difficult to directly quantify and compare\nbiological differences between individuals. Nonlinear methods such as kernels\nand neural networks achieve predictive accuracy but act as black boxes,\noffering little biological interpretability.\n  To address these limitations, we adapt the Linear Optimal Transport (LOT)\nframework to this setting, embedding irregular point clouds into a\nfixed-dimensional Euclidean space while preserving distributional structure.\nThis embedding provides a principled linear representation that preserves\noptimal transport geometry while enabling downstream analysis. It also forms a\nregistration between any two patients, enabling direct comparison of their\ncellular distributions. Within this space, LOT enables: (i) \\textbf{accurate\nand interpretable classification} of COVID-19 patient states, where classifier\nweights map back to specific markers and spatial regions driving predictions;\nand (ii) \\textbf{synthetic data generation} for patient-derived organoids,\nexploiting the linearity of the LOT embedding. LOT barycenters yield averaged\ncellular profiles representing combined conditions or samples, supporting drug\ninteraction testing.\n  Together, these results establish LOT as a unified framework that bridges\npredictive performance, interpretability, and generative modeling. By\ntransforming heterogeneous point clouds into structured embeddings directly\ntraceable to the original data, LOT opens new opportunities for understanding\nimmune variation and treatment effects in high-dimensional biological systems.\n","authors":["Tianxiang Wang","Yingtong Ke","Dhananjay Bhaskar","Smita Krishnaswamy","Alexander Cloninger"],"pdf_url":"https://arxiv.org/pdf/2510.22033v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.26026v1","updated":"2025-10-29T23:45:44Z","published":"2025-10-29T23:45:44Z","title":"Conformal Prediction Beyond the Horizon: Distribution-Free Inference for\n  Policy Evaluation","summary":"  Reliable uncertainty quantification is crucial for reinforcement learning\n(RL) in high-stakes settings. We propose a unified conformal prediction\nframework for infinite-horizon policy evaluation that constructs\ndistribution-free prediction intervals {for returns} in both on-policy and\noff-policy settings. Our method integrates distributional RL with conformal\ncalibration, addressing challenges such as unobserved returns, temporal\ndependencies, and distributional shifts. We propose a modular pseudo-return\nconstruction based on truncated rollouts and a time-aware calibration strategy\nusing experience replay and weighted subsampling. These innovations mitigate\nmodel bias and restore approximate exchangeability, enabling uncertainty\nquantification even under policy shifts. Our theoretical analysis provides\ncoverage guarantees that account for model misspecification and importance\nweight estimation. Empirical results, including experiments in synthetic and\nbenchmark environments like Mountain Car, show that our method significantly\nimproves coverage and reliability over standard distributional RL baselines.\n","authors":["Feichen Gan","Youcun Lu","Yingying Zhang","Yukun Liu"],"pdf_url":"https://arxiv.org/pdf/2510.26026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.05014v3","updated":"2025-10-29T23:44:26Z","published":"2025-10-06T16:53:56Z","title":"Think Then Embed: Generative Context Improves Multimodal Embedding","summary":"  There is a growing interest in Universal Multimodal Embeddings (UME), where\nmodels are required to generate task-specific representations. While recent\nstudies show that Multimodal Large Language Models (MLLMs) perform well on such\ntasks, they treat MLLMs solely as encoders, overlooking their generative\ncapacity. However, such an encoding paradigm becomes less effective as\ninstructions become more complex and require compositional reasoning. Inspired\nby the proven effectiveness of chain-of-thought reasoning, we propose a general\nThink-Then-Embed (TTE) framework for UME, composed of a reasoner and an\nembedder. The reasoner MLLM first generates reasoning traces that explain\ncomplex queries, followed by an embedder that produces representations\nconditioned on both the original query and the intermediate reasoning. This\nexplicit reasoning step enables more nuanced understanding of complex\nmultimodal instructions. Our contributions are threefold. First, by leveraging\na powerful MLLM reasoner, we achieve state-of-the-art performance on the\nMMEB-V2 benchmark, surpassing proprietary models trained on massive in-house\ndatasets. Second, to reduce the dependency on large MLLM reasoners, we finetune\na smaller MLLM reasoner using high-quality embedding-centric reasoning traces,\nachieving the best performance among open-source models with a 7% absolute gain\nover recently proposed models. Third, we investigate strategies for integrating\nthe reasoner and embedder into a unified model for improved efficiency without\nsacrificing performance.\n","authors":["Xuanming Cui","Jianpeng Cheng","Hong-you Chen","Satya Narayan Shukla","Abhijeet Awasthi","Xichen Pan","Chaitanya Ahuja","Shlok Kumar Mishra","Yonghuan Yang","Jun Xiao","Qi Guo","Ser-Nam Lim","Aashu Singh","Xiangjun Fan"],"pdf_url":"https://arxiv.org/pdf/2510.05014v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26025v1","updated":"2025-10-29T23:40:40Z","published":"2025-10-29T23:40:40Z","title":"Exploring Human-AI Conceptual Alignment through the Prism of Chess","summary":"  Do AI systems truly understand human concepts or merely mimic surface\npatterns? We investigate this through chess, where human creativity meets\nprecise strategic concepts. Analyzing a 270M-parameter transformer that\nachieves grandmaster-level play, we uncover a striking paradox: while early\nlayers encode human concepts like center control and knight outposts with up to\n85\\% accuracy, deeper layers, despite driving superior performance, drift\ntoward alien representations, dropping to 50-65\\% accuracy. To test conceptual\nrobustness beyond memorization, we introduce the first Chess960 dataset: 240\nexpert-annotated positions across 6 strategic concepts. When opening theory is\neliminated through randomized starting positions, concept recognition drops\n10-20\\% across all methods, revealing the model's reliance on memorized\npatterns rather than abstract understanding. Our layer-wise analysis exposes a\nfundamental tension in current architectures: the representations that win\ngames diverge from those that align with human thinking. These findings suggest\nthat as AI systems optimize for performance, they develop increasingly alien\nintelligence, a critical challenge for creative AI applications requiring\ngenuine human-AI collaboration. Dataset and code are available at:\nhttps://github.com/slomasov/ChessConceptsLLM.\n","authors":["Semyon Lomaso","Judah Goldfeder","Mehmet Hamza Erol","Matthew So","Yao Yan","Addison Howard","Nathan Kutz","Ravid Shwartz Ziv"],"pdf_url":"https://arxiv.org/pdf/2510.26025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02820v3","updated":"2025-10-29T23:29:35Z","published":"2025-05-05T17:47:49Z","title":"AutoLibra: Agent Metric Induction from Open-Ended Human Feedback","summary":"  Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose **AutoLibra**, a framework for\nagent evaluation, that transforms open-ended human feedback *e.g.* \"If you find\nthat the button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\" into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra serve human prompt engineers for\ndiagonalize agent failures and improve prompts iterative. Moreover, we find\nthat AutoLibra can induce metrics for automatic optimization for agents, which\nmakes agents improve through self-regulation. Our results suggest that\nAutoLibra is a powerful task-agnostic tool for evaluating and improving\nlanguage agents.\n","authors":["Hao Zhu","Phil Cuvin","Xinkai Yu","Charlotte Ka Yee Yan","Jason Zhang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2505.02820v3.pdf","comment":"https://github.com/Open-Social-World/autolibra"},{"id":"http://arxiv.org/abs/2510.26020v1","updated":"2025-10-29T23:28:53Z","published":"2025-10-29T23:28:53Z","title":"PORTool: Tool-Use LLM Training with Rewarded Tree","summary":"  Current tool-use large language models (LLMs) are trained on static datasets,\nenabling them to interact with external tools and perform multi-step,\ntool-integrated reasoning, which produces tool-call trajectories. However,\nthese models imitate how a query is resolved in a generic tool-call routine,\nthereby failing to explore possible solutions and demonstrating limited\nperformance in an evolved, dynamic tool-call environment. In this work, we\npropose PORTool, a reinforcement learning (RL) method that encourages a\ntool-use LLM to explore various trajectories yielding the correct answer.\nSpecifically, this method starts with generating multiple rollouts for a given\nquery, and some of them share the first few tool-call steps, thereby forming a\ntree-like structure. Next, we assign rewards to each step, based on its ability\nto produce a correct answer and make successful tool calls. A shared step\nacross different trajectories receives the same reward, while different steps\nunder the same fork receive different rewards. Finally, these step-wise rewards\nare used to calculate fork-relative advantages, blended with\ntrajectory-relative advantages, to train the LLM for tool use. The experiments\nutilize 17 tools to address user queries, covering both time-sensitive and\ntime-invariant topics. We conduct ablation studies to systematically justify\nthe necessity and the design robustness of step-wise rewards. Furthermore, we\ncompare the proposed PORTool with other training approaches and demonstrate\nsignificant improvements in final accuracy and the number of tool-call steps.\n","authors":["Feijie Wu","Weiwu Zhu","Yuxiang Zhang","Soumya Chatterjee","Jiarong Zhu","Fan Mo","Rodin Luo","Jing Gao"],"pdf_url":"https://arxiv.org/pdf/2510.26020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02682v2","updated":"2025-10-29T23:22:45Z","published":"2024-03-05T06:10:22Z","title":"Time Weaver: A Conditional Time Series Generation Model","summary":"  Imagine generating a city's electricity demand pattern based on weather, the\npresence of an electric vehicle, and location, which could be used for capacity\nplanning during a winter freeze. Such real-world time series are often enriched\nwith paired heterogeneous contextual metadata (e.g., weather and location).\nCurrent approaches to time series generation often ignore this paired metadata.\nAdditionally, the heterogeneity in metadata poses several practical challenges\nin adapting existing conditional generation approaches from the image, audio,\nand video domains to the time series domain. To address this gap, we introduce\nTIME WEAVER, a novel diffusion-based model that leverages the heterogeneous\nmetadata in the form of categorical, continuous, and even time-variant\nvariables to significantly improve time series generation. Additionally, we\nshow that naive extensions of standard evaluation metrics from the image to the\ntime series domain are insufficient. These metrics do not penalize conditional\ngeneration approaches for their poor specificity in reproducing the\nmetadata-specific features in the generated time series. Thus, we innovate a\nnovel evaluation metric that accurately captures the specificity of conditional\ngeneration and the realism of the generated time series. We show that TIME\nWEAVER outperforms state-of-the-art benchmarks, such as Generative Adversarial\nNetworks (GANs), by up to 30% in downstream classification tasks on real-world\nenergy, medical, air quality, and traffic datasets.\n","authors":["Sai Shankar Narasimhan","Shubhankar Agarwal","Oguzhan Akcin","Sujay Sanghavi","Sandeep Chinchali"],"pdf_url":"https://arxiv.org/pdf/2403.02682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26014v1","updated":"2025-10-29T23:11:01Z","published":"2025-10-29T23:11:01Z","title":"Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis","summary":"  Survival analysis is a task to model the time until an event of interest\noccurs, widely used in clinical and biomedical research. A key challenge is to\nmodel patient heterogeneity while also adapting risk predictions to both\nindividual characteristics and temporal dynamics. We propose a dual\nmixture-of-experts (MoE) framework for discrete-time survival analysis. Our\napproach combines a feature-encoder MoE for subgroup-aware representation\nlearning with a hazard MoE that leverages patient features and time embeddings\nto capture temporal dynamics. This dual-MoE design flexibly integrates with\nexisting deep learning based survival pipelines. On METABRIC and GBSG breast\ncancer datasets, our method consistently improves performance, boosting the\ntime-dependent C-index up to 0.04 on the test sets, and yields further gains\nwhen incorporated into the Consurv framework.\n","authors":["Hyeonjun Lee","Hyungseob Shin","Gunhee Nam","Hyeonsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2510.26014v1.pdf","comment":"Accepted to NeurIPS 2025 workshop Learning from Time Series for\n  Health (TS4H)"},{"id":"http://arxiv.org/abs/2406.13989v2","updated":"2025-10-29T23:06:02Z","published":"2024-06-20T04:32:34Z","title":"Random pairing MLE for estimation of item parameters in Rasch model","summary":"  The Rasch model, a classical model in the item response theory, is widely\nused in psychometrics to model the relationship between individuals' latent\ntraits and their binary responses to assessments or questionnaires. In this\npaper, we introduce a new likelihood-based estimator -- random pairing maximum\nlikelihood estimator ($\\mathrm{RP\\text{-}MLE}$) and its bootstrapped variant\nmultiple random pairing MLE ($\\mathrm{MRP\\text{-}MLE}$) which faithfully\nestimate the item parameters in the Rasch model. The new estimators have\nseveral appealing features compared to existing ones. First, both work for\nsparse observations, an increasingly important scenario in the big data era.\nSecond, both estimators are provably minimax optimal in terms of finite sample\n$\\ell_{\\infty}$ estimation error. Lastly, both admit precise distributional\ncharacterization that allows uncertainty quantification on the item parameters,\ne.g., construction of confidence intervals for the item parameters. The main\nidea underlying $\\mathrm{RP\\text{-}MLE}$ and $\\mathrm{MRP\\text{-}MLE}$ is to\nrandomly pair user-item responses to form item-item comparisons. This is\ncarefully designed to reduce the problem size while retaining statistical\nindependence. We also provide empirical evidence of the efficacy of the two new\nestimators using both simulated and real data.\n","authors":["Yuepeng Yang","Cong Ma"],"pdf_url":"https://arxiv.org/pdf/2406.13989v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12191v2","updated":"2025-10-29T23:02:31Z","published":"2025-05-18T01:37:58Z","title":"Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised\n  Learning from Data Curriculum","summary":"  Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2.\n","authors":["Wenquan Lu","Jiaqi Zhang","Hugues Van Assel","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2505.12191v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.23639v2","updated":"2025-10-29T22:50:10Z","published":"2025-10-24T15:56:40Z","title":"Integrating Genomics into Multimodal EHR Foundation Models","summary":"  This paper introduces an innovative Electronic Health Record (EHR) foundation\nmodel that integrates Polygenic Risk Scores (PRS) as a foundational data\nmodality, moving beyond traditional EHR-only approaches to build more holistic\nhealth profiles. Leveraging the extensive and diverse data from the All of Us\n(AoU) Research Program, this multimodal framework aims to learn complex\nrelationships between clinical data and genetic predispositions. The\nmethodology extends advancements in generative AI to the EHR foundation model\nspace, enhancing predictive capabilities and interpretability. Evaluation on\nAoU data demonstrates the model's predictive value for the onset of various\nconditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay\nbetween PRS and EHR data. The work also explores transfer learning for custom\nclassification tasks, showcasing the architecture's versatility and efficiency.\nThis approach is pivotal for unlocking new insights into disease prediction,\nproactive health management, risk stratification, and personalized treatment\nstrategies, laying the groundwork for more personalized, equitable, and\nactionable real-world evidence generation in healthcare.\n","authors":["Jonathan Amar","Edward Liu","Alessandra Breschi","Liangliang Zhang","Pouya Kheradpour","Sylvia Li","Lisa Soleymani Lehmann","Alessandro Giulianelli","Matt Edwards","Yugang Jia","David Nola","Raghav Mani","Pankaj Vats","Jesse Tetreault","T. J. Chen","Cory Y. McLean"],"pdf_url":"https://arxiv.org/pdf/2510.23639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12652v2","updated":"2025-10-29T22:50:00Z","published":"2024-10-16T15:16:04Z","title":"Constrained Posterior Sampling: Time Series Generation with Hard\n  Constraints","summary":"  Generating realistic time series samples is crucial for stress-testing models\nand protecting user privacy by using synthetic data. In engineering and\nsafety-critical applications, these samples must meet certain hard constraints\nthat are domain-specific or naturally imposed by physics or nature. Consider,\nfor example, generating electricity demand patterns with constraints on peak\ndemand times. This can be used to stress-test the functioning of power grids\nduring adverse weather conditions. Existing approaches for generating\nconstrained time series are either not scalable or degrade sample quality. To\naddress these challenges, we introduce Constrained Posterior Sampling (CPS), a\ndiffusion-based sampling algorithm that aims to project the posterior mean\nestimate into the constraint set after each denoising update. Notably, CPS\nscales to a large number of constraints ($\\sim100$) without requiring\nadditional training. We provide theoretical justifications highlighting the\nimpact of our projection step on sampling. Empirically, CPS outperforms\nstate-of-the-art methods in sample quality and similarity to real time series\nby around 70\\% and 22\\%, respectively, on real-world stocks, traffic, and air\nquality datasets.\n","authors":["Sai Shankar Narasimhan","Shubhankar Agarwal","Litu Rout","Sanjay Shakkottai","Sandeep P. Chinchali"],"pdf_url":"https://arxiv.org/pdf/2410.12652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17608v2","updated":"2025-10-29T22:44:11Z","published":"2023-05-28T02:12:00Z","title":"Reward Collapse in Aligning Large Language Models","summary":"  The extraordinary capabilities of large language models (LLMs) such as\nChatGPT and GPT-4 are in part unleashed by aligning them with reward models\nthat are trained on human preferences, which are often represented as rankings\nof responses to prompts. In this paper, we document the phenomenon of\n\\textit{reward collapse}, an empirical observation where the prevailing\nranking-based approach results in an \\textit{identical} reward distribution\n\\textit{regardless} of the prompts during the terminal phase of training. This\noutcome is undesirable as open-ended prompts like ``write a short story about\nyour best friend'' should yield a continuous range of rewards for their\ncompletions, while specific prompts like ``what is the capital of New Zealand''\nshould generate either high or low rewards. Our theoretical investigation\nreveals that reward collapse is primarily due to the insufficiency of the\nranking-based objective function to incorporate prompt-related information\nduring optimization. This insight allows us to derive closed-form expressions\nfor the reward distribution associated with a set of utility functions in an\nasymptotic regime. To overcome reward collapse, we introduce a prompt-aware\noptimization scheme that provably admits a prompt-dependent reward distribution\nwithin the interpolating regime. Our experimental results suggest that our\nproposed prompt-aware utility functions significantly alleviate reward collapse\nduring the training of reward models.\n","authors":["Ziang Song","Tianle Cai","Jason D. Lee","Weijie J. Su"],"pdf_url":"https://arxiv.org/pdf/2305.17608v2.pdf","comment":"Accepted for publication in the Journal of Data Science (JDS),\n  reference JDS1201"},{"id":"http://arxiv.org/abs/2510.26008v1","updated":"2025-10-29T22:39:09Z","published":"2025-10-29T22:39:09Z","title":"Detecting Anomalies in Machine Learning Infrastructure via Hardware\n  Telemetry","summary":"  Modern machine learning (ML) has grown into a tightly coupled, full-stack\necosystem that combines hardware, software, network, and applications. Many\nusers rely on cloud providers for elastic, isolated, and cost-efficient\nresources. Unfortunately, these platforms as a service use virtualization,\nwhich means operators have little insight into the users' workloads. This\nhinders resource optimizations by the operator, which is essential to ensure\ncost efficiency and minimize execution time. In this paper, we argue that\nworkload knowledge is unnecessary for system-level optimization. We propose\nSystem-X, which takes a \\emph{hardware-centric} approach, relying only on\nhardware signals -- fully accessible by operators. Using low-level signals\ncollected from the system, System-X detects anomalies through an unsupervised\nlearning pipeline. The pipeline is developed by analyzing over 30 popular ML\nmodels on various hardware platforms, ensuring adaptability to emerging\nworkloads and unknown deployment patterns. Using System-X, we successfully\nidentified both network and system configuration issues, accelerating the\nDeepSeek model by 5.97%.\n","authors":["Ziji Chen","Steven Chien","Peng Qian","Noa Zilberman"],"pdf_url":"https://arxiv.org/pdf/2510.26008v1.pdf","comment":"12 pages, 9 figures, submitted to nsdi 26"},{"id":"http://arxiv.org/abs/2510.26007v1","updated":"2025-10-29T22:35:34Z","published":"2025-10-29T22:35:34Z","title":"The Quest for Reliable Metrics of Responsible AI","summary":"  The development of Artificial Intelligence (AI), including AI in Science\n(AIS), should be done following the principles of responsible AI. Progress in\nresponsible AI is often quantified through evaluation metrics, yet there has\nbeen less work on assessing the robustness and reliability of the metrics\nthemselves. We reflect on prior work that examines the robustness of fairness\nmetrics for recommender systems as a type of AI application and summarise their\nkey takeaways into a set of non-exhaustive guidelines for developing reliable\nmetrics of responsible AI. Our guidelines apply to a broad spectrum of AI\napplications, including AIS.\n","authors":["Theresia Veronika Rampisela","Maria Maistro","Tuukka Ruotsalo","Christina Lioma"],"pdf_url":"https://arxiv.org/pdf/2510.26007v1.pdf","comment":"Accepted for presentation at the AI in Science Summit 2025"},{"id":"http://arxiv.org/abs/2411.10501v2","updated":"2025-10-29T22:32:43Z","published":"2024-11-15T11:19:25Z","title":"OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion\n  Models","summary":"  We consider the problem of text-to-video generation tasks with precise\ncontrol for various applications such as camera movement control and\nvideo-to-video editing. Most methods tacking this problem rely on providing\nuser-defined controls, such as binary masks or camera movement embeddings. In\nour approach we propose OnlyFlow, an approach leveraging the optical flow\nfirstly extracted from an input video to condition the motion of generated\nvideos. Using a text prompt and an input video, OnlyFlow allows the user to\ngenerate videos that respect the motion of the input video as well as the text\nprompt. This is implemented through an optical flow estimation model applied on\nthe input video, which is then fed to a trainable optical flow encoder. The\noutput feature maps are then injected into the text-to-video backbone model. We\nperform quantitative, qualitative and user preference studies to show that\nOnlyFlow positively compares to state-of-the-art methods on a wide range of\ntasks, even though OnlyFlow was not specifically trained for such tasks.\nOnlyFlow thus constitutes a versatile, lightweight yet efficient method for\ncontrolling motion in text-to-video generation. Models and code will be made\navailable on GitHub and HuggingFace.\n","authors":["Mathis Koroglu","Hugo Caselles-Dupré","Guillaume Jeanneret Sanmiguel","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2411.10501v2.pdf","comment":"8 pages, 1 supplementary page, 9 figures"},{"id":"http://arxiv.org/abs/2503.02878v3","updated":"2025-10-29T22:28:23Z","published":"2025-03-04T18:58:11Z","title":"Language Models can Self-Improve at State-Value Estimation for Better\n  Search","summary":"  Collecting ground-truth rewards or human demonstrations for multi-step\nreasoning tasks is often prohibitively expensive, particularly in interactive\ndomains such as web tasks. We introduce Self-Taught Lookahead (STL), a\nreward-free framework that improves language model-based value functions by\nreasoning explicitly about state transitions. STL can be viewed as a\nchain-of-thought analogue of the value iteration algorithm: instead of\nregressing directly on numeric values, a value LLM is trained to simulate a\nstep of lookahead in natural language - predicting the next action, resulting\nstate, and rationale for its value, thereby refining value estimates without\nany labeled data. This self-supervised procedure yields more accurate\nstate-value predictions, which in turn enable lightweight search algorithms to\nexpand fewer states while maintaining strong performance. Empirically,\nSTL-trained value models built on moderately sized (8B parameter) open-weight\nLLMs boost web agent success rates by 39%, achieving comparable performance\nwith proprietary models. STL also generalizes to multi-hop QA and math puzzles.\nWe find that STL enables small open-source models to guide efficient search,\nreducing inference costs by integrating explicit reasoning with value learning.\n","authors":["Ethan Mendes","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2503.02878v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.26000v1","updated":"2025-10-29T22:25:43Z","published":"2025-10-29T22:25:43Z","title":"Infrequent Exploration in Linear Bandits","summary":"  We study the problem of infrequent exploration in linear bandits, addressing\na significant yet overlooked gap between fully adaptive exploratory methods\n(e.g., UCB and Thompson Sampling), which explore potentially at every time\nstep, and purely greedy approaches, which require stringent diversity\nassumptions to succeed. Continuous exploration can be impractical or unethical\nin safety-critical or costly domains, while purely greedy strategies typically\nfail without adequate contextual diversity. To bridge these extremes, we\nintroduce a simple and practical framework, INFEX, explicitly designed for\ninfrequent exploration. INFEX executes a base exploratory policy according to a\ngiven schedule while predominantly choosing greedy actions in between. Despite\nits simplicity, our theoretical analysis demonstrates that INFEX achieves\ninstance-dependent regret matching standard provably efficient algorithms,\nprovided the exploration frequency exceeds a logarithmic threshold.\nAdditionally, INFEX is a general, modular framework that allows seamless\nintegration of any fully adaptive exploration method, enabling wide\napplicability and ease of adoption. By restricting intensive exploratory\ncomputations to infrequent intervals, our approach can also enhance\ncomputational efficiency. Empirical evaluations confirm our theoretical\nfindings, showing state-of-the-art regret performance and runtime improvements\nover existing methods.\n","authors":["Harin Lee","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2510.26000v1.pdf","comment":"NeurIPS 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2510.22094v2","updated":"2025-10-29T22:11:33Z","published":"2025-10-25T00:21:16Z","title":"Hierarchical Graph Networks for Accurate Weather Forecasting via\n  Lightweight Training","summary":"  Climate events arise from intricate, multivariate dynamics governed by\nglobal-scale drivers, profoundly impacting food, energy, and infrastructure.\nYet, accurate weather prediction remains elusive due to physical processes\nunfolding across diverse spatio-temporal scales, which fixed-resolution methods\ncannot capture. Hierarchical Graph Neural Networks (HGNNs) offer a multiscale\nrepresentation, but nonlinear downward mappings often erase global trends,\nweakening the integration of physics into forecasts. We introduce HiFlowCast\nand its ensemble variant HiAntFlow, HGNNs that embed physics within a\nmultiscale prediction framework. Two innovations underpin their design: a\nLatent-Memory-Retention mechanism that preserves global trends during downward\ntraversal, and a Latent-to-Physics branch that integrates PDE solution fields\nacross diverse scales. Our Flow models cut errors by over 5% at 13-day lead\ntimes and by 5-8% under 1st and 99th quantile extremes, improving reliability\nfor rare events. Leveraging pretrained model weights, they converge within a\nsingle epoch, reducing training cost and their carbon footprint. Such\nefficiency is vital as the growing scale of machine learning challenges\nsustainability and limits research accessibility. Code and model weights are in\nthe supplementary materials.\n","authors":["Thomas Bailie","S. Karthik Mukkavilli","Varvara Vetrova","Yun Sing Koh"],"pdf_url":"https://arxiv.org/pdf/2510.22094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25993v1","updated":"2025-10-29T22:09:53Z","published":"2025-10-29T22:09:53Z","title":"Efficient Online Learning with Predictive Coding Networks: Exploiting\n  Temporal Correlations","summary":"  Robotic systems operating at the edge require efficient online learning\nalgorithms that can continuously adapt to changing environments while\nprocessing streaming sensory data. Traditional backpropagation, while\neffective, conflicts with biological plausibility principles and may be\nsuboptimal for continuous adaptation scenarios. The Predictive Coding (PC)\nframework offers a biologically plausible alternative with local, Hebbian-like\nupdate rules, making it suitable for neuromorphic hardware implementation.\nHowever, PC's main limitation is its computational overhead due to multiple\ninference iterations during training. We present Predictive Coding Network with\nTemporal Amortization (PCN-TA), which preserves latent states across temporal\nframes. By leveraging temporal correlations, PCN-TA significantly reduces\ncomputational demands while maintaining learning performance. Our experiments\non the COIL-20 robotic perception dataset demonstrate that PCN-TA achieves 10%\nfewer weight updates compared to backpropagation and requires 50% fewer\ninference steps than baseline PC networks. These efficiency gains directly\ntranslate to reduced computational overhead for moving another step toward edge\ndeployment and real-time adaptation support in resource-constrained robotic\nsystems. The biologically-inspired nature of our approach also makes it a\npromising candidate for future neuromorphic hardware implementations, enabling\nefficient online learning at the edge.\n","authors":["Darius Masoum Zadeh-Jousdani","Elvin Hajizada","Eyke Hüllermeier"],"pdf_url":"https://arxiv.org/pdf/2510.25993v1.pdf","comment":"Accepted at EdgeAI4R Workshop, IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2505.18125v2","updated":"2025-10-29T22:07:41Z","published":"2025-05-23T17:34:28Z","title":"TabSTAR: A Tabular Foundation Model for Tabular Data with Text Fields","summary":"  While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees. However, recent advancements are\npaving the way for Tabular Foundation Models, which can leverage real-world\nknowledge and generalize across diverse datasets, particularly when the data\ncontains free-text. Although incorporating language model capabilities into\ntabular tasks has been explored, most existing methods utilize static,\ntarget-agnostic textual representations, limiting their effectiveness. We\nintroduce TabSTAR: a Tabular Foundation Model with Semantically Target-Aware\nRepresentations. TabSTAR is designed to enable transfer learning on tabular\ndata with textual features, with an architecture free of dataset-specific\nparameters. It unfreezes a pretrained text encoder and takes as input target\ntokens, which provide the model with the context needed to learn task-specific\nembeddings. TabSTAR achieves state-of-the-art performance for both medium- and\nlarge-sized datasets across known benchmarks of classification tasks with text\nfeatures, and its pretraining phase exhibits scaling laws in the number of\ndatasets, offering a pathway for further performance improvements.\n","authors":["Alan Arazi","Eilam Shapira","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2505.18125v2.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25992v1","updated":"2025-10-29T22:05:08Z","published":"2025-10-29T22:05:08Z","title":"Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning","summary":"  Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.\n","authors":["Yihe Deng","I-Hung Hsu","Jun Yan","Zifeng Wang","Rujun Han","Gufeng Zhang","Yanfei Chen","Wei Wang","Tomas Pfister","Chen-Yu Lee"],"pdf_url":"https://arxiv.org/pdf/2510.25992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12052v2","updated":"2025-10-29T22:00:08Z","published":"2024-11-18T20:46:02Z","title":"HoGA: Higher-Order Graph Attention via Diversity-Aware k-Hop Sampling","summary":"  Graphs model latent variable relationships in many real-world systems, and\nMessage Passing Neural Networks (MPNNs) are widely used to learn such\nstructures for downstream tasks. While edge-based MPNNs effectively capture\nlocal interactions, their expressive power is theoretically bounded, limiting\nthe discovery of higher-order relationships. We introduce the Higher-Order\nGraph Attention (HoGA) module, which constructs a k-order attention matrix by\nsampling subgraphs to maximize diversity among feature vectors. Unlike existing\nhigher-order attention methods that greedily resample similar k-order\nrelationships, HoGA targets diverse modalities in higher-order topology,\nreducing redundancy and expanding the range of captured substructures. Applied\nto two single-hop attention models, HoGA achieves at least a 5% accuracy gain\non all benchmark node classification datasets and outperforms recent baselines\non six of eight datasets. Code is available at\nhttps://github.com/TB862/Higher_Order.\n","authors":["Thomas Bailie","Yun Sing Koh","Karthik Mukkavilli"],"pdf_url":"https://arxiv.org/pdf/2411.12052v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.12344v2","updated":"2025-10-29T21:58:04Z","published":"2025-09-15T18:13:28Z","title":"FEDONet : Fourier-Embedded DeepONet for Spectrally Accurate Operator\n  Learning","summary":"  Deep Operator Networks (DeepONets) have recently emerged as powerful\ndata-driven frameworks for learning nonlinear operators, particularly suited\nfor approximating solutions to partial differential equations. Despite their\npromising capabilities, the standard implementation of DeepONets, which\ntypically employs fully connected linear layers in the trunk network, can\nencounter limitations in capturing complex spatial structures inherent to\nvarious PDEs. To address this limitation, we introduce Fourier-embedded trunk\nnetworks within the DeepONet architecture, leveraging random Fourier feature\nmappings to enrich spatial representation capabilities. Our proposed\nFourier-embedded DeepONet (FEDONet) demonstrates superior performance compared\nto the traditional DeepONet across a comprehensive suite of PDE-driven\ndatasets, including the two-dimensional Poisson, Burgers', Lorenz-63, Eikonal,\nAllen-Cahn, and the Kuramoto-Sivashinsky equation. Empirical evaluations of\nFEDONet consistently show significant improvements in solution reconstruction\naccuracy, with average relative $L^2$ performance gains ranging between\n2-3$\\times$ compared to the DeepONet baseline. This study highlights the\neffectiveness of Fourier embeddings in enhancing neural operator learning,\noffering a robust and broadly applicable methodology for PDE surrogate\nmodeling.\n","authors":["Arth Sojitra","Mrigank Dhingra","Omer San"],"pdf_url":"https://arxiv.org/pdf/2509.12344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25986v1","updated":"2025-10-29T21:42:36Z","published":"2025-10-29T21:42:36Z","title":"A General and Streamlined Differentiable Optimization Framework","summary":"  Differentiating through constrained optimization problems is increasingly\ncentral to learning, control, and large-scale decision-making systems, yet\npractical integration remains challenging due to solver specialization and\ninterface mismatches. This paper presents a general and streamlined\nframework-an updated DiffOpt.jl-that unifies modeling and differentiation\nwithin the Julia optimization stack. The framework computes forward - and\nreverse-mode solution and objective sensitivities for smooth, potentially\nnonconvex programs by differentiating the KKT system under standard regularity\nassumptions. A first-class, JuMP-native parameter-centric API allows users to\ndeclare named parameters and obtain derivatives directly with respect to them -\neven when a parameter appears in multiple constraints and objectives -\neliminating brittle bookkeeping from coefficient-level interfaces. We\nillustrate these capabilities on convex and nonconvex models, including\neconomic dispatch, mean-variance portfolio selection with conic risk\nconstraints, and nonlinear robot inverse kinematics. Two companion studies\nfurther demonstrate impact at scale: gradient-based iterative methods for\nstrategic bidding in energy markets and Sobolev-style training of end-to-end\noptimization proxies using solver-accurate sensitivities. Together, these\nresults demonstrate that differentiable optimization can be deployed as a\nroutine tool for experimentation, learning, calibration, and design-without\ndeviating from standard JuMP modeling practices and while retaining access to a\nbroad ecosystem of solvers.\n","authors":["Andrew W. Rosemberg","Joaquim Dias Garcia","François Pacaud","Robert B. Parker","Benoît Legat","Kaarthik Sundar","Russell Bent","Pascal Van Hentenryck"],"pdf_url":"https://arxiv.org/pdf/2510.25986v1.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.25983v1","updated":"2025-10-29T21:33:59Z","published":"2025-10-29T21:33:59Z","title":"Contrastive Predictive Coding Done Right for Mutual Information\n  Estimation","summary":"  The InfoNCE objective, originally introduced for contrastive representation\nlearning, has become a popular choice for mutual information (MI) estimation,\ndespite its indirect connection to MI. In this paper, we demonstrate why\nInfoNCE should not be regarded as a valid MI estimator, and we introduce a\nsimple modification, which we refer to as InfoNCE-anchor, for accurate MI\nestimation. Our modification introduces an auxiliary anchor class, enabling\nconsistent density ratio estimation and yielding a plug-in MI estimator with\nsignificantly reduced bias. Beyond this, we generalize our framework using\nproper scoring rules, which recover InfoNCE-anchor as a special case when the\nlog score is employed. This formulation unifies a broad spectrum of contrastive\nobjectives, including NCE, InfoNCE, and $f$-divergence variants, under a single\nprincipled framework. Empirically, we find that InfoNCE-anchor with the log\nscore achieves the most accurate MI estimates; however, in self-supervised\nrepresentation learning experiments, we find that the anchor does not improve\nthe downstream task performance. These findings corroborate that contrastive\nrepresentation learning benefits not from accurate MI estimation per se, but\nfrom the learning of structured density ratios.\n","authors":["J. Jon Ryu","Pavan Yeddanapudi","Xiangxiang Xu","Gregory W. Wornell"],"pdf_url":"https://arxiv.org/pdf/2510.25983v1.pdf","comment":"26 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.25982v1","updated":"2025-10-29T21:30:30Z","published":"2025-10-29T21:30:30Z","title":"Enabling Fast and Accurate Neutral Atom Readout through Image Denoising","summary":"  Neutral atom quantum computers hold promise for scaling up to hundreds of\nthousands of qubits, but their progress is constrained by slow qubit readout.\nMeasuring qubits currently takes milliseconds-much longer than the underlying\nquantum gate operations-making readout the primary bottleneck in deploying\nquantum error correction. Because each round of QEC depends on measurement,\nlong readout times increase cycle duration and slow down program execution.\nReducing the readout duration speeds up cycles and reduces decoherence errors\nthat accumulate while qubits idle, but it also lowers the number of collected\nphotons, making measurements noisier and more error-prone. This tradeoff leaves\nneutral atom systems stuck between slow but accurate readout and fast but\nunreliable readout.\n  We show that image denoising can resolve this tension. Our framework,\nGANDALF, uses explicit denoising using image translation to reconstruct clear\nsignals from short, low-photon measurements, enabling reliable classification\nat up to 1.6x shorter readout times. Combined with lightweight classifiers and\na pipelined readout design, our approach both reduces logical error rate by up\nto 35x and overall QEC cycle time up to 1.77x compared to state-of-the-art\nCNN-based readout for Cesium (Cs) Neutral Atom arrays.\n","authors":["Chaithanya Naik Mude","Linipun Phuttitarn","Satvik Maurya","Kunal Sinha","Mark Saffman","Swamit Tannu"],"pdf_url":"https://arxiv.org/pdf/2510.25982v1.pdf","comment":"12 pages, 15 figures"},{"id":"http://arxiv.org/abs/2510.25979v1","updated":"2025-10-29T21:26:17Z","published":"2025-10-29T21:26:17Z","title":"AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache","summary":"  Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.\n","authors":["Dinghong Song","Yuan Feng","Yiwei Wang","Shangye Chen","Cyril Guyot","Filip Blagojevic","Hyeran Jeon","Pengfei Su","Dong Li"],"pdf_url":"https://arxiv.org/pdf/2510.25979v1.pdf","comment":"10 pages, 6 figures, submitted to Ninth Annual Conference on Machine\n  Learning and Systems (MLSys'26)"},{"id":"http://arxiv.org/abs/2510.16629v2","updated":"2025-10-29T21:20:51Z","published":"2025-10-18T19:58:31Z","title":"On the Impossibility of Retrain Equivalence in Machine Unlearning","summary":"  Machine unlearning seeks to selectively remove the \"influence\" of specific\ntraining data on a model's outputs. The ideal goal is Retrain\nEquivalence--behavior identical to a model trained from scratch on only the\nretained data. This goal was formulated for models trained on i.i.d. data\nbatches, but modern pipelines often involve multi-stage training, with each\nstage having a distinct data distribution and objective. Examples include LLM\nfine-tuning for alignment, reasoning ability, etc. Our study shows via theory\nand experiments that this shift to multi-stage training introduces a\nfundamental barrier for machine unlearning. The theory indicates that the\noutcome of local unlearning--methods that only use gradients computed on the\nforget set--is path-dependent. That is, a model's behavior during unlearning is\ninfluenced by the order of its training stages during learning, making it\nimpossible for path-oblivious algorithms to universally achieve Retrain\nEquivalence. We empirically demonstrate the same phenomenon in LLM\npost-training across Llama and Qwen models (1B to 14B) with gradient ascent,\nNPO, and SimNPO local unlearning algorithms. Models fine-tuned via different\norderings of identical training stages diverge in behavior during unlearning,\nwith the degradation in GSM8K accuracy after unlearning varying by over 20%\nacross paths. We also observe that some learning paths consistently produce\nmodels that unlearn slowly. During unlearning, whether the probability mass\ngets squeezed into paraphrasing or alternative concepts is also path-dependent.\nThese results consistently show that Retrain Equivalence is an ill-posed target\nfor local unlearning algorithms, so long as the target models are trained in\nstages. In situations where access to models' training histories is hard, the\ncurrent work calls for rethinking the definition and desiderata of machine\nunlearning.\n","authors":["Jiatong Yu","Yinghui He","Anirudh Goyal","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2510.16629v2.pdf","comment":"Code available at\n  https://princeton-pli.github.io/impossibility-unlearning/"},{"id":"http://arxiv.org/abs/2510.25974v1","updated":"2025-10-29T21:17:50Z","published":"2025-10-29T21:17:50Z","title":"Risks and Opportunities in Human-Machine Teaming in Operationalizing\n  Machine Learning Target Variables","summary":"  Predictive modeling has the potential to enhance human decision-making.\nHowever, many predictive models fail in practice due to problematic problem\nformulation in cases where the prediction target is an abstract concept or\nconstruct and practitioners need to define an appropriate target variable as a\nproxy to operationalize the construct of interest. The choice of an appropriate\nproxy target variable is rarely self-evident in practice, requiring both domain\nknowledge and iterative data modeling. This process is inherently\ncollaborative, involving both domain experts and data scientists. In this work,\nwe explore how human-machine teaming can support this process by accelerating\niterations while preserving human judgment. We study the impact of two\nhuman-machine teaming strategies on proxy construction: 1) relevance-first:\nhumans leading the process by selecting relevant proxies, and 2)\nperformance-first: machines leading the process by recommending proxies based\non predictive performance. Based on a controlled user study of a proxy\nconstruction task (N = 20), we show that the performance-first strategy\nfacilitated faster iterations and decision-making, but also biased users\ntowards well-performing proxies that are misaligned with the application goal.\nOur study highlights the opportunities and risks of human-machine teaming in\noperationalizing machine learning target variables, yielding insights for\nfuture research to explore the opportunities and mitigate the risks.\n","authors":["Mengtian Guo","David Gotz","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2510.25974v1.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.05901v3","updated":"2025-10-29T21:11:25Z","published":"2024-09-05T20:45:44Z","title":"Diffusion Map Autoencoder","summary":"  Diffusion-Map-AutoEncoder (DMAE) pairs a diffusion-map encoder (using the\nNystr\\\"om method) with linear or RBF Gaussian-Process latent mean decoders,\nyielding closed-form inductive mappings and strong reconstructions.\n","authors":["Julio Candanedo"],"pdf_url":"https://arxiv.org/pdf/2409.05901v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25962v1","updated":"2025-10-29T21:01:31Z","published":"2025-10-29T21:01:31Z","title":"On the Dataless Training of Neural Networks","summary":"  This paper surveys studies on the use of neural networks for optimization in\nthe training-data-free setting. Specifically, we examine the dataless\napplication of neural network architectures in optimization by\nre-parameterizing problems using fully connected (or MLP), convolutional,\ngraph, and quadratic neural networks. Although MLPs have been used to solve\nlinear programs a few decades ago, this approach has recently gained increasing\nattention due to its promising results across diverse applications, including\nthose based on combinatorial optimization, inverse problems, and partial\ndifferential equations. The motivation for this setting stems from two key\n(possibly over-lapping) factors: (i) data-driven learning approaches are still\nunderdeveloped and have yet to demonstrate strong results, as seen in\ncombinatorial optimization, and (ii) the availability of training data is\ninherently limited, such as in medical image reconstruction and other\nscientific applications. In this paper, we define the dataless setting and\ncategorize it into two variants based on how a problem instance -- defined by a\nsingle datum -- is encoded onto the neural network: (i) architecture-agnostic\nmethods and (ii) architecture-specific methods. Additionally, we discuss\nsimilarities and clarify distinctions between the dataless neural network (dNN)\nsettings and related concepts such as zero-shot learning, one-shot learning,\nlifting in optimization, and over-parameterization.\n","authors":["Alvaro Velasquez","Susmit Jha","Ismail R. Alkhouri"],"pdf_url":"https://arxiv.org/pdf/2510.25962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25954v1","updated":"2025-10-29T20:53:07Z","published":"2025-10-29T20:53:07Z","title":"Application and Validation of Geospatial Foundation Model Data for the\n  Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi","summary":"  The reliability of routine health data in low and middle-income countries\n(LMICs) is often constrained by reporting delays and incomplete coverage,\nnecessitating the exploration of novel data sources and analytics. Geospatial\nFoundation Models (GeoFMs) offer a promising avenue by synthesizing diverse\nspatial, temporal, and behavioral data into mathematical embeddings that can be\nefficiently used for downstream prediction tasks. This study evaluated the\npredictive performance of three GeoFM embedding sources - Google Population\nDynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite\nimagery), and mobile phone call detail records (CDR) - for modeling 15 routine\nhealth programmatic outputs in Malawi, and compared their utility to\ntraditional geospatial interpolation methods. We used XGBoost models on data\nfrom 552 health catchment areas (January 2021-May 2023), assessing performance\nwith R2, and using an 80/20 training and test data split with 5-fold\ncross-validation used in training. While predictive performance was mixed, the\nembedding-based approaches improved upon baseline geostatistical methods in 13\nof 15 (87%) indicators tested. A Multi-GeoFM model integrating all three\nembedding sources produced the most robust predictions, achieving average\n5-fold cross validated R2 values for indicators like population density (0.63),\nnew HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,\n0.68, and 0.55, respectively. Prediction was poor for prediction targets with\nlow primary data availability, such as TB and malnutrition cases. These results\ndemonstrate that GeoFM embeddings imbue a modest predictive improvement for\nselect health and demographic outcomes in an LMIC context. We conclude that the\nintegration of multiple GeoFM sources is an efficient and valuable tool for\nsupplementing and strengthening constrained routine health information systems.\n","authors":["Lynn Metz","Rachel Haggard","Michael Moszczynski","Samer Asbah","Chris Mwase","Patricia Khomani","Tyler Smith","Hannah Cooper","Annie Mwale","Arbaaz Muslim","Gautam Prasad","Mimi Sun","Tomer Shekel","Joydeep Paul","Anna Carter","Shravya Shetty","Dylan Green"],"pdf_url":"https://arxiv.org/pdf/2510.25954v1.pdf","comment":"13 pages, 3010 words, 2 tables, 2 figures"},{"id":"http://arxiv.org/abs/2510.25952v1","updated":"2025-10-29T20:52:01Z","published":"2025-10-29T20:52:01Z","title":"Modular Linear Tokenization (MLT)","summary":"  This paper introduces Modular Linear Tokenization (MLT), a reversible and\ndeterministic technique for encoding high-cardinality categorical identifiers\ninto compact numerical vectors. Unlike traditional hashing or one-hot\nencodings, MLT preserves bijective mappings by leveraging modular arithmetic\nover finite fields and invertible linear transformations. The method offers\nexplicit control of dimensionality and computational scalability while\nmaintaining full reversibility, even for millions of identifiers. Experimental\nresults on the MovieLens 20M dataset show that MLT achieves comparable\npredictive performance to supervised embeddings while requiring significantly\nfewer parameters and lower training cost. An open-source implementation of MLT\nis available on PyPI (https://pypi.org/project/light-mlt/) and GitHub\n(https://github.com/tcharliesschmitz/light-mlt).\n","authors":["Tcharlies Schmitz"],"pdf_url":"https://arxiv.org/pdf/2510.25952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25947v1","updated":"2025-10-29T20:46:03Z","published":"2025-10-29T20:46:03Z","title":"Revisiting Multilingual Data Mixtures in Language Model Pretraining","summary":"  The impact of different multilingual data mixtures in pretraining large\nlanguage models (LLMs) has been a topic of ongoing debate, often raising\nconcerns about potential trade-offs between language coverage and model\nperformance (i.e., the curse of multilinguality). In this work, we investigate\nthese assumptions by training 1.1B and 3B parameter LLMs on diverse\nmultilingual corpora, varying the number of languages from 25 to 400. Our study\nchallenges common beliefs surrounding multilingual training. First, we find\nthat combining English and multilingual data does not necessarily degrade the\nin-language performance of either group, provided that languages have a\nsufficient number of tokens included in the pretraining corpus. Second, we\nobserve that using English as a pivot language (i.e., a high-resource language\nthat serves as a catalyst for multilingual generalization) yields benefits\nacross language families, and contrary to expectations, selecting a pivot\nlanguage from within a specific family does not consistently improve\nperformance for languages within that family. Lastly, we do not observe a\nsignificant \"curse of multilinguality\" as the number of training languages\nincreases in models at this scale. Our findings suggest that multilingual data,\nwhen balanced appropriately, can enhance language model capabilities without\ncompromising performance, even in low-resource settings\n","authors":["Negar Foroutan","Paul Teiletche","Ayush Kumar Tarun","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2510.25947v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2503.04852v3","updated":"2025-10-29T20:44:13Z","published":"2025-03-06T03:40:01Z","title":"CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data","summary":"  True intelligence hinges on the ability to uncover and leverage hidden causal\nrelations. Despite significant progress in AI and computer vision (CV), there\nremains a lack of benchmarks for assessing models' abilities to infer latent\ncausality from complex visual data. In this paper, we introduce\n\\textsc{\\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates\nstructured data (tables) with corresponding visual representations (images) to\nevaluate causal reasoning. Designed within a systematic framework, Causal3D\ncomprises 19 3D-scene datasets capturing diverse causal relations, views, and\nbackgrounds, enabling evaluations across scenes of varying complexity. We\nassess multiple state-of-the-art methods, including classical causal discovery,\ncausal representation learning, and large/vision-language models (LLMs/VLMs).\nOur experiments show that as causal structures grow more complex without prior\nknowledge, performance declines significantly, highlighting the challenges even\nadvanced methods face in complex causal scenarios. Causal3D serves as a vital\nresource for advancing causal reasoning in CV and fostering trustworthy AI in\ncritical domains.\n","authors":["Disheng Liu","Yiran Qiao","Wuche Liu","Yiren Lu","Yunlai Zhou","Tuo Liang","Yu Yin","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04852v3.pdf","comment":"Datasets link:\n  https://huggingface.co/datasets/LLDDSS/Causal3D_Dataset"},{"id":"http://arxiv.org/abs/2510.25943v1","updated":"2025-10-29T20:38:03Z","published":"2025-10-29T20:38:03Z","title":"InputDSA: Demixing then Comparing Recurrent and Externally Driven\n  Dynamics","summary":"  In control problems and basic scientific modeling, it is important to compare\nobservations with dynamical simulations. For example, comparing two neural\nsystems can shed light on the nature of emergent computations in the brain and\ndeep neural networks. Recently, Ostrow et al. (2023) introduced Dynamical\nSimilarity Analysis (DSA), a method to measure the similarity of two systems\nbased on their recurrent dynamics rather than geometry or topology. However,\nDSA does not consider how inputs affect the dynamics, meaning that two similar\nsystems, if driven differently, may be classified as different. Because\nreal-world dynamical systems are rarely autonomous, it is important to account\nfor the effects of input drive. To this end, we introduce a novel metric for\ncomparing both intrinsic (recurrent) and input-driven dynamics, called InputDSA\n(iDSA). InputDSA extends the DSA framework by estimating and comparing both\ninput and intrinsic dynamic operators using a variant of Dynamic Mode\nDecomposition with control (DMDc) based on subspace identification. We\ndemonstrate that InputDSA can successfully compare partially observed,\ninput-driven systems from noisy data. We show that when the true inputs are\nunknown, surrogate inputs can be substituted without a major deterioration in\nsimilarity estimates. We apply InputDSA on Recurrent Neural Networks (RNNs)\ntrained with Deep Reinforcement Learning, identifying that high-performing\nnetworks are dynamically similar to one another, while low-performing networks\nare more diverse. Lastly, we apply InputDSA to neural data recorded from rats\nperforming a cognitive task, demonstrating that it identifies a transition from\ninput-driven evidence accumulation to intrinsically-driven decision-making. Our\nwork demonstrates that InputDSA is a robust and efficient method for comparing\nintrinsic dynamics and the effect of external input on dynamical systems.\n","authors":["Ann Huang","Mitchell Ostrow","Satpreet H. Singh","Leo Kozachkov","Ila Fiete","Kanaka Rajan"],"pdf_url":"https://arxiv.org/pdf/2510.25943v1.pdf","comment":"36 pages, 14 figures"},{"id":"http://arxiv.org/abs/2505.15811v2","updated":"2025-10-29T20:37:11Z","published":"2025-05-21T17:59:21Z","title":"On the creation of narrow AI: hierarchy and nonlocality of neural\n  network skills","summary":"  We study the problem of creating strong, yet narrow, AI systems. While recent\nAI progress has been driven by the training of large general-purpose foundation\nmodels, the creation of smaller models specialized for narrow domains could be\nvaluable for both efficiency and safety. In this work, we explore two\nchallenges involved in creating such systems, having to do with basic\nproperties of how neural networks learn and structure their representations.\nThe first challenge regards when it is possible to train narrow models from\nscratch. Through experiments on a synthetic task, we find that it is sometimes\nnecessary to train networks on a wide distribution of data to learn certain\nnarrow skills within that distribution. This effect arises when skills depend\non each other hierarchically, and training on a broad distribution introduces a\ncurriculum which substantially accelerates learning. The second challenge\nregards how to transfer particular skills from large general models into small\nspecialized models. We find that model skills are often not perfectly localized\nto a particular set of prunable components. However, we find that methods based\non pruning can still outperform distillation. We investigate the use of a\nregularization objective to align desired skills with prunable components while\nunlearning unnecessary skills.\n","authors":["Eric J. Michaud","Asher Parker-Sartori","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2505.15811v2.pdf","comment":"NeurIPS 2025; 20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.03348v5","updated":"2025-10-29T20:32:11Z","published":"2024-10-04T12:12:36Z","title":"Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning","summary":"  Neurosymbolic learning enables the integration of symbolic reasoning with\ndeep learning but faces significant challenges in scaling to complex symbolic\nprograms, large datasets, or both. We introduce DOLPHIN, a framework that\ntackles these challenges by supporting neurosymbolic programs in Python,\nexecuting complex symbolic reasoning on the CPU while vectorizing probabilistic\ncomputations and gradient propagation on the GPU. Across 13 benchmarks spanning\ntasks over text, image, and video data, with symbolic reasoning features like\nrecursion and black-box functions, DOLPHIN converges to state-of-the-art\naccuracies on the more complex benchmarks while existing frameworks such as\nScallop, ISED, and IndeCateR+ fail to converge within the time limit. On\nsimpler benchmarks, DOLPHIN matches their performance, while achieving these\nresults 1.71x to 62x faster than the baselines. Overall, DOLPHIN advances the\nscalability of neurosymbolic frameworks, achieving state-of-the-art efficiency\nand convergence on difficult benchmarks where existing frameworks struggle. The\ncode is published at https://github.com/Dolphin-NeSy/Dolphin.\n","authors":["Aaditya Naik","Jason Liu","Claire Wang","Amish Sethi","Saikat Dutta","Mayur Naik","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2410.03348v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25934v1","updated":"2025-10-29T20:12:42Z","published":"2025-10-29T20:12:42Z","title":"Robust GNN Watermarking via Implicit Perception of Topological\n  Invariants","summary":"  Graph Neural Networks (GNNs) are valuable intellectual property, yet many\nwatermarks rely on backdoor triggers that break under common model edits and\ncreate ownership ambiguity. We present InvGNN-WM, which ties ownership to a\nmodel's implicit perception of a graph invariant, enabling trigger-free,\nblack-box verification with negligible task impact. A lightweight head predicts\nnormalized algebraic connectivity on an owner-private carrier set; a\nsign-sensitive decoder outputs bits, and a calibrated threshold controls the\nfalse-positive rate. Across diverse node and graph classification datasets and\nbackbones, InvGNN-WM matches clean accuracy while yielding higher watermark\naccuracy than trigger- and compression-based baselines. It remains strong under\nunstructured pruning, fine-tuning, and post-training quantization; plain\nknowledge distillation (KD) weakens the mark, while KD with a watermark loss\n(KD+WM) restores it. We provide guarantees for imperceptibility and robustness,\nand we prove that exact removal is NP-complete.\n","authors":["Jipeng Li","Yannning Shen"],"pdf_url":"https://arxiv.org/pdf/2510.25934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25933v1","updated":"2025-10-29T20:12:36Z","published":"2025-10-29T20:12:36Z","title":"Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual\n  Accuracy by Directed Exoskeleton Reasoning","summary":"  We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS\nGrounding public subset within a $\\pm 5$ pp equivalence margin.\n  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI\n69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference\nis 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's\n$d = 0.023$). TOST establishes equivalence at $\\pm 5$ pp (not at $\\pm 3$ pp).\nWhen purchased as managed APIs, Humans-Junior's base model\n(Phi-3.5-mini-instruct) is $\\approx 19\\times$ less expensive than GPT-4o on\nMicrosoft AI Foundry pricing; self-hosted or edge deployments can drive\nincremental inference cost toward zero. Measured vs estimated pricing sources\nare tabulated in Appendix E.\n  Method. Our approach combines minimal directed \"Exoskeleton Reasoning\"\nscaffolds with behavioral fine-tuning that teaches protocol compliance\n(epistemic discipline) rather than domain answers. Fine-tuning alone adds\nlittle; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance\n($\\approx 25\\%$). In prompt-only settings on frontier models (Q1--Q100;\nnon-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and\nGemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.\n  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within\n$\\pm 5$ pp on Q1--Q500). Cloud pricing shows $\\approx 19\\times$ lower cost\nversus GPT-4o, and self-hosted/edge deployments can approach zero marginal\ncost. Pricing sources are listed in Appendix E. Frontier prompt-only gains\n(Q1--Q100; non-comparable) and optimized-prompt exploratory results under\nearlier judges are summarized in Appendix F.\n  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,\nFine-Tuning, Model Alignment, Cost-Efficient AI\n","authors":["Nissan Yaron","Dan Bystritsky","Ben-Etzion Yaron"],"pdf_url":"https://arxiv.org/pdf/2510.25933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25926v1","updated":"2025-10-29T19:54:04Z","published":"2025-10-29T19:54:04Z","title":"Active Learning with Task-Driven Representations for Messy Pools","summary":"  Active learning has the potential to be especially useful for messy,\nuncurated pools where datapoints vary in relevance to the target task. However,\nstate-of-the-art approaches to this problem currently rely on using fixed,\nunsupervised representations of the pool, focusing on modifying the acquisition\nfunction instead. We show that this model setup can undermine their\neffectiveness at dealing with messy pools, as such representations can fail to\ncapture important information relevant to the task. To address this, we propose\nusing task-driven representations that are periodically updated during the\nactive learning process using the previously collected labels. We introduce two\nspecific strategies for learning these representations, one based on directly\nlearning semi-supervised representations and the other based on supervised\nfine-tuning of an initial unsupervised representation. We find that both\nsignificantly improve empirical performance over using unsupervised or\npretrained representations.\n","authors":["Kianoosh Ashouritaklimi","Tom Rainforth"],"pdf_url":"https://arxiv.org/pdf/2510.25926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25924v1","updated":"2025-10-29T19:53:51Z","published":"2025-10-29T19:53:51Z","title":"Transferring Causal Effects using Proxies","summary":"  We consider the problem of estimating a causal effect in a multi-domain\nsetting. The causal effect of interest is confounded by an unobserved\nconfounder and can change between the different domains. We assume that we have\naccess to a proxy of the hidden confounder and that all variables are discrete\nor categorical. We propose methodology to estimate the causal effect in the\ntarget domain, where we assume to observe only the proxy variable. Under these\nconditions, we prove identifiability (even when treatment and response\nvariables are continuous). We introduce two estimation techniques, prove\nconsistency, and derive confidence intervals. The theoretical results are\nsupported by simulation studies and a real-world example studying the causal\neffect of website rankings on consumer choices.\n","authors":["Manuel Iglesias-Alonso","Felix Schur","Julius von Kügelgen","Jonas Peters"],"pdf_url":"https://arxiv.org/pdf/2510.25924v1.pdf","comment":"Advances in Neural Information Processing Systems (NeurIPS 2025)\n  camera-ready version"},{"id":"http://arxiv.org/abs/2510.25897v1","updated":"2025-10-29T18:59:17Z","published":"2025-10-29T18:59:17Z","title":"MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and\n  efficiency","summary":"  Current text-to-image generative models are trained on large uncurated\ndatasets to enable diverse generation capabilities. However, this does not\nalign well with user preferences. Recently, reward models have been\nspecifically designed to perform post-hoc selection of generated images and\nalign them to a reward, typically user preference. This discarding of\ninformative data together with the optimizing for a single reward tend to harm\ndiversity, semantic fidelity and efficiency. Instead of this post-processing,\nwe propose to condition the model on multiple reward models during training to\nlet the model learn user preferences directly. We show that this not only\ndramatically improves the visual quality of the generated images but it also\nsignificantly speeds up the training. Our proposed method, called MIRO,\nachieves state-of-the-art performances on the GenEval compositional benchmark\nand user-preference scores (PickAScore, ImageReward, HPSv2).\n","authors":["Nicolas Dufour","Lucas Degeorge","Arijit Ghosh","Vicky Kalogeiton","David Picard"],"pdf_url":"https://arxiv.org/pdf/2510.25897v1.pdf","comment":"Project page: https://nicolas-dufour.github.io/miro"},{"id":"http://arxiv.org/abs/2407.11873v3","updated":"2025-10-29T18:52:40Z","published":"2024-07-16T15:59:49Z","title":"Infinite-dimensional Mahalanobis Distance with Applications to\n  Kernelized Novelty Detection","summary":"  The Mahalanobis distance is a classical tool used to measure the\ncovariance-adjusted distance between points in $\\bbR^d$. In this work, we\nextend the concept of Mahalanobis distance to separable Banach spaces by\nreinterpreting it as a Cameron-Martin norm associated with a probability\nmeasure. This approach leads to a basis-free, data-driven notion of anomaly\ndistance through the so-called variance norm, which can naturally be estimated\nusing empirical measures of a sample. Our framework generalizes the classical\n$\\bbR^d$, functional $(L^2[0,1])^d$, and kernelized settings; importantly, it\nincorporates non-injective covariance operators. We prove that the variance\nnorm is invariant under invertible bounded linear transformations of the data,\nextending previous results which are limited to unitary operators. In the\nHilbert space setting, we connect the variance norm to the RKHS of the\ncovariance operator, and establish consistency and convergence results for\nestimation using empirical measures with Tikhonov regularization. Using the\nvariance norm, we introduce the notion of a kernelized nearest-neighbour\nMahalanobis distance, and study some of its finite-sample concentration\nproperties. In an empirical study on 12 real-world data sets, we demonstrate\nthat the kernelized nearest-neighbour Mahalanobis distance outperforms the\ntraditional kernelized Mahalanobis distance for multivariate time series\nnovelty detection, using state-of-the-art time series kernels such as the\nsignature, global alignment, and Volterra reservoir kernels.\n","authors":["Nikita Zozoulenko","Thomas Cass","Lukas Gonon"],"pdf_url":"https://arxiv.org/pdf/2407.11873v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25892v1","updated":"2025-10-29T18:49:24Z","published":"2025-10-29T18:49:24Z","title":"Topology-Aware Active Learning on Graphs","summary":"  We propose a graph-topological approach to active learning that directly\ntargets the core challenge of exploration versus exploitation under scarce\nlabel budgets. To guide exploration, we introduce a coreset construction\nalgorithm based on Balanced Forman Curvature (BFC), which selects\nrepresentative initial labels that reflect the graph's cluster structure. This\nmethod includes a data-driven stopping criterion that signals when the graph\nhas been sufficiently explored. We further use BFC to dynamically trigger the\nshift from exploration to exploitation within active learning routines,\nreplacing hand-tuned heuristics. To improve exploitation, we introduce a\nlocalized graph rewiring strategy that efficiently incorporates multiscale\ninformation around labeled nodes, enhancing label propagation while preserving\nsparsity. Experiments on benchmark classification tasks show that our methods\nconsistently outperform existing graph-based semi-supervised baselines at low\nlabel rates.\n","authors":["Harris Hardiman-Mostow","Jack Mauro","Adrien Weihs","Andrea L. Bertozzi"],"pdf_url":"https://arxiv.org/pdf/2510.25892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.01223v3","updated":"2025-10-29T18:37:50Z","published":"2025-04-01T22:22:25Z","title":"Explainable post-training bias mitigation with distribution-based\n  fairness metrics","summary":"  We develop a novel bias mitigation framework with distribution-based fairness\nconstraints suitable for producing demographically blind and explainable\nmachine-learning models across a wide range of fairness levels. This is\naccomplished through post-processing, allowing fairer models to be generated\nefficiently without retraining the underlying model. Our framework, which is\nbased on stochastic gradient descent, can be applied to a wide range of model\ntypes, with a particular emphasis on the post-processing of gradient-boosted\ndecision trees. Additionally, we design a broad family of global fairness\nmetrics, along with differentiable and consistent estimators compatible with\nour framework, building on previous work. We empirically test our methodology\non a variety of datasets and compare it with alternative post-processing\napproaches, including Bayesian search, optimal transport projection, and direct\nneural network training.\n","authors":["Ryan Franks","Alexey Miroshnikov","Konstandinos Kotsiopoulos"],"pdf_url":"https://arxiv.org/pdf/2504.01223v3.pdf","comment":"45 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.25889v1","updated":"2025-10-29T18:37:39Z","published":"2025-10-29T18:37:39Z","title":"$π_\\texttt{RL}$: Online RL Fine-tuning for Flow-based\n  Vision-Language-Action Models","summary":"  Vision-Language-Action (VLA) models enable robots to understand and perform\ncomplex tasks from multimodal input. Although recent work explores using\nreinforcement learning (RL) to automate the laborious data collection process\nin scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based\nVLAs (e.g., $\\pi_0$, $\\pi_{0.5}$) remains challenging due to intractable action\nlog-likelihoods from iterative denoising.\n  We address this challenge with $\\pi_{\\text{RL}}$, an open-source framework\nfor training flow-based VLAs in parallel simulation. $\\pi_{\\text{RL}}$\nimplements two RL algorithms: (1) {Flow-Noise} models the denoising process as\na discrete-time MDP with a learnable noise network for exact log-likelihood\ncomputation. (2) {Flow-SDE} integrates denoising with agent-environment\ninteraction, formulating a two-layer MDP that employs ODE-to-SDE conversion for\nefficient RL exploration.\n  We evaluate $\\pi_{\\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO,\n$\\pi_{\\text{RL}}$ boosts few-shot SFT models $\\pi_0$ and $\\pi_{0.5}$ from 57.6%\nto 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train\n$\\pi_{\\text{RL}}$ in 320 parallel environments, improving $\\pi_0$ from 41.6% to\n85.7% and $\\pi_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks,\ndemonstrating scalable multitask RL under heterogeneous simulation.\n  Overall, $\\pi_{\\text{RL}}$ achieves significant performance gains and\nstronger generalization over SFT-models, validating the effectiveness of online\nRL for flow-based VLAs.\n","authors":["Kang Chen","Zhihao Liu","Tonghe Zhang","Zhen Guo","Si Xu","Hao Lin","Hongzhi Zang","Quanlu Zhang","Zhaofei Yu","Guoliang Fan","Tiejun Huang","Yu Wang","Chao Yu"],"pdf_url":"https://arxiv.org/pdf/2510.25889v1.pdf","comment":"Preprint, work in progress. 24 pages"},{"id":"http://arxiv.org/abs/2510.25884v1","updated":"2025-10-29T18:32:53Z","published":"2025-10-29T18:32:53Z","title":"Approximating Human Preferences Using a Multi-Judge Learned System","summary":"  Aligning LLM-based judges with human preferences is a significant challenge,\nas they are difficult to calibrate and often suffer from rubric sensitivity,\nbias, and instability. Overcoming this challenge advances key applications,\nsuch as creating reliable reward models for Reinforcement Learning from Human\nFeedback (RLHF) and building effective routing systems that select the\nbest-suited model for a given user query. In this work, we propose a framework\nfor modeling diverse, persona-based preferences by learning to aggregate\noutputs from multiple rubric-conditioned judges. We investigate the performance\nof this approach against naive baselines and assess its robustness through case\nstudies on both human and LLM-judges biases. Our primary contributions include\na persona-based method for synthesizing preference labels at scale and two\ndistinct implementations of our aggregator: Generalized Additive Model (GAM)\nand a Multi-Layer Perceptron (MLP).\n","authors":["Eitán Sprejer","Fernando Avalos","Augusto Bernardi","Jose Pedro Brito de Azevedo Faustino","Jacob Haimes","Narmeen Fatimah Oozeer"],"pdf_url":"https://arxiv.org/pdf/2510.25884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.16336v2","updated":"2025-10-29T18:17:28Z","published":"2025-09-19T18:24:41Z","title":"Neural Atlas Graphs for Dynamic Scene Decomposition and Editing","summary":"  Learning editable high-resolution scene representations for dynamic scenes is\nan open problem with applications across the domains from autonomous driving to\ncreative editing - the most successful approaches today make a trade-off\nbetween editability and supporting scene complexity: neural atlases represent\ndynamic scenes as two deforming image layers, foreground and background, which\nare editable in 2D, but break down when multiple objects occlude and interact.\nIn contrast, scene graph models make use of annotated data such as masks and\nbounding boxes from autonomous-driving datasets to capture complex 3D spatial\nrelationships, but their implicit volumetric node representations are\nchallenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a\nhybrid high-resolution scene representation, where every graph node is a\nview-dependent neural atlas, facilitating both 2D appearance editing and 3D\nordering and positioning of scene elements. Fit at test-time, NAGs achieve\nstate-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR\nincrease compared to existing methods - and make environmental editing possible\nin high resolution and visual quality - creating counterfactual driving\nscenarios with new backgrounds and edited vehicle appearance. We find that the\nmethod also generalizes beyond driving scenes and compares favorably - by more\nthan 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS\nvideo dataset with a diverse set of human and animal-centric scenes.\n  Project Page: https://princeton-computational-imaging.github.io/nag/\n","authors":["Jan Philipp Schneider","Pratik Singh Bisht","Ilya Chugunov","Andreas Kolb","Michael Moeller","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2509.16336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25867v1","updated":"2025-10-29T18:10:44Z","published":"2025-10-29T18:10:44Z","title":"MedVLSynther: Synthesizing High-Quality Visual Question Answering from\n  Medical Documents with Generator-Verifier LMMs","summary":"  Large Multimodal Models (LMMs) are increasingly capable of answering medical\nquestions that require joint reasoning over images and text, yet training\ngeneral medical VQA systems is impeded by the lack of large, openly usable,\nhigh-quality corpora. We present MedVLSynther, a rubric-guided\ngenerator-verifier framework that synthesizes high-quality multiple-choice VQA\nitems directly from open biomedical literature by conditioning on figures,\ncaptions, and in-text references. The generator produces self-contained stems\nand parallel, mutually exclusive options under a machine-checkable JSON schema;\na multi-stage verifier enforces essential gates (self-containment, single\ncorrect answer, clinical validity, image-text consistency), awards fine-grained\npositive points, and penalizes common failure modes before acceptance. Applying\nthis pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over\n14,803 images spanning 13 imaging modalities and 28 anatomical regions.\nTraining open-weight LMMs with reinforcement learning using verifiable rewards\nimproves accuracy across six medical VQA benchmarks, achieving averages of\n55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA,\noutperforming strong medical LMMs. A Ablations verify that both generation and\nverification are necessary and that more verified data consistently helps, and\na targeted contamination analysis detects no leakage from evaluation suites. By\noperating entirely on open literature and open-weight models, MedVLSynther\noffers an auditable, reproducible, and privacy-preserving path to scalable\nmedical VQA training data.\n","authors":["Xiaoke Huang","Ningsen Wang","Hui Liu","Xianfeng Tang","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2510.25867v1.pdf","comment":"Project page, code, data, and models:\n  https://ucsc-vlaa.github.io/MedVLSynther/"},{"id":"http://arxiv.org/abs/2505.10361v2","updated":"2025-10-29T18:07:27Z","published":"2025-05-15T14:52:16Z","title":"Plasticity as the Mirror of Empowerment","summary":"  Agents are minimally entities that are influenced by their past observations\nand act to influence future observations. This latter capacity is captured by\nempowerment, which has served as a vital framing concept across artificial\nintelligence and cognitive science. This former capacity, however, is equally\nfoundational: In what ways, and to what extent, can an agent be influenced by\nwhat it observes? In this paper, we ground this concept in a universal\nagent-centric measure that we refer to as plasticity, and reveal a fundamental\nconnection to empowerment. Following a set of desiderata on a suitable\ndefinition, we define plasticity using a new information-theoretic quantity we\ncall the generalized directed information. We show that this new quantity\nstrictly generalizes the directed information introduced by Massey (1990) while\npreserving all of its desirable properties. Under this definition, we find that\nplasticity is well thought of as the mirror of empowerment: The two concepts\nare defined using the same measure, with only the direction of influence\nreversed. Our main result establishes a tension between the plasticity and\nempowerment of an agent, suggesting that agent design needs to be mindful of\nboth characteristics. We explore the implications of these findings, and\nsuggest that plasticity, empowerment, and their relationship are essential to\nunderstanding agency\n","authors":["David Abel","Michael Bowling","André Barreto","Will Dabney","Shi Dong","Steven Hansen","Anna Harutyunyan","Khimya Khetarpal","Clare Lyle","Razvan Pascanu","Georgios Piliouras","Doina Precup","Jonathan Richens","Mark Rowland","Tom Schaul","Satinder Singh"],"pdf_url":"https://arxiv.org/pdf/2505.10361v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25850v1","updated":"2025-10-29T18:00:16Z","published":"2025-10-29T18:00:16Z","title":"Debate2Create: Robot Co-design via Large Language Model Debates","summary":"  Automating the co-design of a robot's morphology and control is a\nlong-standing challenge due to the vast design space and the tight coupling\nbetween body and behavior. We introduce Debate2Create (D2C), a framework in\nwhich large language model (LLM) agents engage in a structured dialectical\ndebate to jointly optimize a robot's design and its reward function. In each\nround, a design agent proposes targeted morphological modifications, and a\ncontrol agent devises a reward function tailored to exploit the new design. A\npanel of pluralistic judges then evaluates the design-control pair in\nsimulation and provides feedback that guides the next round of debate. Through\niterative debates, the agents progressively refine their proposals, producing\nincreasingly effective robot designs. Notably, D2C yields diverse and\nspecialized morphologies despite no explicit diversity objective. On a\nquadruped locomotion benchmark, D2C discovers designs that travel 73% farther\nthan the default, demonstrating that structured LLM-based debate can serve as a\npowerful mechanism for emergent robot co-design. Our results suggest that\nmulti-agent debate, when coupled with physics-grounded feedback, is a promising\nnew paradigm for automated robot design.\n","authors":["Kevin Qiu","Marek Cygan"],"pdf_url":"https://arxiv.org/pdf/2510.25850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25770v1","updated":"2025-10-29T17:59:16Z","published":"2025-10-29T17:59:16Z","title":"E-Scores for (In)Correctness Assessment of Generative Model Outputs","summary":"  While generative models, especially large language models (LLMs), are\nubiquitous in today's world, principled mechanisms to assess their\n(in)correctness are limited. Using the conformal prediction framework, previous\nworks construct sets of LLM responses where the probability of including an\nincorrect response, or error, is capped at a desired user-defined tolerance\nlevel. However, since these methods are based on p-values, they are susceptible\nto p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the\nguarantees. We therefore leverage e-values to complement generative model\noutputs with e-scores as a measure of incorrectness. In addition to achieving\nthe same statistical guarantees as before, e-scores provide users flexibility\nin adaptively choosing tolerance levels after observing the e-scores\nthemselves, by upper bounding a post-hoc notion of error called size\ndistortion. We experimentally demonstrate their efficacy in assessing LLM\noutputs for different correctness types: mathematical factuality and property\nconstraints satisfaction.\n","authors":["Guneet S. Dhillon","Javier González","Teodora Pandeva","Alicia Curth"],"pdf_url":"https://arxiv.org/pdf/2510.25770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25769v1","updated":"2025-10-29T17:59:06Z","published":"2025-10-29T17:59:06Z","title":"Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE\n  Solutions","summary":"  Stochastic differential equations (SDEs) are well suited to modelling noisy\nand irregularly sampled time series found in finance, physics, and machine\nlearning. Traditional approaches require costly numerical solvers to sample\nbetween arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and\ntheir latent variants, which directly learn (latent) SDE transition laws using\nconditional normalising flows with architectural constraints that preserve\nproperties inherited from stochastic flows. This enables one-shot sampling\nbetween arbitrary states and yields up to two orders of magnitude speed-ups at\nlarge time gaps. Experiments on synthetic SDE simulations and on real-world\ntracking and video data show that NSFs maintain distributional accuracy\ncomparable to numerical approaches while dramatically reducing computation for\narbitrary time-point sampling.\n","authors":["Naoki Kiyohara","Edward Johns","Yingzhen Li"],"pdf_url":"https://arxiv.org/pdf/2510.25769v1.pdf","comment":"NeurIPS 2025 (poster). Project page:\n  https://nkiyohara.github.io/nsf-neurips2025/"},{"id":"http://arxiv.org/abs/2510.18905v2","updated":"2025-10-29T17:57:23Z","published":"2025-10-21T01:03:46Z","title":"3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and\n  Latency","summary":"  AI inference scaling is often tuned through 1D heuristics (a fixed reasoning\npasses) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail\nto consider cost and latency constraints. We introduce a 3D optimization\nframework that jointly calibrates accuracy, cost, and latency within a unified\ndecision space, enabling constraints-aware inference scaling. Using Monte Carlo\nsimulations across three representative scenarios and nine simulated large\nlanguage models, we evaluate four optimization methods to address the 3D\nmulti-objective optimization (MOO) problem. Framing inference scaling in MOO\nshapes a feasible space that 1D and 2D optimizations fail to capture, enabling\nenvironmentadaptive selection of the inference scaling k. Results show that\nknee-point optimization achieves the best balance, while accuracy-maximization\nremains favorable when precision is prioritized. The framework establishes a\ntheoretical foundation for deployment-aware inference scaling across diverse\noperational contexts.\n","authors":["Minseok Jung","Abhas Ricky","Muhammad Rameez Chatni"],"pdf_url":"https://arxiv.org/pdf/2510.18905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01939v3","updated":"2025-10-29T17:57:03Z","published":"2025-07-02T17:49:52Z","title":"SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars","summary":"  In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.\n","authors":["Xiaosheng Zhao","Yang Huang","Guirong Xue","Xiao Kong","Jifeng Liu","Xiaoyu Tang","Timothy C. Beers","Yuan-Sen Ting","A-Li Luo"],"pdf_url":"https://arxiv.org/pdf/2507.01939v3.pdf","comment":"27 pages, 8 figures, 5 tables. Minor update: added corrected\n  acknowledgments and corrected a misstated hyperparameter value (noted in\n  footnote) for reproducibility. Submitted to AAS Journals. Comments welcome"},{"id":"http://arxiv.org/abs/2510.25759v1","updated":"2025-10-29T17:55:17Z","published":"2025-10-29T17:55:17Z","title":"Synthetic Data Reveals Generalization Gaps in Correlated Multiple\n  Instance Learning","summary":"  Multiple instance learning (MIL) is often used in medical imaging to classify\nhigh-resolution 2D images by processing patches or classify 3D volumes by\nprocessing slices. However, conventional MIL approaches treat instances\nseparately, ignoring contextual relationships such as the appearance of nearby\npatches or slices that can be essential in real applications. We design a\nsynthetic classification task where accounting for adjacent instance features\nis crucial for accurate prediction. We demonstrate the limitations of\noff-the-shelf MIL approaches by quantifying their performance compared to the\noptimal Bayes estimator for this task, which is available in closed-form. We\nempirically show that newer correlated MIL methods still struggle to generalize\nas well as possible when trained from scratch on tens of thousands of\ninstances.\n","authors":["Ethan Harvey","Dennis Johan Loevlie","Michael C. Hughes"],"pdf_url":"https://arxiv.org/pdf/2510.25759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25755v1","updated":"2025-10-29T17:52:39Z","published":"2025-10-29T17:52:39Z","title":"MLPrE -- A tool for preprocessing and exploratory data analysis prior to\n  machine learning model construction","summary":"  With the recent growth of Deep Learning for AI, there is a need for tools to\nmeet the demand of data flowing into those models. In some cases, source data\nmay exist in multiple formats, and therefore the source data must be\ninvestigated and properly engineered for a Machine Learning model or graph\ndatabase. Overhead and lack of scalability with existing workflows limit\nintegration within a larger processing pipeline such as Apache Airflow, driving\nthe need for a robust, extensible, and lightweight tool to preprocess arbitrary\ndatasets that scales with data type and size. To address this, we present\nMachine Learning Preprocessing and Exploratory Data Analysis, MLPrE, in which\nSparkDataFrames were utilized to hold data during processing and ensure\nscalability. A generalizable JSON input file format was utilized to describe\nstepwise changes to that DataFrame. Stages were implemented for input and\noutput, filtering, basic statistics, feature engineering, and exploratory data\nanalysis. A total of 69 stages were implemented into MLPrE, of which we\nhighlight and demonstrate key stages using six diverse datasets. We further\nhighlight MLPrE's ability to independently process multiple fields in flat\nfiles and recombine them, otherwise requiring an additional pipeline, using a\nUniProt glossary term dataset. Building on this advantage, we demonstrated the\nclustering stage with available wine quality data. Lastly, we demonstrate the\npreparation of data for a graph database in the final stages of MLPrE using\nphosphosite kinase data. Overall, our MLPrE tool offers a generalizable and\nscalable tool for preprocessing and early data analysis, filling a critical\nneed for such a tool given the ever expanding use of machine learning. This\ntool serves to accelerate and simplify early stage development in larger\nworkflows.\n","authors":["David S Maxwell","Michael Darkoh","Sidharth R Samudrala","Caroline Chung","Stephanie T Schmidt","Bissan Al-Lazikani"],"pdf_url":"https://arxiv.org/pdf/2510.25755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17022v2","updated":"2025-10-29T17:52:01Z","published":"2025-10-19T22:04:57Z","title":"Curiosity-driven RL for symbolic equation solving","summary":"  We explore if RL can be useful for symbolic mathematics. Previous work showed\ncontrastive learning can solve linear equations in one variable. We show\nmodel-free PPO \\cite{schulman2017proximal} augmented with curiosity-based\nexploration and graph-based actions can solve nonlinear equations such as those\ninvolving radicals, exponentials, and trig functions. Our work suggests\ncuriosity-based exploration may be useful for general symbolic reasoning tasks.\n","authors":["Kevin P. O'Keeffe"],"pdf_url":"https://arxiv.org/pdf/2510.17022v2.pdf","comment":"Accepted at the NeurIPS 2025 MATH-AI Workshop"},{"id":"http://arxiv.org/abs/2510.25753v1","updated":"2025-10-29T17:51:57Z","published":"2025-10-29T17:51:57Z","title":"How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for\n  Transformers with MLPs","summary":"  Pretrained Transformers demonstrate remarkable in-context learning (ICL)\ncapabilities, enabling them to adapt to new tasks from demonstrations without\nparameter updates. However, theoretical studies often rely on simplified\narchitectures (e.g., omitting MLPs), data models (e.g., linear regression with\nisotropic inputs), and single-source training, limiting their relevance to\nrealistic settings. In this work, we study ICL in pretrained Transformers with\nnonlinear MLP heads on nonlinear tasks drawn from multiple data sources with\nheterogeneous input, task, and noise distributions. We analyze a model where\nthe MLP comprises two layers, with the first layer trained via a single\ngradient step and the second layer fully optimized. Under high-dimensional\nasymptotics, we prove that such models are equivalent in ICL error to\nstructured polynomial predictors, leveraging results from the theory of\nGaussian universality and orthogonal polynomials. This equivalence reveals that\nnonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear\ntasks, compared to linear baselines. It also enables a precise analysis of data\nmixing effects: we identify key properties of high-quality data sources (low\nnoise, structured covariances) and show that feature learning emerges only when\nthe task covariance exhibits sufficient structure. These results are validated\nempirically across various activation functions, model sizes, and data\ndistributions. Finally, we experiment with a real-world scenario involving\nmultilingual sentiment analysis where each language is treated as a different\nsource. Our experimental results for this case exemplify how our findings\nextend to real-world cases. Overall, our work advances the theoretical\nfoundations of ICL in Transformers and provides actionable insight into the\nrole of architecture and data in ICL.\n","authors":["Samet Demir","Zafer Dogan"],"pdf_url":"https://arxiv.org/pdf/2510.25753v1.pdf","comment":"NeurIPS 2025, 24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.18549v3","updated":"2025-10-29T17:50:44Z","published":"2025-07-24T16:13:56Z","title":"The Price equation reveals a universal force-metric-bias law of\n  algorithmic learning and natural selection","summary":"  Diverse learning algorithms, optimization methods, and natural selection\nshare a common mathematical structure, despite their apparent differences. Here\nI show that a simple notational partitioning of change by the Price equation\nreveals a universal force-metric-bias (FMB) law: $\\Delta\\mathbf{\\theta} =\n\\mathbf{M}\\,\\mathbf{f} + \\mathbf{b} + \\mathbf{\\xi}$. The force $\\mathbf{f}$\ndrives improvement in parameters, $\\Delta\\mathbf{\\theta}$, in proportion to the\nslope of performance with respect to the parameters. The metric $\\mathbf{M}$\nrescales movement by inverse curvature. The bias $\\mathbf{b}$ adds momentum or\nchanges in the frame of reference. The noise $\\mathbf{\\xi}$ enables\nexploration. This framework unifies natural selection, Bayesian updating,\nNewton's method, stochastic gradient descent, stochastic Langevin dynamics,\nAdam optimization, and most other algorithms as special cases of the same\nunderlying process. The Price equation also reveals why Fisher information,\nKullback-Leibler divergence, and d'Alembert's principle arise naturally in\nlearning dynamics. By exposing this common structure, the FMB law provides a\nprincipled foundation for understanding, comparing, and designing learning\nalgorithms across disciplines.\n","authors":["Steven A. Frank"],"pdf_url":"https://arxiv.org/pdf/2507.18549v3.pdf","comment":"Version 2: fixed definition of force in abstract; Version 3: added\n  citations and some minor editing"},{"id":"http://arxiv.org/abs/2510.25752v1","updated":"2025-10-29T17:49:40Z","published":"2025-10-29T17:49:40Z","title":"Meshless solutions of PDE inverse problems on irregular geometries","summary":"  Solving inverse and optimization problems over solutions of nonlinear partial\ndifferential equations (PDEs) on complex spatial domains is a long-standing\nchallenge. Here we introduce a method that parameterizes the solution using\nspectral bases on arbitrary spatiotemporal domains, whereby the basis is\ndefined on a hyperrectangle containing the true domain. We find the\ncoefficients of the basis expansion by solving an optimization problem whereby\nboth the equations, the boundary conditions and any optimization targets are\nenforced by a loss function, building on a key idea from Physics-Informed\nNeural Networks (PINNs). Since the representation of the function natively has\nexponential convergence, so does the solution of the optimization problem, as\nlong as it can be solved efficiently. We find empirically that the optimization\nprotocols developed for machine learning find solutions with exponential\nconvergence on a wide range of equations. The method naturally allows for the\nincorporation of data assimilation by including additional terms in the loss\nfunction, and for the efficient solution of optimization problems over the PDE\nsolutions.\n","authors":["James V. Roggeveen","Michael P. Brenner"],"pdf_url":"https://arxiv.org/pdf/2510.25752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20762v3","updated":"2025-10-29T17:47:39Z","published":"2025-03-26T17:50:13Z","title":"ASGO: Adaptive Structured Gradient Optimization","summary":"  Training deep neural networks is a structured optimization problem, because\nthe parameters are naturally represented by matrices and tensors rather than by\nvectors. Under this structural representation, it has been widely observed that\ngradients are low-rank and Hessians are approximately block diagonal. These\nstructured properties are crucial for designing efficient optimization\nalgorithms, but are not utilized by many current popular optimizers like Adam.\nIn this paper, we present a novel optimization algorithm ASGO that capitalizes\non these properties by employing a preconditioner that is adaptively updated\nusing structured gradients. By a fine-grained theoretical analysis, ASGO is\nproven to achieve superior convergence rates compared to existing structured\ngradient methods. Based on this convergence theory, we further demonstrate that\nASGO can benefit from low-rank gradients and block diagonal Hessians. We also\ndiscuss practical modifications of ASGO and empirically verify ASGO's\neffectiveness on language model tasks. Code is available at\nhttps://github.com/infinity-stars/ASGO.\n","authors":["Kang An","Yuxing Liu","Rui Pan","Yi Ren","Shiqian Ma","Donald Goldfarb","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20762v3.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2502.21269v3","updated":"2025-10-29T17:46:23Z","published":"2025-02-28T17:45:26Z","title":"Dynamical Decoupling of Generalization and Overfitting in Large\n  Two-Layer Networks","summary":"  Understanding the inductive bias and generalization properties of large\noverparametrized machine learning models requires to characterize the dynamics\nof the training algorithm. We study the learning dynamics of large two-layer\nneural networks via dynamical mean field theory, a well established technique\nof non-equilibrium statistical physics. We show that, for large network width\n$m$, and large number of samples per input dimension $n/d$, the training\ndynamics exhibits a separation of timescales which implies: $(i)$~The emergence\nof a slow time scale associated with the growth in Gaussian/Rademacher\ncomplexity of the network; $(ii)$~Inductive bias towards small complexity if\nthe initialization has small enough complexity; $(iii)$~A dynamical decoupling\nbetween feature learning and overfitting regimes; $(iv)$~A non-monotone\nbehavior of the test error, associated `feature unlearning' regime at large\ntimes.\n","authors":["Andrea Montanari","Pierfrancesco Urbani"],"pdf_url":"https://arxiv.org/pdf/2502.21269v3.pdf","comment":"88 pages; 63 pdf figures"},{"id":"http://arxiv.org/abs/2510.25739v1","updated":"2025-10-29T17:43:31Z","published":"2025-10-29T17:43:31Z","title":"Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image\n  Generation","summary":"  Autoregressive (AR) image generation models are capable of producing\nhigh-fidelity images but often suffer from slow inference due to their\ninherently sequential, token-by-token decoding process. Speculative decoding,\nwhich employs a lightweight draft model to approximate the output of a larger\nAR model, has shown promise in accelerating text generation without\ncompromising quality. However, its application to image generation remains\nlargely underexplored. The challenges stem from a significantly larger sampling\nspace, which complicates the alignment between the draft and target model\noutputs, coupled with the inadequate use of the two-dimensional spatial\nstructure inherent in images, thereby limiting the modeling of local\ndependencies. To overcome these challenges, we introduce Hawk, a new approach\nthat harnesses the spatial structure of images to guide the speculative model\ntoward more accurate and efficient predictions. Experimental results on\nmultiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR\nmodels, while preserving both image fidelity and diversity.\n","authors":["Zhi-Kai Chen","Jun-Peng Jiang","Han-Jia Ye","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2510.25739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25731v1","updated":"2025-10-29T17:37:27Z","published":"2025-10-29T17:37:27Z","title":"LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries","summary":"  We introduce a method for efficiently solving initial-boundary value problems\n(IBVPs) that uses Lie symmetries to enforce the associated partial differential\nequation (PDE) exactly by construction. By leveraging symmetry transformations,\nthe model inherently incorporates the physical laws and learns solutions from\ninitial and boundary data. As a result, the loss directly measures the model's\naccuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our\nmethod enables rigorous error estimation. The approach yields compact models,\nfacilitating an efficient optimization. We implement LieSolver and demonstrate\nits application to linear homogeneous PDEs with a range of initial conditions,\nshowing that it is faster and more accurate than physics-informed neural\nnetworks (PINNs). Overall, our method improves both computational efficiency\nand the reliability of predictions for PDE-constrained problems.\n","authors":["René P. Klausen","Ivan Timofeev","Johannes Frank","Jonas Naujoks","Thomas Wiegand","Sebastian Lapuschkin","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2510.25731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25729v1","updated":"2025-10-29T17:34:10Z","published":"2025-10-29T17:34:10Z","title":"Physics-Guided Conditional Diffusion Networks for Microwave Image\n  Reconstruction","summary":"  A conditional latent-diffusion based framework for solving the\nelectromagnetic inverse scattering problem associated with microwave imaging is\nintroduced. This generative machine-learning model explicitly mirrors the\nnon-uniqueness of the ill-posed inverse problem. Unlike existing inverse\nsolvers utilizing deterministic machine learning techniques that produce a\nsingle reconstruction, the proposed latent-diffusion model generates multiple\nplausible permittivity maps conditioned on measured scattered-field data,\nthereby generating several potential instances in the range-space of the\nnon-unique inverse mapping. A forward electromagnetic solver is integrated into\nthe reconstruction pipeline as a physics-based evaluation mechanism. The space\nof candidate reconstructions form a distribution of possibilities consistent\nwith the conditioning data and the member of this space yielding the lowest\nscattered-field data discrepancy between the predicted and measured scattered\nfields is reported as the final solution. Synthetic and experimental labeled\ndatasets are used for training and evaluation of the model. An innovative\nlabeled synthetic dataset is created that exemplifies a varied set of\nscattering features. Training of the model using this new dataset produces high\nquality permittivity reconstructions achieving improved generalization with\nexcellent fidelity to shape recognition. The results highlight the potential of\nhybrid generative physics frameworks as a promising direction for robust,\ndata-driven microwave imaging.\n","authors":["Shirin Chehelgami","Joe LoVetri","Vahab Khoshdel"],"pdf_url":"https://arxiv.org/pdf/2510.25729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.21355v2","updated":"2025-10-29T17:23:18Z","published":"2025-06-26T15:08:18Z","title":"SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context\n  Learning","summary":"  Multimodal in-context learning (ICL) remains underexplored despite\nsignificant potential for domains such as medicine. Clinicians routinely\nencounter diverse, specialized tasks requiring adaptation from limited\nexamples, such as drawing insights from a few relevant prior cases or\nconsidering a constrained set of differential diagnoses. While multimodal large\nlanguage models (MLLMs) have shown advances in medical visual question\nanswering (VQA), their ability to learn multimodal tasks from context is\nlargely unknown. We introduce SMMILE, the first expert-driven multimodal ICL\nbenchmark for medical tasks. Eleven medical experts curated problems, each\nincluding a multimodal query and multimodal in-context examples as task\ndemonstrations. SMMILE encompasses 111 problems (517 question-image-answer\ntriplets) covering 6 medical specialties and 13 imaging modalities. We further\nintroduce SMMILE++, an augmented variant with 1038 permuted problems. A\ncomprehensive evaluation of 15 MLLMs demonstrates that most models exhibit\nmoderate to poor multimodal ICL ability in medical tasks. In open-ended\nevaluations, ICL contributes only an 8% average improvement over zero-shot on\nSMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant\nin-context examples: even a single noisy or irrelevant example can degrade\nperformance by up to 9.5%. Moreover, we observe that MLLMs are affected by a\nrecency bias, where placing the most relevant example last can lead to\nsubstantial performance improvements of up to 71%. Our findings highlight\ncritical limitations and biases in current MLLMs when learning multimodal\nmedical tasks from context. SMMILE is available at\nhttps://smmile-benchmark.github.io.\n","authors":["Melanie Rieff","Maya Varma","Ossian Rabow","Subathra Adithan","Julie Kim","Ken Chang","Hannah Lee","Nidhi Rohatgi","Christian Bluethgen","Mohamed S. Muneer","Jean-Benoit Delbrouck","Michael Moor"],"pdf_url":"https://arxiv.org/pdf/2506.21355v2.pdf","comment":"NeurIPS 2025 (Datasets & Benchmarks Track)"},{"id":"http://arxiv.org/abs/2510.25818v1","updated":"2025-10-29T17:17:32Z","published":"2025-10-29T17:17:32Z","title":"ScaleDiff: Higher-Resolution Image Synthesis via Efficient and\n  Model-Agnostic Diffusion","summary":"  Text-to-image diffusion models often exhibit degraded performance when\ngenerating images beyond their training resolution. Recent training-free\nmethods can mitigate this limitation, but they often require substantial\ncomputation or are incompatible with recent Diffusion Transformer models. In\nthis paper, we propose ScaleDiff, a model-agnostic and highly efficient\nframework for extending the resolution of pretrained diffusion models without\nany additional training. A core component of our framework is Neighborhood\nPatch Attention (NPA), an efficient mechanism that reduces computational\nredundancy in the self-attention layer with non-overlapping patches. We\nintegrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing\n(LFM) to better generate fine details. Furthermore, we apply Structure Guidance\nto enhance global structure during the denoising process. Experimental results\ndemonstrate that ScaleDiff achieves state-of-the-art performance among\ntraining-free methods in terms of both image quality and inference speed on\nboth U-Net and Diffusion Transformer architectures.\n","authors":["Sungho Koh","SeungJu Cha","Hyunwoo Oh","Kwanyoung Lee","Dong-Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2510.25818v1.pdf","comment":"NeurIPS 2025. Code: https://github.com/KSH00906/ScaleDiff"},{"id":"http://arxiv.org/abs/2502.02270v2","updated":"2025-10-29T17:15:10Z","published":"2025-02-04T12:31:00Z","title":"Exact Sequence Interpolation with Transformers","summary":"  We prove that transformers can exactly interpolate datasets of finite input\nsequences in $\\mathbb{R}^d$, $d\\geq 2$, with corresponding output sequences of\nsmaller or equal length. Specifically, given $N$ sequences of arbitrary but\nfinite lengths in $\\mathbb{R}^d$ and output sequences of lengths $m^1, \\dots,\nm^N \\in \\mathbb{N}$, we construct a transformer with $\\mathcal{O}(\\sum_{j=1}^N\nm^j)$ blocks and $\\mathcal{O}(d \\sum_{j=1}^N m^j)$ parameters that exactly\ninterpolates the dataset. Our construction provides complexity estimates that\nare independent of the input sequence length, by alternating feed-forward and\nself-attention layers and by capitalizing on the clustering effect inherent to\nthe latter. Our novel constructive method also uses low-rank parameter matrices\nin the self-attention mechanism, a common feature of practical transformer\nimplementations. These results are first established in the hardmax\nself-attention setting, where the geometric structure permits an explicit and\nquantitative analysis, and are then extended to the softmax setting. Finally,\nwe demonstrate the applicability of our exact interpolation construction to\nlearning problems, in particular by providing convergence guarantees to a\nglobal minimizer under regularized training strategies. Our analysis\ncontributes to the theoretical understanding of transformer models, offering an\nexplanation for their excellent performance in exact sequence-to-sequence\ninterpolation tasks.\n","authors":["Albert Alcalde","Giovanni Fantuzzi","Enrique Zuazua"],"pdf_url":"https://arxiv.org/pdf/2502.02270v2.pdf","comment":"27 pages, 9 figures. Funded by the European Union (Horizon Europe\n  MSCA project ModConFlex, grant number 101073558)"},{"id":"http://arxiv.org/abs/2510.25704v1","updated":"2025-10-29T17:12:21Z","published":"2025-10-29T17:12:21Z","title":"Scaling flow-based approaches for topology sampling in $\\mathrm{SU}(3)$\n  gauge theory","summary":"  We develop a methodology based on out-of-equilibrium simulations to mitigate\ntopological freezing when approaching the continuum limit of lattice gauge\ntheories. We reduce the autocorrelation of the topological charge employing\nopen boundary conditions, while removing exactly their unphysical effects using\na non-equilibrium Monte Carlo approach in which periodic boundary conditions\nare gradually switched on. We perform a detailed analysis of the computational\ncosts of this strategy in the case of the four-dimensional $\\mathrm{SU}(3)$\nYang-Mills theory. After achieving full control of the scaling, we outline a\nclear strategy to sample topology efficiently in the continuum limit, which we\ncheck at lattice spacings as small as $0.045$ fm. We also generalize this\napproach by designing a customized Stochastic Normalizing Flow for evolutions\nin the boundary conditions, obtaining superior performances with respect to the\npurely stochastic non-equilibrium approach, and paving the way for more\nefficient future flow-based solutions.\n","authors":["Claudio Bonanno","Andrea Bulgarelli","Elia Cellini","Alessandro Nada","Dario Panfalone","Davide Vadacchino","Lorenzo Verzichelli"],"pdf_url":"https://arxiv.org/pdf/2510.25704v1.pdf","comment":"1+39 pages, 14 figures"},{"id":"http://arxiv.org/abs/2310.02806v3","updated":"2025-10-29T17:05:43Z","published":"2023-10-04T13:33:37Z","title":"MP-FVM: Enhancing Finite Volume Method for Water Infiltration Modeling\n  in Unsaturated Soils via Message-passing Encoder-decoder Network","summary":"  The spatiotemporal water flow dynamics in unsaturated soils can generally be\nmodeled by the Richards equation. To overcome the computational challenges\nassociated with solving this highly nonlinear partial differential equation\n(PDE), we present a novel solution algorithm, which we name as the MP-FVM\n(Message Passing-Finite Volume Method), to holistically integrate adaptive\nfixed-point iteration scheme, encoder-decoder neural network architecture,\nSobolev training, and message passing mechanism in a finite volume\ndiscretization framework. We thoroughly discuss the need and benefits of\nintroducing these components to achieve synergistic improvements in accuracy\nand stability of the solution. We also show that our MP-FVM algorithm can\naccurately solve the mixed-form $n$-dimensional Richards equation with\nguaranteed convergence under reasonable assumptions. Through several\nillustrative examples, we demonstrate that our MP-FVM algorithm not only\nachieves superior accuracy, but also better preserves the underlying physical\nlaws and mass conservation of the Richards equation compared to\nstate-of-the-art solution algorithms and the commercial HYDRUS solver.\n","authors":["Zeyuan Song","Zheyu Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.02806v3.pdf","comment":"36 pages, 14 figures, Accepted by Computers and Geotechnics"},{"id":"http://arxiv.org/abs/2510.25696v1","updated":"2025-10-29T17:00:36Z","published":"2025-10-29T17:00:36Z","title":"Convolutional Spiking-based GRU Cell for Spatio-temporal Data","summary":"  Spike-based temporal messaging enables SNNs to efficiently process both\npurely temporal and spatio-temporal time-series or event-driven data. Combining\nSNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,\ngives rise to a robust framework for sequential data processing; however,\ntraditional RNNs often lose local details when handling long sequences.\nPrevious approaches, such as SpikGRU, fail to capture fine-grained local\ndependencies in event-based spatio-temporal data. In this paper, we introduce\nthe Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional\noperations to preserve local structure and dependencies while integrating the\ntemporal precision of spiking neurons with the efficient gating mechanisms of\nGRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,\nSHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our\nexperiments show that CS-GRU outperforms state-of-the-art GRU variants by an\naverage of 4.35%, achieving over 90% accuracy on sequential tasks and up to\n99.31% on MNIST. It is worth noting that our solution achieves 69% higher\nefficiency compared to SpikGRU. The code is available at:\nhttps://github.com/YesmineAbdennadher/CS-GRU.\n","authors":["Yesmine Abdennadher","Eleonora Cicciarella","Michele Rossi"],"pdf_url":"https://arxiv.org/pdf/2510.25696v1.pdf","comment":"6 pages, 1 figure. Published in 2025 IEEE International Workshop On\n  Machine Learning for Signal Processing, Aug. 31-Sep. 3, 2025, Istanbul,\n  Turkey"},{"id":"http://arxiv.org/abs/2510.23323v2","updated":"2025-10-29T16:59:39Z","published":"2025-10-24T14:47:49Z","title":"Towards Scaling Deep Neural Networks with Predictive Coding: Theory and\n  Practice","summary":"  Backpropagation (BP) is the standard algorithm for training the deep neural\nnetworks that power modern artificial intelligence including large language\nmodels. However, BP is energy inefficient and unlikely to be implemented by the\nbrain. This thesis studies an alternative, potentially more efficient\nbrain-inspired algorithm called predictive coding (PC). Unlike BP, PC networks\n(PCNs) perform inference by iterative equilibration of neuron activities before\nlearning or weight updates. Recent work has suggested that this iterative\ninference procedure provides a range of benefits over BP, such as faster\ntraining. However, these advantages have not been consistently observed, the\ninference and learning dynamics of PCNs are still poorly understood, and deep\nPCNs remain practically untrainable. Here, we make significant progress towards\nscaling PCNs by taking a theoretical approach grounded in optimisation theory.\nFirst, we show that the learning dynamics of PC can be understood as an\napproximate trust-region method using second-order information, despite\nexplicitly using only first-order local updates. Second, going beyond this\napproximation, we show that PC can in principle make use of arbitrarily\nhigher-order information, such that for feedforward networks the effective\nlandscape on which PC learns is far more benign and robust to vanishing\ngradients than the (mean squared error) loss landscape. Third, motivated by a\nstudy of the inference dynamics of PCNs, we propose a new parameterisation\ncalled \"$\\mu$PC\", which for the first time allows stable training of 100+ layer\nnetworks with little tuning and competitive performance on simple tasks.\nOverall, this thesis significantly advances our fundamental understanding of\nthe inference and learning dynamics of PCNs, while highlighting the need for\nfuture research to focus on hardware co-design if PC is to compete with BP at\nscale.\n","authors":["Francesco Innocenti"],"pdf_url":"https://arxiv.org/pdf/2510.23323v2.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2510.25693v1","updated":"2025-10-29T16:57:54Z","published":"2025-10-29T16:57:54Z","title":"PyDPF: A Python Package for Differentiable Particle Filtering","summary":"  State-space models (SSMs) are a widely used tool in time series analysis. In\nthe complex systems that arise from real-world data, it is common to employ\nparticle filtering (PF), an efficient Monte Carlo method for estimating the\nhidden state corresponding to a sequence of observations. Applying particle\nfiltering requires specifying both the parametric form and the parameters of\nthe system, which are often unknown and must be estimated. Gradient-based\noptimisation techniques cannot be applied directly to standard particle\nfilters, as the filters themselves are not differentiable. However, several\nrecently proposed methods modify the resampling step to make particle filtering\ndifferentiable. In this paper, we present an implementation of several such\ndifferentiable particle filters (DPFs) with a unified API built on the popular\nPyTorch framework. Our implementation makes these algorithms easily accessible\nto a broader research community and facilitates straightforward comparison\nbetween them. We validate our framework by reproducing experiments from several\nexisting studies and demonstrate how DPFs can be applied to address several\ncommon challenges with state space modelling.\n","authors":["John-Joseph Brady","Benjamin Cox","Víctor Elvira","Yunpeng Li"],"pdf_url":"https://arxiv.org/pdf/2510.25693v1.pdf","comment":"42 pages, 0 figures, under review at the Journal of Statistical\n  Software, the python package can be found at https://pypi.org/project/pydpf/\n  , the full documentation at\n  https://python-dpf.readthedocs.io/en/latest/#documentation-index , and the\n  source code including experiments and replication material at\n  https://github.com/John-JoB/pydpf"},{"id":"http://arxiv.org/abs/2510.25692v1","updated":"2025-10-29T16:57:33Z","published":"2025-10-29T16:57:33Z","title":"A Configuration-First Framework for Reproducible, Low-Code Localization","summary":"  Machine learning is increasingly permeating radio-based localization\nservices. To keep results credible and comparable, everyday workflows should\nmake rigorous experiment specification and exact repeatability the default,\nwithout blocking advanced experimentation. However, in practice, researchers\nface a three-way gap that could be filled by a framework that offers (i) low\ncoding effort for end-to-end studies, (ii) reproducibility by default including\nversioned code, data, and configurations, controlled randomness, isolated runs,\nand recorded artifacts, and (iii) built-in extensibility so new models,\nmetrics, and stages can be added with minimal integration effort. Existing\ntools rarely deliver all three for machine learning in general and localization\nworkflows in particular. In this paper we introduce LOCALIZE, a low-code,\nconfiguration-first framework for radio localization in which experiments are\ndeclared in human-readable configuration, a workflow orchestrator runs\nstandardized pipelines from data preparation to reporting, and all artifacts,\nsuch as datasets, models, metrics, and reports, are versioned. The\npreconfigured, versioned datasets reduce initial setup and boilerplate,\nspeeding up model development and evaluation. The design, with clear extension\npoints, allows experts to add components without reworking the infrastructure.\nIn a qualitative comparison and a head-to-head study against a plain Jupyter\nnotebook baseline, we show that the framework reduces authoring effort while\nmaintaining comparable runtime and memory behavior. Furthermore, using a\nBluetooth Low Energy dataset, we show that scaling across training data (1x to\n10x) keeps orchestration overheads bounded as data grows. Overall, the\nframework makes reproducible machine-learning-based localization\nexperimentation practical, accessible, and extensible.\n","authors":["Tim Strnad","Blaž Bertalanič","Carolina Fortuna"],"pdf_url":"https://arxiv.org/pdf/2510.25692v1.pdf","comment":"20 pages, 7 figures. Preprint submitted to ACM Transactions on\n  Software Engineering and Methodology (TOSEM), 2025"},{"id":"http://arxiv.org/abs/2510.25687v1","updated":"2025-10-29T16:50:54Z","published":"2025-10-29T16:50:54Z","title":"Model Inversion Attacks Meet Cryptographic Fuzzy Extractors","summary":"  Model inversion attacks pose an open challenge to privacy-sensitive\napplications that use machine learning (ML) models. For example, face\nauthentication systems use modern ML models to compute embedding vectors from\nface images of the enrolled users and store them. If leaked, inversion attacks\ncan accurately reconstruct user faces from the leaked vectors. There is no\nsystematic characterization of properties needed in an ideal defense against\nmodel inversion, even for the canonical example application of a face\nauthentication system susceptible to data breaches, despite a decade of\nbest-effort solutions.\n  In this paper, we formalize the desired properties of a provably strong\ndefense against model inversion and connect it, for the first time, to the\ncryptographic concept of fuzzy extractors. We further show that existing fuzzy\nextractors are insecure for use in ML-based face authentication. We do so\nthrough a new model inversion attack called PIPE, which achieves a success rate\nof over 89% in most cases against prior schemes. We then propose L2FE-Hash, the\nfirst candidate fuzzy extractor which supports standard Euclidean distance\ncomparators as needed in many ML-based applications, including face\nauthentication. We formally characterize its computational security guarantees,\neven in the extreme threat model of full breach of stored secrets, and\nempirically show its usable accuracy in face authentication for practical face\ndistributions. It offers attack-agnostic security without requiring any\nre-training of the ML model it protects. Empirically, it nullifies both prior\nstate-of-the-art inversion attacks as well as our new PIPE attack.\n","authors":["Mallika Prabhakar","Louise Xu","Prateek Saxena"],"pdf_url":"https://arxiv.org/pdf/2510.25687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02804v3","updated":"2025-10-29T16:49:31Z","published":"2023-12-05T14:44:58Z","title":"Score-Aware Policy-Gradient and Performance Guarantees using Local\n  Lyapunov Stability","summary":"  In this paper, we introduce a policy-gradient method for model-based\nreinforcement learning (RL) that exploits a type of stationary distributions\ncommonly obtained from Markov decision processes (MDPs) in stochastic networks,\nqueueing systems, and statistical mechanics. Specifically, when the stationary\ndistribution of the MDP belongs to an exponential family that is parametrized\nby policy parameters, we can improve existing policy gradient methods for\naverage-reward RL. Our key identification is a family of gradient estimators,\ncalled score-aware gradient estimators (SAGEs), that enable policy gradient\nestimation without relying on value-function estimation in the aforementioned\nsetting. We show that SAGE-based policy-gradient locally converges, and we\nobtain its regret. This includes cases when the state space of the MDP is\ncountable and unstable policies can exist. Under appropriate assumptions such\nas starting sufficiently close to a maximizer and the existence of a local\nLyapunov function, the policy under SAGE-based stochastic gradient ascent has\nan overwhelming probability of converging to the associated optimal policy.\nFurthermore, we conduct a numerical comparison between a SAGE-based\npolicy-gradient method and an actor-critic method on several examples inspired\nfrom stochastic networks, queueing systems, and models derived from statistical\nphysics. Our results demonstrate that a SAGE-based method finds\nclose-to-optimal policies faster than an actor-critic method.\n","authors":["Céline Comte","Matthieu Jonckheere","Jaron Sanders","Albert Senen-Cerda"],"pdf_url":"https://arxiv.org/pdf/2312.02804v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25683v1","updated":"2025-10-29T16:47:24Z","published":"2025-10-29T16:47:24Z","title":"Graph Network-based Structural Simulator: Graph Neural Networks for\n  Structural Dynamics","summary":"  Graph Neural Networks (GNNs) have recently been explored as surrogate models\nfor numerical simulations. While their applications in computational fluid\ndynamics have been investigated, little attention has been given to structural\nproblems, especially for dynamic cases. To address this gap, we introduce the\nGraph Network-based Structural Simulator (GNSS), a GNN framework for surrogate\nmodeling of dynamic structural problems.\n  GNSS follows the encode-process-decode paradigm typical of GNN-based machine\nlearning models, and its design makes it particularly suited for dynamic\nsimulations thanks to three key features: (i) expressing node kinematics in\nnode-fixed local frames, which avoids catastrophic cancellation in\nfinite-difference velocities; (ii) employing a sign-aware regression loss,\nwhich reduces phase errors in long rollouts; and (iii) using a\nwavelength-informed connectivity radius, which optimizes graph construction.\n  We evaluate GNSS on a case study involving a beam excited by a 50kHz\nHanning-modulated pulse. The results show that GNSS accurately reproduces the\nphysics of the problem over hundreds of timesteps and generalizes to unseen\nloading conditions, where existing GNNs fail to converge or deliver meaningful\npredictions.\n  Compared with explicit finite element baselines, GNSS achieves substantial\ninference speedups while preserving spatial and temporal fidelity. These\nfindings demonstrate that locality-preserving GNNs with physics-consistent\nupdate rules are a competitive alternative for dynamic, wave-dominated\nstructural simulations.\n","authors":["Alessandro Lucchetti","Francesco Cadini","Marco Giglio","Luca Lomazzi"],"pdf_url":"https://arxiv.org/pdf/2510.25683v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2510.25674v1","updated":"2025-10-29T16:42:07Z","published":"2025-10-29T16:42:07Z","title":"Mechanistic Interpretability of RNNs emulating Hidden Markov Models","summary":"  Recurrent neural networks (RNNs) provide a powerful approach in neuroscience\nto infer latent dynamics in neural populations and to generate hypotheses about\nthe neural computations underlying behavior. However, past work has focused on\nrelatively simple, input-driven, and largely deterministic behaviors - little\nis known about the mechanisms that would allow RNNs to generate the richer,\nspontaneous, and potentially stochastic behaviors observed in natural settings.\nModeling with Hidden Markov Models (HMMs) has revealed a segmentation of\nnatural behaviors into discrete latent states with stochastic transitions\nbetween them, a type of dynamics that may appear at odds with the continuous\nstate spaces implemented by RNNs. Here we first show that RNNs can replicate\nHMM emission statistics and then reverse-engineer the trained networks to\nuncover the mechanisms they implement. In the absence of inputs, the activity\nof trained RNNs collapses towards a single fixed point. When driven by\nstochastic input, trajectories instead exhibit noise-sustained dynamics along\nclosed orbits. Rotation along these orbits modulates the emission probabilities\nand is governed by transitions between regions of slow, noise-driven dynamics\nconnected by fast, deterministic transitions. The trained RNNs develop highly\nstructured connectivity, with a small set of \"kick neurons\" initiating\ntransitions between these regions. This mechanism emerges during training as\nthe network shifts into a regime of stochastic resonance, enabling it to\nperform probabilistic computations. Analyses across multiple HMM architectures\n- fully connected, cyclic, and linear-chain - reveal that this solution\ngeneralizes through the modular reuse of the same dynamical motif, suggesting a\ncompositional principle by which RNNs can emulate complex discrete latent\ndynamics.\n","authors":["Elia Torre","Michele Viscione","Lucas Pompe","Benjamin F Grewe","Valerio Mante"],"pdf_url":"https://arxiv.org/pdf/2510.25674v1.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.25816v1","updated":"2025-10-29T16:41:44Z","published":"2025-10-29T16:41:44Z","title":"Beyond Long Context: When Semantics Matter More than Tokens","summary":"  Electronic Health Records (EHR) store clinical documentation as base64\nencoded attachments in FHIR DocumentReference resources, which makes semantic\nquestion answering difficult. Traditional vector database methods often miss\nnuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)\nmethod, introduced by Lopez et al. 2025, uses entity aware retrieval and\nachieved improved performance with an F1 score of 0.90 versus 0.86 for\nembedding based retrieval, while using over 70 percent fewer tokens. We\ndeveloped a Clinical Notes QA Evaluation Platform to validate CLEAR against\nzero shot large context inference and traditional chunk based retrieval\naugmented generation. The platform was tested on 12 clinical notes ranging from\n10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a\n58.3 percent win rate, an average semantic similarity of 0.878, and used 78\npercent fewer tokens than wide context processing. The largest performance\ngains occurred on long notes, with a 75 percent win rate for documents\nexceeding 65,000 tokens. These findings confirm that entity aware retrieval\nimproves both efficiency and accuracy in clinical natural language processing.\nThe evaluation framework provides a reusable and transparent benchmark for\nassessing clinical question answering systems where semantic precision and\ncomputational efficiency are critical.\n","authors":["Tarun Kumar Chawdhury","Jon D. Duke"],"pdf_url":"https://arxiv.org/pdf/2510.25816v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.25670v1","updated":"2025-10-29T16:36:00Z","published":"2025-10-29T16:36:00Z","title":"Spectral Perturbation Bounds for Low-Rank Approximation with\n  Applications to Privacy","summary":"  A central challenge in machine learning is to understand how noise or\nmeasurement errors affect low-rank approximations, particularly in the spectral\nnorm. This question is especially important in differentially private low-rank\napproximation, where one aims to preserve the top-$p$ structure of a\ndata-derived matrix while ensuring privacy. Prior work often analyzes Frobenius\nnorm error or changes in reconstruction quality, but these metrics can over- or\nunder-estimate true subspace distortion. The spectral norm, by contrast,\ncaptures worst-case directional error and provides the strongest utility\nguarantees. We establish new high-probability spectral-norm perturbation bounds\nfor symmetric matrices that refine the classical Eckart--Young--Mirsky theorem\nand explicitly capture interactions between a matrix $A \\in \\mathbb{R}^{n\n\\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and\nnorm conditions, our bounds yield sharp estimates for $\\|(A + E)_p - A_p\\|$,\nwhere $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up\nto a factor of $\\sqrt{n}$. As an application, we derive improved utility\nguarantees for differentially private PCA, resolving an open problem in the\nliterature. Our analysis relies on a novel contour bootstrapping method from\ncomplex analysis and extends it to a broad class of spectral functionals,\nincluding polynomials and matrix exponentials. Empirical results on real-world\ndatasets confirm that our bounds closely track the actual spectral error under\ndiverse perturbation regimes.\n","authors":["Phuc Tran","Nisheeth K. Vishnoi","Van H. Vu"],"pdf_url":"https://arxiv.org/pdf/2510.25670v1.pdf","comment":"NeurIPS 2025"}]},"2025-10-28T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2510.25025v1","updated":"2025-10-28T22:54:19Z","published":"2025-10-28T22:54:19Z","title":"Secure Retrieval-Augmented Generation against Poisoning Attacks","summary":"  Large language models (LLMs) have transformed natural language processing\n(NLP), enabling applications from content generation to decision support.\nRetrieval-Augmented Generation (RAG) improves LLMs by incorporating external\nknowledge but also introduces security risks, particularly from data poisoning,\nwhere the attacker injects poisoned texts into the knowledge database to\nmanipulate system outputs. While various defenses have been proposed, they\noften struggle against advanced attacks. To address this, we introduce RAGuard,\na detection framework designed to identify poisoned texts. RAGuard first\nexpands the retrieval scope to increase the proportion of clean texts, reducing\nthe likelihood of retrieving poisoned content. It then applies chunk-wise\nperplexity filtering to detect abnormal variations and text similarity\nfiltering to flag highly similar texts. This non-parametric approach enhances\nRAG security, and experiments on large-scale datasets demonstrate its\neffectiveness in detecting and mitigating poisoning attacks, including strong\nadaptive attacks.\n","authors":["Zirui Cheng","Jikai Sun","Anjun Gao","Yueyang Quan","Zhuqing Liu","Xiaohua Hu","Minghong Fang"],"pdf_url":"https://arxiv.org/pdf/2510.25025v1.pdf","comment":"To appear in IEEE BigData 2025"},{"id":"http://arxiv.org/abs/2510.24870v1","updated":"2025-10-28T18:21:19Z","published":"2025-10-28T18:21:19Z","title":"Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented\n  Generation","summary":"  We introduce MiRAGE, an evaluation framework for retrieval-augmented\ngeneration (RAG) from multimodal sources. As audiovisual media becomes a\nprevalent source of information online, it is essential for RAG systems to\nintegrate information from these sources into generation. However, existing\nevaluations for RAG are text-centric, limiting their applicability to\nmultimodal, reasoning intensive settings because they don't verify information\nagainst sources. MiRAGE is a claim-centric approach to multimodal RAG\nevaluation, consisting of InfoF1, evaluating factuality and information\ncoverage, and CiteF1, measuring citation support and completeness. We show that\nMiRAGE, when applied by humans, strongly aligns with extrinsic quality\njudgments. We additionally introduce automatic variants of MiRAGE and three\nprominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the\nlimitations of text-centric work and laying the groundwork for automatic\nevaluation. We release open-source implementations and outline how to assess\nmultimodal RAG.\n","authors":["Alexander Martin","William Walden","Reno Kriz","Dengjia Zhang","Kate Sanders","Eugene Yang","Chihsheng Jin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2510.24870v1.pdf","comment":"https://github.com/alexmartin1722/mirage"},{"id":"http://arxiv.org/abs/2510.24701v1","updated":"2025-10-28T17:53:02Z","published":"2025-10-28T17:53:02Z","title":"Tongyi DeepResearch Technical Report","summary":"  We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.\n","authors":[" Tongyi DeepResearch Team","Baixuan Li","Bo Zhang","Dingchu Zhang","Fei Huang","Guangyu Li","Guoxin Chen","Huifeng Yin","Jialong Wu","Jingren Zhou","Kuan Li","Liangcai Su","Litu Ou","Liwen Zhang","Pengjun Xie","Rui Ye","Wenbiao Yin","Xinmiao Yu","Xinyu Wang","Xixi Wu","Xuanzhong Chen","Yida Zhao","Zhen Zhang","Zhengwei Tao","Zhongwang Zhang","Zile Qiao","Chenxi Wang","Donglei Yu","Gang Fu","Haiyang Shen","Jiayin Yang","Jun Lin","Junkai Zhang","Kui Zeng","Li Yang","Hailong Yin","Maojia Song","Ming Yan","Peng Xia","Qian Xiao","Rui Min","Ruixue Ding","Runnan Fang","Shaowei Chen","Shen Huang","Shihang Wang","Shihao Cai","Weizhou Shen","Xiaobin Wang","Xin Guan","Xinyu Geng","Yingcheng Shi","Yuning Wu","Zhuo Chen","Zijian Li","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2510.24701v1.pdf","comment":"https://tongyi-agent.github.io/blog"},{"id":"http://arxiv.org/abs/2510.24652v1","updated":"2025-10-28T17:18:30Z","published":"2025-10-28T17:18:30Z","title":"Optimizing Retrieval for RAG via Reinforced Contrastive Learning","summary":"  As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day.\n","authors":["Jiawei Zhou","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2510.24652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03187v2","updated":"2025-10-28T17:05:07Z","published":"2025-05-30T19:29:18Z","title":"Comparing Retrieval Strategies to Capture Interdisciplinary Scientific\n  Research: A Bibliometric Evaluation of the Integration of Neuroscience and\n  Computer Science","summary":"  Interdisciplinary scientific research is increasingly important in knowledge\nproduction, funding policies, and academic discussions on scholarly\ncommunication. While many studies focus on interdisciplinary corpora defined a\npriori -- usually through keyword-based searches within assumed\ninterdisciplinary domains -- few explore interdisciplinarity as an emergent\nintersection between two distinct fields. Thus, methodological proposals for\nbuilding databases at the intersection of two fields of knowledge are scarce.\nThe goal of this article is to develop and compare different strategies for\ndefining an interdisciplinary corpus between two bodies of knowledge. As a case\nstudy, we focus on the intersection between neuroscience and computer science.\nTo this end, we develop and compare four retrieval strategies, two of them\nbased on keywords and two based on citation and reference patterns. Our results\nshow that the reference-based strategy provides better retrieval, pseudorecall,\nand F1. While we focus on comparing strategies for the study of the\nintersection between the fields of neuroscience and computer science, this\nmethodological reflection is applicable to a wide range of interdisciplinary\ndomains.\n","authors":["Malena Mendez Isla","Agustin Mauro","Diego Kozlowski"],"pdf_url":"https://arxiv.org/pdf/2506.03187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21729v2","updated":"2025-10-28T16:15:47Z","published":"2025-09-30T00:25:47Z","title":"CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known\n  Document Corpora","summary":"  Dense embedding models have become critical for modern information retrieval,\nparticularly in RAG pipelines, but their performance often degrades when\napplied to specialized corpora outside their pre-training distribution. To\naddress thi we introduce CustomIR, a framework for unsupervised adaptation of\npre-trained language embedding models to domain-specific corpora using\nsynthetically generated query-document pairs. CustomIR leverages large language\nmodels (LLMs) to create diverse queries grounded in a known target corpus,\npaired with LLM-verified hard negatives, eliminating the need for costly human\nannotation. Experiments on enterprise email and messaging datasets show that\nCustomIR consistently improves retrieval effectiveness with small models\ngaining up to 2.3 points in Recall@10. This performance increase allows these\nsmall models to rival the performance of much larger alternatives, allowing for\ncheaper RAG deployments. These results highlight that targeted synthetic\nfine-tuning offers a scalable and cost-efficient strategy for increasing\ndomain-specific performance.\n","authors":["Nathan Paull"],"pdf_url":"https://arxiv.org/pdf/2510.21729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24469v1","updated":"2025-10-28T14:36:22Z","published":"2025-10-28T14:36:22Z","title":"Iterative Critique-Refine Framework for Enhancing LLM Personalization","summary":"  Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic.\n","authors":["Durga Prasad Maram","Dhruvin Gandhi","Zonghai Yao","Gayathri Akkinapalli","Franck Dernoncourt","Yu Wang","Ryan A. Rossi","Nesreen K. Ahmed"],"pdf_url":"https://arxiv.org/pdf/2510.24469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24431v1","updated":"2025-10-28T13:58:36Z","published":"2025-10-28T13:58:36Z","title":"MiniOneRec: An Open-Source Framework for Scaling Generative\n  Recommendation","summary":"  The recent success of large language models (LLMs) has renewed interest in\nwhether recommender systems can achieve similar scaling benefits. Conventional\nrecommenders, dominated by massive embedding tables, tend to plateau as\nembedding dimensions grow. In contrast, the emerging generative paradigm\nreplaces embeddings with compact Semantic ID (SID) sequences produced by\nautoregressive Transformers. Yet most industrial deployments remain\nproprietary, leaving two fundamental questions open: (1) Do the expected\nscaling laws hold on public benchmarks? (2) What is the minimal post-training\nrecipe that enables competitive performance?\n  We present MiniOneRec, to the best of our knowledge, the first fully\nopen-source generative recommendation framework, which provides an end-to-end\nworkflow spanning SID construction, supervised fine-tuning, and\nrecommendation-oriented reinforcement learning. We generate SIDs via a Residual\nQuantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters\non the Amazon Review dataset. Our experiments reveal a consistent downward\ntrend in both training and evaluation losses with increasing model size,\nvalidating the parameter efficiency of the generative approach. To further\nenhance performance, we propose a lightweight yet effective post-training\npipeline that (1) enforces full-process SID alignment and (2) applies\nreinforcement learning with constrained decoding and hybrid rewards. Together,\nthese techniques yield significant improvements in both ranking accuracy and\ncandidate diversity.\n","authors":["Xiaoyu Kong","Leheng Sheng","Junfei Tan","Yuxin Chen","Jiancan Wu","An Zhang","Xiang Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2510.24431v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2510.24430v1","updated":"2025-10-28T13:57:23Z","published":"2025-10-28T13:57:23Z","title":"From Time and Place to Preference: LLM-Driven Geo-Temporal Context in\n  Recommendations","summary":"  Most recommender systems treat timestamps as numeric or cyclical values,\noverlooking real-world context such as holidays, events, and seasonal patterns.\nWe propose a scalable framework that uses large language models (LLMs) to\ngenerate geo-temporal embeddings from only a timestamp and coarse location,\ncapturing holidays, seasonal trends, and local/global events. We then introduce\na geo-temporal embedding informativeness test as a lightweight diagnostic,\ndemonstrating on MovieLens, LastFM, and a production dataset that these\nembeddings provide predictive signal consistent with the outcomes of full model\nintegrations. Geo-temporal embeddings are incorporated into sequential models\nthrough (1) direct feature fusion with metadata embeddings or (2) an auxiliary\nloss that enforces semantic and geo-temporal alignment. Our findings highlight\nthe need for adaptive or hybrid recommendation strategies, and we release a\ncontext-enriched MovieLens dataset to support future research.\n","authors":["Yejin Kim","Shaghayegh Agah","Mayur Nankani","Neeraj Sharma","Feifei Peng","Maria Peifer","Sardar Hamidian","H Howie Huang"],"pdf_url":"https://arxiv.org/pdf/2510.24430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24402v1","updated":"2025-10-28T13:16:36Z","published":"2025-10-28T13:16:36Z","title":"Metadata-Driven Retrieval-Augmented Generation for Financial Question\n  Answering","summary":"  Retrieval-Augmented Generation (RAG) struggles on long, structured financial\nfilings where relevant evidence is sparse and cross-referenced. This paper\npresents a systematic investigation of advanced metadata-driven\nRetrieval-Augmented Generation (RAG) techniques, proposing and evaluating a\nnovel, multi-stage RAG architecture that leverages LLM-generated metadata. We\nintroduce a sophisticated indexing pipeline to create contextually rich\ndocument chunks and benchmark a spectrum of enhancements, including\npre-retrieval filtering, post-retrieval reranking, and enriched embeddings,\nbenchmarked on the FinanceBench dataset. Our results reveal that while a\npowerful reranker is essential for precision, the most significant performance\ngains come from embedding chunk metadata directly with text (\"contextual\nchunks\"). Our proposed optimal architecture combines LLM-driven pre-retrieval\noptimizations with these contextual embeddings to achieve superior performance.\nAdditionally, we present a custom metadata reranker that offers a compelling,\ncost-effective alternative to commercial solutions, highlighting a practical\ntrade-off between peak performance and operational efficiency. This study\nprovides a blueprint for building robust, metadata-aware RAG systems for\nfinancial document analysis.\n","authors":["Michail Dadopoulos","Anestis Ladas","Stratos Moschidis","Ioannis Negkakis"],"pdf_url":"https://arxiv.org/pdf/2510.24402v1.pdf","comment":"Preprint version submitted to the International Journal of Accounting\n  Information Systems; currently under major revision. 20 pages, 1 figure, 1\n  table"},{"id":"http://arxiv.org/abs/2510.14788v2","updated":"2025-10-28T12:58:38Z","published":"2025-10-16T15:20:49Z","title":"Cross-Scenario Unified Modeling of User Interests at Billion Scale","summary":"  User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms.\n","authors":["Manjie Xu","Cheng Chen","Xin Jia","Jingyi Zhou","Yongji Wu","Zejian Wang","Chi Zhang","Kai Zuo","Yibo Chen","Xu Tang","Yao Hu","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2510.14788v2.pdf","comment":"https://github.com/ariesssxu/RedSeqRec"},{"id":"http://arxiv.org/abs/2510.24369v1","updated":"2025-10-28T12:46:33Z","published":"2025-10-28T12:46:33Z","title":"DUET: Dual Model Co-Training for Entire Space CTR Prediction","summary":"  The pre-ranking stage plays a pivotal role in large-scale recommender systems\nbut faces an intrinsic trade-off between model expressiveness and computational\nefficiency. Owing to the massive candidate pool and strict latency constraints,\nindustry systems often rely on lightweight two-tower architectures, which are\ncomputationally efficient yet limited in estimation capability. As a result,\nthey struggle to capture the complex synergistic and suppressive relationships\namong candidate items, which are essential for producing contextually coherent\nand diverse recommendation lists. Moreover, this simplicity further amplifies\nthe Sample Selection Bias (SSB) problem, as coarse-grained models trained on\nbiased exposure data must generalize to a much larger candidate space with\ndistinct distributions.\n  To address these issues, we propose \\textbf{DUET} (\\textbf{DU}al Model\nCo-Training for \\textbf{E}ntire Space C\\textbf{T}R Prediction), a set-wise\npre-ranking framework that achieves expressive modeling under tight\ncomputational budgets. Instead of scoring items independently, DUET performs\nset-level prediction over the entire candidate subset in a single forward pass,\nenabling information-aware interactions among candidates while amortizing the\ncomputational cost across the set. Moreover, a dual model co-training mechanism\nextends supervision to unexposed items via mutual pseudo-label refinement,\neffectively mitigating SSB. Validated through extensive offline experiments and\nonline A/B testing, DUET consistently outperforms state-of-the-art baselines\nand achieves improvements across multiple core business metrics. At present,\nDUET has been fully deployed in Kuaishou and Kuaishou Lite Apps, serving the\nmain traffic for hundreds of millions of users.\n","authors":["Yutian Xiao","Meng Yuan","Fuzhen Zhuang","Wei Chen","Shukuan Wang","Shanqi Liu","Chao Feng","Wenhui Yu","Xiang Li","Lantao Hu","Han Li","Zhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.20900v4","updated":"2025-10-28T09:32:28Z","published":"2025-08-28T15:29:51Z","title":"OneRec-V2 Technical Report","summary":"  Recent breakthroughs in generative AI have transformed recommender systems\nthrough end-to-end generation. OneRec reformulates recommendation as an\nautoregressive generation task, achieving high Model FLOPs Utilization. While\nOneRec-V1 has shown significant empirical success in real-world deployment, two\ncritical challenges hinder its scalability and performance: (1) inefficient\ncomputational allocation where 97.66% of resources are consumed by sequence\nencoding rather than generation, and (2) limitations in reinforcement learning\nrelying solely on reward models.\n  To address these challenges, we propose OneRec-V2, featuring: (1) Lazy\nDecoder-Only Architecture: Eliminates encoder bottlenecks, reducing total\ncomputation by 94% and training resources by 90%, enabling successful scaling\nto 8B parameters. (2) Preference Alignment with Real-World User Interactions:\nIncorporates Duration-Aware Reward Shaping and Adaptive Ratio Clipping to\nbetter align with user preferences using real-world feedback.\n  Extensive A/B tests on Kuaishou demonstrate OneRec-V2's effectiveness,\nimproving App Stay Time by 0.467%/0.741% while balancing multi-objective\nrecommendations. This work advances generative recommendation scalability and\nalignment with real-world feedback, representing a step forward in the\ndevelopment of end-to-end recommender systems.\n","authors":["Guorui Zhou","Hengrui Hu","Hongtao Cheng","Huanjie Wang","Jiaxin Deng","Jinghao Zhang","Kuo Cai","Lejian Ren","Lu Ren","Liao Yu","Pengfei Zheng","Qiang Luo","Qianqian Wang","Qigen Hu","Rui Huang","Ruiming Tang","Shiyao Wang","Shujie Yang","Tao Wu","Wuchao Li","Xinchen Luo","Xingmei Wang","Yi Su","Yunfan Wu","Zexuan Cheng","Zhanyu Liu","Zixing Zhang","Bin Zhang","Boxuan Wang","Chaoyi Ma","Chengru Song","Chenhui Wang","Chenglong Chu","Di Wang","Dongxue Meng","Dunju Zang","Fan Yang","Fangyu Zhang","Feng Jiang","Fuxing Zhang","Gang Wang","Guowang Zhang","Han Li","Honghui Bao","Hongyang Cao","Jiaming Huang","Jiapeng Chen","Jiaqiang Liu","Jinghui Jia","Kun Gai","Lantao Hu","Liang Zeng","Qiang Wang","Qidong Zhou","Rongzhou Zhang","Shengzhe Wang","Shihui He","Shuang Yang","Siyang Mao","Sui Huang","Tiantian He","Tingting Gao","Wei Yuan","Xiao Liang","Xiaoxiao Xu","Xugang Liu","Yan Wang","Yang Zhou","Yi Wang","Yiwu Liu","Yue Song","Yufei Zhang","Yunfeng Zhao","Zhixin Ling","Ziming Li"],"pdf_url":"https://arxiv.org/pdf/2508.20900v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17281v2","updated":"2025-10-28T04:01:30Z","published":"2025-10-20T08:16:12Z","title":"MemoryBench: A Benchmark for Memory and Continual Learning in LLM\n  Systems","summary":"  Scaling up data, parameters, and test-time computation has been the\nmainstream methods to improve LLM systems (LLMsys), but their upper bounds are\nalmost reached due to the gradual depletion of high-quality data and marginal\ngains obtained from larger computational resource consumption. Inspired by the\nabilities of human and traditional AI systems in learning from practice,\nconstructing memory and continual learning frameworks for LLMsys has become an\nimportant and popular research direction in recent literature. Yet, existing\nbenchmarks for LLM memory often focus on evaluating the system on homogeneous\nreading comprehension tasks with long-form inputs rather than testing their\nabilities to learn from accumulated user feedback in service time. Therefore,\nwe propose a user feedback simulation framework and a comprehensive benchmark\ncovering multiple domains, languages, and types of tasks to evaluate the\ncontinual learning abilities of LLMsys. Experiments show that the effectiveness\nand efficiency of state-of-the-art baselines are far from satisfying, and we\nhope this benchmark could pave the way for future studies on LLM memory and\noptimization algorithms.\n","authors":["Qingyao Ai","Yichen Tang","Changyue Wang","Jianming Long","Weihang Su","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2510.17281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21727v2","updated":"2025-10-28T02:31:06Z","published":"2025-09-27T16:50:03Z","title":"Your Dense Retriever is Secretly an Expeditious Reasoner","summary":"  Dense retrievers enhance retrieval by encoding queries and documents into\ncontinuous vectors, but they often struggle with reasoning-intensive queries.\nAlthough Large Language Models (LLMs) can reformulate queries to capture\ncomplex reasoning, applying them universally incurs significant computational\ncost. In this work, we propose Adaptive Query Reasoning (AdaQR), a hybrid query\nrewriting framework. Within this framework, a Reasoner Router dynamically\ndirects each query to either fast dense reasoning or deep LLM reasoning. The\ndense reasoning is achieved by the Dense Reasoner, which performs LLM-style\nreasoning directly in the embedding space, enabling a controllable trade-off\nbetween efficiency and accuracy. Experiments on large-scale retrieval\nbenchmarks BRIGHT show that AdaQR reduces reasoning cost by 28% while\npreserving-or even improving-retrieval performance by 7%.\n","authors":["Yichi Zhang","Jun Bai","Zhixin Cai","Shuhan Qin","Zhuofan Chen","Jinghua Guan","Wenge Rong"],"pdf_url":"https://arxiv.org/pdf/2510.21727v2.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2510.23990v1","updated":"2025-10-28T01:49:10Z","published":"2025-10-28T01:49:10Z","title":"Resource-Efficient LLM Application for Structured Transformation of\n  Unstructured Financial Contracts","summary":"  The transformation of unstructured legal contracts into standardized,\nmachine-readable formats is essential for automating financial workflows. The\nCommon Domain Model (CDM) provides a standardized framework for this purpose,\nbut converting complex legal documents like Credit Support Annexes (CSAs) into\nCDM representations remains a significant challenge. In this paper, we present\nan extension of the CDMizer framework, a template-driven solution that ensures\nsyntactic correctness and adherence to the CDM schema during contract-to-CDM\nconversion. We apply this extended framework to a real-world task, comparing\nits performance with a benchmark developed by the International Swaps and\nDerivatives Association (ISDA) for CSA clause extraction. Our results show that\nCDMizer, when integrated with a significantly smaller, open-source Large\nLanguage Model (LLM), achieves competitive performance in terms of accuracy and\nefficiency against larger, proprietary models. This work underscores the\npotential of resource-efficient solutions to automate legal contract\ntransformation, offering a cost-effective and scalable approach that can meet\nthe needs of financial institutions with constrained resources or strict data\nprivacy requirements.\n","authors":["Maruf Ahmed Mridul","Oshani Seneviratne"],"pdf_url":"https://arxiv.org/pdf/2510.23990v1.pdf","comment":"5 pages, 1 figure, 2 tables"}],"Multimedia":[{"id":"http://arxiv.org/abs/2510.25002v1","updated":"2025-10-28T22:02:36Z","published":"2025-10-28T22:02:36Z","title":"Resi-VidTok: An Efficient and Decomposed Progressive Tokenization\n  Framework for Ultra-Low-Rate and Lightweight Video Transmission","summary":"  Real-time transmission of video over wireless networks remains highly\nchallenging, even with advanced deep models, particularly under severe channel\nconditions such as limited bandwidth and weak connectivity. In this paper, we\npropose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for\nultra-low-rate and lightweight video transmission that delivers strong\nrobustness while preserving perceptual and semantic fidelity on commodity\ndigital hardware. By reorganizing spatio--temporal content into a discrete,\nimportance-ordered token stream composed of key tokens and refinement tokens,\nResi-VidTok enables progressive encoding, prefix-decodable reconstruction, and\ngraceful quality degradation under constrained channels. A key contribution is\na resilient 1D tokenization pipeline for video that integrates differential\ntemporal token coding, explicitly supporting reliable recovery from incomplete\ntoken sets using a single shared framewise decoder--without auxiliary temporal\nextractors or heavy generative models. Furthermore, stride-controlled frame\nsparsification combined with a lightweight decoder-side interpolator reduces\ntransmission load while maintaining motion continuity. Finally, a\nchannel-adaptive source--channel coding and modulation scheme dynamically\nallocates rate and protection according to token importance and channel\ncondition, yielding stable quality across adverse SNRs. Evaluation results\nindicate robust visual and semantic consistency at channel bandwidth ratios\n(CBR) as low as 0.0004 and real-time reconstruction at over 30 fps,\ndemonstrating the practicality of Resi-VidTok for energy-efficient,\nlatency-sensitive, and reliability-critical wireless applications.\n","authors":["Zhenyu Liu","Yi Ma","Rahim Tafazolli","Zhi Ding"],"pdf_url":"https://arxiv.org/pdf/2510.25002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24827v1","updated":"2025-10-28T16:04:03Z","published":"2025-10-28T16:04:03Z","title":"MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal\n  Interaction for Multimodal Emotion Recognition","summary":"  Multimodal emotion recognition is crucial for future human-computer\ninteraction. However, accurate emotion recognition still faces significant\nchallenges due to differences between different modalities and the difficulty\nof characterizing unimodal emotional information. To solve these problems, a\nhybrid network model based on multipath cross-modal interaction (MCIHN) is\nproposed. First, adversarial autoencoders (AAE) are constructed separately for\neach modality. The AAE learns discriminative emotion features and reconstructs\nthe features through a decoder to obtain more discriminative information about\nthe emotion classes. Then, the latent codes from the AAE of different\nmodalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to\nreduce the discrepancy between modalities, establish the emotional relationship\nbetween interacting modalities, and generate the interaction features between\ndifferent modalities. Multimodal fusion using the Feature Fusion module (FFM)\nfor better emotion recognition. Experiments were conducted on publicly\navailable SIMS and MOSI datasets, demonstrating that MCIHN achieves superior\nperformance.\n","authors":["Haoyang Zhang","Zhou Yang","Ke Sun","Yucai Pang","Guoliang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24827v1.pdf","comment":"The paper will be published in the MMAsia2025 conference proceedings"},{"id":"http://arxiv.org/abs/2509.17336v2","updated":"2025-10-28T14:31:14Z","published":"2025-09-22T03:13:58Z","title":"Mano Technical Report","summary":"  Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.\n","authors":["Tianyu Fu","Anyang Su","Chenxu Zhao","Hanning Wang","Minghui Wu","Zhe Yu","Fei Hu","Mingjia Shi","Wei Dong","Jiayao Wang","Yuyang Chen","Ruiyang Yu","Siran Peng","Menglin Li","Nan Huang","Haitian Wei","Jiawei Yu","Yi Xin","Xilin Zhao","Kai Gu","Ping Jiang","Sifan Zhou","Shuo Wang"],"pdf_url":"https://arxiv.org/pdf/2509.17336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05229v2","updated":"2025-10-28T08:05:02Z","published":"2025-05-08T13:21:10Z","title":"Does CLIP perceive art the same way we do?","summary":"  CLIP has emerged as a powerful multimodal model capable of connecting images\nand text through joint embeddings, but to what extent does it 'see' the same\nway humans do - especially when interpreting artworks? In this paper, we\ninvestigate CLIP's ability to extract high-level semantic and stylistic\ninformation from paintings, including both human-created and AI-generated\nimagery. We evaluate its perception across multiple dimensions: content, scene\nunderstanding, artistic style, historical period, and the presence of visual\ndeformations or artifacts. By designing targeted probing tasks and comparing\nCLIP's responses to human annotations and expert benchmarks, we explore its\nalignment with human perceptual and contextual understanding. Our findings\nreveal both strengths and limitations in CLIP's visual representations,\nparticularly in relation to aesthetic cues and artistic intent. We further\ndiscuss the implications of these insights for using CLIP as a guidance\nmechanism during generative processes, such as style transfer or prompt-based\nimage synthesis. Our work highlights the need for deeper interpretability in\nmultimodal systems, especially when applied to creative domains where nuance\nand subjectivity play a central role.\n","authors":["Andrea Asperti","Leonardo Dessì","Maria Chiara Tonetti","Nico Wu"],"pdf_url":"https://arxiv.org/pdf/2505.05229v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24161v1","updated":"2025-10-28T07:58:39Z","published":"2025-10-28T07:58:39Z","title":"BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and\n  Cross-Embodiment Learning","summary":"  Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.\n","authors":["Wentao Tan","Bowen Wang","Heng Zhi","Chenyu Liu","Zhe Li","Jian Liu","Zengrong Lin","Yukun Dai","Yipeng Chen","Wenjie Yang","Enci Xie","Hao Xue","Baixu Ji","Chen Xu","Zhibin Wang","Tianshi Wang","Lei Zhu","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2510.24161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24103v1","updated":"2025-10-28T06:16:47Z","published":"2025-10-28T06:16:47Z","title":"Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain\n  Video-to-Audio Generation","summary":"  We present MGAudio, a novel flow-based framework for open-domain\nvideo-to-audio generation, which introduces model-guided dual-role alignment as\na central design principle. Unlike prior approaches that rely on\nclassifier-based or classifier-free guidance, MGAudio enables the generative\nmodel to guide itself through a dedicated training objective designed for\nvideo-conditioned audio generation. The framework integrates three main\ncomponents: (1) a scalable flow-based Transformer model, (2) a dual-role\nalignment mechanism where the audio-visual encoder serves both as a\nconditioning module and as a feature aligner to improve generation quality, and\n(3) a model-guided objective that enhances cross-modal coherence and audio\nrealism. MGAudio achieves state-of-the-art performance on VGGSound, reducing\nFAD to 0.40, substantially surpassing the best classifier-free guidance\nbaselines, and consistently outperforms existing methods across FD, IS, and\nalignment metrics. It also generalizes well to the challenging UnAV-100\nbenchmark. These results highlight model-guided dual-role alignment as a\npowerful and scalable paradigm for conditional video-to-audio generation. Code\nis available at: https://github.com/pantheon5100/mgaudio\n","authors":["Kang Zhang","Trung X. Pham","Suyeon Lee","Axi Niu","Arda Senocak","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2510.24103v1.pdf","comment":"accepted by NeurIPS 2025"}],"Robotics":[{"id":"http://arxiv.org/abs/2510.24994v1","updated":"2025-10-28T21:48:00Z","published":"2025-10-28T21:48:00Z","title":"Defect Mitigation for Robot Arm-based Additive Manufacturing Utilizing\n  Intelligent Control and IOT","summary":"  This paper presents an integrated robotic fused deposition modeling additive\nmanufacturing system featuring closed-loop thermal control and intelligent\nin-situ defect correction using a 6-degree of freedom robotic arm and an Oak-D\ncamera. The robot arm end effector was modified to mount an E3D hotend\nthermally regulated by an IoT microcontroller, enabling precise temperature\ncontrol through real-time feedback. Filament extrusion system was synchronized\nwith robotic motion, coordinated via ROS2, ensuring consistent deposition along\ncomplex trajectories. A vision system based on OpenCV detects layer-wise\ndefects position, commanding autonomous re-extrusion at identified sites.\nExperimental validation demonstrated successful defect mitigation in printing\noperations. The integrated system effectively addresses challenges real-time\nquality assurance. Inverse kinematics were used for motion planning, while\nhomography transformations corrected camera perspectives for accurate defect\nlocalization. The intelligent system successfully mitigated surface anomalies\nwithout interrupting the print process. By combining real-time thermal\nregulation, motion control, and intelligent defect detection & correction, this\narchitecture establishes a scalable and adaptive robotic additive manufacturing\nframework suitable for aerospace, biomedical, and industrial applications.\n","authors":["Matsive Ali","Blake Gassen","Sen Liu"],"pdf_url":"https://arxiv.org/pdf/2510.24994v1.pdf","comment":"This Paper Has Accepted at ASME 2025 International Mechanical\n  Engineering Congress and Exposition (IMECE 2025)"},{"id":"http://arxiv.org/abs/2506.17488v2","updated":"2025-10-28T21:28:31Z","published":"2025-06-20T21:49:17Z","title":"Online Adaptation for Flying Quadrotors in Tight Formations","summary":"  The task of flying in tight formations is challenging for teams of quadrotors\nbecause the complex aerodynamic wake interactions can destabilize individual\nteam members as well as the team. Furthermore, these aerodynamic effects are\nhighly nonlinear and fast-paced, making them difficult to model and predict. To\novercome these challenges, we present L1 KNODE-DW MPC, an adaptive, mixed\nexpert learning based control framework that allows individual quadrotors to\naccurately track trajectories while adapting to time-varying aerodynamic\ninteractions during formation flights. We evaluate L1 KNODE-DW MPC in two\ndifferent three-quadrotor formations and show that it outperforms several MPC\nbaselines. Our results show that the proposed framework is capable of enabling\nthe three-quadrotor team to remain vertically aligned in close proximity\nthroughout the flight. These findings show that the L1 adaptive module\ncompensates for unmodeled disturbances most effectively when paired with an\naccurate dynamics model. A video showcasing our framework and the physical\nexperiments is available here: https://youtu.be/9QX1Q5Ut9Rs\n","authors":["Pei-An Hsieh","Kong Yao Chee","M. Ani Hsieh"],"pdf_url":"https://arxiv.org/pdf/2506.17488v2.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.24972v1","updated":"2025-10-28T21:12:54Z","published":"2025-10-28T21:12:54Z","title":"Smooth path planning with safety margins using Piece-Wise Bezier curves","summary":"  In this paper, we propose a computationally efficient quadratic programming\n(QP) approach for generating smooth, $C^1$ continuous paths for mobile robots\nusing piece-wise quadratic Bezier (PWB) curves. Our method explicitly\nincorporates safety margins within a structured optimization framework,\nbalancing trajectory smoothness and robustness with manageable numerical\ncomplexity suitable for real-time and embedded applications. Comparative\nsimulations demonstrate clear advantages over traditional piece-wise linear\n(PWL) path planning methods, showing reduced trajectory deviations, enhanced\nrobustness, and improved overall path quality. These benefits are validated\nthrough simulations using a Pure-Pursuit controller in representative\nscenarios, highlighting the practical effectiveness and scalability of our\napproach for safe navigation.\n","authors":["Iancu Andrei","Marius Kloetzer","Cristian Mahulea","Catalin Dosoftei"],"pdf_url":"https://arxiv.org/pdf/2510.24972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10961v2","updated":"2025-10-28T20:57:42Z","published":"2025-07-15T03:45:26Z","title":"EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for\n  Spatially Generalizable Contact-rich Tasks","summary":"  This paper presents a framework for learning vision-based robotic policies\nfor contact-rich manipulation tasks that generalize spatially across task\nconfigurations. We focus on achieving robust spatial generalization of the\npolicy for the peg-in-hole (PiH) task trained from a small number of\ndemonstrations. We propose EquiContact, a hierarchical policy composed of a\nhigh-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF)\nand a novel low-level compliant visuomotor policy (Geometric Compliant ACT,\nG-CompACT). G-CompACT operates using only localized observations (geometrically\nconsistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB\nimages) and produces actions defined in the end-effector frame. Through these\ndesign choices, we show that the entire EquiContact pipeline is\nSE(3)-equivariant, from perception to force control. We also outline three key\ncomponents for spatially generalizable contact-rich policies: compliance,\nlocalized policies, and induced equivariance. Real-world experiments on PiH,\nscrewing, and surface wiping tasks demonstrate a near-perfect success rate and\nrobust generalization to unseen spatial configurations, validating the proposed\nframework and principles. The experimental videos can be found on the project\nwebsite: https://sites.google.com/berkeley.edu/equicontact\n","authors":["Joohwan Seo","Arvind Kruthiventy","Soomi Lee","Megan Teng","Xiang Zhang","Seoyeon Choi","Jongeun Choi","Roberto Horowitz"],"pdf_url":"https://arxiv.org/pdf/2507.10961v2.pdf","comment":"Submitted to RA-L"},{"id":"http://arxiv.org/abs/2509.07216v2","updated":"2025-10-28T20:45:56Z","published":"2025-09-08T20:52:59Z","title":"Quantum Machine Learning and Grover's Algorithm for Quantum Optimization\n  of Robotic Manipulators","summary":"  Optimizing high-degree of freedom robotic manipulators requires searching\ncomplex, high-dimensional configuration spaces, a task that is computationally\nchallenging for classical methods. This paper introduces a quantum native\nframework that integrates quantum machine learning with Grover's algorithm to\nsolve kinematic optimization problems efficiently. A parameterized quantum\ncircuit is trained to approximate the forward kinematics model, which then\nconstructs an oracle to identify optimal configurations. Grover's algorithm\nleverages this oracle to provide a quadratic reduction in search complexity.\nDemonstrated on simulated 1-DoF, 2-DoF, and dual-arm manipulator tasks, the\nmethod achieves significant speedups-up to 93x over classical optimizers like\nNelder Mead as problem dimensionality increases. This work establishes a\nfoundational, quantum-native framework for robot kinematic optimization,\neffectively bridging quantum computing and robotics problems.\n","authors":["Hassen Nigatu","Shi Gaokun","Li Jituo","Wang Jin","Lu Guodong","Howard Li"],"pdf_url":"https://arxiv.org/pdf/2509.07216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24949v1","updated":"2025-10-28T20:31:19Z","published":"2025-10-28T20:31:19Z","title":"SCOUT: A Lightweight Framework for Scenario Coverage Assessment in\n  Autonomous Driving","summary":"  Assessing scenario coverage is crucial for evaluating the robustness of\nautonomous agents, yet existing methods rely on expensive human annotations or\ncomputationally intensive Large Vision-Language Models (LVLMs). These\napproaches are impractical for large-scale deployment due to cost and\nefficiency constraints. To address these shortcomings, we propose SCOUT\n(Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate\nmodel designed to predict scenario coverage labels directly from an agent's\nlatent sensor representations. SCOUT is trained through a distillation process,\nlearning to approximate LVLM-generated coverage labels while eliminating the\nneed for continuous LVLM inference or human annotation. By leveraging\nprecomputed perception features, SCOUT avoids redundant computations and\nenables fast, scalable scenario coverage estimation. We evaluate our method\nacross a large dataset of real-life autonomous navigation scenarios,\ndemonstrating that it maintains high accuracy while significantly reducing\ncomputational cost. Our results show that SCOUT provides an effective and\npractical alternative for large-scale coverage analysis. While its performance\ndepends on the quality of LVLM-generated training labels, SCOUT represents a\nmajor step toward efficient scenario coverage oversight in autonomous systems.\n","authors":["Anil Yildiz","Sarah M. Thornton","Carl Hildebrandt","Sreeja Roy-Singh","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2510.24949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06210v2","updated":"2025-10-28T18:18:29Z","published":"2023-10-09T23:42:33Z","title":"CAT-RRT: Motion Planning that Admits Contact One Link at a Time","summary":"  Current motion planning approaches rely on binary collision checking to\nevaluate the validity of a state and thereby dictate where the robot is allowed\nto move. This approach leaves little room for robots to engage in contact with\nan object, as is often necessary when operating in densely cluttered spaces. In\nthis work, we propose an alternative method that considers contact states as\nhigh-cost states that the robot should avoid but can traverse if necessary to\ncomplete a task. More specifically, we introduce Contact Admissible\nTransition-based Rapidly exploring Random Trees (CAT-RRT), a planner that uses\na novel per-link cost heuristic to find a path by traversing high-cost obstacle\nregions. Through extensive testing, we find that state-of-the-art optimization\nplanners tend to over-explore low-cost states, which leads to slow and\ninefficient convergence to contact regions. Conversely, CAT-RRT searches both\nlow and high-cost regions simultaneously with an adaptive thresholding\nmechanism carried out at each robot link. This leads to paths with a balance\nbetween efficiency, path length, and contact cost.\n","authors":["Nataliya Nechyporenko","Caleb Escobedo","Shreyas Kadekodi","Alessandro Roncone"],"pdf_url":"https://arxiv.org/pdf/2310.06210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04881v2","updated":"2025-10-28T18:11:08Z","published":"2025-06-05T11:00:31Z","title":"Efficient Path Planning and Task Allocation Algorithm for Boolean\n  Specifications","summary":"  This paper presents a novel path-planning and task assignment algorithm for\nmulti-robot systems that should fulfill a global Boolean specification. The\nproposed method is based on Integer Linear Programming (ILP) formulations,\nwhich are combined with structural insights from Petri nets to improve\nscalability and computational efficiency. By proving that the \\emph{constraint\nmatrix} is totally unimodular (TU) for certain classes of problems, the ILP\nformulation can be relaxed into a Linear Programming (LP) problem without\nlosing the integrality of the solution. This relaxation eliminates complex\ncombinatorial techniques, significantly reducing computational overhead and\nthus ensuring scalability for large-scale systems. Using the approach proposed\nin this paper, we can solve path-planning problems for teams made up to 500\nrobots. The method guarantees computational tractability, handles collision\navoidance and reduces computational demands through iterative LP optimization\ntechniques. Case studies demonstrate the efficiency of the algorithm in\ngenerating scalable, collision-free paths for large robot teams navigating in\ncomplex environments. While the conservative nature of collision avoidance\nintroduces additional constraints, and thus, computational requirements, the\nsolution remains practical and impactful for diverse applications. The\nalgorithm is particularly applicable to real-world scenarios, including\nwarehouse logistics where autonomous robots must efficiently coordinate tasks\nor search-and-rescue operations in various environments. This work contributes\nboth theoretically and practically to scalable multi-robot path planning and\ntask allocation, offering an efficient framework for coordinating autonomous\nagents in shared environments.\n","authors":["Ioana Hustiu","Roozbeh Abolpour","Cristian Mahulea","Marius Kloetzer"],"pdf_url":"https://arxiv.org/pdf/2506.04881v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12153v2","updated":"2025-10-28T18:06:02Z","published":"2025-05-17T22:02:44Z","title":"Federated Deep Reinforcement Learning for Privacy-Preserving\n  Robotic-Assisted Surgery","summary":"  The integration of Reinforcement Learning (RL) into robotic-assisted surgery\n(RAS) holds significant promise for advancing surgical precision, adaptability,\nand autonomous decision-making. However, the development of robust RL models in\nclinical settings is hindered by key challenges, including stringent patient\ndata privacy regulations, limited access to diverse surgical datasets, and high\nprocedural variability. To address these limitations, this paper presents a\nFederated Deep Reinforcement Learning (FDRL) framework that enables\ndecentralized training of RL models across multiple healthcare institutions\nwithout exposing sensitive patient information. A central innovation of the\nproposed framework is its dynamic policy adaptation mechanism, which allows\nsurgical robots to select and tailor patient-specific policies in real-time,\nthereby ensuring personalized and Optimised interventions. To uphold rigorous\nprivacy standards while facilitating collaborative learning, the FDRL framework\nincorporates secure aggregation, differential privacy, and homomorphic\nencryption techniques. Experimental results demonstrate a 60\\% reduction in\nprivacy leakage compared to conventional methods, with surgical precision\nmaintained within a 1.5\\% margin of a centralized baseline. This work\nestablishes a foundational approach for adaptive, secure, and patient-centric\nAI-driven surgical robotics, offering a pathway toward clinical translation and\nscalable deployment across diverse healthcare environments.\n","authors":["Sana Hafeez","Sundas Rafat Mulkana","Muhammad Ali Imran","Michele Sevegnani"],"pdf_url":"https://arxiv.org/pdf/2505.12153v2.pdf","comment":"11 pages, 7 figures, conference"},{"id":"http://arxiv.org/abs/2510.24692v1","updated":"2025-10-28T17:50:30Z","published":"2025-10-28T17:50:30Z","title":"Embodying Physical Computing into Soft Robots","summary":"  Softening and onboarding computers and controllers is one of the final\nfrontiers in soft robotics towards their robustness and intelligence for\neveryday use. In this regard, embodying soft and physical computing presents\nexciting potential. Physical computing seeks to encode inputs into a mechanical\ncomputing kernel and leverage the internal interactions among this kernel's\nconstituent elements to compute the output. Moreover, such input-to-output\nevolution can be re-programmable. This perspective paper proposes a framework\nfor embodying physical computing into soft robots and discusses three unique\nstrategies in the literature: analog oscillators, physical reservoir computing,\nand physical algorithmic computing. These embodied computers enable the soft\nrobot to perform complex behaviors that would otherwise require CMOS-based\nelectronics -- including coordinated locomotion with obstacle avoidance,\npayload weight and orientation classification, and programmable operation based\non logical rules. This paper will detail the working principles of these\nembodied physical computing methods, survey the current state-of-the-art, and\npresent a perspective for future development.\n","authors":["Jun Wang","Ziyang Zhou","Ardalan Kahak","Suyi Li"],"pdf_url":"https://arxiv.org/pdf/2510.24692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24683v1","updated":"2025-10-28T17:46:06Z","published":"2025-10-28T17:46:06Z","title":"A Framework for the Systematic Evaluation of Obstacle Avoidance and\n  Object-Aware Controllers","summary":"  Real-time control is an essential aspect of safe robot operation in the real\nworld with dynamic objects. We present a framework for the analysis of\nobject-aware controllers, methods for altering a robot's motion to anticipate\nand avoid possible collisions. This framework is focused on three design\nconsiderations: kinematics, motion profiles, and virtual constraints.\nAdditionally, the analysis in this work relies on verification of robot\nbehaviors using fundamental robot-obstacle experimental scenarios. To showcase\nthe effectiveness of our method we compare three representative object-aware\ncontrollers. The comparison uses metrics originating from the design\nconsiderations. From the analysis, we find that the design of object-aware\ncontrollers often lacks kinematic considerations, continuity of control points,\nand stability in movement profiles. We conclude that this framework can be used\nin the future to design, compare, and benchmark obstacle avoidance methods.\n","authors":["Caleb Escobedo","Nataliya Nechyporenko","Shreyas Kadekodi","Alessandro Roncone"],"pdf_url":"https://arxiv.org/pdf/2510.24683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24680v1","updated":"2025-10-28T17:45:26Z","published":"2025-10-28T17:45:26Z","title":"Fare: Failure Resilience in Learned Visual Navigation Control","summary":"  While imitation learning (IL) enables effective visual navigation, IL\npolicies are prone to unpredictable failures in out-of-distribution (OOD)\nscenarios. We advance the notion of failure-resilient policies, which not only\ndetect failures but also recover from them automatically. Failure recognition\nthat identifies the factors causing failure is key to informing recovery: e.g.\npinpointing image regions triggering failure detections can provide cues to\nguide recovery. We present Fare, a framework to construct failure-resilient IL\npolicies, embedding OOD-detection and recognition in them without using\nexplicit failure data, and pairing them with recovery heuristics. Real-world\nexperiments show that Fare enables failure recovery across two different policy\narchitectures, enabling robust long-range navigation in complex environments.\n","authors":["Zishuo Wang","Joel Loo","David Hsu"],"pdf_url":"https://arxiv.org/pdf/2510.24680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24676v1","updated":"2025-10-28T17:40:52Z","published":"2025-10-28T17:40:52Z","title":"Feature Matching-Based Gait Phase Prediction for Obstacle Crossing\n  Control of Powered Transfemoral Prosthesis","summary":"  For amputees with powered transfemoral prosthetics, navigating obstacles or\ncomplex terrain remains challenging. This study addresses this issue by using\nan inertial sensor on the sound ankle to guide obstacle-crossing movements. A\ngenetic algorithm computes the optimal neural network structure to predict the\nrequired angles of the thigh and knee joints. A gait progression prediction\nalgorithm determines the actuation angle index for the prosthetic knee motor,\nultimately defining the necessary thigh and knee angles and gait progression.\nResults show that when the standard deviation of Gaussian noise added to the\nthigh angle data is less than 1, the method can effectively eliminate noise\ninterference, achieving 100\\% accuracy in gait phase estimation under 150 Hz,\nwith thigh angle prediction error being 8.71\\% and knee angle prediction error\nbeing 6.78\\%. These findings demonstrate the method's ability to accurately\npredict gait progression and joint angles, offering significant practical value\nfor obstacle negotiation in powered transfemoral prosthetics.\n","authors":["Jiaxuan Zhang","Yuquan Leng","Yixuan Guo","Chenglong Fu"],"pdf_url":"https://arxiv.org/pdf/2510.24676v1.pdf","comment":"6 pages, conference"},{"id":"http://arxiv.org/abs/2510.24671v1","updated":"2025-10-28T17:36:52Z","published":"2025-10-28T17:36:52Z","title":"Multi-Agent Scenario Generation in Roundabouts with a\n  Transformer-enhanced Conditional Variational Autoencoder","summary":"  With the increasing integration of intelligent driving functions into\nserial-produced vehicles, ensuring their functionality and robustness poses\ngreater challenges. Compared to traditional road testing, scenario-based\nvirtual testing offers significant advantages in terms of time and cost\nefficiency, reproducibility, and exploration of edge cases. We propose a\nTransformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for\ngenerating multi-agent traffic scenarios in roundabouts, which are\ncharacterized by high vehicle dynamics and complex layouts, yet remain\nrelatively underexplored in current research. The results show that the\nproposed model can accurately reconstruct original scenarios and generate\nrealistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators\n(KPIs) are employed to evaluate the interactive behavior in the generated\nscenarios. Analysis of the latent space reveals partial disentanglement, with\nseveral latent dimensions exhibiting distinct and interpretable effects on\nscenario attributes such as vehicle entry timing, exit timing, and velocity\nprofiles. The results demonstrate the model's capability to generate scenarios\nfor the validation of intelligent driving functions involving multi-agent\ninteractions, as well as to augment data for their development and iterative\nimprovement.\n","authors":["Li Li","Tobias Brinkmann","Till Temmen","Markus Eisenbarth","Jakob Andert"],"pdf_url":"https://arxiv.org/pdf/2510.24671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20774v2","updated":"2025-10-28T17:10:50Z","published":"2025-10-23T17:47:12Z","title":"FieldGen: From Teleoperated Pre-Manipulation Trajectories to\n  Field-Guided Data Generation","summary":"  Large-scale and diverse datasets are vital for training robust robotic\nmanipulation policies, yet existing data collection methods struggle to balance\nscale, diversity, and quality. Simulation offers scalability but suffers from\nsim-to-real gaps, while teleoperation yields high-quality demonstrations with\nlimited diversity and high labor cost. We introduce FieldGen, a field-guided\ndata generation framework that enables scalable, diverse, and high-quality\nreal-world data collection with minimal human supervision. FieldGen decomposes\nmanipulation into two stages: a pre-manipulation phase, allowing trajectory\ndiversity, and a fine manipulation phase requiring expert precision. Human\ndemonstrations capture key contact and pose information, after which an\nattraction field automatically generates diverse trajectories converging to\nsuccessful configurations. This decoupled design combines scalable trajectory\ndiversity with precise supervision. Moreover, FieldGen-Reward augments\ngenerated data with reward annotations to further enhance policy learning.\nExperiments demonstrate that policies trained with FieldGen achieve higher\nsuccess rates and improved stability compared to teleoperation-based baselines,\nwhile significantly reducing human effort in long-term real-world data\ncollection. Webpage is available at https://fieldgen.github.io/.\n","authors":["Wenhao Wang","Kehe Ye","Xinyu Zhou","Tianxing Chen","Cao Min","Qiaoming Zhu","Xiaokang Yang","Ping Luo","Yongjian Shen","Yang Yang","Maoqing Yao","Yao Mu"],"pdf_url":"https://arxiv.org/pdf/2510.20774v2.pdf","comment":"Webpage: https://fieldgen.github.io/"},{"id":"http://arxiv.org/abs/2510.24623v1","updated":"2025-10-28T16:51:50Z","published":"2025-10-28T16:51:50Z","title":"GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization","summary":"  In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline\ndesigned to localize a mobile robot in large-scale outdoor environments using\nprior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing\non the perceived ground area and utilizes the place recognition network R2D2,\nor alternatively, the non-learning approach Scale-Invariant Feature Transform\n(SIFT), to identify and select keypoints for BEV image map registration. Our\nresults demonstrate that GroundLoc outperforms state-of-the-art methods on the\nSemanticKITTI and HeLiPR datasets across various sensors. In the multi-session\nlocalization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)\nwell below 50 cm on all Ouster OS2 128 sequences while meeting online runtime\nrequirements. The system supports various sensor models, as evidenced by\nevaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,\nand Livox Avia sensors. The prior maps are stored as 2D raster image maps,\nwhich can be created from a single drive and require only 4 MB of storage per\nsquare kilometer. The source code is available at\nhttps://github.com/dcmlr/groundloc.\n","authors":["Nicolai Steinke","Daniel Goehring"],"pdf_url":"https://arxiv.org/pdf/2510.24623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24584v1","updated":"2025-10-28T16:16:31Z","published":"2025-10-28T16:16:31Z","title":"Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using\n  Reinforcement Learning","summary":"  This paper presents a curriculum-based reinforcement learning framework for\ntraining precise and high-performance jumping policies for the robot `Olympus'.\nSeparate policies are developed for vertical and horizontal jumps, leveraging a\nsimple yet effective strategy. First, we densify the inherently sparse jumping\nreward using the laws of projectile motion. Next, a reference state\ninitialization scheme is employed to accelerate the exploration of dynamic\njumping behaviors without reliance on reference trajectories. We also present a\nwalking policy that, when combined with the jumping policies, unlocks versatile\nand dynamic locomotion capabilities. Comprehensive testing validates walking on\nvaried terrain surfaces and jumping performance that exceeds previous works,\neffectively crossing the Sim2Real gap. Experimental validation demonstrates\nhorizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to\n1.0 m. Additionally, we show that with only minor modifications, the proposed\nmethod can be used to learn omnidirectional jumping.\n","authors":["Jørgen Anker Olsen","Lars Rønhaug Pettersen","Kostas Alexis"],"pdf_url":"https://arxiv.org/pdf/2510.24584v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2510.24571v1","updated":"2025-10-28T16:07:39Z","published":"2025-10-28T16:07:39Z","title":"Spatiotemporal Calibration of Doppler Velocity Logs for Underwater\n  Robots","summary":"  The calibration of extrinsic parameters and clock offsets between sensors for\nhigh-accuracy performance in underwater SLAM systems remains insufficiently\nexplored. Existing methods for Doppler Velocity Log (DVL) calibration are\neither constrained to specific sensor configurations or rely on oversimplified\nassumptions, and none jointly estimate translational extrinsics and time\noffsets. We propose a Unified Iterative Calibration (UIC) framework for general\nDVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a\nGaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC\nalternates between efficient GP-based motion state updates and gradient-based\ncalibration variable updates, supported by a provably statistically consistent\nsequential initialization scheme. The proposed UIC can be applied to IMU,\ncameras and other modalities as co-sensors. We release an open-source\nDVL-camera calibration toolbox. Beyond underwater applications, several aspects\nof UIC-such as the integration of GP priors for MAP-based calibration and the\ndesign of provably reliable initialization procedures-are broadly applicable to\nother multi-sensor calibration problems. Finally, simulations and real-world\ntests validate our approach.\n","authors":["Hongxu Zhao","Guangyang Zeng","Yunling Shao","Tengfei Zhang","Junfeng Wu"],"pdf_url":"https://arxiv.org/pdf/2510.24571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24554v1","updated":"2025-10-28T15:51:14Z","published":"2025-10-28T15:51:14Z","title":"An Adaptive Inspection Planning Approach Towards Routine Monitoring in\n  Uncertain Environments","summary":"  In this work, we present a hierarchical framework designed to support robotic\ninspection under environment uncertainty. By leveraging a known environment\nmodel, existing methods plan and safely track inspection routes to visit points\nof interest. However, discrepancies between the model and actual site\nconditions, caused by either natural or human activities, can alter the surface\nmorphology or introduce path obstructions. To address this challenge, the\nproposed framework divides the inspection task into: (a) generating the initial\nglobal view-plan for region of interests based on a historical map and (b)\nlocal view replanning to adapt to the current morphology of the inspection\nscene. The proposed hierarchy preserves global coverage objectives while\nenabling reactive adaptation to the local surface morphology. This enables the\nlocal autonomy to remain robust against environment uncertainty and complete\nthe inspection tasks. We validate the approach through deployments in\nreal-world subterranean mines using quadrupedal robot.\n","authors":["Vignesh Kottayam Viswanathan","Yifan Bai","Scott Fredriksson","Sumeet Satpute","Christoforos Kanellakis","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2510.24554v1.pdf","comment":"Submitted for ICRA 2026"},{"id":"http://arxiv.org/abs/2510.24533v1","updated":"2025-10-28T15:39:07Z","published":"2025-10-28T15:39:07Z","title":"GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater\n  Robots","summary":"  Accurate visual inertial simultaneous localization and mapping (VI SLAM) for\nunderwater robots remains a significant challenge due to frequent visual\ndegeneracy and insufficient inertial measurement unit (IMU) motion excitation.\nIn this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system\ndesigned to address these issues. By leveraging the stereo camera's direct\ndepth estimation ability, we eliminate the need to estimate scale during IMU\ninitialization, enabling stable operation even under low acceleration dynamics.\nWith precise gravity initialization, we decouple the pitch and roll from the\npose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point\n(PnP) problem for pose tracking. This allows the use of a minimal 3-point\nsolver, which significantly reduces computational time to reject outliers\nwithin a Random Sample Consensus framework. We further propose a\nbias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the\nrelative pose converges to the true value as the feature number increases. To\nhandle dynamic motion, we refine the full 6-DOF pose while jointly estimating\nthe IMU covariance, enabling adaptive weighting of the gravity prior. Extensive\nexperiments on simulated and real-world data demonstrate that GeVI-SLAM\nachieves higher accuracy and greater stability compared to state-of-the-art\nmethods.\n","authors":["Yuan Shen","Yuze Hong","Guangyang Zeng","Tengfei Zhang","Pui Yi Chui","Ziyang Hong","Junfeng Wu"],"pdf_url":"https://arxiv.org/pdf/2510.24533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24515v1","updated":"2025-10-28T15:27:26Z","published":"2025-10-28T15:27:26Z","title":"Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot\n  Systems","summary":"  The Team Orienteering Problem (TOP) generalizes many real-world multi-robot\nscheduling and routing tasks that occur in autonomous mobility, aerial\nlogistics, and surveillance applications. While many flavors of the TOP exist\nfor planning in multi-robot systems, they assume that all the robots cooperate\ntoward a single objective; thus, they do not extend to settings where the\nrobots compete in reward-scarce environments. We propose Stochastic\nPrize-Collecting Games (SPCG) as an extension of the TOP to plan in the\npresence of self-interested robots operating on a graph, under energy\nconstraints and stochastic transitions. A theoretical study on complete and\nstar graphs establishes that there is a unique pure Nash equilibrium in SPCGs\nthat coincides with the optimal routing solution of an equivalent TOP given a\nrank-based conflict resolution rule. This work proposes two algorithms: Ordinal\nRank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in\ntemporarily-formed local neighborhoods during the games' stages, and Fictitious\nOrdinal Response Learning (FORL) to obtain best-response policies against one's\nsenior-rank opponents. Empirical evaluations conducted on road networks and\nsynthetic graphs under both dynamic and stationary prize distributions show\nthat 1) the state-aliasing induced by OR-conditioning enables learning policies\nthat scale more efficiently to large team sizes than those trained with the\nglobal index, and 2) Policies trained with FORL generalize better to imbalanced\nprize distributions than those with other multi-agent training methods.\nFinally, the learned policies in the SPCG achieved between 87% and 95%\noptimality compared to an equivalent TOP solution obtained by mixed-integer\nlinear programming.\n","authors":["Malintha Fernando","Petter Ögren","Silun Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.24515v1.pdf","comment":"Submitted to IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2510.24508v1","updated":"2025-10-28T15:19:14Z","published":"2025-10-28T15:19:14Z","title":"Supervisory Measurement-Guided Noise Covariance Estimation","summary":"  Reliable state estimation hinges on accurate specification of sensor noise\ncovariances, which weigh heterogeneous measurements. In practice, these\ncovariances are difficult to identify due to environmental variability,\nfront-end preprocessing, and other reasons. We address this by formulating\nnoise covariance estimation as a bilevel optimization that, from a Bayesian\nperspective, factorizes the joint likelihood of so-called odometry and\nsupervisory measurements, thereby balancing information utilization with\ncomputational efficiency. The factorization converts the nested Bayesian\ndependency into a chain structure, enabling efficient parallel computation: at\nthe lower level, an invariant extended Kalman filter with state augmentation\nestimates trajectories, while a derivative filter computes analytical gradients\nin parallel for upper-level gradient updates. The upper level refines the\ncovariance to guide the lower-level estimation. Experiments on synthetic and\nreal-world datasets show that our method achieves higher efficiency over\nexisting baselines.\n","authors":["Haoying Li","Yifan Peng","Junfeng Wu"],"pdf_url":"https://arxiv.org/pdf/2510.24508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11607v3","updated":"2025-10-28T15:15:35Z","published":"2024-11-18T14:29:22Z","title":"Performance evaluation of a ROS2 based Automated Driving System","summary":"  Automated driving is currently a prominent area of scientific work. In the\nfuture, highly automated driving and new Advanced Driver Assistance Systems\nwill become reality. While Advanced Driver Assistance Systems and automated\ndriving functions for certain domains are already commercially available,\nubiquitous automated driving in complex scenarios remains a subject of ongoing\nresearch. Contrarily to single-purpose Electronic Control Units, the software\nfor automated driving is often executed on high performance PCs. The Robot\nOperating System 2 (ROS2) is commonly used to connect components in an\nautomated driving system. Due to the time critical nature of automated driving\nsystems, the performance of the framework is especially important. In this\npaper, a thorough performance evaluation of ROS2 is conducted, both in terms of\ntimeliness and error rate. The results show that ROS2 is a suitable framework\nfor automated driving systems.\n","authors":["Jorin Kouril","Bernd Schäufele","Ilja Radusch","Bettina Schnor"],"pdf_url":"https://arxiv.org/pdf/2411.11607v3.pdf","comment":"Published and presented at VEHITS 2024, Proceedings of the 10th\n  International Conference on Vehicle Technology and Intelligent Transport\n  Systems - VEHITS; 2024"},{"id":"http://arxiv.org/abs/2510.24482v1","updated":"2025-10-28T14:54:12Z","published":"2025-10-28T14:54:12Z","title":"Sample-efficient and Scalable Exploration in Continuous-Time RL","summary":"  Reinforcement learning algorithms are typically designed for discrete-time\ndynamics, even though the underlying real-world control systems are often\ncontinuous in time. In this paper, we study the problem of continuous-time\nreinforcement learning, where the unknown system dynamics are represented using\nnonlinear ordinary differential equations (ODEs). We leverage probabilistic\nmodels, such as Gaussian processes and Bayesian neural networks, to learn an\nuncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily\nmaximizes a weighted sum of the extrinsic reward and model epistemic\nuncertainty. This yields a scalable and sample-efficient approach to\ncontinuous-time model-based RL. We show that COMBRL achieves sublinear regret\nin the reward-driven setting, and in the unsupervised RL setting (i.e., without\nextrinsic rewards), we provide a sample complexity bound. In our experiments,\nwe evaluate COMBRL in both standard and unsupervised RL settings and\ndemonstrate that it scales better, is more sample-efficient than prior methods,\nand outperforms baselines across several deep RL tasks.\n","authors":["Klemens Iten","Lenart Treven","Bhavya Sukhija","Florian Dörfler","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2510.24482v1.pdf","comment":"26 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2510.24461v1","updated":"2025-10-28T14:28:40Z","published":"2025-10-28T14:28:40Z","title":"Adaptive Surrogate Gradients for Sequential Reinforcement Learning in\n  Spiking Neural Networks","summary":"  Neuromorphic computing systems are set to revolutionize energy-constrained\nrobotics by achieving orders-of-magnitude efficiency gains, while enabling\nnative temporal processing. Spiking Neural Networks (SNNs) represent a\npromising algorithmic approach for these systems, yet their application to\ncomplex control tasks faces two critical challenges: (1) the non-differentiable\nnature of spiking neurons necessitates surrogate gradients with unclear\noptimization properties, and (2) the stateful dynamics of SNNs require training\non sequences, which in reinforcement learning (RL) is hindered by limited\nsequence lengths during early training, preventing the network from bridging\nits warm-up period.\n  We address these challenges by systematically analyzing surrogate gradient\nslope settings, showing that shallower slopes increase gradient magnitude in\ndeeper layers but reduce alignment with true gradients. In supervised learning,\nwe find no clear preference for fixed or scheduled slopes. The effect is much\nmore pronounced in RL settings, where shallower slopes or scheduled slopes lead\nto a 2.1x improvement in both training and final deployed performance. Next, we\npropose a novel training approach that leverages a privileged guiding policy to\nbootstrap the learning process, while still exploiting online environment\ninteractions with the spiking policy. Combining our method with an adaptive\nslope schedule for a real-world drone position control task, we achieve an\naverage return of 400 points, substantially outperforming prior techniques,\nincluding Behavioral Cloning and TD3BC, which achieve at most --200 points\nunder the same conditions. This work advances both the theoretical\nunderstanding of surrogate gradient learning in SNNs and practical training\nmethodologies for neuromorphic controllers demonstrated in real-world robotic\nsystems.\n","authors":["Korneel Van den Berghe","Stein Stroobants","Vijay Janapa Reddi","G. C. H. E. de Croon"],"pdf_url":"https://arxiv.org/pdf/2510.24461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24457v1","updated":"2025-10-28T14:24:47Z","published":"2025-10-28T14:24:47Z","title":"Flatness-based trajectory planning for 3D overhead cranes with friction\n  compensation and collision avoidance","summary":"  This paper presents an optimal trajectory generation method for 3D overhead\ncranes by leveraging differential flatness. This framework enables the direct\ninclusion of complex physical and dynamic constraints, such as nonlinear\nfriction and collision avoidance for both payload and rope. Our approach allows\nfor aggressive movements by constraining payload swing only at the final point.\nA comparative simulation study validates our approach, demonstrating that\nneglecting dry friction leads to actuator saturation and collisions. The\nresults show that friction modeling is a fundamental requirement for fast and\nsafe crane trajectories.\n","authors":["Jorge Vicente-Martinez","Edgar Ramirez-Laboreo"],"pdf_url":"https://arxiv.org/pdf/2510.24457v1.pdf","comment":"8 pages, 11 figures"},{"id":"http://arxiv.org/abs/2508.20072v2","updated":"2025-10-28T14:22:20Z","published":"2025-08-27T17:39:11Z","title":"Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding\n  in Vision-Language-Action Policies","summary":"  Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions into robot actions. However, prevailing VLAs either\ngenerate actions auto-regressively in a fixed left-to-right order or attach\nseparate MLP or diffusion heads outside the backbone, leading to fragmented\ninformation pathways and specialized training requirements that hinder a\nunified, scalable architecture. We present Discrete Diffusion VLA, a\nunified-transformer policy that models discretized action chunks with discrete\ndiffusion. The design retains diffusion's progressive refinement paradigm while\nremaining natively compatible with the discrete token interface of VLMs. Our\nmethod achieves an adaptive decoding order that resolves easy action elements\nbefore harder ones and uses secondary re-masking to revisit uncertain\npredictions across refinement rounds, which improves consistency and enables\nrobust error correction. This unified decoder preserves pre-trained\nvision-language priors, supports parallel decoding, breaks the autoregressive\nbottleneck, and reduces the number of function evaluations. Discrete Diffusion\nVLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on\nSimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge, improving over\nautoregressive, MLP decoder and continuous diffusion baselines. These findings\nindicate that discrete-diffusion VLA supports precise action modeling and\nconsistent training, laying groundwork for scaling VLA to larger models and\ndatasets. Our project page is https://github.com/Liang-ZX/DiscreteDiffusionVLA\n","authors":["Zhixuan Liang","Yizhuo Li","Tianshuo Yang","Chengyue Wu","Sitong Mao","Tian Nian","Liuao Pei","Shunbo Zhou","Xiaokang Yang","Jiangmiao Pang","Yao Mu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2508.20072v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2510.25725v1","updated":"2025-10-28T14:16:10Z","published":"2025-10-28T14:16:10Z","title":"A Humanoid Visual-Tactile-Action Dataset for Contact-Rich Manipulation","summary":"  Contact-rich manipulation has become increasingly important in robot\nlearning. However, previous studies on robot learning datasets have focused on\nrigid objects and underrepresented the diversity of pressure conditions for\nreal-world manipulation. To address this gap, we present a humanoid\nvisual-tactile-action dataset designed for manipulating deformable soft\nobjects. The dataset was collected via teleoperation using a humanoid robot\nequipped with dexterous hands, capturing multi-modal interactions under varying\npressure conditions. This work also motivates future research on models with\nadvanced optimization strategies capable of effectively leveraging the\ncomplexity and diversity of tactile signals.\n","authors":["Eunju Kwon","Seungwon Oh","In-Chang Baek","Yucheon Park","Gyungbo Kim","JaeYoung Moon","Yunho Choi","Kyung-Joong Kim"],"pdf_url":"https://arxiv.org/pdf/2510.25725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24410v1","updated":"2025-10-28T13:22:24Z","published":"2025-10-28T13:22:24Z","title":"A Hybrid Approach for Visual Multi-Object Tracking","summary":"  This paper proposes a visual multi-object tracking method that jointly\nemploys stochastic and deterministic mechanisms to ensure identifier\nconsistency for unknown and time-varying target numbers under nonlinear\ndynamics. A stochastic particle filter addresses nonlinear dynamics and\nnon-Gaussian noise, with support from particle swarm optimization (PSO) to\nguide particles toward state distribution modes and mitigate divergence through\nproposed fitness measures incorporating motion consistency, appearance\nsimilarity, and social-interaction cues with neighboring targets. Deterministic\nassociation further enforces identifier consistency via a proposed cost matrix\nincorporating spatial consistency between particles and current detections,\ndetection confidences, and track penalties. Subsequently, a novel scheme is\nproposed for the smooth updating of target states while preserving their\nidentities, particularly for weak tracks during interactions with other targets\nand prolonged occlusions. Moreover, velocity regression over past states\nprovides trend-seed velocities, enhancing particle sampling and state updates.\nThe proposed tracker is designed to operate flexibly for both pre-recorded\nvideos and camera live streams, where future frames are unavailable.\nExperimental results confirm superior performance compared to state-of-the-art\ntrackers. The source-code reference implementations of both the proposed method\nand compared-trackers are provided on GitHub:\nhttps://github.com/SDU-VelKoTek/GenTrack2\n","authors":["Toan Van Nguyen","Rasmus G. K. Christiansen","Dirk Kraft","Leon Bodenhagen"],"pdf_url":"https://arxiv.org/pdf/2510.24410v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2510.24399v1","updated":"2025-10-28T13:13:20Z","published":"2025-10-28T13:13:20Z","title":"GenTrack: A New Generation of Multi-Object Tracking","summary":"  This paper introduces a novel multi-object tracking (MOT) method, dubbed\nGenTrack, whose main contributions include: a hybrid tracking approach\nemploying both stochastic and deterministic manners to robustly handle unknown\nand time-varying numbers of targets, particularly in maintaining target\nidentity (ID) consistency and managing nonlinear dynamics, leveraging particle\nswarm optimization (PSO) with some proposed fitness measures to guide\nstochastic particles toward their target distribution modes, enabling effective\ntracking even with weak and noisy object detectors, integration of social\ninteractions among targets to enhance PSO-guided particles as well as improve\ncontinuous updates of both strong (matched) and weak (unmatched) tracks,\nthereby reducing ID switches and track loss, especially during occlusions, a\nGenTrack-based redefined visual MOT baseline incorporating a comprehensive\nstate and observation model based on space consistency, appearance, detection\nconfidence, track penalties, and social scores for systematic and efficient\ntarget updates, and the first-ever publicly available source-code reference\nimplementation with minimal dependencies, featuring three variants, including\nGenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.\nExperimental results have shown that GenTrack provides superior performance on\nstandard benchmarks and real-world scenarios compared to state-of-the-art\ntrackers, with integrated implementations of baselines for fair comparison.\nPotential directions for future work are also discussed. The source-code\nreference implementations of both the proposed method and compared-trackers are\nprovided on GitHub: https://github.com/SDU-VelKoTek/GenTrack\n","authors":["Toan Van Nguyen","Rasmus G. K. Christiansen","Dirk Kraft","Leon Bodenhagen"],"pdf_url":"https://arxiv.org/pdf/2510.24399v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2510.20974v2","updated":"2025-10-28T11:58:54Z","published":"2025-10-23T20:06:29Z","title":"Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization","summary":"  Reinforcement Learning (RL) from raw visual input has achieved impressive\nsuccesses in recent years, yet it remains fragile to out-of-distribution\nvariations such as changes in lighting, color, and viewpoint. Point Cloud\nReinforcement Learning (PC-RL) offers a promising alternative by mitigating\nappearance-based brittleness, but its sensitivity to camera pose mismatches\ncontinues to undermine reliability in realistic settings. To address this\nchallenge, we propose PCA Point Cloud (PPC), a canonicalization framework\nspecifically tailored for downstream robotic control. PPC maps point clouds\nunder arbitrary rigid-body transformations to a unique canonical pose, aligning\nobservations to a consistent frame, thereby substantially decreasing\nviewpoint-induced inconsistencies. In our experiments, we show that PPC\nimproves robustness to unseen camera poses across challenging robotic tasks,\nproviding a principled alternative to domain randomization.\n","authors":["Michael Bezick","Vittorio Giammarino","Ahmed H. Qureshi"],"pdf_url":"https://arxiv.org/pdf/2510.20974v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24335v1","updated":"2025-10-28T11:57:33Z","published":"2025-10-28T11:57:33Z","title":"NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation","summary":"  We present NVSim, a framework that automatically constructs large-scale,\nnavigable indoor simulators from only common image sequences, overcoming the\ncost and scalability limitations of traditional 3D scanning. Our approach\nadapts 3D Gaussian Splatting to address visual artifacts on sparsely observed\nfloors a common issue in robotic traversal data. We introduce Floor-Aware\nGaussian Splatting to ensure a clean, navigable ground plane, and a novel\nmesh-free traversability checking algorithm that constructs a topological graph\nby directly analyzing rendered views. We demonstrate our system's ability to\ngenerate valid, large-scale navigation graphs from real-world data. A video\ndemonstration is avilable at https://youtu.be/tTiIQt6nXC8\n","authors":["Mingyu Jeong","Eunsung Kim","Sehun Park","Andrew Jaeyong Choi"],"pdf_url":"https://arxiv.org/pdf/2510.24335v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2510.24315v1","updated":"2025-10-28T11:24:58Z","published":"2025-10-28T11:24:58Z","title":"Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground\n  Cooperation","summary":"  CoNi-MPC provides an efficient framework for UAV control in air-ground\ncooperative tasks by relying exclusively on relative states, eliminating the\nneed for global state estimation. However, its lack of environmental\ninformation poses significant challenges for obstacle avoidance. To address\nthis issue, we propose a novel obstacle avoidance algorithm, Cooperative\nNon-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for\nUAV-UGV cooperative scenarios without reliance on global state estimation or\nobstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data\nfrom the UAV to generate a modulation matrix, which directly adjusts the\nquadrotor's velocity to achieve obstacle avoidance. This modulation-based\nmethod enables real-time generation of collision-free trajectories within the\nUGV's non-inertial frame, significantly reducing computational demands (less\nthan 5 ms per iteration) while maintaining safety in dynamic and unpredictable\nenvironments. The key contributions of this work include: (1) a\nmodulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV\ncooperation in non-inertial frames without global states; (2) rapid, real-time\ntrajectory generation based solely on single-frame LiDAR data, removing the\nneed for obstacle modeling or prediction; and (3) adaptability to both static\nand dynamic environments, thus extending applicability to featureless or\nunknown scenarios.\n","authors":["Baozhe Zhang","Xinwei Chen","Qingcheng Chen","Chao Xu","Fei Gao","Yanjun Cao"],"pdf_url":"https://arxiv.org/pdf/2510.24315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10755v3","updated":"2025-10-28T11:05:00Z","published":"2025-05-15T23:47:58Z","title":"Procedural Generation of Articulated Simulation-Ready Assets","summary":"  We introduce Infinigen-Articulated, a toolkit for generating realistic,\nprocedurally generated articulated assets for robotics simulation. We include\nprocedural generators for 18 common articulated object categories along with\nhigh-level utilities for use creating custom articulated assets in Blender. We\nalso provide an export pipeline to integrate the resulting assets along with\ntheir physical properties into common robotics simulators. Experiments\ndemonstrate that assets sampled from these generators are effective for movable\nobject segmentation, training generalizable reinforcement learning policies,\nand sim-to-real transfer of imitation learning policies.\n","authors":["Abhishek Joshi","Beining Han","Jack Nugent","Max Gonzalez Saez-Diez","Yiming Zuo","Jonathan Liu","Hongyu Wen","Stamatis Alexandropoulos","Karhan Kayan","Anna Calveri","Tao Sun","Gaowen Liu","Yi Shao","Alexander Raistrick","Jia Deng"],"pdf_url":"https://arxiv.org/pdf/2505.10755v3.pdf","comment":"Updated to include information on newly implemented assets, new\n  experimental results (both simulation and real world), and additional\n  features including material and dynamics parameters"},{"id":"http://arxiv.org/abs/2504.09230v2","updated":"2025-10-28T10:58:38Z","published":"2025-04-12T14:15:27Z","title":"Concurrent-Allocation Task Execution for Multi-Robot\n  Path-Crossing-Minimal Navigation in Obstacle Environments","summary":"  Reducing undesirable path crossings among trajectories of different robots is\nvital in multi-robot navigation missions, which not only reduces detours and\nconflict scenarios, but also enhances navigation efficiency and boosts\nproductivity. Despite recent progress in multi-robot path-crossing-minimal\n(MPCM) navigation, the majority of approaches depend on the minimal\nsquared-distance reassignment of suitable desired points to robots directly.\nHowever, if obstacles occupy the passing space, calculating the actual\nrobot-point distances becomes complex or intractable, which may render the MPCM\nnavigation in obstacle environments inefficient or even infeasible.\n  In this paper, the concurrent-allocation task execution (CATE) algorithm is\npresented to address this problem (i.e., MPCM navigation in obstacle\nenvironments). First, the path-crossing-related elements in terms of (i) robot\nallocation, (ii) desired-point convergence, and (iii) collision and obstacle\navoidance are encoded into integer and control barrier function (CBF)\nconstraints. Then, the proposed constraints are used in an online constrained\noptimization framework, which implicitly yet effectively minimizes the possible\npath crossings and trajectory length in obstacle environments by minimizing the\ndesired point allocation cost and slack variables in CBF constraints\nsimultaneously. In this way, the MPCM navigation in obstacle environments can\nbe achieved with flexible spatial orderings. Note that the feasibility of\nsolutions and the asymptotic convergence property of the proposed CATE\nalgorithm in obstacle environments are both guaranteed, and the calculation\nburden is also reduced by concurrently calculating the optimal allocation and\nthe control input directly without the path planning process.\n","authors":["Bin-Bin Hu","Weijia Yao","Yanxin Zhou","Henglai Wei","Chen Lv"],"pdf_url":"https://arxiv.org/pdf/2504.09230v2.pdf","comment":"Accepted in IEEE Transactions on Robotics"},{"id":"http://arxiv.org/abs/2502.01484v2","updated":"2025-10-28T10:26:35Z","published":"2025-02-03T16:20:03Z","title":"Robot Cell Modeling via Exploratory Robot Motions: A Novel and\n  Accessible Data-Driven Approach","summary":"  Generating a collision-free robot motion is crucial for safe applications in\nreal-world settings. This requires an accurate model of all obstacle shapes\nwithin the constrained robot cell, which is particularly challenging and\ntime-consuming. The difficulty is heightened in flexible production lines,\nwhere the environment model must be updated each time the robot cell is\nmodified. Furthermore, sensor-based methods often necessitate costly hardware\nand calibration procedures and can be influenced by environmental factors\n(e.g., light conditions or reflections). To address these challenges, we\npresent a novel data-driven approach to modeling a cluttered workspace,\nleveraging solely the robot internal joint encoders to capture exploratory\nmotions. By computing the corresponding swept volume (SV), we generate a\n(conservative) mesh of the environment that is subsequently used for collision\nchecking within established path planning and control methods. Our method\nsignificantly reduces the complexity and cost of classical environment modeling\nby removing the need for computer-aided design (CAD) files and external\nsensors. We validate the approach with the KUKA LBR iisy collaborative robot in\na pick-and-place scenario. In less than three minutes of exploratory robot\nmotions and less than four additional minutes of computation time, we obtain an\naccurate model that enables collision-free motions. Our approach is intuitive\nand easy to use, making it accessible to users without specialized technical\nknowledge. It is applicable to all types of industrial robots or cobots.\n","authors":["Gaetano Meli","Niels Dehio"],"pdf_url":"https://arxiv.org/pdf/2502.01484v2.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.24261v1","updated":"2025-10-28T10:17:11Z","published":"2025-10-28T10:17:11Z","title":"DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic\n  Manipulation","summary":"  Learning generalizable robotic manipulation policies remains a key challenge\ndue to the scarcity of diverse real-world training data. While recent\napproaches have attempted to mitigate this through self-supervised\nrepresentation learning, most either rely on 2D vision pretraining paradigms\nsuch as masked image modeling, which primarily focus on static semantics or\nscene geometry, or utilize large-scale video prediction models that emphasize\n2D dynamics, thus failing to jointly learn the geometry, semantics, and\ndynamics required for effective manipulation. In this paper, we present\nDynaRend, a representation learning framework that learns 3D-aware and\ndynamics-informed triplane features via masked reconstruction and future\nprediction using differentiable volumetric rendering. By pretraining on\nmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, future\ndynamics, and task semantics in a unified triplane representation. The learned\nrepresentations can be effectively transferred to downstream robotic\nmanipulation tasks via action value map prediction. We evaluate DynaRend on two\nchallenging benchmarks, RLBench and Colosseum, as well as in real-world robotic\nexperiments, demonstrating substantial improvements in policy success rate,\ngeneralization to environmental perturbations, and real-world applicability\nacross diverse manipulation tasks.\n","authors":["Jingyi Tian","Le Wang","Sanping Zhou","Sen Wang","Jiayi Li","Gang Hua"],"pdf_url":"https://arxiv.org/pdf/2510.24261v1.pdf","comment":"Accepted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24259v1","updated":"2025-10-28T10:13:43Z","published":"2025-10-28T10:13:43Z","title":"Can LLMs Translate Human Instructions into a Reinforcement Learning\n  Agent's Internal Emergent Symbolic Representation?","summary":"  Emergent symbolic representations are critical for enabling developmental\nlearning agents to plan and generalize across tasks. In this work, we\ninvestigate whether large language models (LLMs) can translate human natural\nlanguage instructions into the internal symbolic representations that emerge\nduring hierarchical reinforcement learning. We apply a structured evaluation\nframework to measure the translation performance of commonly seen LLMs -- GPT,\nClaude, Deepseek and Grok -- across different internal symbolic partitions\ngenerated by a hierarchical reinforcement learning algorithm in the Ant Maze\nand Ant Fall environments. Our findings reveal that although LLMs demonstrate\nsome ability to translate natural language into a symbolic representation of\nthe environment dynamics, their performance is highly sensitive to partition\ngranularity and task complexity. The results expose limitations in current LLMs\ncapacity for representation alignment, highlighting the need for further\nresearch on robust alignment between language and internal agent\nrepresentations.\n","authors":["Ziqi Ma","Sao Mai Nguyen","Philippe Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24257v1","updated":"2025-10-28T10:10:59Z","published":"2025-10-28T10:10:59Z","title":"Manipulate as Human: Learning Task-oriented Manipulation Skills by\n  Adversarial Motion Priors","summary":"  In recent years, there has been growing interest in developing robots and\nautonomous systems that can interact with human in a more natural and intuitive\nway. One of the key challenges in achieving this goal is to enable these\nsystems to manipulate objects and tools in a manner that is similar to that of\nhumans. In this paper, we propose a novel approach for learning human-style\nmanipulation skills by using adversarial motion priors, which we name HMAMP.\nThe approach leverages adversarial networks to model the complex dynamics of\ntool and object manipulation, as well as the aim of the manipulation task. The\ndiscriminator is trained using a combination of real-world data and simulation\ndata executed by the agent, which is designed to train a policy that generates\nrealistic motion trajectories that match the statistical properties of human\nmotion. We evaluated HMAMP on one challenging manipulation task: hammering, and\nthe results indicate that HMAMP is capable of learning human-style manipulation\nskills that outperform current baseline methods. Additionally, we demonstrate\nthat HMAMP has potential for real-world applications by performing real robot\narm hammering tasks. In general, HMAMP represents a significant step towards\ndeveloping robots and autonomous systems that can interact with humans in a\nmore natural and intuitive way, by learning to manipulate tools and objects in\na manner similar to how humans do.\n","authors":["Ziqi Ma","Changda Tian","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2510.24257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.00037v3","updated":"2025-10-28T09:55:21Z","published":"2025-09-26T14:42:23Z","title":"On Robustness of Vision-Language-Action Model against Multi-Modal\n  Perturbations","summary":"  In Vision-Language-Action (VLA) models, robustness to real-world\nperturbations is critical for deployment. Existing methods target simple visual\ndisturbances, overlooking the broader multi-modal perturbations that arise in\nactions, instructions, environments, and observations. Here, we first evaluate\nthe robustness of mainstream VLAs under 17 perturbations across four\nmodalities. We find (1) actions as the most fragile modality, (2) Existing\nvisual-robust VLA do not gain robustness in other modality, and (3) pi0\ndemonstrates superior robustness with a diffusion-based action head. To build\nmulti-modal robust VLAs, we propose RobustVLA against perturbations in VLA\ninputs and outputs. For output robustness, we perform offline robust\noptimization against worst-case action noise that maximizes mismatch in flow\nmatching objective. This can be seen as adversarial training, label smoothing,\nand outlier penalization. For input robustness, we enforce consistent actions\nacross input variations that preserve task semantics. To account for multiple\nperturbations, we formulate robustness as a multi-armed bandit problem and\napply an upper confidence bound algorithm to automatically identify the most\nharmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers\nabsolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the\nOpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference\nthan existing visual-robust VLAs, and a 10.4% gain under mixed perturbations.\nOur RobustVLA is particularly effective on real-world FR5 robot with limited\ndemonstrations, showing absolute gains by 65.6% under perturbations of four\nmodalities.\n","authors":["Jianing Guo","Zhenhong Wu","Chang Tu","Yiyao Ma","Xiangqi Kong","Zhiqian Liu","Jiaming Ji","Shuning Zhang","Yuanpei Chen","Kai Chen","Qi Dou","Yaodong Yang","Xianglong Liu","Huijie Zhao","Weifeng Lv","Simin Li"],"pdf_url":"https://arxiv.org/pdf/2510.00037v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24194v1","updated":"2025-10-28T08:57:27Z","published":"2025-10-28T08:57:27Z","title":"Blindfolded Experts Generalize Better: Insights from Robotic\n  Manipulation and Videogames","summary":"  Behavioral cloning is a simple yet effective technique for learning\nsequential decision-making from demonstrations. Recently, it has gained\nprominence as the core of foundation models for the physical world, where\nachieving generalization requires countless demonstrations of a multitude of\ntasks. Typically, a human expert with full information on the task demonstrates\na (nearly) optimal behavior. In this paper, we propose to hide some of the\ntask's information from the demonstrator. This ``blindfolded'' expert is\ncompelled to employ non-trivial exploration to solve the task. We show that\ncloning the blindfolded expert generalizes better to unseen tasks than its\nfully-informed counterpart. We conduct experiments of real-world robot peg\ninsertion tasks with (limited) human demonstrations, alongside videogames from\nthe Procgen benchmark. Additionally, we support our findings with theoretical\nanalysis, which confirms that the generalization error scales with\n$\\sqrt{I/m}$, where $I$ measures the amount of task information available to\nthe demonstrator, and $m$ is the number of demonstrated tasks. Both theory and\npractice indicate that cloning blindfolded experts generalizes better with\nfewer demonstrated tasks. Project page with videos and code:\nhttps://sites.google.com/view/blindfoldedexperts/home\n","authors":["Ev Zisselman","Mirco Mutti","Shelly Francis-Meretzki","Elisei Shafer","Aviv Tamar"],"pdf_url":"https://arxiv.org/pdf/2510.24194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01356v2","updated":"2025-10-28T08:42:04Z","published":"2025-06-02T06:20:09Z","title":"Two-Stage Learning of Stabilizing Neural Controllers via Zubov Sampling\n  and Iterative Domain Expansion","summary":"  Learning-based neural network (NN) control policies have shown impressive\nempirical performance. However, obtaining stability guarantees and estimates of\nthe region of attraction of these learned neural controllers is challenging due\nto the lack of stable and scalable training and verification algorithms.\nAlthough previous works in this area have achieved great success, much\nconservatism remains in their frameworks. In this work, we propose a novel\ntwo-stage training framework to jointly synthesize a controller and a Lyapunov\nfunction for continuous-time systems. By leveraging a Zubov-inspired region of\nattraction characterization to directly estimate stability boundaries, we\npropose a novel training-data sampling strategy and a domain-updating mechanism\nthat significantly reduces the conservatism in training. Moreover, unlike\nexisting works on continuous-time systems that rely on an SMT solver to\nformally verify the Lyapunov condition, we extend state-of-the-art neural\nnetwork verifier $\\alpha,\\!\\beta$-CROWN with the capability of performing\nautomatic bound propagation through the Jacobian of dynamical systems and a\nnovel verification scheme that avoids expensive bisection. To demonstrate the\neffectiveness of our approach, we conduct numerical experiments by synthesizing\nand verifying controllers on several challenging nonlinear systems across\nmultiple dimensions. We show that our training can yield region of attractions\nwith volume $5 - 1.5\\cdot 10^{5}$ times larger compared to the baselines, and\nour verification on continuous systems can be up to $40-10{,}000$ times faster\ncompared to the traditional SMT solver dReal. Our code is available at\nhttps://github.com/Verified-Intelligence/Two-Stage_Neural_Controller_Training.\n","authors":["Haoyu Li","Xiangru Zhong","Bin Hu","Huan Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.01356v2.pdf","comment":"NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.22672v2","updated":"2025-10-28T08:39:14Z","published":"2025-10-26T13:27:59Z","title":"Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and\n  Exocentric Views","summary":"  We introduce Look and Tell, a multimodal dataset for studying referential\ncommunication across egocentric and exocentric perspectives. Using Meta Project\nAria smart glasses and stationary cameras, we recorded synchronized gaze,\nspeech, and video as 25 participants instructed a partner to identify\ningredients in a kitchen. Combined with 3D scene reconstructions, this setup\nprovides a benchmark for evaluating how different spatial representations (2D\nvs. 3D; ego vs. exo) affect multimodal grounding. The dataset contains 3.67\nhours of recordings, including 2,707 richly annotated referential expressions,\nand is designed to advance the development of embodied agents that can\nunderstand and engage in situated dialogue.\n","authors":["Anna Deichler","Jonas Beskow"],"pdf_url":"https://arxiv.org/pdf/2510.22672v2.pdf","comment":"10 pages, 6 figures, 2 tables. Accepted to the NeurIPS 2025 Workshop\n  on SPACE in Vision, Language, and Embodied AI (SpaVLE). Dataset:\n  https://huggingface.co/datasets/annadeichler/KTH-ARIA-referential"},{"id":"http://arxiv.org/abs/2503.08930v2","updated":"2025-10-28T08:30:20Z","published":"2025-03-11T22:18:57Z","title":"Acoustic Neural 3D Reconstruction Under Pose Drift","summary":"  We consider the problem of optimizing neural implicit surfaces for 3D\nreconstruction using acoustic images collected with drifting sensor poses. The\naccuracy of current state-of-the-art 3D acoustic modeling algorithms is highly\ndependent on accurate pose estimation; small errors in sensor pose can lead to\nsevere reconstruction artifacts. In this paper, we propose an algorithm that\njointly optimizes the neural scene representation and sonar poses. Our\nalgorithm does so by parameterizing the 6DoF poses as learnable parameters and\nbackpropagating gradients through the neural renderer and implicit\nrepresentation. We validated our algorithm on both real and simulated datasets.\nIt produces high-fidelity 3D reconstructions even under significant pose drift.\n","authors":["Tianxiang Lin","Mohamad Qadri","Kevin Zhang","Adithya Pediredla","Christopher A. Metzler","Michael Kaess"],"pdf_url":"https://arxiv.org/pdf/2503.08930v2.pdf","comment":"8 pages, 8 figures. This paper is accepted by 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2501.15806v2","updated":"2025-10-28T08:19:30Z","published":"2025-01-27T06:28:29Z","title":"Autonomous Horizon-based Asteroid Navigation With\n  Observability-constrained Maneuvers","summary":"  Small body exploration is a pertinent challenge due to low gravity\nenvironments and strong sensitivity to perturbations like Solar Radiation\nPressure (SRP). Thus, autonomous methods are being developed to enable safe\nnavigation and control around small bodies. These methods often involve using\nOptical Navigation (OpNav) to determine the spacecraft's location. Ensuring\nOpNav reliability would allow the spacecraft to maintain an accurate state\nestimate throughout its mission. This research presents an\nobservability-constrained Lyapunov controller that steers a spacecraft to a\ndesired target orbit while guaranteeing continuous OpNav observability. We\ndesign observability path constraints to avoid regions where horizon-based\nOpNav methods exhibit poor performance, ensuring control input that maintains\ngood observability. This controller is implemented with a framework that\nsimulates small body dynamics, synthetic image generation, edge detection,\nhorizon-based OpNav, and filtering. We evaluate the approach in two\nrepresentative scenarios, orbit maintenance and approach with circularization,\naround spherical and ellipsoidal target bodies. In Monte Carlo simulations, the\nproposed approach improves the rate of attaining target orbits without\nobservability violations by up to 94% compared to an unconstrained Lyapunov\nbaseline, demonstrating improved robustness over conventional methods.\n","authors":["Aditya Arjun Anibha","Kenshiro Oguri"],"pdf_url":"https://arxiv.org/pdf/2501.15806v2.pdf","comment":"52 pages, 18 figures, published in the Journal of the Astronautical\n  Sciences"},{"id":"http://arxiv.org/abs/2510.24161v1","updated":"2025-10-28T07:58:39Z","published":"2025-10-28T07:58:39Z","title":"BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and\n  Cross-Embodiment Learning","summary":"  Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.\n","authors":["Wentao Tan","Bowen Wang","Heng Zhi","Chenyu Liu","Zhe Li","Jian Liu","Zengrong Lin","Yukun Dai","Yipeng Chen","Wenjie Yang","Enci Xie","Hao Xue","Baixu Ji","Chen Xu","Zhibin Wang","Tianshi Wang","Lei Zhu","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2510.24161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24118v1","updated":"2025-10-28T06:42:21Z","published":"2025-10-28T06:42:21Z","title":"LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal\n  Open-vocabulary Multi-goal Visual Navigation","summary":"  Navigating to a designated goal using visual information is a fundamental\ncapability for intelligent robots. Most classical visual navigation methods are\nrestricted to single-goal, single-modality, and closed set goal settings. To\naddress the practical demands of multi-modal, open-vocabulary goal queries and\nmulti-goal visual navigation, we propose LagMemo, a navigation system that\nleverages a language 3D Gaussian Splatting memory. During exploration, LagMemo\nconstructs a unified 3D language memory. With incoming task goals, the system\nqueries the memory, predicts candidate goal locations, and integrates a local\nperception-based verification mechanism to dynamically match and validate goals\nduring navigation. For fair and rigorous evaluation, we curate GOAT-Core, a\nhigh-quality core split distilled from GOAT-Bench tailored to multi-modal\nopen-vocabulary multi-goal visual navigation. Experimental results show that\nLagMemo's memory module enables effective multi-modal open-vocabulary goal\nlocalization, and that LagMemo outperforms state-of-the-art methods in\nmulti-goal visual navigation. Project page:\nhttps://weekgoodday.github.io/lagmemo\n","authors":["Haotian Zhou","Xiaole Wang","He Li","Fusheng Sun","Shengyu Guo","Guolei Qi","Jianghuan Xu","Huijing Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.24118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24109v1","updated":"2025-10-28T06:28:21Z","published":"2025-10-28T06:28:21Z","title":"PFEA: An LLM-based High-Level Natural Language Planning and Feedback\n  Embodied Agent for Human-Centered AI","summary":"  The rapid advancement of Large Language Models (LLMs) has marked a\nsignificant breakthrough in Artificial Intelligence (AI), ushering in a new era\nof Human-centered Artificial Intelligence (HAI). HAI aims to better serve human\nwelfare and needs, thereby placing higher demands on the intelligence level of\nrobots, particularly in aspects such as natural language interaction, complex\ntask planning, and execution. Intelligent agents powered by LLMs have opened up\nnew pathways for realizing HAI. However, existing LLM-based embodied agents\noften lack the ability to plan and execute complex natural language control\ntasks online. This paper explores the implementation of intelligent robotic\nmanipulating agents based on Vision-Language Models (VLMs) in the physical\nworld. We propose a novel embodied agent framework for robots, which comprises\na human-robot voice interaction module, a vision-language agent module and an\naction execution module. The vision-language agent itself includes a\nvision-based task planner, a natural language instruction converter, and a task\nperformance feedback evaluator. Experimental results demonstrate that our agent\nachieves a 28\\% higher average task success rate in both simulated and real\nenvironments compared to approaches relying solely on LLM+CLIP, significantly\nimproving the execution success rate of high-level natural language instruction\ntasks.\n","authors":["Wenbin Ding","Jun Chen","Mingjia Chen","Fei Xie","Qi Mao","Philip Dames"],"pdf_url":"https://arxiv.org/pdf/2510.24109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24108v1","updated":"2025-10-28T06:26:36Z","published":"2025-10-28T06:26:36Z","title":"ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory\n  Scoring","summary":"  End-to-end autonomous driving maps raw sensor inputs directly into\nego-vehicle trajectories to avoid cascading errors from perception modules and\nto leverage rich semantic cues. Existing frameworks largely rely on Imitation\nLearning (IL), which can be limited by sub-optimal expert demonstrations and\ncovariate shift during deployment. On the other hand, Reinforcement Learning\n(RL) has recently shown potential in scaling up with simulations, but is\ntypically confined to low-dimensional symbolic inputs (e.g. 3D objects and\nmaps), falling short of full end-to-end learning from raw sensor data. We\nintroduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory\nScoring), a framework that combines the strengths of both worlds: sensor inputs\nwithout losing information and RL training for robust planning. To the best of\nour knowledge, ZTRS is the first framework that eliminates IL entirely by only\nlearning from rewards while operating directly on high-dimensional sensor data.\nZTRS utilizes offline reinforcement learning with our proposed Exhaustive\nPolicy Optimization (EPO), a variant of policy gradient tailored for enumerable\nactions and rewards. ZTRS demonstrates strong performance across three\nbenchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop\nplanning in challenging real-world and synthetic scenarios), and HUGSIM\n(simulated closed-loop driving). Specifically, ZTRS achieves the\nstate-of-the-art result on Navhard and outperforms IL-based baselines on\nHUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.\n","authors":["Zhenxin Li","Wenhao Yao","Zi Wang","Xinglong Sun","Jingde Chen","Nadine Chang","Maying Shen","Jingyu Song","Zuxuan Wu","Shiyi Lan","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2510.24108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24095v1","updated":"2025-10-28T06:08:25Z","published":"2025-10-28T06:08:25Z","title":"Learning Parameterized Skills from Demonstrations","summary":"  We present DEPS, an end-to-end algorithm for discovering parameterized skills\nfrom expert demonstrations. Our method learns parameterized skill policies\njointly with a meta-policy that selects the appropriate discrete skill and\ncontinuous parameters at each timestep. Using a combination of temporal\nvariational inference and information-theoretic regularization methods, we\naddress the challenge of degeneracy common in latent variable models, ensuring\nthat the learned skills are temporally extended, semantically meaningful, and\nadaptable. We empirically show that learning parameterized skills from\nmultitask expert demonstrations significantly improves generalization to unseen\ntasks. Our method outperforms multitask as well as skill learning baselines on\nboth LIBERO and MetaWorld benchmarks. We also demonstrate that DEPS discovers\ninterpretable parameterized skills, such as an object grasping skill whose\ncontinuous arguments define the grasp location.\n","authors":["Vedant Gupta","Haotian Fu","Calvin Luo","Yiding Jiang","George Konidaris"],"pdf_url":"https://arxiv.org/pdf/2510.24095v1.pdf","comment":"Neurips 2025"},{"id":"http://arxiv.org/abs/2510.24069v1","updated":"2025-10-28T05:03:39Z","published":"2025-10-28T05:03:39Z","title":"Dynamically-Consistent Trajectory Optimization for Legged Robots via\n  Contact Point Decomposition","summary":"  To generate reliable motion for legged robots through trajectory\noptimization, it is crucial to simultaneously compute the robot's path and\ncontact sequence, as well as accurately consider the dynamics in the problem\nformulation. In this paper, we present a phase-based trajectory optimization\nthat ensures the feasibility of translational dynamics and friction cone\nconstraints throughout the entire trajectory. Specifically, our approach\nleverages the superposition properties of linear differential equations to\ndecouple the translational dynamics for each contact point, which operates\nunder different phase sequences. Furthermore, we utilize the differentiation\nmatrix of B{\\'e}zier polynomials to derive an analytical relationship between\nthe robot's position and force, thereby ensuring the consistent satisfaction of\ntranslational dynamics. Additionally, by exploiting the convex closure property\nof B{\\'e}zier polynomials, our method ensures compliance with friction cone\nconstraints. Using the aforementioned approach, the proposed trajectory\noptimization framework can generate dynamically reliable motions with various\ngait sequences for legged robots. We validate our framework using a quadruped\nrobot model, focusing on the feasibility of dynamics and motion generation.\n","authors":["Sangmin Kim","Hajun Kim","Gijeong Kim","Min-Gyu Kim","Hae-Won Park"],"pdf_url":"https://arxiv.org/pdf/2510.24069v1.pdf","comment":"8 pages, 4 figures, IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT\n  VERSION. ACCEPTED OCTOBER, 2025"},{"id":"http://arxiv.org/abs/2510.24067v1","updated":"2025-10-28T05:01:04Z","published":"2025-10-28T05:01:04Z","title":"Balanced Collaborative Exploration via Distributed Topological Graph\n  Voronoi Partition","summary":"  This work addresses the collaborative multi-robot autonomous online\nexploration problem, particularly focusing on distributed exploration planning\nfor dynamically balanced exploration area partition and task allocation among a\nteam of mobile robots operating in obstacle-dense non-convex environments.\n  We present a novel topological map structure that simultaneously\ncharacterizes both spatial connectivity and global exploration completeness of\nthe environment. The topological map is updated incrementally to utilize known\nspatial information for updating reachable spaces, while exploration targets\nare planned in a receding horizon fashion under global coverage guidance.\n  A distributed weighted topological graph Voronoi algorithm is introduced\nimplementing balanced graph space partitions of the fused topological maps.\nTheoretical guarantees are provided for distributed consensus convergence and\nequitable graph space partitions with constant bounds.\n  A local planner optimizes the visitation sequence of exploration targets\nwithin the balanced partitioned graph space to minimize travel distance, while\ngenerating safe, smooth, and dynamically feasible motion trajectories.\n  Comprehensive benchmarking against state-of-the-art methods demonstrates\nsignificant improvements in exploration efficiency, completeness, and workload\nbalance across the robot team.\n","authors":["Tianyi Ding","Ronghao Zheng","Senlin Zhang","Meiqin Liu"],"pdf_url":"https://arxiv.org/pdf/2510.24067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24055v1","updated":"2025-10-28T04:27:03Z","published":"2025-10-28T04:27:03Z","title":"Language-Conditioned Representations and Mixture-of-Experts Policy for\n  Robust Multi-Task Robotic Manipulation","summary":"  Perceptual ambiguity and task conflict limit multitask robotic manipulation\nvia imitation learning. We propose a framework combining a Language-Conditioned\nVisual Representation (LCVR) module and a Language-conditioned\nMixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual\nambiguities by grounding visual features with language instructions, enabling\ndifferentiation between visually similar tasks. To mitigate task conflict,\nLMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal\naction distributions, stabilized by gradient modulation. On real-robot\nbenchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion\nPolicy (DP) success rates by 33.75% and 25%, respectively. The full framework\nachieves a 79% average success, outperforming the advanced baseline by 21%. Our\nwork shows that combining semantic grounding and expert specialization enables\nrobust, efficient multi-task manipulation\n","authors":["Xiucheng Zhang","Yang Jiang","Hongwei Qing","Jiashuo Bai"],"pdf_url":"https://arxiv.org/pdf/2510.24055v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2510.24052v1","updated":"2025-10-28T04:22:02Z","published":"2025-10-28T04:22:02Z","title":"SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through\n  Synthetic Data Integration","summary":"  Recent advancements in deep learning and the availability of high-quality\nreal-world driving datasets have propelled end-to-end autonomous driving.\nDespite this progress, relying solely on real-world data limits the variety of\ndriving scenarios for training. Synthetic scenario generation has emerged as a\npromising solution to enrich the diversity of training data; however, its\napplication within E2E AD models remains largely unexplored. This is primarily\ndue to the absence of a designated ego vehicle and the associated sensor\ninputs, such as camera or LiDAR, typically provided in real-world scenarios. To\naddress this gap, we introduce SynAD, the first framework designed to enhance\nreal-world E2E AD models using synthetic data. Our method designates the agent\nwith the most comprehensive driving information as the ego vehicle in a\nmulti-agent synthetic scenario. We further project path-level scenarios onto\nmaps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view\nfeatures without relying on sensor inputs. Finally, we devise a training\nstrategy that effectively integrates these map-based synthetic data with real\ndriving data. Experimental results demonstrate that SynAD effectively\nintegrates all components and notably enhances safety performance. By bridging\nsynthetic scenario generation and E2E AD, SynAD paves the way for more\ncomprehensive and robust autonomous driving models.\n","authors":["Jongsuk Kim","Jaeyoung Lee","Gyojin Han","Dongjae Lee","Minki Jeong","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2510.24052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24029v1","updated":"2025-10-28T03:24:02Z","published":"2025-10-28T03:24:02Z","title":"Improved Accuracy of Robot Localization Using 3-D LiDAR in a\n  Hippocampus-Inspired Model","summary":"  Boundary Vector Cells (BVCs) are a class of neurons in the brains of\nvertebrates that encode environmental boundaries at specific distances and\nallocentric directions, playing a central role in forming place fields in the\nhippocampus. Most computational BVC models are restricted to two-dimensional\n(2D) environments, making them prone to spatial ambiguities in the presence of\nhorizontal symmetries in the environment. To address this limitation, we\nincorporate vertical angular sensitivity into the BVC framework, thereby\nenabling robust boundary detection in three dimensions, and leading to\nsignificantly more accurate spatial localization in a biologically-inspired\nrobot model.\n  The proposed model processes LiDAR data to capture vertical contours, thereby\ndisambiguating locations that would be indistinguishable under a purely 2D\nrepresentation. Experimental results show that in environments with minimal\nvertical variation, the proposed 3D model matches the performance of a 2D\nbaseline; yet, as 3D complexity increases, it yields substantially more\ndistinct place fields and markedly reduces spatial aliasing. These findings\nshow that adding a vertical dimension to BVC-based localization can\nsignificantly enhance navigation and mapping in real-world 3D spaces while\nretaining performance parity in simpler, near-planar scenarios.\n","authors":["Andrew Gerstenslager","Bekarys Dukenbaev","Ali A. Minai"],"pdf_url":"https://arxiv.org/pdf/2510.24029v1.pdf","comment":"8 pages, 9 figures, Presented at the 2025 International Joint\n  Conference on Neural Networks, Rome, July 2025"},{"id":"http://arxiv.org/abs/2508.05186v3","updated":"2025-10-28T03:21:38Z","published":"2025-08-07T09:21:20Z","title":"Learning to See and Act: Task-Aware View Planning for Robotic\n  Manipulation","summary":"  Recent vision-language-action (VLA) models for multi-task robotic\nmanipulation commonly rely on static viewpoints and shared visual encoders,\nwhich limit 3D perception and cause task interference, hindering robustness and\ngeneralization. In this work, we propose Task-Aware View Planning (TAVP), a\nframework designed to overcome these challenges by integrating active view\nplanning with task-specific representation learning. TAVP employs an efficient\nexploration policy, accelerated by a novel pseudo-environment, to actively\nacquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE)\nvisual encoder to disentangle features across different tasks, boosting both\nrepresentation fidelity and task generalization. By learning to see the world\nin a task-aware way, TAVP generates more complete and discriminative visual\nrepresentations, demonstrating significantly enhanced action prediction across\na wide array of manipulation challenges. Extensive experiments on RLBench tasks\nshow that our proposed TAVP model achieves superior performance over\nstate-of-the-art fixed-view approaches. Visual results and code are provided\nat: https://hcplab-sysu.github.io/TAVP.\n","authors":["Yongjie Bai","Zhouxia Wang","Yang Liu","Weixing Chen","Ziliang Chen","Mingtong Dai","Yongsen Zheng","Lingbo Liu","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2508.05186v3.pdf","comment":"14 pages, 8 figures, project page: https://hcplab-sysu.github.io/TAVP"},{"id":"http://arxiv.org/abs/2509.19804v2","updated":"2025-10-28T03:10:25Z","published":"2025-09-24T06:40:57Z","title":"DynaFlow: Dynamics-embedded Flow Matching for Physically Consistent\n  Motion Generation from State-only Demonstrations","summary":"  This paper introduces DynaFlow, a novel framework that embeds a\ndifferentiable simulator directly into a flow matching model. By generating\ntrajectories in the action space and mapping them to dynamically feasible state\ntrajectories via the simulator, DynaFlow ensures all outputs are physically\nconsistent by construction. This end-to-end differentiable architecture enables\ntraining on state-only demonstrations, allowing the model to simultaneously\ngenerate physically consistent state trajectories while inferring the\nunderlying action sequences required to produce them. We demonstrate the\neffectiveness of our approach through quantitative evaluations and showcase its\nreal-world applicability by deploying the generated actions onto a physical Go1\nquadruped robot. The robot successfully reproduces diverse gait present in the\ndataset, executes long-horizon motions in open-loop control and translates\ninfeasible kinematic demonstrations into dynamically executable, stylistic\nbehaviors. These hardware experiments validate that DynaFlow produces\ndeployable, highly effective motions on real-world hardware from state-only\ndemonstrations, effectively bridging the gap between kinematic data and\nreal-world execution.\n","authors":["Sowoo Lee","Dongyun Kang","Jaehyun Park","Hae-Won Park"],"pdf_url":"https://arxiv.org/pdf/2509.19804v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2510.22917v2","updated":"2025-10-28T02:49:09Z","published":"2025-10-27T01:43:56Z","title":"HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown\n  Environment","summary":"  Objective-oriented navigation(ObjNav) enables robot to navigate to target\nobject directly and autonomously in an unknown environment. Effective\nperception in navigation in unknown environment is critical for autonomous\nrobots. While egocentric observations from RGB-D sensors provide abundant local\ninformation, real-time top-down maps offer valuable global context for ObjNav.\nNevertheless, the majority of existing studies focus on a single source, seldom\nintegrating these two complementary perceptual modalities, despite the fact\nthat humans naturally attend to both. With the rapid advancement of\nVision-Language Models(VLMs), we propose Hybrid Perception Navigation\n(HyPerNav), leveraging VLMs' strong reasoning and vision-language understanding\ncapabilities to jointly perceive both local and global information to enhance\nthe effectiveness and intelligence of navigation in unknown environments. In\nboth massive simulation evaluation and real-world validation, our methods\nachieved state-of-the-art performance against popular baselines. Benefiting\nfrom hybrid perception approach, our method captures richer cues and finds the\nobjects more effectively, by simultaneously leveraging information\nunderstanding from egocentric observations and the top-down map. Our ablation\nstudy further proved that either of the hybrid perception contributes to the\nnavigation performance.\n","authors":["Zecheng Yin","Hao Zhao","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2510.22917v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2506.00034v2","updated":"2025-10-28T02:15:21Z","published":"2025-05-27T01:43:02Z","title":"GaussianFusion: Gaussian-Based Multi-Sensor Fusion for End-to-End\n  Autonomous Driving","summary":"  Multi-sensor fusion is crucial for improving the performance and robustness\nof end-to-end autonomous driving systems. Existing methods predominantly adopt\neither attention-based flatten fusion or bird's eye view fusion through\ngeometric transformations. However, these approaches often suffer from limited\ninterpretability or dense computational overhead. In this paper, we introduce\nGaussianFusion, a Gaussian-based multi-sensor fusion framework for end-to-end\nautonomous driving. Our method employs intuitive and compact Gaussian\nrepresentations as intermediate carriers to aggregate information from diverse\nsensors. Specifically, we initialize a set of 2D Gaussians uniformly across the\ndriving scene, where each Gaussian is parameterized by physical attributes and\nequipped with explicit and implicit features. These Gaussians are progressively\nrefined by integrating multi-modal features. The explicit features capture rich\nsemantic and spatial information about the traffic scene, while the implicit\nfeatures provide complementary cues beneficial for trajectory planning. To\nfully exploit rich spatial and semantic information in Gaussians, we design a\ncascade planning head that iteratively refines trajectory predictions through\ninteractions with Gaussians. Extensive experiments on the NAVSIM and\nBench2Drive benchmarks demonstrate the effectiveness and robustness of the\nproposed GaussianFusion framework. The source code will be released at\nhttps://github.com/Say2L/GaussianFusion.\n","authors":["Shuai Liu","Quanmin Liang","Zefeng Li","Boyang Li","Kai Huang"],"pdf_url":"https://arxiv.org/pdf/2506.00034v2.pdf","comment":"Accepted at NeurIPS2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2506.18443v2","updated":"2025-10-28T01:59:34Z","published":"2025-06-23T09:27:22Z","title":"Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation","summary":"  Achieving reliable ego motion estimation for agile robots, e.g., aerobatic\naircraft, remains challenging because most robot sensors fail to respond timely\nand clearly to highly dynamic robot motions, often resulting in measurement\nblurring, distortion, and delays. In this paper, we propose an IMU-free and\nfeature-association-free framework to achieve aggressive ego-motion velocity\nestimation of a robot platform in highly dynamic scenarios by combining two\ntypes of exteroceptive sensors, an event camera and a millimeter wave radar,\nFirst, we used instantaneous raw events and Doppler measurements to derive\nrotational and translational velocities directly. Without a sophisticated\nassociation process between measurement frames, the proposed method is more\nrobust in texture-less and structureless environments and is more\ncomputationally efficient for edge computing devices. Then, in the back-end, we\npropose a continuous-time state-space model to fuse the hybrid time-based and\nevent-based measurements to estimate the ego-motion velocity in a fixed-lagged\nsmoother fashion. In the end, we validate our velometer framework extensively\nin self-collected experiment datasets. The results indicate that our IMU-free\nand association-free ego motion estimation framework can achieve reliable and\nefficient velocity output in challenging environments. The source code,\nillustrative video and dataset are available at\nhttps://github.com/ZzhYgwh/TwistEstimator.\n","authors":["Yang Lyu","Zhenghao Zou","Yanfeng Li","Xiaohu Guo","Chunhui Zhao","Quan Pan"],"pdf_url":"https://arxiv.org/pdf/2506.18443v2.pdf","comment":"2025.10.28 version v2 for TwistEstimator"},{"id":"http://arxiv.org/abs/2510.23997v1","updated":"2025-10-28T01:59:34Z","published":"2025-10-28T01:59:34Z","title":"VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion","summary":"  Recent advancements in legged robot locomotion have facilitated traversal\nover increasingly complex terrains. Despite this progress, many existing\napproaches rely on end-to-end deep reinforcement learning (DRL), which poses\nlimitations in terms of safety and interpretability, especially when\ngeneralizing to novel terrains. To overcome these challenges, we introduce\nVOCALoco, a modular skill-selection framework that dynamically adapts\nlocomotion strategies based on perceptual input. Given a set of pre-trained\nlocomotion policies, VOCALoco evaluates their viability and energy-consumption\nby predicting both the safety of execution and the anticipated cost of\ntransport over a fixed planning horizon. This joint assessment enables the\nselection of policies that are both safe and energy-efficient, given the\nobserved local terrain. We evaluate our approach on staircase locomotion tasks,\ndemonstrating its performance in both simulated and real-world scenarios using\na quadrupedal robot. Empirical results show that VOCALoco achieves improved\nrobustness and safety during stair ascent and descent compared to a\nconventional end-to-end DRL policy\n","authors":["Stanley Wu","Mohamad H. Danesh","Simon Li","Hanna Yurchyk","Amin Abyaneh","Anas El Houssaini","David Meger","Hsiu-Chin Lin"],"pdf_url":"https://arxiv.org/pdf/2510.23997v1.pdf","comment":"Accepted in IEEE Robotics and Automation Letters (RAL), 2025. 8\n  pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.23988v1","updated":"2025-10-28T01:44:52Z","published":"2025-10-28T01:44:52Z","title":"A Survey on Collaborative SLAM with 3D Gaussian Splatting","summary":"  This survey comprehensively reviews the evolving field of multi-robot\ncollaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian\nSplatting (3DGS). As an explicit scene representation, 3DGS has enabled\nunprecedented real-time, high-fidelity rendering, ideal for robotics. However,\nits use in multi-robot systems introduces significant challenges in maintaining\nglobal consistency, managing communication, and fusing data from heterogeneous\nsources. We systematically categorize approaches by their architecture --\ncentralized, distributed -- and analyze core components like multi-agent\nconsistency and alignment, communication-efficient, Gaussian representation,\nsemantic distillation, fusion and pose optimization, and real-time scalability.\nIn addition, a summary of critical datasets and evaluation metrics is provided\nto contextualize performance. Finally, we identify key open challenges and\nchart future research directions, including lifelong mapping, semantic\nassociation and mapping, multi-model for robustness, and bridging the Sim2Real\ngap.\n","authors":["Phuc Nguyen Xuan","Thanh Nguyen Canh","Huu-Hung Nguyen","Nak Young Chong","Xiem HoangVan"],"pdf_url":"https://arxiv.org/pdf/2510.23988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23502v3","updated":"2025-10-28T01:42:48Z","published":"2025-03-30T16:24:22Z","title":"Boosting Omnidirectional Stereo Matching with a Pre-trained Depth\n  Foundation Model","summary":"  Omnidirectional depth perception is essential for mobile robotics\napplications that require scene understanding across a full 360{\\deg} field of\nview. Camera-based setups offer a cost-effective option by using stereo depth\nestimation to generate dense, high-resolution depth maps without relying on\nexpensive active sensing. However, existing omnidirectional stereo matching\napproaches achieve only limited depth accuracy across diverse environments,\ndepth ranges, and lighting conditions, due to the scarcity of real-world data.\nWe present DFI-OmniStereo, a novel omnidirectional stereo matching method that\nleverages a large-scale pre-trained foundation model for relative monocular\ndepth estimation within an iterative optimization-based stereo matching\narchitecture. We introduce a dedicated two-stage training strategy to utilize\nthe relative monocular depth features for our omnidirectional stereo matching\nbefore scale-invariant fine-tuning. DFI-OmniStereo achieves state-of-the-art\nresults on the real-world Helvipad dataset, reducing disparity MAE by\napproximately 16% compared to the previous best omnidirectional stereo method.\n","authors":["Jannik Endres","Oliver Hahn","Charles Corbière","Simone Schaub-Meyer","Stefan Roth","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2503.23502v3.pdf","comment":"Accepted at IROS 2025. Project page:\n  https://vita-epfl.github.io/DFI-OmniStereo-website/"},{"id":"http://arxiv.org/abs/2410.15536v3","updated":"2025-10-28T01:05:53Z","published":"2024-10-20T23:33:06Z","title":"GRS: Generating Robotic Simulation Tasks from Real-World Images","summary":"  We introduce GRS (Generating Robotic Simulation tasks), a system addressing\nreal-to-sim for robotic simulations. GRS creates digital twin simulations from\nsingle RGB-D observations with solvable tasks for virtual agent training. Using\nvision-language models (VLMs), our pipeline operates in three stages: 1) scene\ncomprehension with SAM2 for segmentation and object description, 2) matching\nobjects with simulation-ready assets, and 3) generating appropriate tasks. We\nensure simulation-task alignment through generated test suites and introduce a\nrouter that iteratively refines both simulation and test code. Experiments\ndemonstrate our system's effectiveness in object correspondence and task\nenvironment generation through our novel router mechanism.\n","authors":["Alex Zook","Fan-Yun Sun","Josef Spjut","Valts Blukis","Stan Birchfield","Jonathan Tremblay"],"pdf_url":"https://arxiv.org/pdf/2410.15536v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17191v2","updated":"2025-10-28T00:58:56Z","published":"2025-10-20T06:09:57Z","title":"SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End\n  Autonomous Driving","summary":"  End-to-end autonomous driving has emerged as a promising paradigm for\nachieving robust and intelligent driving policies. However, existing end-to-end\nmethods still face significant challenges, such as suboptimal decision-making\nin complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring\nFusion), a novel framework that enhances end-to-end planning by leveraging the\ncognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory\nfusion techniques. We utilize the conventional scorers and the novel\nVLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative\naggregation and a powerful VLM-based fusioner for qualitative, context-aware\ndecision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End\nDriving Challenge, our SimpleVSF framework demonstrates state-of-the-art\nperformance, achieving a superior balance between safety, comfort, and\nefficiency.\n","authors":["Peiru Zheng","Yun Zhao","Zhan Gong","Hong Zhu","Shaohua Wu"],"pdf_url":"https://arxiv.org/pdf/2510.17191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23963v1","updated":"2025-10-28T00:37:20Z","published":"2025-10-28T00:37:20Z","title":"Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping","summary":"  This paper presents a soft robot finger capable of adaptive-twist deformation\nto grasp objects by wrapping them. For a soft hand to grasp and pick-up one\nobject from densely contained multiple objects, a soft finger requires the\nadaptive-twist deformation function in both in-plane and out-of-plane\ndirections. The function allows the finger to be inserted deeply into a limited\ngap among objects. Once inserted, the soft finger requires appropriate control\nof grasping force normal to contact surface, thereby maintaining the twisted\ndeformation. In this paper, we refer to this type of grasping as grasping by\nwrapping. To achieve these two functions by a single actuation source, we\npropose a variable stiffness mechanism that can adaptively change the stiffness\nas the pressure is higher. We conduct a finite element analysis (FEA) on the\nproposed mechanism and determine its design parameter based on the FEA result.\nUsing the developed soft finger, we report basic experimental results and\ndemonstrations on grasping various objects.\n","authors":["Hiroki Ishikawa","Kyosuke Ishibashi","Ko Yamamoto"],"pdf_url":"https://arxiv.org/pdf/2510.23963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23954v1","updated":"2025-10-28T00:18:40Z","published":"2025-10-28T00:18:40Z","title":"A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots\n  with Multiple Tubes and Tendons","summary":"  Tendon-actuated concentric tube mechanisms combine the advantages of\ntendon-driven continuum robots and concentric tube robots while addressing\ntheir respective limitations. They overcome the restricted degrees of freedom\noften seen in tendon-driven designs, and mitigate issues such as snapping\ninstability associated with concentric tube robots. However, a complete and\ngeneral mechanical model for these systems remains an open problem. In this\nwork, we propose a Cosserat rod-based framework for modeling the general case\nof $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \\{1,\n\\ldots, n\\}$. The model allows each tube to twist and elongate while enforcing\na shared centerline for bending. We validate the proposed framework through\nexperiments with two-tube and three tube assemblies under various tendon\nrouting configurations, achieving tip prediction errors $<4\\%$ of the robot's\ntotal length. We further demonstrate the model's generality by applying it to\nexisting robots in the field, where maximum tip deviations remain around $5\\%$\nof the total length. This model provides a foundation for accurate shape\nestimation and control of advanced tendon-actuated concentric tube robots.\n","authors":["Pejman Kheradmand","Behnam Moradkhani","Raghavasimhan Sankaranarayanan","Kent K. Yamamoto","Tanner J. Zachem","Patrick J. Codd","Yash Chitalia","Pierre E. Dupont"],"pdf_url":"https://arxiv.org/pdf/2510.23954v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2510.03786v2","updated":"2025-10-28T23:43:17Z","published":"2025-10-04T11:25:10Z","title":"MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based\n  Fusion for Medical Image Segmentation","summary":"  In recent years, deep learning has shown near-expert performance in\nsegmenting complex medical tissues and tumors. However, existing models are\noften task-specific, with performance varying across modalities and anatomical\nregions. Balancing model complexity and performance remains challenging,\nparticularly in clinical settings where both accuracy and efficiency are\ncritical. To address these issues, we propose a hybrid segmentation\narchitecture featuring a three-branch encoder that integrates CNNs,\nTransformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture\nlocal, global, and long-range dependencies. A multi-scale attention-based CNN\ndecoder reconstructs fine-grained segmentation maps while preserving contextual\nconsistency. Additionally, a co-attention gate enhances feature selection by\nemphasizing relevant spatial and semantic information across scales during both\nencoding and decoding, improving feature interaction and cross-scale\ncommunication. Extensive experiments on multiple benchmark datasets show that\nour approach outperforms state-of-the-art methods in accuracy and\ngeneralization, while maintaining comparable computational complexity. By\neffectively balancing efficiency and effectiveness, our architecture offers a\npractical and scalable solution for diverse medical imaging tasks. Source code\nand trained models will be publicly released upon acceptance to support\nreproducibility and further research.\n","authors":["T-Mai Bui","Fares Bougourzi","Fadi Dornaika","Vinh Truong Hoang"],"pdf_url":"https://arxiv.org/pdf/2510.03786v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25032v1","updated":"2025-10-28T23:21:00Z","published":"2025-10-28T23:21:00Z","title":"Efficient License Plate Recognition via Pseudo-Labeled Supervision with\n  Grounding DINO and YOLOv8","summary":"  Developing a highly accurate automatic license plate recognition system\n(ALPR) is challenging due to environmental factors such as lighting, rain, and\ndust. Additional difficulties include high vehicle speeds, varying camera\nangles, and low-quality or low-resolution images. ALPR is vital in traffic\ncontrol, parking, vehicle tracking, toll collection, and law enforcement\napplications. This paper proposes a deep learning strategy using YOLOv8 for\nlicense plate detection and recognition tasks. This method seeks to enhance the\nperformance of the model using datasets from Ontario, Quebec, California, and\nNew York State. It achieved an impressive recall rate of 94% on the dataset\nfrom the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and\n91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised\nlearning framework, combining a small set of manually labeled data with\npseudo-labels generated by Grounding DINO to train our detection model.\nGrounding DINO, a powerful vision-language model, automatically annotates many\nimages with bounding boxes for license plates, thereby minimizing the reliance\non labor-intensive manual labeling. By integrating human-verified and\nmodel-generated annotations, we can scale our dataset efficiently while\nmaintaining label quality, which significantly enhances the training process\nand overall model performance. Furthermore, it reports character error rates\nfor both datasets, providing additional insight into system performance.\n","authors":["Zahra Ebrahimi Vargoorani","Amir Mohammad Ghoreyshi","Ching Yee Suen"],"pdf_url":"https://arxiv.org/pdf/2510.25032v1.pdf","comment":"6 pages, 8 figures. Presented at 2025 IEEE International Workshop on\n  Machine Learning for Signal Processing (MLSP), August 31 - September 3, 2025,\n  Istanbul, Turkey"},{"id":"http://arxiv.org/abs/2412.08619v3","updated":"2025-10-28T22:43:29Z","published":"2024-12-11T18:40:16Z","title":"Physics Context Builders: A Modular Framework for Physical Reasoning in\n  Vision-Language Models","summary":"  Physical reasoning remains a significant challenge for Vision-Language Models\n(VLMs). This limitation arises from an inability to translate learned knowledge\ninto predictions about physical behavior. Although continual fine-tuning can\nmitigate this issue, it is expensive for large models and impractical to\nperform repeatedly for every task. This necessitates the creation of modular\nand scalable ways to teach VLMs about physical reasoning. To that end, we\nintroduce Physics Context Builders (PCBs), a modular framework where\nspecialized smaller VLMs are fine-tuned to generate detailed physical scene\ndescriptions. These can be used as physical contexts to enhance the reasoning\ncapabilities of larger VLMs. PCBs enable the separation of visual perception\nfrom reasoning, allowing us to analyze their relative contributions to physical\nunderstanding. We perform experiments on CLEVRER and on Falling Tower, a\nstability detection dataset with both simulated and real-world scenes, to\ndemonstrate that PCBs provide substantial performance improvements, increasing\naverage accuracy by up to 13.8% on complex physical reasoning tasks. Notably,\nPCBs also show strong Sim2Real transfer, successfully generalizing from\nsimulated training data to real-world scenes.\n","authors":["Vahid Balazadeh","Mohammadmehdi Ataei","Hyunmin Cheong","Amir Hosein Khasahmadi","Rahul G. Krishnan"],"pdf_url":"https://arxiv.org/pdf/2412.08619v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.10266v2","updated":"2025-10-28T22:32:18Z","published":"2025-09-12T14:08:06Z","title":"SignMouth: Leveraging Mouthing Cues for Sign Language Translation by\n  Multimodal Contrastive Fusion","summary":"  Sign language translation (SLT) aims to translate natural language from sign\nlanguage videos, serving as a vital bridge for inclusive communication. While\nrecent advances leverage powerful visual backbones and large language models,\nmost approaches mainly focus on manual signals (hand gestures) and tend to\noverlook non-manual cues like mouthing. In fact, mouthing conveys essential\nlinguistic information in sign languages and plays a crucial role in\ndisambiguating visually similar signs. In this paper, we propose SignClip, a\nnovel framework to improve the accuracy of sign language translation. It fuses\nmanual and non-manual cues, specifically spatial gesture and lip movement\nfeatures. Besides, SignClip introduces a hierarchical contrastive learning\nframework with multi-level alignment objectives, ensuring semantic consistency\nacross sign-lip and visual-text modalities. Extensive experiments on two\nbenchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our\napproach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip\nsurpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from\n24.32 to 24.71, and ROUGE from 46.57 to 48.38.\n","authors":["Wenfang Wu","Tingting Yuan","Yupeng Li","Daling Wang","Xiaoming Fu"],"pdf_url":"https://arxiv.org/pdf/2509.10266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.25002v1","updated":"2025-10-28T22:02:36Z","published":"2025-10-28T22:02:36Z","title":"Resi-VidTok: An Efficient and Decomposed Progressive Tokenization\n  Framework for Ultra-Low-Rate and Lightweight Video Transmission","summary":"  Real-time transmission of video over wireless networks remains highly\nchallenging, even with advanced deep models, particularly under severe channel\nconditions such as limited bandwidth and weak connectivity. In this paper, we\npropose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for\nultra-low-rate and lightweight video transmission that delivers strong\nrobustness while preserving perceptual and semantic fidelity on commodity\ndigital hardware. By reorganizing spatio--temporal content into a discrete,\nimportance-ordered token stream composed of key tokens and refinement tokens,\nResi-VidTok enables progressive encoding, prefix-decodable reconstruction, and\ngraceful quality degradation under constrained channels. A key contribution is\na resilient 1D tokenization pipeline for video that integrates differential\ntemporal token coding, explicitly supporting reliable recovery from incomplete\ntoken sets using a single shared framewise decoder--without auxiliary temporal\nextractors or heavy generative models. Furthermore, stride-controlled frame\nsparsification combined with a lightweight decoder-side interpolator reduces\ntransmission load while maintaining motion continuity. Finally, a\nchannel-adaptive source--channel coding and modulation scheme dynamically\nallocates rate and protection according to token importance and channel\ncondition, yielding stable quality across adverse SNRs. Evaluation results\nindicate robust visual and semantic consistency at channel bandwidth ratios\n(CBR) as low as 0.0004 and real-time reconstruction at over 30 fps,\ndemonstrating the practicality of Resi-VidTok for energy-efficient,\nlatency-sensitive, and reliability-critical wireless applications.\n","authors":["Zhenyu Liu","Yi Ma","Rahim Tafazolli","Zhi Ding"],"pdf_url":"https://arxiv.org/pdf/2510.25002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22918v4","updated":"2025-10-28T21:55:57Z","published":"2025-05-28T22:39:12Z","title":"Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape","summary":"  Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. Experimental results\non T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate that\nRe-ttention requires as few as 3.1% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference.\n","authors":["Ruichen Chen","Keith G. Mills","Liyao Jiang","Chao Gao","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2505.22918v4.pdf","comment":"author comment: This version was previously removed by arXiv\n  administrators as the submitter did not have the rights to agree to the\n  license at the time of submission. The authors have now obtained the\n  necessary permissions, and the paper is resubmitted accordingly"},{"id":"http://arxiv.org/abs/2510.24980v1","updated":"2025-10-28T21:23:32Z","published":"2025-10-28T21:23:32Z","title":"FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for\n  Pressure Ulcer Severity Classification with Reasoning","summary":"  Pressure ulcers (PUs) are a serious and prevalent healthcare concern.\nAccurate classification of PU severity (Stages I-IV) is essential for proper\ntreatment but remains challenging due to subtle visual distinctions and\nsubjective interpretation, leading to variability among clinicians. Prior\nAI-based approaches using Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViTs) achieved promising accuracy but offered limited\ninterpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal\nmodel), a fine-tuned multimodal large language model (MLLM) with an agentic\nself-reflection mechanism for pressure ulcer severity classification. Inspired\nby clinician-style diagnostic reassessment, FT-ARM iteratively refines its\npredictions by reasoning over visual features and encoded clinical knowledge\nfrom text, enhancing both accuracy and consistency. On the publicly available\nPressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B,\nachieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based\nmodels by +4%. Unlike earlier CNN/ViT studies that relied solely on offline\nevaluations, FT-ARM is designed and tested for live inference, reflecting\nreal-time deployment conditions. Furthermore, it produces clinically grounded\nnatural-language explanations, improving interpretability and trust. By\nintegrating fine-tuning and reflective reasoning across multimodal inputs,\nFT-ARM advances the reliability, transparency, and clinical applicability of\nautomated wound assessment systems, addressing the critical need for consistent\nand explainable PU staging to support improved patient care.\n","authors":["Reza Saadati Fard","Emmanuel Agu","Palawat Busaranuvong","Deepak Kumar","Shefalika Gautam","Bengisu Tulu","Diane Strong","Lorraine Loretz"],"pdf_url":"https://arxiv.org/pdf/2510.24980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09066v5","updated":"2025-10-28T21:13:11Z","published":"2024-03-14T03:13:01Z","title":"Hyperparameters in Continual Learning: A Reality Check","summary":"  Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a\nCL scenario) while balancing the trade-off between plasticity (learning new\ntasks) and stability (retaining prior knowledge). The dominantly adopted\nconventional evaluation protocol for CL algorithms selects the best\nhyperparameters (e.g., learning rate, mini-batch size, regularization\nstrengths, etc.) within a given scenario and then evaluates the algorithms\nusing these hyperparameters in the same scenario. However, this protocol has\nsignificant shortcomings: it overestimates the CL capacity of algorithms and\nrelies on unrealistic hyperparameter tuning, which is not feasible for\nreal-world applications. From the fundamental principles of evaluation in\nmachine learning, we argue that the evaluation of CL algorithms should focus on\nassessing the generalizability of their CL capacity to unseen scenarios. Based\non this, we propose the Generalizable Two-phase Evaluation Protocol (GTEP)\nconsisting of hyperparameter tuning and evaluation phases. Both phases share\nthe same scenario configuration (e.g., number of tasks) but are generated from\ndifferent datasets. Hyperparameters of CL algorithms are tuned in the first\nphase and applied in the second phase to evaluate the algorithms. We apply this\nprotocol to class-incremental learning, both with and without pretrained\nmodels. Across more than 8,000 experiments, our results show that most\nstate-of-the-art algorithms fail to replicate their reported performance,\nhighlighting that their CL capacity has been significantly overestimated in the\nconventional evaluation protocol. Our implementation can be found in\nhttps://github.com/csm9493/GTEP.\n","authors":["Sungmin Cha","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2403.09066v5.pdf","comment":"TMLR 2025 camera ready version"},{"id":"http://arxiv.org/abs/2507.22017v2","updated":"2025-10-28T20:48:51Z","published":"2025-07-29T17:06:22Z","title":"Cyst-X: A Federated AI System Outperforms Clinical Guidelines to Detect\n  Pancreatic Cancer Precursors and Reduce Unnecessary Surgery","summary":"  Pancreatic cancer is projected to be the second-deadliest cancer by 2030,\nmaking early detection critical. Intraductal papillary mucinous neoplasms\n(IPMNs), key cancer precursors, present a clinical dilemma, as current\nguidelines struggle to stratify malignancy risk, leading to unnecessary\nsurgeries or missed diagnoses. Here, we developed Cyst-X, an AI framework for\nIPMN risk prediction trained on a unique, multi-center dataset of 1,461 MRI\nscans from 764 patients. Cyst-X achieves significantly higher accuracy (AUC =\n0.82) than both the established Kyoto guidelines (AUC = 0.75) and expert\nradiologists, particularly in correct identification of high-risk lesions.\nClinically, this translates to a 20% increase in cancer detection sensitivity\n(87.8% vs. 64.1%) for high-risk lesions. We demonstrate that this performance\nis maintained in a federated learning setting, allowing for collaborative model\ntraining without compromising patient privacy. To accelerate research in early\npancreatic cancer detection, we publicly release the Cyst-X dataset and models,\nproviding the first large-scale, multi-center MRI resource for pancreatic cyst\nanalysis.\n","authors":["Hongyi Pan","Gorkem Durak","Elif Keles","Deniz Seyithanoglu","Zheyuan Zhang","Alpay Medetalibeyoglu","Halil Ertugrul Aktas","Andrea Mia Bejar","Ziliang Hong","Yavuz Taktak","Gulbiz Dagoglu Kartal","Mehmet Sukru Erturk","Timurhan Cebeci","Maria Jaramillo Gonzalez","Yury Velichko","Lili Zhao","Emil Agarunov","Federica Proietto Salanitri","Concetto Spampinato","Pallavi Tiwari","Ziyue Xu","Sachin Jambawalikar","Ivo G. Schoots","Marco J. Bruno","Chenchang Huang","Candice W. Bolan","Tamas Gonda","Frank H. Miller","Rajesh N. Keswani","Michael B. Wallace","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2507.22017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24949v1","updated":"2025-10-28T20:31:19Z","published":"2025-10-28T20:31:19Z","title":"SCOUT: A Lightweight Framework for Scenario Coverage Assessment in\n  Autonomous Driving","summary":"  Assessing scenario coverage is crucial for evaluating the robustness of\nautonomous agents, yet existing methods rely on expensive human annotations or\ncomputationally intensive Large Vision-Language Models (LVLMs). These\napproaches are impractical for large-scale deployment due to cost and\nefficiency constraints. To address these shortcomings, we propose SCOUT\n(Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate\nmodel designed to predict scenario coverage labels directly from an agent's\nlatent sensor representations. SCOUT is trained through a distillation process,\nlearning to approximate LVLM-generated coverage labels while eliminating the\nneed for continuous LVLM inference or human annotation. By leveraging\nprecomputed perception features, SCOUT avoids redundant computations and\nenables fast, scalable scenario coverage estimation. We evaluate our method\nacross a large dataset of real-life autonomous navigation scenarios,\ndemonstrating that it maintains high accuracy while significantly reducing\ncomputational cost. Our results show that SCOUT provides an effective and\npractical alternative for large-scale coverage analysis. While its performance\ndepends on the quality of LVLM-generated training labels, SCOUT represents a\nmajor step toward efficient scenario coverage oversight in autonomous systems.\n","authors":["Anil Yildiz","Sarah M. Thornton","Carl Hildebrandt","Sreeja Roy-Singh","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2510.24949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22589v2","updated":"2025-10-28T20:08:24Z","published":"2025-10-26T09:09:52Z","title":"PSScreen V2: Partially Supervised Multiple Retinal Disease Screening","summary":"  In this work, we propose PSScreen V2, a partially supervised self-training\nframework for multiple retinal disease screening. Unlike previous methods that\nrely on fully labelled or single-domain datasets, PSScreen V2 is designed to\nlearn from multiple partially labelled datasets with different distributions,\naddressing both label absence and domain shift challenges. To this end,\nPSScreen V2 adopts a three-branch architecture with one teacher and two student\nnetworks. The teacher branch generates pseudo labels from weakly augmented\nimages to address missing labels, while the two student branches introduce\nnovel feature augmentation strategies: Low-Frequency Dropout (LF-Dropout),\nwhich enhances domain robustness by randomly discarding domain-related\nlow-frequency components, and Low-Frequency Uncertainty (LF-Uncert), which\nestimates uncertain domain variability via adversarially learned Gaussian\nperturbations of low-frequency statistics. Extensive experiments on multiple\nin-domain and out-of-domain fundus datasets demonstrate that PSScreen V2\nachieves state-of-the-art performance and superior domain generalization\nability. Furthermore, compatibility tests with diverse backbones, including the\nvision foundation model DINOv2, as well as evaluations on chest X-ray datasets,\nhighlight the universality and adaptability of the proposed framework. The\ncodes are available at https://github.com/boyiZheng99/PSScreen_V2.\n","authors":["Boyi Zheng","Yalin Zheng","Hrvoje Bogunović","Qing Liu"],"pdf_url":"https://arxiv.org/pdf/2510.22589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24936v1","updated":"2025-10-28T20:06:08Z","published":"2025-10-28T20:06:08Z","title":"IBIS: A Powerful Hybrid Architecture for Human Activity Recognition","summary":"  The increasing interest in Wi-Fi sensing stems from its potential to capture\nenvironmental data in a low-cost, non-intrusive way, making it ideal for\napplications like healthcare, space occupancy analysis, and gesture-based IoT\ncontrol. However, a major limitation in this field is the common problem of\noverfitting, where models perform well on training data but fail to generalize\nto new data. To overcome this, we introduce a novel hybrid architecture that\nintegrates Inception-BiLSTM with a Support Vector Machine (SVM), which we refer\nto as IBIS. Our IBIS approach is uniquely engineered to improve model\ngeneralization and create more robust classification boundaries. By applying\nthis method to Doppler-derived data, we achieve a movement recognition accuracy\nof nearly 99%. Comprehensive performance metrics and confusion matrices confirm\nthe significant effectiveness of our proposed solution.\n","authors":["Alison M. Fernandes","Hermes I. Del Monego","Bruno S. Chang","Anelise Munaretto","Hélder M. Fontes","Rui L. Campos"],"pdf_url":"https://arxiv.org/pdf/2510.24936v1.pdf","comment":"8 pages. 8 figures. Wireless Days Conference, December 2025"},{"id":"http://arxiv.org/abs/2510.24919v1","updated":"2025-10-28T19:44:20Z","published":"2025-10-28T19:44:20Z","title":"Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient\n  Modulation for Harmonized Multimodal Learning","summary":"  In multimodal learning, dominant modalities often overshadow others, limiting\ngeneralization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM),\na model-agnostic framework that applies to many modalities and supports early\nand late fusion scenarios. In every iteration, M-SAM in three steps optimizes\nlearning. \\textbf{First, it identifies the dominant modality} based on\nmodalities' contribution in the accuracy using Shapley. \\textbf{Second, it\ndecomposes the loss landscape}, or in another language, it modulates the loss\nto prioritize the robustness of the model in favor of the dominant modality,\nand \\textbf{third, M-SAM updates the weights} by backpropagation of modulated\ngradients. This ensures robust learning for the dominant modality while\nenhancing contributions from others, allowing the model to explore and exploit\ncomplementary features that strengthen overall performance. Extensive\nexperiments on four diverse datasets show that M-SAM outperforms the latest\nstate-of-the-art optimization and gradient manipulation methods and\nsignificantly balances and improves multimodal learning.\n","authors":["Hossein R. Nowdeh","Jie Ji","Xiaolong Ma","Fatemeh Afghah"],"pdf_url":"https://arxiv.org/pdf/2510.24919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24907v1","updated":"2025-10-28T19:19:35Z","published":"2025-10-28T19:19:35Z","title":"Understanding Multi-View Transformers","summary":"  Multi-view transformers such as DUSt3R are revolutionizing 3D vision by\nsolving 3D tasks in a feed-forward manner. However, contrary to previous\noptimization-based pipelines, the inner mechanisms of multi-view transformers\nare unclear. Their black-box nature makes further improvements beyond data\nscaling challenging and complicates usage in safety- and reliability-critical\napplications. Here, we present an approach for probing and visualizing 3D\nrepresentations from the residual connections of the multi-view transformers'\nlayers. In this manner, we investigate a variant of the DUSt3R model, shedding\nlight on the development of its latent state across blocks, the role of the\nindividual layers, and suggest how it differs from methods with stronger\ninductive biases of explicit global pose. Finally, we show that the\ninvestigated variant of DUSt3R estimates correspondences that are refined with\nreconstructed geometry. The code used for the analysis is available at\nhttps://github.com/JulienGaubil/und3rstand .\n","authors":["Michal Stary","Julien Gaubil","Ayush Tewari","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2510.24907v1.pdf","comment":"Presented at the ICCV 2025 E2E3D Workshop"},{"id":"http://arxiv.org/abs/2506.03089v2","updated":"2025-10-28T19:19:25Z","published":"2025-06-03T17:13:51Z","title":"Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End\n  Improves CNN Robustness","summary":"  Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end (VOneBlock) that mimics the primate primary visual cortex (V1) can\nimprove overall model robustness. Expanding on this, we introduce Early Vision\nNetworks (EVNets), a new class of hybrid CNNs that combine the VOneBlock with a\nnovel SubcorticalBlock, whose architecture draws from computational models in\nneuroscience and is parameterized to maximize alignment with subcortical\nresponses reported across multiple experimental studies. Without being\noptimized to do so, the assembly of the SubcorticalBlock with the VOneBlock\nimproved V1 alignment across most standard V1 benchmarks, and better modeled\nextra-classical receptive field phenomena. In addition, EVNets exhibit stronger\nemergent shape bias and outperform the base CNN architecture by 9.3% on an\naggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 6.2% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches.\n","authors":["Lucas Piper","Arlindo L. Oliveira","Tiago Marques"],"pdf_url":"https://arxiv.org/pdf/2506.03089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24904v1","updated":"2025-10-28T19:12:22Z","published":"2025-10-28T19:12:22Z","title":"VividCam: Learning Unconventional Camera Motions from Virtual Synthetic\n  Videos","summary":"  Although recent text-to-video generative models are getting more capable of\nfollowing external camera controls, imposed by either text descriptions or\ncamera trajectories, they still struggle to generalize to unconventional camera\nmotions, which is crucial in creating truly original and artistic videos. The\nchallenge lies in the difficulty of finding sufficient training videos with the\nintended uncommon camera motions. To address this challenge, we propose\nVividCam, a training paradigm that enables diffusion models to learn complex\ncamera motions from synthetic videos, releasing the reliance on collecting\nrealistic training videos. VividCam incorporates multiple disentanglement\nstrategies that isolates camera motion learning from synthetic appearance\nartifacts, ensuring more robust motion representation and mitigating domain\nshift. We demonstrate that our design synthesizes a wide range of precisely\ncontrolled and complex camera motions using surprisingly simple synthetic data.\nNotably, this synthetic data often consists of basic geometries within a\nlow-poly 3D scene and can be efficiently rendered by engines like Unity. Our\nvideo results can be found in https://wuqiuche.github.io/VividCamDemoPage/ .\n","authors":["Qiucheng Wu","Handong Zhao","Zhixin Shu","Jing Shi","Yang Zhang","Shiyu Chang"],"pdf_url":"https://arxiv.org/pdf/2510.24904v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.24902v1","updated":"2025-10-28T19:04:53Z","published":"2025-10-28T19:04:53Z","title":"Pixels to Signals: A Real-Time Framework for Traffic Demand Estimation","summary":"  Traffic congestion is becoming a challenge in the rapidly growing urban\ncities, resulting in increasing delays and inefficiencies within urban\ntransportation systems. To address this issue a comprehensive methodology is\ndesigned to optimize traffic flow and minimize delays. The framework is\nstructured with three primary components: (a) vehicle detection, (b) traffic\nprediction, and (c) traffic signal optimization. This paper presents the first\ncomponent, vehicle detection. The methodology involves analyzing multiple\nsequential frames from a camera feed to compute the background, i.e. the\nunderlying roadway, by averaging pixel values over time. The computed\nbackground is then utilized to extract the foreground, where the Density-Based\nSpatial Clustering of Applications with Noise (DBSCAN) algorithm is applied to\ndetect vehicles. With its computational efficiency and minimal infrastructure\nmodification requirements, the proposed methodology offers a practical and\nscalable solution for real-world deployment.\n","authors":["H Mhatre","M Vyas","A Mittal"],"pdf_url":"https://arxiv.org/pdf/2510.24902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.21609v3","updated":"2025-10-28T18:57:29Z","published":"2025-09-25T21:21:00Z","title":"VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster\n  Assessment","summary":"  Immediate damage assessment is essential after natural catastrophes; yet,\nconventional hand evaluation techniques are sluggish and perilous. Although\nsatellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives\nof impacted regions, current computer vision methodologies generally yield just\nclassification labels or segmentation masks, so constraining their capacity to\ndeliver a thorough situational comprehension. We introduce the Vision Language\nCaption Enhancer (VLCE), a multimodal system designed to produce comprehensive,\ncontextually-informed explanations of disaster imagery. VLCE employs a\ndual-architecture approach: a CNN-LSTM model with a ResNet50 backbone\npretrained on EuroSat satellite imagery for the xBD dataset, and a Vision\nTransformer (ViT) model pretrained on UAV pictures for the RescueNet dataset.\nBoth systems utilize external semantic knowledge from ConceptNet and WordNet to\nexpand vocabulary coverage and improve description accuracy. We assess VLCE in\ncomparison to leading vision-language models (LLaVA and QwenVL) utilizing\nCLIPScore for semantic alignment and InfoMetIC for caption informativeness.\nExperimental findings indicate that VLCE markedly surpasses baseline models,\nattaining a maximum of 95.33% on InfoMetIC while preserving competitive\nsemantic alignment. Our dual-architecture system demonstrates significant\npotential for improving disaster damage assessment by automating the production\nof actionable, information-dense descriptions from satellite and drone photos.\n","authors":["Md. Mahfuzur Rahman","Kishor Datta Gupta","Marufa Kamal","Fahad Rahman","Sunzida Siddique","Ahmed Rafi Hasan","Mohd Ariful Haque","Roy George"],"pdf_url":"https://arxiv.org/pdf/2509.21609v3.pdf","comment":"29 pages, 40 figures, 3 algorithms"},{"id":"http://arxiv.org/abs/2510.24887v1","updated":"2025-10-28T18:43:17Z","published":"2025-10-28T18:43:17Z","title":"Proper Body Landmark Subset Enables More Accurate and 5X Faster\n  Recognition of Isolated Signs in LIBRAS","summary":"  This paper investigates the feasibility of using lightweight body landmark\ndetection for the recognition of isolated signs in Brazilian Sign Language\n(LIBRAS). Although the skeleton-based approach by Alves et al. (2024) enabled\nsubstantial improvements in recognition performance, the use of OpenPose for\nlandmark extraction hindered time performance. In a preliminary investigation,\nwe observed that simply replacing OpenPose with the lightweight MediaPipe,\nwhile improving processing speed, significantly reduced accuracy. To overcome\nthis limitation, we explored landmark subset selection strategies aimed at\noptimizing recognition performance. Experimental results showed that a proper\nlandmark subset achieves comparable or superior performance to state-of-the-art\nmethods while reducing processing time by more than 5X compared to Alves et al.\n(2024). As an additional contribution, we demonstrated that spline-based\nimputation effectively mitigates missing landmark issues, leading to\nsubstantial accuracy gains. These findings highlight that careful landmark\nselection, combined with simple imputation techniques, enables efficient and\naccurate isolated sign recognition, paving the way for scalable Sign Language\nRecognition systems.\n","authors":["Daniele L. V. dos Santos","Thiago B. Pereira","Carlos Eduardo G. R. Alves","Richard J. M. G. Tello","Francisco de A. Boldt","Thiago M. Paixão"],"pdf_url":"https://arxiv.org/pdf/2510.24887v1.pdf","comment":"Submitted to Int. Conf. on Computer Vision Theory and Applications\n  (VISAPP 2026)"},{"id":"http://arxiv.org/abs/2510.24885v1","updated":"2025-10-28T18:39:03Z","published":"2025-10-28T18:39:03Z","title":"FruitProm: Probabilistic Maturity Estimation and Detection of Fruits and\n  Vegetables","summary":"  Maturity estimation of fruits and vegetables is a critical task for\nagricultural automation, directly impacting yield prediction and robotic\nharvesting. Current deep learning approaches predominantly treat maturity as a\ndiscrete classification problem (e.g., unripe, ripe, overripe). This rigid\nformulation, however, fundamentally conflicts with the continuous nature of the\nbiological ripening process, leading to information loss and ambiguous class\nboundaries. In this paper, we challenge this paradigm by reframing maturity\nestimation as a continuous, probabilistic learning task. We propose a novel\narchitectural modification to the state-of-the-art, real-time object detector,\nRT-DETRv2, by introducing a dedicated probabilistic head. This head enables the\nmodel to predict a continuous distribution over the maturity spectrum for each\ndetected object, simultaneously learning the mean maturity state and its\nassociated uncertainty. This uncertainty measure is crucial for downstream\ndecision-making in robotics, providing a confidence score for tasks like\nselective harvesting. Our model not only provides a far richer and more\nbiologically plausible representation of plant maturity but also maintains\nexceptional detection performance, achieving a mean Average Precision (mAP) of\n85.6\\% on a challenging, large-scale fruit dataset. We demonstrate through\nextensive experiments that our probabilistic approach offers more granular and\naccurate maturity assessments than its classification-based counterparts,\npaving the way for more intelligent, uncertainty-aware automated systems in\nmodern agriculture\n","authors":["Sidharth Rai","Rahul Harsha Cheppally","Benjamin Vail","Keziban Yalçın Dokumacı","Ajay Sharda"],"pdf_url":"https://arxiv.org/pdf/2510.24885v1.pdf","comment":"Sidharth Rai, Rahul Harsha Cheppally contributed equally to this work"},{"id":"http://arxiv.org/abs/2510.24870v1","updated":"2025-10-28T18:21:19Z","published":"2025-10-28T18:21:19Z","title":"Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented\n  Generation","summary":"  We introduce MiRAGE, an evaluation framework for retrieval-augmented\ngeneration (RAG) from multimodal sources. As audiovisual media becomes a\nprevalent source of information online, it is essential for RAG systems to\nintegrate information from these sources into generation. However, existing\nevaluations for RAG are text-centric, limiting their applicability to\nmultimodal, reasoning intensive settings because they don't verify information\nagainst sources. MiRAGE is a claim-centric approach to multimodal RAG\nevaluation, consisting of InfoF1, evaluating factuality and information\ncoverage, and CiteF1, measuring citation support and completeness. We show that\nMiRAGE, when applied by humans, strongly aligns with extrinsic quality\njudgments. We additionally introduce automatic variants of MiRAGE and three\nprominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the\nlimitations of text-centric work and laying the groundwork for automatic\nevaluation. We release open-source implementations and outline how to assess\nmultimodal RAG.\n","authors":["Alexander Martin","William Walden","Reno Kriz","Dengjia Zhang","Kate Sanders","Eugene Yang","Chihsheng Jin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2510.24870v1.pdf","comment":"https://github.com/alexmartin1722/mirage"},{"id":"http://arxiv.org/abs/2510.05034v5","updated":"2025-10-28T18:02:26Z","published":"2025-10-06T17:10:44Z","title":"Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models","summary":"  Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training\n","authors":["Yolo Yunlong Tang","Jing Bi","Pinxin Liu","Zhenyu Pan","Zhangyun Tan","Qianxiang Shen","Jiani Liu","Hang Hua","Junjia Guo","Yunzhong Xiao","Chao Huang","Zhiyuan Wang","Susan Liang","Xinyi Liu","Yizhi Song","Junhua Huang","Jia-Xing Zhong","Bozheng Li","Daiqing Qi","Ziyun Zeng","Ali Vosoughi","Luchuan Song","Zeliang Zhang","Daiki Shimada","Han Liu","Jiebo Luo","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.05034v5.pdf","comment":"Version v1.1"},{"id":"http://arxiv.org/abs/2510.24718v1","updated":"2025-10-28T17:59:58Z","published":"2025-10-28T17:59:58Z","title":"Generative View Stitching","summary":"  Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.\n","authors":["Chonghyuk Song","Michal Stary","Boyuan Chen","George Kopanas","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2510.24718v1.pdf","comment":"Project website: https://andrewsonga.github.io/gvs"},{"id":"http://arxiv.org/abs/2510.24717v1","updated":"2025-10-28T17:59:57Z","published":"2025-10-28T17:59:57Z","title":"Uniform Discrete Diffusion with Metric Path for Video Generation","summary":"  Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA\n","authors":["Haoge Deng","Ting Pan","Fan Zhang","Yang Liu","Zhuoyan Luo","Yufeng Cui","Wenxuan Wang","Chunhua Shen","Shiguang Shan","Zhaoxiang Zhang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2510.24717v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2510.24711v1","updated":"2025-10-28T17:59:02Z","published":"2025-10-28T17:59:02Z","title":"Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance","summary":"  Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.\n","authors":["Yujie Wei","Shiwei Zhang","Hangjie Yuan","Yujin Han","Zhekai Chen","Jiayu Wang","Difan Zou","Xihui Liu","Yingya Zhang","Yu Liu","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2510.24711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24709v1","updated":"2025-10-28T17:57:05Z","published":"2025-10-28T17:57:05Z","title":"Does Object Binding Naturally Emerge in Large Pretrained Vision\n  Transformers?","summary":"  Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.\n","authors":["Yihao Li","Saeed Salehi","Lyle Ungar","Konrad P. Kording"],"pdf_url":"https://arxiv.org/pdf/2510.24709v1.pdf","comment":"Accepted as a Spotlight at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.24688v1","updated":"2025-10-28T17:49:42Z","published":"2025-10-28T17:49:42Z","title":"MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with\n  Relation-Aware Fusion for 3D Object Detection","summary":"  Infrastructure-based perception plays a crucial role in intelligent\ntransportation systems, offering global situational awareness and enabling\ncooperative autonomy. However, existing camera-based detection models often\nunderperform in such scenarios due to challenges such as multi-view\ninfrastructure setup, diverse camera configurations, degraded visual inputs,\nand various road layouts. We introduce MIC-BEV, a Transformer-based\nbird's-eye-view (BEV) perception framework for infrastructure-based\nmulti-camera 3D object detection. MIC-BEV flexibly supports a variable number\nof cameras with heterogeneous intrinsic and extrinsic parameters and\ndemonstrates strong robustness under sensor degradation. The proposed\ngraph-enhanced fusion module in MIC-BEV integrates multi-view image features\ninto the BEV space by exploiting geometric relationships between cameras and\nBEV cells alongside latent visual cues. To support training and evaluation, we\nintroduce M2I, a synthetic dataset for infrastructure-based object detection,\nfeaturing diverse camera configurations, road layouts, and environmental\nconditions. Extensive experiments on both M2I and the real-world dataset\nRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D\nobject detection. It also remains robust under challenging conditions,\nincluding extreme weather and sensor degradation. These results highlight the\npotential of MIC-BEV for real-world deployment. The dataset and source code are\navailable at: https://github.com/HandsomeYun/MIC-BEV.\n","authors":["Yun Zhang","Zhaoliang Zheng","Johnson Liu","Zhiyu Huang","Zewei Zhou","Zonglin Meng","Tianhui Cai","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2510.24688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07862v2","updated":"2025-10-28T17:37:03Z","published":"2025-02-11T17:19:44Z","title":"ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise\n  and Compute Resources","summary":"  Multimodal deep learning systems are deployed in dynamic scenarios due to the\nrobustness afforded by multiple sensing modalities. Nevertheless, they struggle\nwith varying compute resource availability (due to multi-tenancy, device\nheterogeneity, etc.) and fluctuating quality of inputs (from sensor feed\ncorruption, environmental noise, etc.). Statically provisioned multimodal\nsystems cannot adapt when compute resources change over time, while existing\ndynamic networks struggle with strict compute budgets. Additionally, both\nsystems often neglect the impact of variations in modality quality.\nConsequently, modalities suffering substantial corruption may needlessly\nconsume resources better allocated towards other modalities. We propose ADMN, a\nlayer-wise Adaptive Depth Multimodal Network capable of tackling both\nchallenges: it adjusts the total number of active layers across all modalities\nto meet strict compute resource constraints and continually reallocates layers\nacross input modalities according to their modality quality. Our evaluations\nshowcase ADMN can match the accuracy of state-of-the-art networks while\nreducing up to 75% of their floating-point operations.\n","authors":["Jason Wu","Yuyang Yuan","Kang Yang","Lance Kaplan","Mani Srivastava"],"pdf_url":"https://arxiv.org/pdf/2502.07862v2.pdf","comment":"Accepted to Neurips 2025"},{"id":"http://arxiv.org/abs/2505.20426v3","updated":"2025-10-28T17:35:54Z","published":"2025-05-26T18:20:22Z","title":"MMPerspective: Do MLLMs Understand Perspective? A Comprehensive\n  Benchmark for Perspective Perception, Reasoning, and Robustness","summary":"  Understanding perspective is fundamental to human visual perception, yet the\nextent to which multimodal large language models (MLLMs) internalize\nperspective geometry remains unclear. We introduce MMPerspective, the first\nbenchmark specifically designed to systematically evaluate MLLMs' understanding\nof perspective through 10 carefully crafted tasks across three complementary\ndimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark\ncomprises 2,711 real-world and synthetic image instances with 5,083\nquestion-answer pairs that probe key capabilities, such as vanishing point\nperception and counting, perspective type reasoning, line relationship\nunderstanding in 3D space, invariance to perspective-preserving\ntransformations, etc. Through a comprehensive evaluation of 43 state-of-the-art\nMLLMs, we uncover significant limitations: while models demonstrate competence\non surface-level perceptual tasks, they struggle with compositional reasoning\nand maintaining spatial consistency under perturbations. Our analysis further\nreveals intriguing patterns between model architecture, scale, and perspective\ncapabilities, highlighting both robustness bottlenecks and the benefits of\nchain-of-thought prompting. MMPerspective establishes a valuable testbed for\ndiagnosing and advancing spatial understanding in vision-language systems.\nResources available at: https://yunlong10.github.io/MMPerspective/\n","authors":["Yolo Yunlong Tang","Pinxin Liu","Zhangyun Tan","Mingqian Feng","Rui Mao","Chao Huang","Jing Bi","Yunzhong Xiao","Susan Liang","Hang Hua","Ali Vosoughi","Luchuan Song","Zeliang Zhang","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2505.20426v3.pdf","comment":"Accepted to NeurIPS 2025 DB Track"},{"id":"http://arxiv.org/abs/2510.24667v1","updated":"2025-10-28T17:35:02Z","published":"2025-10-28T17:35:02Z","title":"SAGE: Structure-Aware Generative Video Transitions between Diverse Clips","summary":"  Video transitions aim to synthesize intermediate frames between two clips,\nbut naive approaches such as linear blending introduce artifacts that limit\nprofessional use or break temporal coherence. Traditional techniques\n(cross-fades, morphing, frame interpolation) and recent generative inbetweening\nmethods can produce high-quality plausible intermediates, but they struggle\nwith bridging diverse clips involving large temporal gaps or significant\nsemantic differences, leaving a gap for content-aware and visually coherent\ntransitions. We address this challenge by drawing on artistic workflows,\ndistilling strategies such as aligning silhouettes and interpolating salient\nfeatures to preserve structure and perceptual continuity. Building on this, we\npropose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot\napproach that combines structural guidance, provided via line maps and motion\nflow, with generative synthesis, enabling smooth, semantically consistent\ntransitions without fine-tuning. Extensive experiments and comparison with\ncurrent alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate\nthat SAGE outperforms both classical and generative baselines on quantitative\nmetrics and user studies for producing transitions between diverse clips. Code\nto be released on acceptance.\n","authors":["Mia Kan","Yilin Liu","Niloy Mitra"],"pdf_url":"https://arxiv.org/pdf/2510.24667v1.pdf","comment":"Website: https://kan32501.github.io/sage.github.io/"},{"id":"http://arxiv.org/abs/2510.24657v1","updated":"2025-10-28T17:22:44Z","published":"2025-10-28T17:22:44Z","title":"Group Relative Attention Guidance for Image Editing","summary":"  Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.\n","authors":["Xuanpu Zhang","Xuesong Niu","Ruidong Chen","Dan Song","Jianhao Zeng","Penghui Du","Haoxiang Cao","Kai Wu","An-an Liu"],"pdf_url":"https://arxiv.org/pdf/2510.24657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24653v1","updated":"2025-10-28T17:18:43Z","published":"2025-10-28T17:18:43Z","title":"Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making\n  Datasets in Digital Pathology","summary":"  Interpretation of giga-pixel whole-slide images (WSIs) is an important but\ndifficult task for pathologists. Their diagnostic accuracy is estimated to\naverage around 70%. Adding a second pathologist does not substantially improve\ndecision consistency. The field lacks adequate behavioral data to explain\ndiagnostic errors and inconsistencies. To fill in this gap, we present\nPathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual\nsearch and decision-making processes of the full diagnostic workflow during\ncancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse\ninteraction, stimulus tracking, viewport navigation, and diagnostic decision\ndata (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data\ncollection process emphasizes ecological validity through an\napplication-grounded testbed, called PTAH. In total, we recorded 171,909\nfixations, 263,320 saccades, and 1,867,362 mouse interaction events. In\naddition, such data could also be used to improve the training of both\npathologists and AI systems that might support human experts. All experiments\nwere preregistered at https://osf.io/hj9a7, and the complete dataset along with\nanalysis code is available at https://go.osu.edu/pathogaze.\n","authors":["Veronica Thai","Rui Li","Meng Ling","Shuning Jiang","Jeremy Wolfe","Raghu Machiraju","Yan Hu","Zaibo Li","Anil Parwani","Jian Chen"],"pdf_url":"https://arxiv.org/pdf/2510.24653v1.pdf","comment":"16 pages, 9 figures, submitted to Nature Scientific Data"},{"id":"http://arxiv.org/abs/2510.24640v1","updated":"2025-10-28T17:06:40Z","published":"2025-10-28T17:06:40Z","title":"A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries","summary":"  The rapid advancement of generative AI has enabled the creation of highly\nrealistic forged facial images, posing significant threats to AI security,\ndigital media integrity, and public trust. Face forgery techniques, ranging\nfrom face swapping and attribute editing to powerful diffusion-based image\nsynthesis, are increasingly being used for malicious purposes such as\nmisinformation, identity fraud, and defamation. This growing challenge\nunderscores the urgent need for robust and generalizable face forgery detection\nmethods as a critical component of AI security infrastructure. In this work, we\npropose a novel dual-branch convolutional neural network for face forgery\ndetection that leverages complementary cues from both spatial and frequency\ndomains. The RGB branch captures semantic information, while the frequency\nbranch focuses on high-frequency artifacts that are difficult for generative\nmodels to suppress. A channel attention module is introduced to adaptively fuse\nthese heterogeneous features, highlighting the most informative channels for\nforgery discrimination. To guide the network's learning process, we design a\nunified loss function, FSC Loss, that combines focal loss, supervised\ncontrastive loss, and a frequency center margin loss to enhance class\nseparability and robustness. We evaluate our model on the DiFF benchmark, which\nincludes forged images generated from four representative methods:\ntext-to-image, image-to-image, face swap, and face edit. Our method achieves\nstrong performance across all categories and outperforms average human\naccuracy. These results demonstrate the model's effectiveness and its potential\ncontribution to safeguarding AI ecosystems against visual forgery attacks.\n","authors":["Xin Zhang","Yuqi Song","Fei Zuo"],"pdf_url":"https://arxiv.org/pdf/2510.24640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22693v2","updated":"2025-10-28T16:57:22Z","published":"2025-10-26T14:36:15Z","title":"VADTree: Explainable Training-Free Video Anomaly Detection via\n  Hierarchical Granularity-Aware Tree","summary":"  Video anomaly detection (VAD) focuses on identifying anomalies in videos.\nSupervised methods demand substantial in-domain training data and fail to\ndeliver clear explanations for anomalies. In contrast, training-free methods\nleverage the knowledge reserves and language interactivity of large pre-trained\nmodels to detect anomalies. However, the current fixed-length temporal window\nsampling approaches struggle to accurately capture anomalies with varying\ntemporal spans. Therefore, we propose VADTree that utilizes a Hierarchical\nGranularityaware Tree (HGTree) structure for flexible sampling in VAD. VADTree\nleverages the knowledge embedded in a pre-trained Generic Event Boundary\nDetection (GEBD) model to characterize potential anomaly event boundaries.\nSpecifically, VADTree decomposes the video into generic event nodes based on\nboundary confidence, and performs adaptive coarse-fine hierarchical structuring\nand redundancy removal to construct the HGTree. Then, the multi-dimensional\npriors are injected into the visual language models (VLMs) to enhance the\nnode-wise anomaly perception, and anomaly reasoning for generic event nodes is\nachieved via large language models (LLMs). Finally, an inter-cluster node\ncorrelation method is used to integrate the multi-granularity anomaly scores.\nExtensive experiments on three challenging datasets demonstrate that VADTree\nachieves state-of-the-art performance in training-free settings while\ndrastically reducing the number of sampled video segments. The code will be\navailable at https://github.com/wenlongli10/VADTree.\n","authors":["Wenlong Li","Yifei Xu","Yuan Rao","Zhenhua Wang","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2510.22693v2.pdf","comment":"NeurIPS 2025 poster"},{"id":"http://arxiv.org/abs/2510.24623v1","updated":"2025-10-28T16:51:50Z","published":"2025-10-28T16:51:50Z","title":"GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization","summary":"  In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline\ndesigned to localize a mobile robot in large-scale outdoor environments using\nprior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing\non the perceived ground area and utilizes the place recognition network R2D2,\nor alternatively, the non-learning approach Scale-Invariant Feature Transform\n(SIFT), to identify and select keypoints for BEV image map registration. Our\nresults demonstrate that GroundLoc outperforms state-of-the-art methods on the\nSemanticKITTI and HeLiPR datasets across various sensors. In the multi-session\nlocalization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)\nwell below 50 cm on all Ouster OS2 128 sequences while meeting online runtime\nrequirements. The system supports various sensor models, as evidenced by\nevaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,\nand Livox Avia sensors. The prior maps are stored as 2D raster image maps,\nwhich can be created from a single drive and require only 4 MB of storage per\nsquare kilometer. The source code is available at\nhttps://github.com/dcmlr/groundloc.\n","authors":["Nicolai Steinke","Daniel Goehring"],"pdf_url":"https://arxiv.org/pdf/2510.24623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18513v2","updated":"2025-10-28T16:44:35Z","published":"2025-10-21T10:55:32Z","title":"DWaste: Greener AI for Waste Sorting using Mobile and Edge Devices","summary":"  The rise of convenience packaging has led to generation of enormous waste,\nmaking efficient waste sorting crucial for sustainable waste management. To\naddress this, we developed DWaste, a computer vision-powered platform designed\nfor real-time waste sorting on resource-constrained smartphones and edge\ndevices, including offline functionality. We benchmarked various image\nclassification models (EfficientNetV2S/M, ResNet50/101, MobileNet) and object\ndetection (YOLOv8n, YOLOv11n) including our purposed YOLOv8n-CBAM model using\nour annotated dataset designed for recycling. We found a clear trade-off\nbetween accuracy and resource consumption: the best classifier,\nEfficientNetV2S, achieved high accuracy(~ 96%) but suffered from high latency\n(~ 0.22s) and elevated carbon emissions. In contrast, lightweight object\ndetection models delivered strong performance (up to 80% mAP) with ultra-fast\ninference (~ 0.03s) and significantly smaller model sizes (< 7MB ), making them\nideal for real-time, low-power use. Model quantization further maximized\nefficiency, substantially reducing model size and VRAM usage by up to 75%. Our\nwork demonstrates the successful implementation of \"Greener AI\" models to\nsupport real-time, sustainable waste sorting on edge devices.\n","authors":["Suman Kunwar"],"pdf_url":"https://arxiv.org/pdf/2510.18513v2.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2405.07046v3","updated":"2025-10-28T16:43:19Z","published":"2024-05-11T16:22:00Z","title":"RETTA: Retrieval-Enhanced Test-Time Adaptation for Zero-Shot Video\n  Captioning","summary":"  Despite the significant progress of fully-supervised video captioning,\nzero-shot methods remain much less explored. In this paper, we propose a novel\nzero-shot video captioning framework named Retrieval-Enhanced Test-Time\nAdaptation (RETTA), which takes advantage of existing pretrained large-scale\nvision and language models to directly generate captions with test-time\nadaptation. Specifically, we bridge video and text using four key models: a\ngeneral video-text retrieval model XCLIP, a general image-text matching model\nCLIP, a text alignment model AnglE, and a text generation model GPT-2, due to\ntheir source-code availability. The main challenge is how to enable the text\ngeneration model to be sufficiently aware of the content in a given video so as\nto generate corresponding captions. To address this problem, we propose using\nlearnable tokens as a communication medium among these four frozen models\nGPT-2, XCLIP, CLIP, and AnglE. Different from the conventional way that trains\nthese tokens with training data, we propose to learn these tokens with soft\ntargets of the inference data under several carefully crafted loss functions,\nwhich enable the tokens to absorb video information catered for GPT-2. This\nprocedure can be efficiently done in just a few iterations (we use 16\niterations in the experiments) and does not require ground truth data.\nExtensive experimental results on three widely used datasets, MSR-VTT, MSVD,\nand VATEX, show absolute 5.1%-32.4% improvements in terms of the main metric\nCIDEr compared to several state-of-the-art zero-shot video captioning methods.\n","authors":["Yunchuan Ma","Laiyun Qing","Guorong Li","Yuankai Qi","Amin Beheshti","Quan Z. Sheng","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2405.07046v3.pdf","comment":"Published in Pattern Recognition"},{"id":"http://arxiv.org/abs/2510.24830v1","updated":"2025-10-28T16:42:53Z","published":"2025-10-28T16:42:53Z","title":"The Generation Phases of Flow Matching: a Denoising Perspective","summary":"  Flow matching has achieved remarkable success, yet the factors influencing\nthe quality of its generation process remain poorly understood. In this work,\nwe adopt a denoising perspective and design a framework to empirically probe\nthe generation process. Laying down the formal connections between flow\nmatching models and denoisers, we provide a common ground to compare their\nperformances on generation and denoising. This enables the design of principled\nand controlled perturbations to influence sample generation: noise and drift.\nThis leads to new insights on the distinct dynamical phases of the generative\nprocess, enabling us to precisely characterize at which stage of the generative\nprocess denoisers succeed or fail and why this matters.\n","authors":["Anne Gagneux","Ségolène Martin","Rémi Gribonval","Mathurin Massias"],"pdf_url":"https://arxiv.org/pdf/2510.24830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24579v1","updated":"2025-10-28T16:13:14Z","published":"2025-10-28T16:13:14Z","title":"Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter\n  Correction in Cone-Beam CT","summary":"  Cone-beam CT (CBCT) employs a flat-panel detector to achieve\nthree-dimensional imaging with high spatial resolution. However, CBCT is\nsusceptible to scatter during data acquisition, which introduces CT value bias\nand reduced tissue contrast in the reconstructed images, ultimately degrading\ndiagnostic accuracy. To address this issue, we propose a deep learning-based\nscatter artifact correction method inspired by physical prior knowledge.\nLeveraging the fact that the observed point scatter probability density\ndistribution exhibits rotational symmetry in the projection domain. The method\nuses Gaussian Radial Basis Functions (RBF) to model the point scatter function\nand embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides\nefficient nonlinear mapping capabilities for learning high-dimensional scatter\nfeatures. By incorporating the physical characteristics of the scattered photon\ndistribution together with the complex function mapping capacity of KAN, the\nmodel improves its ability to accurately represent scatter. The effectiveness\nof the method is validated through both synthetic and real-scan experiments.\nExperimental results show that the model can effectively correct the scatter\nartifacts in the reconstructed images and is superior to the current methods in\nterms of quantitative metrics.\n","authors":["Xu Jiang","Huiying Pan","Ligen Shi","Jianing Sun","Wenfeng Xu","Xing Zhao"],"pdf_url":"https://arxiv.org/pdf/2510.24579v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.12427v4","updated":"2025-10-28T16:06:34Z","published":"2025-02-18T01:52:41Z","title":"Frequency-Aware Vision Transformers for High-Fidelity Super-Resolution\n  of Earth System Models","summary":"  Super-resolution (SR) is crucial for enhancing the spatial fidelity of Earth\nSystem Model (ESM) outputs, allowing fine-scale structures vital to climate\nscience to be recovered from coarse simulations. However, traditional deep\nsuper-resolution methods, including convolutional and transformer-based models,\ntend to exhibit spectral bias, reconstructing low-frequency content more\nreadily than valuable high-frequency details. In this work, we introduce two\nfrequency-aware frameworks: the Vision Transformer-Tuned Sinusoidal Implicit\nRepresentation (ViSIR), combining Vision Transformers and sinusoidal\nactivations to mitigate spectral bias, and the Vision Transformer Fourier\nRepresentation Network (ViFOR), which integrates explicit Fourier-based\nfiltering for independent low- and high-frequency learning. Evaluated on the\nE3SM-HR Earth system dataset across surface temperature, shortwave, and\nlongwave fluxes, these models outperform leading CNN, GAN, and vanilla\ntransformer baselines, with ViFOR demonstrating up to 2.6~dB improvements in\nPSNR and significantly higher SSIM. Detailed ablation and scaling studies\nhighlight the benefit of full-field training, the impact of frequency\nhyperparameters, and the potential for generalization. The results establish\nViFOR as a state-of-the-art, scalable solution for climate data downscaling.\nFuture extensions will address temporal super-resolution, multimodal climate\nvariables, automated parameter selection, and integration of physical\nconservation constraints to broaden scientific applicability.\n","authors":["Ehsan Zeraatkar","Salah A Faroughi","Jelena Tešić"],"pdf_url":"https://arxiv.org/pdf/2502.12427v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06415v2","updated":"2025-10-28T16:04:58Z","published":"2025-03-09T03:17:28Z","title":"Polygonal network disorder and the turning distance","summary":"  The turning distance is a well-studied metric for measuring the similarity\nbetween two polygons. This metric is constructed by taking an $L^p$ distance\nbetween step functions which track each shape's tangent angle of a path tracing\nits boundary. In this study, we introduce \\textit{turning disorders} for\npolygonal planar networks, defined by averaging turning distances between\nnetwork faces and \"ordered\" shapes (regular polygons or circles). We derive\nclosed-form expressions of turning distances for special classes of regular\npolygons, related to the divisibility of $m$ and $n$, and also between regular\npolygons and circles. These formulas are used to show that the time for\ncomputing the 2-turning distances reduces to $O((m+n) \\log(m+n))$ when both\nshapes are regular polygons, an improvement from $O(mn\\log(mn))$ operations\nneeded to compute distances between general polygons of $n$ and $m$ sides. We\nalso apply these formulas to several examples of network microstructure with\nvarying disorder. For Archimedean lattices, a class of regular tilings, we can\nexpress turning disorders with exact expressions. We also consider turning\ndisorders applied to two examples of stochastic processes on networks: spring\nnetworks evolving under T1 moves and polygonal rupture processes. We find that\nthe two aspects of defining different turning disorders, the choice of ordered\nshape and whether to apply area-weighting, can capture different notions of\nnetwork disorder.\n","authors":["Alex Dolce","Ryan Lavelle","Bernard Scott","Ashlyn Urbanski","Joseph Klobusicky"],"pdf_url":"https://arxiv.org/pdf/2503.06415v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24827v1","updated":"2025-10-28T16:04:03Z","published":"2025-10-28T16:04:03Z","title":"MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal\n  Interaction for Multimodal Emotion Recognition","summary":"  Multimodal emotion recognition is crucial for future human-computer\ninteraction. However, accurate emotion recognition still faces significant\nchallenges due to differences between different modalities and the difficulty\nof characterizing unimodal emotional information. To solve these problems, a\nhybrid network model based on multipath cross-modal interaction (MCIHN) is\nproposed. First, adversarial autoencoders (AAE) are constructed separately for\neach modality. The AAE learns discriminative emotion features and reconstructs\nthe features through a decoder to obtain more discriminative information about\nthe emotion classes. Then, the latent codes from the AAE of different\nmodalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to\nreduce the discrepancy between modalities, establish the emotional relationship\nbetween interacting modalities, and generate the interaction features between\ndifferent modalities. Multimodal fusion using the Feature Fusion module (FFM)\nfor better emotion recognition. Experiments were conducted on publicly\navailable SIMS and MOSI datasets, demonstrating that MCIHN achieves superior\nperformance.\n","authors":["Haoyang Zhang","Zhou Yang","Ke Sun","Yucai Pang","Guoliang Xu"],"pdf_url":"https://arxiv.org/pdf/2510.24827v1.pdf","comment":"The paper will be published in the MMAsia2025 conference proceedings"},{"id":"http://arxiv.org/abs/2502.05177v3","updated":"2025-10-28T16:02:48Z","published":"2025-02-07T18:59:56Z","title":"Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with\n  Leading Short-Context Accuracy","summary":"  We introduce Long-VITA, a simple yet effective large multi-modal model for\nlong-context visual-language understanding tasks. It is adept at concurrently\nprocessing and analyzing modalities of image, video, and text over 4K frames or\n1M tokens while delivering advanced performances on short-context multi-modal\ntasks. We propose an effective multi-modal training schema that starts with\nlarge language models and proceeds through vision-language alignment, general\nknowledge learning, and two sequential stages of long-sequence fine-tuning. We\nfurther implement context-parallelism distributed inference and logits-masked\nlanguage modeling head to scale Long-VITA to infinitely long inputs of images\nand texts during model inference. Regarding training data, Long-VITA is built\non a mix of 17M samples from public datasets only and demonstrates\nstate-of-the-art performance on various multi-modal benchmarks, compared\nagainst recent cutting-edge models with internal data. Long-VITA is fully\nopen-source and reproducible.. By leveraging our inference designs, Long-VITA\nmodels achieve a remarkable 2x prefill speedup and 4x context length extension\nin a single node with 8 GPUs. We hope Long-VITA can serve as a competitive\nbaseline and offer valuable insights for the open-source community in advancing\nlong-context multi-modal understanding.\n","authors":["Yunhang Shen","Chaoyou Fu","Shaoqi Dong","Xiong Wang","Yi-Fan Zhang","Peixian Chen","Mengdan Zhang","Haoyu Cao","Ke Li","Shaohui Lin","Xiawu Zheng","Yan Zhang","Yiyi Zhou","Ran He","Caifeng Shan","Rongrong Ji","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2502.05177v3.pdf","comment":"https://github.com/VITA-MLLM/Long-VITA"},{"id":"http://arxiv.org/abs/2510.24563v1","updated":"2025-10-28T15:56:36Z","published":"2025-10-28T15:56:36Z","title":"OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents","summary":"  With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.\n","authors":["Hongrui Jia","Jitong Liao","Xi Zhang","Haiyang Xu","Tianbao Xie","Chaoya Jiang","Ming Yan","Si Liu","Wei Ye","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2510.24563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.02293v2","updated":"2025-10-28T15:28:13Z","published":"2025-08-04T11:03:12Z","title":"Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning","summary":"  So-called unsupervised anomaly detection is better described as\nsemi-supervised, as it assumes all training data are nominal. This assumption\nsimplifies training but requires manual data curation, introducing bias and\nlimiting adaptability. We propose Confident Meta-learning (CoMet), a novel\ntraining strategy that enables deep anomaly detection models to learn from\nuncurated datasets where nominal and anomalous samples coexist, eliminating the\nneed for explicit filtering. Our approach integrates Soft Confident Learning,\nwhich assigns lower weights to low-confidence samples, and Meta-Learning, which\nstabilizes training by regularizing updates based on training validation loss\ncovariance. This prevents overfitting and enhances robustness to noisy data.\nCoMet is model-agnostic and can be applied to any anomaly detection method\ntrainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2\nwith two state-of-the-art models demonstrate the effectiveness of our approach,\nconsistently improving over the baseline methods, remaining insensitive to\nanomalies in the training set, and setting a new state-of-the-art across all\ndatasets. Code is available at https://github.com/aqeeelmirza/CoMet\n","authors":["Muhammad Aqeel","Shakiba Sharifi","Marco Cristani","Francesco Setti"],"pdf_url":"https://arxiv.org/pdf/2508.02293v2.pdf","comment":"Accepted to IEEE/CVF International Conference on Computer Vision\n  (ICCV2025)"},{"id":"http://arxiv.org/abs/2510.24514v1","updated":"2025-10-28T15:26:20Z","published":"2025-10-28T15:26:20Z","title":"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs","summary":"  While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.\n","authors":["Huanyu Zhang","Wenshan Wu","Chengzu Li","Ning Shang","Yan Xia","Yangyu Huang","Yifan Zhang","Li Dong","Zhang Zhang","Liang Wang","Tieniu Tan","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2510.24514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24821v1","updated":"2025-10-28T15:24:13Z","published":"2025-10-28T15:24:13Z","title":"Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal\n  Perception and Generation","summary":"  We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a\nsparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion\ntotal parameters, of which only 6.1 billion are active per token. This\narchitecture enables highly efficient scaling (dramatically improving\ncomputational efficiency while significantly expanding model capacity) and\nempowers stronger unified multimodal intelligence across vision, speech, and\nlanguage, representing a key step toward Artificial General Intelligence (AGI).\nCompared to its predecessor, the upgraded version exhibits substantial\nimprovements across multimodal understanding and generation. We significantly\nadvance speech recognition capabilities, achieving state-of-the-art performance\nin contextual ASR and highly competitive results in dialect-aware ASR. In image\ngeneration, Ming-Flash-Omni introduces high-fidelity text rendering and\ndemonstrates marked gains in scene consistency and identity preservation during\nimage editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,\na capability that not only achieves strong standalone segmentation performance\nbut also enhances spatial control in image generation and improves editing\nconsistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in\ntext-to-image generation and generative segmentation, and sets new records on\nall 12 contextual ASR benchmarks, all within a single unified architecture.\n","authors":["Inclusion AI"," :","Bowen Ma","Cheng Zou","Canxiang Yan","Chunxiang Jin","Chunjie Shen","Dandan Zheng","Fudong Wang","Furong Xu","GuangMing Yao","Jun Zhou","Jingdong Chen","Jianing Li","Jianxin Sun","Jiajia Liu","Jianjiang Zhu","Jianping Jiang","Jun Peng","Kaixiang Ji","Kaimeng Ren","Libin Wang","Lixiang Ru","Longhua Tan","Lan Wang","Mochen Bai","Ning Gao","Qingpei Guo","Qinglong Zhang","Qiang Xu","Rui Liu","Ruijie Xiong","Ruobing Zheng","Sirui Gao","Tianqi Li","Tinghao Liu","Weilong Chai","Xinyu Xiao","Xiaomei Wang","Xiaolong Wang","Xiao Lu","Xiaoyu Li","Xingning Dong","Xuzheng Yu","Yi Yuan","Yuting Gao","Yuting Xiao","Yunxiao Sun","Yipeng Chen","Yifan Mao","Yifei Wu","Yongjie Lyu","Ziping Ma","Zhiqiang Fang","Zhihao Qiu","Ziyuan Huang","Zizheng Yang","Zhengyu He"],"pdf_url":"https://arxiv.org/pdf/2510.24821v1.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.17071v2","updated":"2025-10-28T15:20:36Z","published":"2025-03-21T11:54:16Z","title":"Superpowering Open-Vocabulary Object Detectors for X-ray Vision","summary":"  Open-vocabulary object detection (OvOD) is set to revolutionize security\nscreening by enabling systems to recognize any item in X-ray scans. However,\ndeveloping effective OvOD models for X-ray imaging presents unique challenges\ndue to data scarcity and the modality gap that prevents direct adoption of\nRGB-based solutions. To overcome these limitations, we propose RAXO, a\ntraining-free framework that repurposes off-the-shelf RGB OvOD detectors for\nrobust X-ray detection. RAXO builds high-quality X-ray class descriptors using\na dual-source retrieval strategy. It gathers relevant RGB images from the web\nand enriches them via a novel X-ray material transfer mechanism, eliminating\nthe need for labeled databases. These visual descriptors replace text-based\nclassification in OvOD, leveraging intra-modal feature distances for robust\ndetection. Extensive experiments demonstrate that RAXO consistently improves\nOvOD performance, providing an average mAP increase of up to 17.0 points over\nbase detectors. To further support research in this emerging field, we also\nintroduce DET-COMPASS, a new benchmark featuring bounding box annotations for\nover 300 object categories, enabling large-scale evaluation of OvOD in X-ray.\nCode and dataset available at: https://github.com/PAGF188/RAXO.\n","authors":["Pablo Garcia-Fernandez","Lorenzo Vaquero","Mingxuan Liu","Feng Xue","Daniel Cores","Nicu Sebe","Manuel Mucientes","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2503.17071v2.pdf","comment":"Accepted at ICCV 2025"},{"id":"http://arxiv.org/abs/2510.24503v1","updated":"2025-10-28T15:15:14Z","published":"2025-10-28T15:15:14Z","title":"Local Performance vs. Out-of-Distribution Generalization: An Empirical\n  Analysis of Personalized Federated Learning in Heterogeneous Data\n  Environments","summary":"  In the context of Federated Learning with heterogeneous data environments,\nlocal models tend to converge to their own local model optima during local\ntraining steps, deviating from the overall data distributions. Aggregation of\nthese local updates, e.g., with FedAvg, often does not align with the global\nmodel optimum (client drift), resulting in an update that is suboptimal for\nmost clients. Personalized Federated Learning approaches address this challenge\nby exclusively focusing on the average local performances of clients' models on\ntheir own data distribution. Generalization to out-of-distribution samples,\nwhich is a substantial benefit of FedAvg and represents a significant component\nof robustness, appears to be inadequately incorporated into the assessment and\nevaluation processes. This study involves a thorough evaluation of Federated\nLearning approaches, encompassing both their local performance and their\ngeneralization capabilities. Therefore, we examine different stages within a\nsingle communication round to enable a more nuanced understanding of the\nconsidered metrics. Furthermore, we propose and incorporate a modified approach\nof FedAvg, designated as Federated Learning with Individualized Updates (FLIU),\nextending the algorithm by a straightforward individualization step with an\nadaptive personalization factor. We evaluate and compare the approaches\nempirically using MNIST and CIFAR-10 under various distributional conditions,\nincluding benchmark IID and pathological non-IID, as well as additional novel\ntest environments with Dirichlet distribution specifically developed to stress\nthe algorithms on complex data heterogeneity.\n","authors":["Mortesa Hussaini","Jan Theiß","Anthony Stein"],"pdf_url":"https://arxiv.org/pdf/2510.24503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24820v1","updated":"2025-10-28T15:12:15Z","published":"2025-10-28T15:12:15Z","title":"SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing","summary":"  With the rapid advancement of text-to-image (T2I) models, ensuring their\nsafety has become increasingly critical. Existing safety approaches can be\ncategorized into training-time and inference-time methods. While inference-time\nmethods are widely adopted due to their cost-effectiveness, they often suffer\nfrom limitations such as over-refusal and imbalance between safety and utility.\nTo address these challenges, we propose a multi-round safety editing framework\nthat functions as a model-agnostic, plug-and-play module, enabling efficient\nsafety alignment for any text-to-image model. Central to this framework is\nMR-SafeEdit, a multi-round image-text interleaved dataset specifically\nconstructed for safety editing in text-to-image generation. We introduce a\npost-hoc safety editing paradigm that mirrors the human cognitive process of\nidentifying and refining unsafe content. To instantiate this paradigm, we\ndevelop SafeEditor, a unified MLLM capable of multi-round safety editing on\ngenerated images. Experimental results show that SafeEditor surpasses prior\nsafety approaches by reducing over-refusal while achieving a more favorable\nsafety-utility balance.\n","authors":["Ruiyang Zhang","Jiahao Luo","Xiaoru Feng","Qiufan Pang","Yaodong Yang","Juntao Dai"],"pdf_url":"https://arxiv.org/pdf/2510.24820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.24486v1","updated":"2025-10-28T15:00:07Z","published":"2025-10-28T15:00:07Z","title":"Fast and accurate neural reflectance transformation imaging through\n  knowledge distillation","summary":"  Reflectance Transformation Imaging (RTI) is very popular for its ability to\nvisually analyze surfaces by enhancing surface details through interactive\nrelighting, starting from only a few tens of photographs taken with a fixed\ncamera and variable illumination. Traditional methods like Polynomial Texture\nMaps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle\nto accurately capture complex reflectance fields using few per-pixel\ncoefficients and fixed bases, leading to artifacts, especially in highly\nreflective or shadowed areas. The NeuralRTI approach, which exploits a neural\nautoencoder to learn a compact function that better approximates the local\nreflectance as a function of light directions, has been shown to produce\nsuperior quality at comparable storage cost. However, as it performs\ninteractive relighting with custom decoder networks with many parameters, the\nrendering step is computationally expensive and not feasible at full resolution\nfor large images on limited hardware. Earlier attempts to reduce costs by\ndirectly training smaller networks have failed to produce valid results. For\nthis reason, we propose to reduce its computational cost through a novel\nsolution based on Knowledge Distillation (DisK-NeuralRTI). ...\n","authors":["Tinsae G. Dulecha","Leonardo Righetto","Ruggero Pintus","Enrico Gobbetti","Andrea Giachetti"],"pdf_url":"https://arxiv.org/pdf/2510.24486v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2510.14383v2","updated":"2025-10-28T14:50:18Z","published":"2025-10-16T07:31:21Z","title":"DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with\n  Analytical Insights","summary":"  Accurate brain tumor segmentation is significant for clinical diagnosis and\ntreatment but remains challenging due to tumor heterogeneity. Mamba-based State\nSpace Models have demonstrated promising performance. However, despite their\ncomputational efficiency over other neural architectures, they incur\nconsiderable overhead for this task due to their sequential feature computation\nacross multiple spatial axes. Moreover, their robustness across diverse BraTS\ndata partitions remains largely unexplored, leaving a critical gap in reliable\nevaluation. To address this, we first propose a dual-resolution bi-directional\nMamba (DRBD-Mamba), an efficient 3D segmentation model that captures\nmulti-scale long-range dependencies with minimal computational overhead. We\nleverage a space-filling curve to preserve spatial locality during 3D-to-1D\nfeature mapping, thereby reducing reliance on computationally expensive\nmulti-axial feature scans. To enrich feature representation, we propose a gated\nfusion module that adaptively integrates forward and reverse contexts, along\nwith a quantization block that improves robustness. We further propose five\nsystematic folds on BraTS2023 for rigorous evaluation of segmentation\ntechniques under diverse conditions and present analysis of common failure\nscenarios. On the 20% test set used by recent methods, our model achieves Dice\nimprovements of 0.10% for whole tumor, 1.75% for tumor core, and 0.93% for\nenhancing tumor. Evaluations on the proposed systematic folds demonstrate that\nour model maintains competitive whole tumor accuracy while achieving clear\naverage Dice gains of 1.16% for tumor core and 1.68% for enhancing tumor over\nexisting state-of-the-art. Furthermore, our model achieves a 15x efficiency\nimprovement while maintaining high segmentation accuracy, highlighting its\nrobustness and computational advantage over existing methods.\n","authors":["Danish Ali","Ajmal Mian","Naveed Akhtar","Ghulam Mubashar Hassan"],"pdf_url":"https://arxiv.org/pdf/2510.14383v2.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2510.24486v1","updated":"2025-10-28T15:00:07Z","published":"2025-10-28T15:00:07Z","title":"Fast and accurate neural reflectance transformation imaging through\n  knowledge distillation","summary":"  Reflectance Transformation Imaging (RTI) is very popular for its ability to\nvisually analyze surfaces by enhancing surface details through interactive\nrelighting, starting from only a few tens of photographs taken with a fixed\ncamera and variable illumination. Traditional methods like Polynomial Texture\nMaps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle\nto accurately capture complex reflectance fields using few per-pixel\ncoefficients and fixed bases, leading to artifacts, especially in highly\nreflective or shadowed areas. The NeuralRTI approach, which exploits a neural\nautoencoder to learn a compact function that better approximates the local\nreflectance as a function of light directions, has been shown to produce\nsuperior quality at comparable storage cost. However, as it performs\ninteractive relighting with custom decoder networks with many parameters, the\nrendering step is computationally expensive and not feasible at full resolution\nfor large images on limited hardware. Earlier attempts to reduce costs by\ndirectly training smaller networks have failed to produce valid results. For\nthis reason, we propose to reduce its computational cost through a novel\nsolution based on Knowledge Distillation (DisK-NeuralRTI). ...\n","authors":["Tinsae G. Dulecha","Leonardo Righetto","Ruggero Pintus","Enrico Gobbetti","Andrea Giachetti"],"pdf_url":"https://arxiv.org/pdf/2510.24486v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2505.10755v3","updated":"2025-10-28T11:05:00Z","published":"2025-05-15T23:47:58Z","title":"Procedural Generation of Articulated Simulation-Ready Assets","summary":"  We introduce Infinigen-Articulated, a toolkit for generating realistic,\nprocedurally generated articulated assets for robotics simulation. We include\nprocedural generators for 18 common articulated object categories along with\nhigh-level utilities for use creating custom articulated assets in Blender. We\nalso provide an export pipeline to integrate the resulting assets along with\ntheir physical properties into common robotics simulators. Experiments\ndemonstrate that assets sampled from these generators are effective for movable\nobject segmentation, training generalizable reinforcement learning policies,\nand sim-to-real transfer of imitation learning policies.\n","authors":["Abhishek Joshi","Beining Han","Jack Nugent","Max Gonzalez Saez-Diez","Yiming Zuo","Jonathan Liu","Hongyu Wen","Stamatis Alexandropoulos","Karhan Kayan","Anna Calveri","Tao Sun","Gaowen Liu","Yi Shao","Alexander Raistrick","Jia Deng"],"pdf_url":"https://arxiv.org/pdf/2505.10755v3.pdf","comment":"Updated to include information on newly implemented assets, new\n  experimental results (both simulation and real world), and additional\n  features including material and dynamics parameters"}]},"2025-10-27T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2510.18239v2","updated":"2025-10-27T21:18:47Z","published":"2025-10-21T02:53:17Z","title":"LIME: Link-based user-item Interaction Modeling with decoupled xor\n  attention for Efficient test time scaling","summary":"  Scaling large recommendation systems requires advancing three major\nfrontiers: processing longer user histories, expanding candidate sets, and\nincreasing model capacity. While promising, transformers' computational cost\nscales quadratically with the user sequence length and linearly with the number\nof candidates. This trade-off makes it prohibitively expensive to expand\ncandidate sets or increase sequence length at inference, despite the\nsignificant performance improvements.\n  We introduce \\textbf{LIME}, a novel architecture that resolves this\ntrade-off. Through two key innovations, LIME fundamentally reduces\ncomputational complexity. First, low-rank ``link embeddings\" enable\npre-computation of attention weights by decoupling user and candidate\ninteractions, making the inference cost nearly independent of candidate set\nsize. Second, a linear attention mechanism, \\textbf{LIME-XOR}, reduces the\ncomplexity with respect to user sequence length from quadratic ($O(N^2)$) to\nlinear ($O(N)$).\n  Experiments on public and industrial datasets show LIME achieves near-parity\nwith state-of-the-art transformers but with a 10$\\times$ inference speedup on\nlarge candidate sets or long sequence lengths. When tested on a major\nrecommendation platform, LIME improved user engagement while maintaining\nminimal inference costs with respect to candidate set size and user history\nlength, establishing a new paradigm for efficient and expressive recommendation\nsystems.\n","authors":["Yunjiang Jiang","Ayush Agarwal","Yang Liu","Bi Xue"],"pdf_url":"https://arxiv.org/pdf/2510.18239v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2510.23544v1","updated":"2025-10-27T17:19:37Z","published":"2025-10-27T17:19:37Z","title":"LimRank: Less is More for Reasoning-Intensive Information Reranking","summary":"  Existing approaches typically rely on large-scale fine-tuning to adapt LLMs\nfor information reranking tasks, which is computationally expensive. In this\nwork, we demonstrate that modern LLMs can be effectively adapted using only\nminimal, high-quality supervision. To enable this, we design\nLIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating\ndiverse, challenging, and realistic reranking examples. Using this synthetic\ndata, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two\nchallenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and\nFollowIR for instruction-following retrieval. Our experiments demonstrate that\nLIMRANK achieves competitive performance, while being trained on less than 5%\nof the data typically used in prior work. Further ablation studies demonstrate\nthe effectiveness of LIMRANK-SYNTHESIZER and the strong generalization\ncapabilities of LIMRANK across downstream tasks, including scientific\nliterature search and retrieval-augmented generation for knowledge-intensive\nproblem solving.\n","authors":["Tingyu Song","Yilun Zhao","Siyue Zhang","Chen Zhao","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2510.23544v1.pdf","comment":"EMNLP 2025 Main (Short)"},{"id":"http://arxiv.org/abs/2509.16599v3","updated":"2025-10-27T16:02:27Z","published":"2025-09-20T09:50:18Z","title":"Computational-Assisted Systematic Review and Meta-Analysis (CASMA):\n  Effect of a Subclass of GnRH-a on Endometriosis Recurrence","summary":"  Background: Evidence synthesis facilitates evidence-based medicine. This task\nbecomes increasingly difficult to accomplished with applying computational\nsolutions, since the medical literature grows at astonishing rates. Objective:\nThis study evaluates an information retrieval-driven workflow, CASMA, to\nenhance the efficiency, transparency, and reproducibility of systematic\nreviews. Endometriosis recurrence serves as the ideal case due to its complex\nand ambiguous literature. Methods: The hybrid approach integrates PRISMA\nguidelines with fuzzy matching and regular expression (regex) to facilitate\nsemi-automated deduplication and filtered records before manual screening. The\nworkflow synthesised evidence from randomised controlled trials on the efficacy\nof a subclass of gonadotropin-releasing hormone agonists (GnRH-a). A modified\nsplitting method addressed unit-of-analysis errors in multi-arm trials.\nResults: The workflow sharply reduced the screening workload, taking only 11\ndays to fetch and filter 33,444 records. Seven eligible RCTs were synthesized\n(841 patients). The pooled random-effects model yielded a Risk Ratio (RR) of\n$0.64$ ($95\\%$ CI $0.48$ to $0.86$), demonstrating a $36\\%$ reduction in\nrecurrence, with non-significant heterogeneity ($I^2=0.00\\%$, $\\tau^2=0.00$).\nThe findings were robust and stable, as they were backed by sensitivity\nanalyses. Conclusion: This study demonstrates an application of an\ninformation-retrieval-driven workflow for medical evidence synthesis. The\napproach yields valuable clinical results and a generalisable framework to\nscale up the evidence synthesis, bridging the gap between clinical research and\ncomputer science.\n","authors":["Sandro Tsang"],"pdf_url":"https://arxiv.org/pdf/2509.16599v3.pdf","comment":"15 pages, 12 figures and 4 tables. This work describes an information\n  retrieval-driven workflow for medical evidence synthesis, with an application\n  to endometriosis recurrence. The method can be generalized to other\n  systematic reviews. The preregistered protocol is available:\n  https://doi.org/10.17605/OSF.IO/R2DFA"},{"id":"http://arxiv.org/abs/2507.12311v4","updated":"2025-10-27T14:28:50Z","published":"2025-07-16T15:06:29Z","title":"An Ecosystem for Ontology Interoperability","summary":"  Ontology interoperability is one of the complicated issues that restricts the\nuse of ontologies in knowledge graphs (KGs). Different ontologies with\nconflicting and overlapping concepts make it difficult to design, develop, and\ndeploy an interoperable ontology for downstream tasks. We propose an ecosystem\nfor ontology interoperability. The ecosystem employs three state-of-the-art\nsemantic techniques in different phases of the ontology engineering life cycle:\nontology design patterns (ODPs) in the design phase, ontology matching and\nversioning (OM\\&OV) in the develop phase, and ontology-compliant knowledge\ngraphs (OCKGs) in the deploy phase, to achieve better ontology interoperability\nand data integration in real-world applications. A case study of sensor\nobservation in the building domain validates the usefulness of the proposed\necosystem.\n","authors":["Zhangcheng Qiang"],"pdf_url":"https://arxiv.org/pdf/2507.12311v4.pdf","comment":"5 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.15807v2","updated":"2025-10-27T13:12:32Z","published":"2025-05-21T17:59:01Z","title":"The Atlas of In-Context Learning: How Attention Heads Shape In-Context\n  Retrieval Augmentation","summary":"  Large language models are able to exploit in-context learning to access\nexternal knowledge beyond their training data through retrieval-augmentation.\nWhile promising, its inner workings remain unclear. In this work, we shed light\non the mechanism of in-context retrieval augmentation for question answering by\nviewing a prompt as a composition of informational components. We propose an\nattribution-based method to identify specialized attention heads, revealing\nin-context heads that comprehend instructions and retrieve relevant contextual\ninformation, and parametric heads that store entities' relational knowledge. To\nbetter understand their roles, we extract function vectors and modify their\nattention weights to show how they can influence the answer generation process.\nFinally, we leverage the gained insights to trace the sources of knowledge used\nduring inference, paving the way towards more safe and transparent language\nmodels.\n","authors":["Patrick Kahardipraja","Reduan Achtibat","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2505.15807v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2508.12365v2","updated":"2025-10-27T13:03:18Z","published":"2025-08-17T13:48:48Z","title":"TaoSR1: The Thinking Model for E-commerce Relevance Search","summary":"  Query-product relevance prediction is a core task in e-commerce search.\nBERT-based models excel at semantic matching but lack complex reasoning\ncapabilities. While Large Language Models (LLMs) are explored, most still use\ndiscriminative fine-tuning or distill to smaller models for deployment. We\npropose a framework to directly deploy LLMs for this task, addressing key\nchallenges: Chain-of-Thought (CoT) error accumulation, discriminative\nhallucination, and deployment feasibility. Our framework, TaoSR1, involves\nthree stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;\n(2) Offline sampling with a pass@N strategy and Direct Preference Optimization\n(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling\nwith Group Relative Policy Optimization (GRPO) to mitigate discriminative\nhallucination. Additionally, post-CoT processing and a cumulative\nprobability-based partitioning method enable efficient online deployment.\nTaoSR1 significantly outperforms baselines on offline datasets and achieves\nsubstantial gains in online side-by-side human evaluations, introducing a novel\nparadigm for applying CoT reasoning to relevance classification.\n","authors":["Chenhe Dong","Shaowei Yao","Pengkun Jiao","Jianhui Yang","Yiming Jin","Zerui Huang","Xiaojiang Zhou","Dan Ou","Haihong Tang","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2508.12365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.18936v2","updated":"2025-10-27T12:01:51Z","published":"2025-10-21T17:52:13Z","title":"SBAN: A Framework & Multi-Dimensional Dataset for Large Language Model\n  Pre-Training and Software Code Mining","summary":"  This paper introduces SBAN (Source code, Binary, Assembly, and Natural\nLanguage Description), a large-scale, multi-dimensional dataset designed to\nadvance the pre-training and evaluation of large language models (LLMs) for\nsoftware code analysis. SBAN comprises more than 3 million samples, including\n2.9 million benign and 672,000 malware respectively, each represented across\nfour complementary layers: binary code, assembly instructions, natural language\ndescriptions, and source code. This unique multimodal structure enables\nresearch on cross-representation learning, semantic understanding of software,\nand automated malware detection. Beyond security applications, SBAN supports\nbroader tasks such as code translation, code explanation, and other software\nmining tasks involving heterogeneous data. It is particularly suited for\nscalable training of deep models, including transformers and other LLM\narchitectures. By bridging low-level machine representations and high-level\nhuman semantics, SBAN provides a robust foundation for building intelligent\nsystems that reason about code. We believe that this dataset opens new\nopportunities for mining software behavior, improving security analytics, and\nenhancing LLM capabilities in pre-training and fine-tuning tasks for software\ncode mining.\n","authors":["Hamed Jelodar","Mohammad Meymani","Samita Bai","Roozbeh Razavi-Far","Ali A. Ghorbani"],"pdf_url":"https://arxiv.org/pdf/2510.18936v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23224v1","updated":"2025-10-27T11:22:28Z","published":"2025-10-27T11:22:28Z","title":"Accurate and Scalable Multimodal Pathology Retrieval via Attentive\n  Vision-Language Alignment","summary":"  The rapid digitization of histopathology slides has opened up new\npossibilities for computational tools in clinical and research workflows. Among\nthese, content-based slide retrieval stands out, enabling pathologists to\nidentify morphologically and semantically similar cases, thereby supporting\nprecise diagnoses, enhancing consistency across observers, and assisting\nexample-based education. However, effective retrieval of whole slide images\n(WSIs) remains challenging due to their gigapixel scale and the difficulty of\ncapturing subtle semantic differences amid abundant irrelevant content. To\novercome these challenges, we present PathSearch, a retrieval framework that\nunifies fine-grained attentive mosaic representations with global-wise slide\nembeddings aligned through vision-language contrastive learning. Trained on a\ncorpus of 6,926 slide-report pairs, PathSearch captures both fine-grained\nmorphological cues and high-level semantic patterns to enable accurate and\nflexible retrieval. The framework supports two key functionalities: (1)\nmosaic-based image-to-image retrieval, ensuring accurate and efficient slide\nresearch; and (2) multi-modal retrieval, where text queries can directly\nretrieve relevant slides. PathSearch was rigorously evaluated on four public\npathology datasets and three in-house cohorts, covering tasks including\nanatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination,\nand grading across diverse organs such as breast, lung, kidney, liver, and\nstomach. External results show that PathSearch outperforms traditional\nimage-to-image retrieval frameworks. A multi-center reader study further\ndemonstrates that PathSearch improves diagnostic accuracy, boosts confidence,\nand enhances inter-observer agreement among pathologists in real clinical\nscenarios. These results establish PathSearch as a scalable and generalizable\nretrieval solution for digital pathology.\n","authors":["Hongyi Wang","Zhengjie Zhu","Jiabo Ma","Fang Wang","Yue Shi","Bo Luo","Jili Wang","Qiuyu Cai","Xiuming Zhang","Yen-Wei Chen","Lanfen Lin","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2510.23224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.15709v2","updated":"2025-10-27T08:48:20Z","published":"2025-09-19T07:33:50Z","title":"Understanding Embedding Scaling in Collaborative Filtering","summary":"  Scaling recommendation models into large recommendation models has become one\nof the most widely discussed topics. Recent efforts focus on components beyond\nthe scaling embedding dimension, as it is believed that scaling embedding may\nlead to performance degradation. Although there have been some initial\nobservations on embedding, the root cause of their non-scalability remains\nunclear. Moreover, whether performance degradation occurs across different\ntypes of models and datasets is still an unexplored area. Regarding the effect\nof embedding dimensions on performance, we conduct large-scale experiments\nacross 10 datasets with varying sparsity levels and scales, using 4\nrepresentative classical architectures. We surprisingly observe two novel\nphenomena: double-peak and logarithmic. For the former, as the embedding\ndimension increases, performance first improves, then declines, rises again,\nand eventually drops. For the latter, it exhibits a perfect logarithmic curve.\nOur contributions are threefold. First, we discover two novel phenomena when\nscaling collaborative filtering models. Second, we gain an understanding of the\nunderlying causes of the double-peak phenomenon. Lastly, we theoretically\nanalyze the noise robustness of collaborative filtering models, with results\nmatching empirical observations.\n","authors":["Yicheng He","Zhou Kaiyu","Haoyue Bai","Fengbin Zhu","Yonghui Yang"],"pdf_url":"https://arxiv.org/pdf/2509.15709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23104v1","updated":"2025-10-27T08:18:02Z","published":"2025-10-27T08:18:02Z","title":"Leveraging Hierarchical Organization for Medical Multi-document\n  Summarization","summary":"  Medical multi-document summarization (MDS) is a complex task that requires\neffectively managing cross-document relationships. This paper investigates\nwhether incorporating hierarchical structures in the inputs of MDS can improve\na model's ability to organize and contextualize information across documents\ncompared to traditional flat summarization methods. We investigate two ways of\nincorporating hierarchical organization across three large language models\n(LLMs), and conduct comprehensive evaluations of the resulting summaries using\nautomated metrics, model-based metrics, and domain expert evaluation of\npreference, understandability, clarity, complexity, relevance, coverage,\nfactuality, and coherence. Our results show that human experts prefer\nmodel-generated summaries over human-written summaries. Hierarchical approaches\ngenerally preserve factuality, coverage, and coherence of information, while\nalso increasing human preference for summaries. Additionally, we examine\nwhether simulated judgments from GPT-4 align with human judgments, finding\nhigher agreement along more objective evaluation facets. Our findings\ndemonstrate that hierarchical structures can improve the clarity of medical\nsummaries generated by models while maintaining content coverage, providing a\npractical way to improve human preference for generated summaries.\n","authors":["Yi-Li Hsu","Katelyn X. Mei","Lucy Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07543v2","updated":"2025-10-27T07:40:12Z","published":"2025-07-10T08:38:31Z","title":"The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English\n  Corpora","summary":"  Cross-lingual retrieval-augmented generation (RAG) is a critical capability\nfor retrieving and generating answers across languages. Prior work in this\ncontext has mostly focused on generation and relied on benchmarks derived from\nopen-domain sources, most notably Wikipedia. In such settings, retrieval\nchallenges often remain hidden due to language imbalances, overlap with\npretraining data, and memorized content. To address this gap, we study\nArabic-English RAG in a domain-specific setting using benchmarks derived from\nreal-world corporate datasets. Our benchmarks include all combinations of\nlanguages for the user query and the supporting document, drawn independently\nand uniformly at random. This enables a systematic study of multilingual\nretrieval behavior.\n  Our findings reveal that retrieval is a critical bottleneck in cross-lingual\ndomain-specific scenarios, with substantial performance drops occurring when\nthe user query and supporting document languages differ. A key insight is that\nthese failures stem primarily from the retriever's difficulty in ranking\ndocuments across languages. Finally, we propose two simple retrieval strategies\nthat address this source of failure by enforcing equal retrieval from both\nlanguages or by translating the query, resulting in substantial improvements in\ncross-lingual and overall performance. These results highlight meaningful\nopportunities for improving multilingual retrieval, particularly in practical,\nreal-world RAG applications.\n","authors":["Chen Amiraz","Yaroslav Fyodorov","Elad Haramaty","Zohar Karnin","Liane Lewin-Eytan"],"pdf_url":"https://arxiv.org/pdf/2507.07543v2.pdf","comment":"Accepted to ArabicNLP 2025"},{"id":"http://arxiv.org/abs/2510.23077v1","updated":"2025-10-27T07:26:32Z","published":"2025-10-27T07:26:32Z","title":"Think before Recommendation: Autonomous Reasoning-enhanced Recommender","summary":"  The core task of recommender systems is to learn user preferences from\nhistorical user-item interactions. With the rapid development of large language\nmodels (LLMs), recent research has explored leveraging the reasoning\ncapabilities of LLMs to enhance rating prediction tasks. However, existing\ndistillation-based methods suffer from limitations such as the teacher model's\ninsufficient recommendation capability, costly and static supervision, and\nsuperficial transfer of reasoning ability. To address these issues, this paper\nproposes RecZero, a reinforcement learning (RL)-based recommendation paradigm\nthat abandons the traditional multi-model and multi-stage distillation\napproach. Instead, RecZero trains a single LLM through pure RL to autonomously\ndevelop reasoning capabilities for rating prediction. RecZero consists of two\nkey components: (1) \"Think-before-Recommendation\" prompt construction, which\nemploys a structured reasoning template to guide the model in step-wise\nanalysis of user interests, item features, and user-item compatibility; and (2)\nrule-based reward modeling, which adopts group relative policy optimization\n(GRPO) to compute rewards for reasoning trajectories and optimize the LLM.\nAdditionally, the paper explores a hybrid paradigm, RecOne, which combines\nsupervised fine-tuning with RL, initializing the model with cold-start\nreasoning samples and further optimizing it with RL. Experimental results\ndemonstrate that RecZero and RecOne significantly outperform existing baseline\nmethods on multiple benchmark datasets, validating the superiority of the RL\nparadigm in achieving autonomous reasoning-enhanced recommender systems.\n","authors":["Xiaoyu Kong","Junguang Jiang","Bin Liu","Ziru Xu","Han Zhu","Jian Xu","Bo Zheng","Jiancan Wu","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23077v1.pdf","comment":"NeurIPS 2025 poster"},{"id":"http://arxiv.org/abs/2510.23066v1","updated":"2025-10-27T06:56:08Z","published":"2025-10-27T06:56:08Z","title":"Multi-Stage Field Extraction of Financial Documents with OCR and Compact\n  Vision-Language Models","summary":"  Financial documents are essential sources of information for regulators,\nauditors, and financial institutions, particularly for assessing the wealth and\ncompliance of Small and Medium-sized Businesses. However, SMB documents are\noften difficult to parse. They are rarely born digital and instead are\ndistributed as scanned images that are none machine readable. The scans\nthemselves are low in resolution, affected by skew or rotation, and often\ncontain noisy backgrounds. These documents also tend to be heterogeneous,\nmixing narratives, tables, figures, and multilingual content within the same\nreport. Such characteristics pose major challenges for automated information\nextraction, especially when relying on end to end large Vision Language Models,\nwhich are computationally expensive, sensitive to noise, and slow when applied\nto files with hundreds of pages.\n  We propose a multistage pipeline that leverages traditional image processing\nmodels and OCR extraction, together with compact VLMs for structured field\nextraction of large-scale financial documents. Our approach begins with image\npre-processing, including segmentation, orientation detection, and size\nnormalization. Multilingual OCR is then applied to recover page-level text.\nUpon analyzing the text information, pages are retrieved for coherent sections.\nFinally, compact VLMs are operated within these narrowed-down scopes to extract\nstructured financial indicators.\n  Our approach is evaluated using an internal corpus of multi-lingual, scanned\nfinancial documents. The results demonstrate that compact VLMs, together with a\nmultistage pipeline, achieves 8.8 times higher field level accuracy relative to\ndirectly feeding the whole document into large VLMs, only at 0.7 percent of the\nGPU cost and 92.6 percent less end-to-end service latency.\n","authors":["Yichao Jin","Yushuo Wang","Qishuai Zhong","Kent Chiu Jin-Chun","Kenneth Zhu Ke","Donald MacDonald"],"pdf_url":"https://arxiv.org/pdf/2510.23066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23449v3","updated":"2025-10-27T06:39:35Z","published":"2025-05-29T13:56:21Z","title":"CMIE: Combining MLLM Insights with External Evidence for Explainable\n  Out-of-Context Misinformation Detection","summary":"  Multimodal large language models (MLLMs) have demonstrated impressive\ncapabilities in visual reasoning and text generation. While previous studies\nhave explored the application of MLLM for detecting out-of-context (OOC)\nmisinformation, our empirical analysis reveals two persisting challenges of\nthis paradigm. Evaluating the representative GPT-4o model on direct reasoning\nand evidence augmented reasoning, results indicate that MLLM struggle to\ncapture the deeper relationships-specifically, cases in which the image and\ntext are not directly connected but are associated through underlying semantic\nlinks. Moreover, noise in the evidence further impairs detection accuracy. To\naddress these challenges, we propose CMIE, a novel OOC misinformation detection\nframework that incorporates a Coexistence Relationship Generation (CRG)\nstrategy and an Association Scoring (AS) mechanism. CMIE identifies the\nunderlying coexistence relationships between images and text, and selectively\nutilizes relevant evidence to enhance misinformation detection. Experimental\nresults demonstrate that our approach outperforms existing methods.\n","authors":["Fanxiao Li","Jiaying Wu","Canyuan He","Wei Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.23449v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23018v1","updated":"2025-10-27T05:32:13Z","published":"2025-10-27T05:32:13Z","title":"Improving Product Search Relevance with EAR-MP: A Solution for the CIKM\n  2025 AnalytiCup","summary":"  Multilingual e-commerce search is challenging due to linguistic diversity and\nthe noise inherent in user-generated queries. This paper documents the solution\nemployed by our team (EAR-MP) for the CIKM 2025 AnalytiCup, which addresses two\ncore tasks: Query-Category (QC) relevance and Query-Item (QI) relevance. Our\napproach first normalizes the multilingual dataset by translating all text into\nEnglish, then mitigates noise through extensive data cleaning and\nnormalization. For model training, we build on DeBERTa-v3-large and improve\nperformance with label smoothing, self-distillation, and dropout. In addition,\nwe introduce task-specific upgrades, including hierarchical token injection for\nQC and a hybrid scoring mechanism for QI. Under constrained compute, our method\nachieves competitive results, attaining an F1 score of 0.8796 on QC and 0.8744\non QI. These findings underscore the importance of systematic data\npreprocessing and tailored training strategies for building robust,\nresource-efficient multilingual relevance systems.\n","authors":["JaeEun Lim","Soomin Kim","Jaeyong Seo","Iori Ono","Qimu Ran","Jae-woong Lee"],"pdf_url":"https://arxiv.org/pdf/2510.23018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.11080v2","updated":"2025-10-27T05:12:35Z","published":"2025-09-14T04:06:03Z","title":"Membership Inference Attacks on Recommender System: A Survey","summary":"  Recommender systems (RecSys) have been widely applied to various\napplications, including E-commerce, finance, healthcare, social media and have\nbecome increasingly influential in shaping user behavior and decision-making,\nhighlighting their growing impact in various domains. However, recent studies\nhave shown that RecSys are vulnerable to membership inference attacks (MIAs),\nwhich aim to infer whether user interaction record was used to train a target\nmodel or not. MIAs on RecSys models can directly lead to a privacy breach. For\nexample, via identifying the fact that a purchase record that has been used to\ntrain a RecSys associated with a specific user, an attacker can infer that\nuser's special quirks. In recent years, MIAs have been shown to be effective on\nother ML tasks, e.g., classification models and natural language processing.\nHowever, traditional MIAs are ill-suited for RecSys due to the unseen posterior\nprobability. Although MIAs on RecSys form a newly emerging and rapidly growing\nresearch area, there has been no systematic survey on this topic yet. In this\narticle, we conduct the first comprehensive survey on RecSys MIAs. This survey\noffers a comprehensive review of the latest advancements in RecSys MIAs,\nexploring the design principles, challenges, attack and defense associated with\nthis emerging field. We provide a unified taxonomy that categorizes different\nRecSys MIAs based on their characterizations and discuss their pros and cons.\nBased on the limitations and gaps identified in this survey, we point out\nseveral promising future research directions to inspire the researchers who\nwish to follow this area. This survey not only serves as a reference for the\nresearch community but also provides a clear description for researchers\noutside this research domain.\n","authors":["Jiajie He","Xintong Chen","Xinyang Fang","Min-Chun Chen","Yuechun Gu","Keke Chen"],"pdf_url":"https://arxiv.org/pdf/2509.11080v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2510.22956v1","updated":"2025-10-27T03:23:25Z","published":"2025-10-27T03:23:25Z","title":"Tagging-Augmented Generation: Assisting Language Models in Finding\n  Intricate Knowledge In Long Contexts","summary":"  Recent investigations into effective context lengths of modern flagship large\nlanguage models (LLMs) have revealed major limitations in effective question\nanswering (QA) and reasoning over long and complex contexts for even the\nlargest and most impressive cadre of models. While approaches like\nretrieval-augmented generation (RAG) and chunk-based re-ranking attempt to\nmitigate this issue, they are sensitive to chunking, embedding and retrieval\nstrategies and models, and furthermore, rely on extensive pre-processing,\nknowledge acquisition and indexing steps. In this paper, we propose\nTagging-Augmented Generation (TAG), a lightweight data augmentation strategy\nthat boosts LLM performance in long-context scenarios, without degrading and\naltering the integrity and composition of retrieved documents. We validate our\nhypothesis by augmenting two challenging and directly relevant\nquestion-answering benchmarks -- NoLima and NovelQA -- and show that tagging\nthe context or even just adding tag definitions into QA prompts leads to\nconsistent performance gains over the baseline -- up to 17% for 32K token\ncontexts, and 2.9% in complex reasoning question-answering for multi-hop\nqueries requiring knowledge across a wide span of text. Additional details are\navailable at https://sites.google.com/view/tag-emnlp.\n","authors":["Anwesan Pal","Karen Hovsepian","Tinghao Guo","Mengnan Zhao","Somendra Tripathi","Nikos Kanakaris","George Mihaila","Sumit Nigam"],"pdf_url":"https://arxiv.org/pdf/2510.22956v1.pdf","comment":"Paper accepted at EMNLP 2025"},{"id":"http://arxiv.org/abs/2510.22942v1","updated":"2025-10-27T02:56:08Z","published":"2025-10-27T02:56:08Z","title":"GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation","summary":"  Next Point-of-Interest (POI) recommendation is a critical task in modern\nLocation-Based Social Networks (LBSNs), aiming to model the complex\ndecision-making process of human mobility to provide personalized\nrecommendations for a user's next check-in location. Existing POI\nrecommendation models, predominantly based on Graph Neural Networks and\nsequential models, have been extensively studied. However, these models face a\nfundamental limitation: they struggle to simultaneously capture the inherent\nhierarchical structure of spatial choices and the dynamics and irregular shifts\nof user-specific temporal contexts. To overcome this limitation, we propose\nGTR-Mamba, a novel framework for cross-manifold conditioning and routing.\nGTR-Mamba leverages the distinct advantages of different mathematical spaces\nfor different tasks: it models the static, tree-like preference hierarchies in\nhyperbolic geometry, while routing the dynamic sequence updates to a novel\nMamba layer in the computationally stable and efficient Euclidean tangent\nspace. This process is coordinated by a cross-manifold channel that fuses\nspatio-temporal information to explicitly steer the State Space Model (SSM),\nenabling flexible adaptation to contextual changes. Extensive experiments on\nthree real-world datasets demonstrate that GTR-Mamba consistently outperforms\nstate-of-the-art baseline models in next POI recommendation.\n","authors":["Zhuoxuan Li","Jieyuan Pei","Tangwei Ye","Zhongyuan Lai","Zihan Liu","Fengyuan Xu","Qi Zhang","Liang Hu"],"pdf_url":"https://arxiv.org/pdf/2510.22942v1.pdf","comment":"14 pages, 8 figures, 4 tables, submitted to ICDE 2026"},{"id":"http://arxiv.org/abs/2507.05715v2","updated":"2025-10-27T01:44:45Z","published":"2025-07-08T06:58:24Z","title":"From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal\n  Collaborative Filtering Recommendation","summary":"  Most existing multimodal collaborative filtering recommendation (MCFRec)\nmethods rely heavily on ID features and multimodal content to enhance\nrecommendation performance. However, this paper reveals that ID features are\neffective but have limited benefits in multimodal collaborative filtering\nrecommendation. Therefore, this paper systematically deconstruct the pros and\ncons of ID features: (i) they provide initial embedding but lack semantic\nrichness, (ii) they provide a unique identifier for each user and item but\nhinder generalization to untrained data, and (iii) they assist in aligning and\nfusing multimodal features but may lead to representation shift. Based on these\ninsights, this paper proposes IDFREE, an ID-free multimodal collaborative\nFiltering REcommEndation baseline. IDFREE replaces ID features with multimodal\nfeatures and positional encodings to generate semantically meaningful ID-free\nembeddings. For ID-free multimodal collaborative filtering, it further proposes\nan adaptive similarity graph module to construct dynamic user-user and\nitem-item graphs based on multimodal features. Then, an augmented user-item\ngraph encoder is proposed to construct more effective user and item encoding.\nFinally, IDFREE achieves inter-multimodal alignment based on the contrastive\nlearning and uses Softmax loss as recommendation loss. Basic experiments on\nthree public datasets demonstrate that IDFREE outperforms existing ID-based\nMCFRec methods, achieving an average performance gain of 72.24% across standard\nmetrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended\nexperiments further validate our findings on the limitations of ID features in\nMCFRec. The code is released at https://github.com/G-H-Li/IDFREE.\n","authors":["Guohao Li","Li Jing","Jia Wu","Xuefei Li","Kai Zhu","Yue He"],"pdf_url":"https://arxiv.org/pdf/2507.05715v2.pdf","comment":"We identified that our current approach achieves its reported\n  performance only under specific data conditions, and its robustness is weaker\n  than we initially expected"},{"id":"http://arxiv.org/abs/2510.22888v1","updated":"2025-10-27T00:41:07Z","published":"2025-10-27T00:41:07Z","title":"MGFRec: Towards Reinforced Reasoning Recommendation with Multiple\n  Groundings and Feedback","summary":"  The powerful reasoning and generative capabilities of large language models\n(LLMs) have inspired researchers to apply them to reasoning-based\nrecommendation tasks, which require in-depth reasoning about user interests and\nthe generation of recommended items. However, previous reasoning-based\nrecommendation methods have typically performed inference within the language\nspace alone, without incorporating the actual item space. This has led to\nover-interpreting user interests and deviating from real items. Towards this\nresearch gap, we propose performing multiple rounds of grounding during\ninference to help the LLM better understand the actual item space, which could\nensure that its reasoning remains aligned with real items. Furthermore, we\nintroduce a user agent that provides feedback during each grounding step,\nenabling the LLM to better recognize and adapt to user interests. Comprehensive\nexperiments conducted on three Amazon review datasets demonstrate the\neffectiveness of incorporating multiple groundings and feedback. These findings\nunderscore the critical importance of reasoning within the actual item space,\nrather than being confined to the language space, for recommendation tasks.\n","authors":["Shihao Cai","Chongming Gao","Haoyan Liu","Wentao Shi","Jianshan Sun","Ruiming Tang","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2510.22888v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2209.08884v2","updated":"2025-10-27T23:54:59Z","published":"2022-09-19T09:39:29Z","title":"Adaptive 3D Mesh Steganography Based on Feature-Preserving Distortion","summary":"  Current 3D mesh steganography algorithms relying on geometric modification\nare prone to detection by steganalyzers. In traditional steganography, adaptive\nsteganography has proven to be an efficient means of enhancing steganography\nsecurity. Taking inspiration from this, we propose a highly adaptive embedding\nalgorithm, guided by the principle of minimizing a carefully crafted distortion\nthrough efficient steganography codes. Specifically, we tailor a\npayload-limited embedding optimization problem for 3D settings and devise a\nfeature-preserving distortion (FPD) to measure the impact of message embedding.\nThe distortion takes on an additive form and is defined as a weighted\ndifference of the effective steganalytic subfeatures utilized by the current 3D\nsteganalyzers. With practicality in mind, we refine the distortion to enhance\nrobustness and computational efficiency. By minimizing the FPD, our algorithm\ncan preserve mesh features to a considerable extent, including steganalytic and\ngeometric features, while achieving a high embedding capacity. During the\npractical embedding phase, we employ the Q-layered syndrome trellis code (STC).\nHowever, calculating the bit modification probability (BMP) for each layer of\nthe Q-layered STC, given the variation of Q, can be cumbersome. To address this\nissue, we design a universal and automatic approach for the BMP calculation.\nThe experimental results demonstrate that our algorithm achieves\nstate-of-the-art performance in countering 3D steganalysis. Code is available\nat https://github.com/zjhJOJO/3D-steganography-based-on-FPD.git.\n","authors":["Yushu Zhang","Jiahao Zhu","Mignfu Xue","Xinpeng Zhang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2209.08884v2.pdf","comment":"IEEE TVCG, corresponding author Jiahao Zhu, code is available at\n  https://github.com/zjhJOJO/3D-steganography-based-on-FPD.git"},{"id":"http://arxiv.org/abs/2509.04086v2","updated":"2025-10-27T14:28:49Z","published":"2025-09-04T10:32:40Z","title":"TEn-CATG:Text-Enriched Audio-Visual Video Parsing with Multi-Scale\n  Category-Aware Temporal Graph","summary":"  Audio-visual video parsing (AVVP) aims to detect event categories and their\ntemporal boundaries in videos, typically under weak supervision. Existing\nmethods mainly focus on (i) improving temporal modeling using attention-based\narchitectures or (ii) generating richer pseudo-labels to address the absence of\nframe-level annotations. However, attention-based models often overfit noisy\npseudo-labels, leading to cumulative training errors, while pseudo-label\ngeneration approaches distribute attention uniformly across frames, weakening\ntemporal localization accuracy. To address these challenges, we propose\nTEn-CATG, a text-enriched AVVP framework that combines semantic calibration\nwith category-aware temporal reasoning. More specifically, we design a\nbi-directional text fusion (BiT) module by leveraging audio-visual features as\nsemantic anchors to refine text embeddings, which departs from conventional\ntext-to-feature alignment, thereby mitigating noise and enhancing cross-modal\nconsistency. Furthermore, we introduce the category-aware temporal graph (CATG)\nmodule to model temporal relationships by selecting multi-scale temporal\nneighbors and learning category-specific temporal decay factors, enabling\neffective event-dependent temporal reasoning. Extensive experiments demonstrate\nthat TEn-CATG achieves state-of-the-art results across multiple evaluation\nmetrics on benchmark datasets LLP and UnAV-100, highlighting its robustness and\nsuperior ability to capture complex temporal and semantic dependencies in\nweakly supervised AVVP tasks.\n","authors":["Yaru Chen","Faegheh Sardari","Peiliang Zhang","Ruohao Guo","Yang Xiang","Zhenbo Li","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2509.04086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23299v1","updated":"2025-10-27T13:05:27Z","published":"2025-10-27T13:05:27Z","title":"MMSD3.0: A Multi-Image Benchmark for Real-World Multimodal Sarcasm\n  Detection","summary":"  Despite progress in multimodal sarcasm detection, existing datasets and\nmethods predominantly focus on single-image scenarios, overlooking potential\nsemantic and affective relations across multiple images. This leaves a gap in\nmodeling cases where sarcasm is triggered by multi-image cues in real-world\nsettings. To bridge this gap, we introduce MMSD3.0, a new benchmark composed\nentirely of multi-image samples curated from tweets and Amazon reviews. We\nfurther propose the Cross-Image Reasoning Model (CIRM), which performs targeted\ncross-image sequence modeling to capture latent inter-image connections. In\naddition, we introduce a relevance-guided, fine-grained cross-modal fusion\nmechanism based on text-image correspondence to reduce information loss during\nintegration. We establish a comprehensive suite of strong and representative\nbaselines and conduct extensive experiments, showing that MMSD3.0 is an\neffective and reliable benchmark that better reflects real-world conditions.\nMoreover, CIRM demonstrates state-of-the-art performance across MMSD, MMSD2.0\nand MMSD3.0, validating its effectiveness in both single-image and multi-image\nscenarios.\n","authors":["Haochen Zhao","Yuyao Kong","Yongxiu Xu","Gaopeng Gou","Hongbo Xu","Yubin Wang","Haoliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.23299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01737v2","updated":"2025-10-27T08:33:50Z","published":"2024-10-02T16:47:55Z","title":"Robust Modality-incomplete Anomaly Detection: A Modality-instructive\n  Framework with Benchmark","summary":"  Multimodal Industrial Anomaly Detection (MIAD), which utilizes 3D point\nclouds and 2D RGB images to identify abnormal regions in products, plays a\ncrucial role in industrial quality inspection. However, traditional MIAD\nsettings assume that all 2D and 3D modalities are paired, ignoring the fact\nthat multimodal data collected from the real world is often imperfect due to\nmissing modalities. Additionally, models trained on modality-incomplete data\nare prone to overfitting. Therefore, MIAD models that demonstrate robustness\nagainst modality-incomplete data are highly desirable in practice. To address\nthis, we introduce a pioneering study that comprehensively investigates\nModality-Incomplete Industrial Anomaly Detection (MIIAD), and under the\nguidance of experts, we construct the MIIAD Bench with rich modality-missing\nsettings to account for imperfect learning environments with incomplete\nmultimodal information. As expected, we find that most existing MIAD methods\nperform poorly on the MIIAD Bench, leading to significant performance\ndegradation. To tackle this challenge, we propose a novel two-stage Robust\nmodAlity-aware fusing and Detecting framewoRk, abbreviated as RADAR.\nSpecifically: i) We propose Modality-incomplete Instruction to guide the\nmultimodal Transformer to robustly adapt to various modality-incomplete\nscenarios, and implement adaptive parameter learning based on HyperNetwork. ii)\nThen, we construct a Double-Pseudo Hybrid Module to highlight the uniqueness of\nmodality combinations, mitigating overfitting issues and further enhancing the\nrobustness of the MIIAD model. Our experimental results demonstrate that the\nproposed RADAR significantly outperforms traditional MIAD methods on our newly\ncreated MIIAD dataset, proving its practical application value.\n","authors":["Bingchen Miao","Wenqiao Zhang","Juncheng Li","Wangyu Wu","Siliang Tang","Zhaocheng Li","Haochen Shi","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.01737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23056v1","updated":"2025-10-27T06:39:50Z","published":"2025-10-27T06:39:50Z","title":"Enabling American Sign Language Communication Under Low Data Rates","summary":"  In recent years, video conferencing applications have become increasingly\nprevalent, relying heavily on high-speed internet connectivity. When such\nconnectivity is lacking, users often default to audio-only communication, a\nmode that significantly disadvantages American Sign Language (ASL) users, whose\ncommunication relies on hand gestures, body movement, and facial expressions.\nIn this work, we introduce VC4ASL, a system designed to enable ASL\ncommunication over the audio channel of existing video conferencing\napplications, even in the absence of reliable video. VC4ASL integrates\nseamlessly with current platforms without requiring any modifications. Our\napproach establishes a communication channel through audio by encoding and\ntransmitting human pose information, which is then rendered to reconstruct\nsigned content. We propose novel receive-side error detection and correction\nmechanisms that exploit the inherent structural constraints of human pose data.\nTo evaluate the system, we simulate network-degraded environments, generate\npose-based ASL video sequences, and conduct user studies to assess\ncomprehension among ASL users. Experimental results demonstrate that VC4ASL\neffectively facilitates intelligible ASL communication over audio in\nlow-bandwidth scenarios where video transmission is impaired.\n","authors":["Panneer Selvam Santhalingam","Swann Thantsin","Ahmad Kamari","Parth Pathak","Kenneth De Haan"],"pdf_url":"https://arxiv.org/pdf/2510.23056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23449v3","updated":"2025-10-27T06:39:35Z","published":"2025-05-29T13:56:21Z","title":"CMIE: Combining MLLM Insights with External Evidence for Explainable\n  Out-of-Context Misinformation Detection","summary":"  Multimodal large language models (MLLMs) have demonstrated impressive\ncapabilities in visual reasoning and text generation. While previous studies\nhave explored the application of MLLM for detecting out-of-context (OOC)\nmisinformation, our empirical analysis reveals two persisting challenges of\nthis paradigm. Evaluating the representative GPT-4o model on direct reasoning\nand evidence augmented reasoning, results indicate that MLLM struggle to\ncapture the deeper relationships-specifically, cases in which the image and\ntext are not directly connected but are associated through underlying semantic\nlinks. Moreover, noise in the evidence further impairs detection accuracy. To\naddress these challenges, we propose CMIE, a novel OOC misinformation detection\nframework that incorporates a Coexistence Relationship Generation (CRG)\nstrategy and an Association Scoring (AS) mechanism. CMIE identifies the\nunderlying coexistence relationships between images and text, and selectively\nutilizes relevant evidence to enhance misinformation detection. Experimental\nresults demonstrate that our approach outperforms existing methods.\n","authors":["Fanxiao Li","Jiaying Wu","Canyuan He","Wei Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.23449v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05715v2","updated":"2025-10-27T01:44:45Z","published":"2025-07-08T06:58:24Z","title":"From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal\n  Collaborative Filtering Recommendation","summary":"  Most existing multimodal collaborative filtering recommendation (MCFRec)\nmethods rely heavily on ID features and multimodal content to enhance\nrecommendation performance. However, this paper reveals that ID features are\neffective but have limited benefits in multimodal collaborative filtering\nrecommendation. Therefore, this paper systematically deconstruct the pros and\ncons of ID features: (i) they provide initial embedding but lack semantic\nrichness, (ii) they provide a unique identifier for each user and item but\nhinder generalization to untrained data, and (iii) they assist in aligning and\nfusing multimodal features but may lead to representation shift. Based on these\ninsights, this paper proposes IDFREE, an ID-free multimodal collaborative\nFiltering REcommEndation baseline. IDFREE replaces ID features with multimodal\nfeatures and positional encodings to generate semantically meaningful ID-free\nembeddings. For ID-free multimodal collaborative filtering, it further proposes\nan adaptive similarity graph module to construct dynamic user-user and\nitem-item graphs based on multimodal features. Then, an augmented user-item\ngraph encoder is proposed to construct more effective user and item encoding.\nFinally, IDFREE achieves inter-multimodal alignment based on the contrastive\nlearning and uses Softmax loss as recommendation loss. Basic experiments on\nthree public datasets demonstrate that IDFREE outperforms existing ID-based\nMCFRec methods, achieving an average performance gain of 72.24% across standard\nmetrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended\nexperiments further validate our findings on the limitations of ID features in\nMCFRec. The code is released at https://github.com/G-H-Li/IDFREE.\n","authors":["Guohao Li","Li Jing","Jia Wu","Xuefei Li","Kai Zhu","Yue He"],"pdf_url":"https://arxiv.org/pdf/2507.05715v2.pdf","comment":"We identified that our current approach achieves its reported\n  performance only under specific data conditions, and its robustness is weaker\n  than we initially expected"},{"id":"http://arxiv.org/abs/2502.10999v2","updated":"2025-10-27T00:52:27Z","published":"2025-02-16T05:30:18Z","title":"ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering\n  without Font Annotations","summary":"  This work demonstrates that diffusion models can achieve font-controllable\nmultilingual text rendering using just raw images without font label\nannotations.Visual text rendering remains a significant challenge. While recent\nmethods condition diffusion on glyphs, it is impossible to retrieve exact font\nannotations from large-scale, real-world datasets, which prevents\nuser-specified font control. To address this, we propose a data-driven solution\nthat integrates the conditional diffusion model with a text segmentation model,\nutilizing segmentation masks to capture and represent fonts in pixel space in a\nself-supervised manner, thereby eliminating the need for any ground-truth\nlabels and enabling users to customize text rendering with any multilingual\nfont of their choice. The experiment provides a proof of concept of our\nalgorithm in zero-shot text and font editing across diverse fonts and\nlanguages, providing valuable insights for the community and industry toward\nachieving generalized visual text rendering. Code is available at\ngithub.com/bowen-upenn/ControlText.\n","authors":["Bowen Jiang","Yuan Yuan","Xinyi Bai","Zhuoqun Hao","Alyson Yin","Yaojie Hu","Wenyu Liao","Lyle Ungar","Camillo J. Taylor"],"pdf_url":"https://arxiv.org/pdf/2502.10999v2.pdf","comment":"The 2025 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP) Findings"}],"Robotics":[{"id":"http://arxiv.org/abs/2510.23928v1","updated":"2025-10-27T23:25:57Z","published":"2025-10-27T23:25:57Z","title":"Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in\n  Dynamic Environments","summary":"  In this paper, we propose an adaptive keyframe selection method for improved\n3D scene reconstruction in dynamic environments. The proposed method integrates\ntwo complementary modules: an error-based selection module utilizing\nphotometric and structural similarity (SSIM) errors, and a momentum-based\nupdate module that dynamically adjusts keyframe selection thresholds according\nto scene motion dynamics. By dynamically curating the most informative frames,\nour approach addresses a key data bottleneck in real-time perception. This\nallows for the creation of high-quality 3D world representations from a\ncompressed data stream, a critical step towards scalable robot learning and\ndeployment in complex, dynamic environments. Experimental results demonstrate\nsignificant improvements over traditional static keyframe selection strategies,\nsuch as fixed temporal intervals or uniform frame skipping. These findings\nhighlight a meaningful advancement toward adaptive perception systems that can\ndynamically respond to complex and evolving visual scenes. We evaluate our\nproposed adaptive keyframe selection module on two recent state-of-the-art 3D\nreconstruction networks, Spann3r and CUT3R, and observe consistent improvements\nin reconstruction quality across both frameworks. Furthermore, an extensive\nablation study confirms the effectiveness of each individual component in our\nmethod, underlining their contribution to the overall performance gains.\n","authors":["Raman Jha","Yang Zhou","Giuseppe Loianno"],"pdf_url":"https://arxiv.org/pdf/2510.23928v1.pdf","comment":"Under Review for ROBOVIS 2026"},{"id":"http://arxiv.org/abs/2510.23902v1","updated":"2025-10-27T22:19:35Z","published":"2025-10-27T22:19:35Z","title":"Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost\n  Wheeled Quadruped","summary":"  Wheeled-legged robots combine the efficiency of wheels with the obstacle\nnegotiation of legs, yet many state-of-the-art systems rely on costly actuators\nand sensors, and fall-recovery is seldom integrated, especially for\nwheeled-legged morphologies. This work presents a recovery-aware\nvisual-inertial navigation system on a low-cost wheeled quadruped. The proposed\nsystem leverages vision-based perception from a depth camera and deep\nreinforcement learning policies for robust locomotion and autonomous recovery\nfrom falls across diverse terrains. Simulation experiments show agile mobility\nwith low-torque actuators over irregular terrain and reliably recover from\nexternal perturbations and self-induced failures. We further show goal directed\nnavigation in structured indoor spaces with low-cost perception. Overall, this\napproach lowers the barrier to deploying autonomous navigation and robust\nlocomotion policies in budget-constrained robotic platforms.\n","authors":["Jans Solano","Diego Quiroz"],"pdf_url":"https://arxiv.org/pdf/2510.23902v1.pdf","comment":"Accepted at the IROS 2025 Workshop on Wheeled-Legged Robots"},{"id":"http://arxiv.org/abs/2510.23899v1","updated":"2025-10-27T22:12:21Z","published":"2025-10-27T22:12:21Z","title":"Coordinated Autonomous Drones for Human-Centered Fire Evacuation in\n  Partially Observable Urban Environments","summary":"  Autonomous drone technology holds significant promise for enhancing search\nand rescue operations during evacuations by guiding humans toward safety and\nsupporting broader emergency response efforts. However, their application in\ndynamic, real-time evacuation support remains limited. Existing models often\noverlook the psychological and emotional complexity of human behavior under\nextreme stress. In real-world fire scenarios, evacuees frequently deviate from\ndesignated safe routes due to panic and uncertainty. To address these\nchallenges, this paper presents a multi-agent coordination framework in which\nautonomous Unmanned Aerial Vehicles (UAVs) assist human evacuees in real-time\nby locating, intercepting, and guiding them to safety under uncertain\nconditions. We model the problem as a Partially Observable Markov Decision\nProcess (POMDP), where two heterogeneous UAV agents, a high-level rescuer (HLR)\nand a low-level rescuer (LLR), coordinate through shared observations and\ncomplementary capabilities. Human behavior is captured using an agent-based\nmodel grounded in empirical psychology, where panic dynamically affects\ndecision-making and movement in response to environmental stimuli. The\nenvironment features stochastic fire spread, unknown evacuee locations, and\nlimited visibility, requiring UAVs to plan over long horizons to search for\nhumans and adapt in real-time. Our framework employs the Proximal Policy\nOptimization (PPO) algorithm with recurrent policies to enable robust\ndecision-making in partially observable settings. Simulation results\ndemonstrate that the UAV team can rapidly locate and intercept evacuees,\nsignificantly reducing the time required for them to reach safety compared to\nscenarios without UAV assistance.\n","authors":["Maria G. Mendoza","Addison Kalanther","Daniel Bostwick","Emma Stephan","Chinmay Maheshwari","Shankar Sastry"],"pdf_url":"https://arxiv.org/pdf/2510.23899v1.pdf","comment":"Accepted to IEEE Global Humanitarian Technology Conference (GHTC\n  2025). 8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2510.23895v1","updated":"2025-10-27T22:05:54Z","published":"2025-10-27T22:05:54Z","title":"Modeling and Scheduling of Fusion Patterns in Autonomous Driving Systems\n  (Extended Version)","summary":"  In Autonomous Driving Systems (ADS), Directed Acyclic Graphs (DAGs) are\nwidely used to model complex data dependencies and inter-task communication.\nHowever, existing DAG scheduling approaches oversimplify data fusion tasks by\nassuming fixed triggering mechanisms, failing to capture the diverse fusion\npatterns found in real-world ADS software stacks. In this paper, we propose a\nsystematic framework for analyzing various fusion patterns and their\nperformance implications in ADS. Our framework models three distinct fusion\ntask types: timer-triggered, wait-for-all, and immediate fusion, which\ncomprehensively represent real-world fusion behaviors. Our Integer Linear\nProgramming (ILP)-based approach enables an optimization of multiple real-time\nperformance metrics, including reaction time, time disparity, age of\ninformation, and response time, while generating deterministic offline\nschedules directly applicable to real platforms. Evaluation using real-world\nADS case studies, Raspberry Pi implementation, and randomly generated DAGs\ndemonstrates that our framework handles diverse fusion patterns beyond the\nscope of existing work, and achieves substantial performance improvements in\ncomparable scenarios.\n","authors":["Hoora Sobhani","Hyoseung Kim"],"pdf_url":"https://arxiv.org/pdf/2510.23895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16372v3","updated":"2025-10-27T21:59:34Z","published":"2025-02-22T22:26:30Z","title":"COMPASS: Cross-embodiment Mobility Policy via Residual RL and Skill\n  Synthesis","summary":"  As robots are increasingly deployed in diverse application domains, enabling\nrobust mobility across different embodiments has become a critical challenge.\nClassical mobility stacks, though effective on specific platforms, require\nextensive per-robot tuning and do not scale easily to new embodiments.\nLearning-based approaches, such as imitation learning (IL), offer alternatives,\nbut face significant limitations on the need for high-quality demonstrations\nfor each embodiment.\n  To address these challenges, we introduce COMPASS, a unified framework that\nenables scalable cross-embodiment mobility using expert demonstrations from\nonly a single embodiment. We first pre-train a mobility policy on a single\nrobot using IL, combining a world model with a policy model. We then apply\nresidual reinforcement learning (RL) to efficiently adapt this policy to\ndiverse embodiments through corrective refinements. Finally, we distill\nspecialist policies into a single generalist policy conditioned on an\nembodiment embedding vector. This design significantly reduces the burden of\ncollecting data while enabling robust generalization across a wide range of\nrobot designs. Our experiments demonstrate that COMPASS scales effectively\nacross diverse robot platforms while maintaining adaptability to various\nenvironment configurations, achieving a generalist policy with a success rate\napproximately 5X higher than the pre-trained IL policy on unseen embodiments,\nand further demonstrates zero-shot sim-to-real transfer.\n","authors":["Wei Liu","Huihua Zhao","Chenran Li","Yuchen Deng","Joydeep Biswas","Soha Pouya","Yan Chang"],"pdf_url":"https://arxiv.org/pdf/2502.16372v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23860v1","updated":"2025-10-27T21:07:17Z","published":"2025-10-27T21:07:17Z","title":"Motivating Students' Self-study with Goal Reminder and Emotional Support","summary":"  While the efficacy of social robots in supporting people in learning tasks\nhas been extensively investigated, their potential impact in assisting students\nin self-studying contexts has not been investigated much. This study explores\nhow a social robot can act as a peer study companion for college students\nduring self-study tasks by delivering task-oriented goal reminder and positive\nemotional support. We conducted an exploratory Wizard-of-Oz study to explore\nhow these robotic support behaviors impacted students' perceived focus,\nproductivity, and engagement in comparison to a robot that only provided\nphysical presence (control). Our study results suggest that participants in the\ngoal reminder and the emotional support conditions reported greater ease of\nuse, with the goal reminder condition additionally showing a higher willingness\nto use the robot in future study sessions. Participants' satisfaction with the\nrobot was correlated with their perception of the robot as a social other, and\nthis perception was found to be a predictor for their level of goal achievement\nin the self-study task. These findings highlight the potential of socially\nassistive robots to support self-study through both functional and emotional\nengagement.\n","authors":["Hyung Chan Cho","Go-Eum Cha","Yanfu Liu","Sooyeon Jeong"],"pdf_url":"https://arxiv.org/pdf/2510.23860v1.pdf","comment":"RO-MAN 2025 accepted paper"},{"id":"http://arxiv.org/abs/2510.23605v1","updated":"2025-10-27T17:59:51Z","published":"2025-10-27T17:59:51Z","title":"Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with\n  Progressive Texture Infilling","summary":"  Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/.\n","authors":["Shuhong Zheng","Ashkan Mirzaei","Igor Gilitschenski"],"pdf_url":"https://arxiv.org/pdf/2510.23605v1.pdf","comment":"NeurIPS 2025, 38 pages, 22 figures"},{"id":"http://arxiv.org/abs/2510.24795v1","updated":"2025-10-27T17:57:33Z","published":"2025-10-27T17:57:33Z","title":"A Survey on Efficient Vision-Language-Action Models","summary":"  Vision-Language-Action models (VLAs) represent a significant frontier in\nembodied intelligence, aiming to bridge digital knowledge with physical-world\ninteraction. While these models have demonstrated remarkable generalist\ncapabilities, their deployment is severely hampered by the substantial\ncomputational and data requirements inherent to their underlying large-scale\nfoundation models. Motivated by the urgent need to address these challenges,\nthis survey presents the first comprehensive review of Efficient\nVision-Language-Action models (Efficient VLAs) across the entire\ndata-model-training process. Specifically, we introduce a unified taxonomy to\nsystematically organize the disparate efforts in this domain, categorizing\ncurrent techniques into three core pillars: (1) Efficient Model Design,\nfocusing on efficient architectures and model compression; (2) Efficient\nTraining, which reduces computational burdens during model learning; and (3)\nEfficient Data Collection, which addresses the bottlenecks in acquiring and\nutilizing robotic data. Through a critical review of state-of-the-art methods\nwithin this framework, this survey not only establishes a foundational\nreference for the community but also summarizes representative applications,\ndelineates key challenges, and charts a roadmap for future research. We\nmaintain a continuously updated project page to track our latest developments:\nhttps://evla-survey.github.io/\n","authors":["Zhaoshu Yu","Bo Wang","Pengpeng Zeng","Haonan Zhang","Ji Zhang","Lianli Gao","Jingkuan Song","Nicu Sebe","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2510.24795v1.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.02874v2","updated":"2025-10-27T17:48:06Z","published":"2025-01-06T09:35:36Z","title":"Steering Flexible Linear Objects in Planar Environments by Two Robot\n  Hands Using Euler's Elastica Solutions","summary":"  The manipulation of flexible objects such as cables, wires and fresh food\nitems by robot hands forms a special challenge in robot grasp mechanics. This\npaper considers the steering of flexible linear objects in planar environments\nby two robot hands. The flexible linear object, modeled as an elastic\nnon-stretchable rod, is manipulated by varying the gripping endpoint positions\nwhile keeping equal endpoint tangents. The flexible linear object shape has a\nclosed form solution in terms of the grasp endpoint positions and tangents,\ncalled Euler's elastica. This paper obtains the elastica solutions under the\noptimal control framework, then uses the elastica solutions to obtain\nclosed-form criteria for non self-intersection, stability and obstacle\navoidance of the flexible linear object. The new tools are incorporated into a\nplanning scheme for steering flexible linear objects in planar environments\npopulated by sparsely spaced obstacles. The scheme is fully implemented and\ndemonstrated with detailed examples.\n","authors":["Aharon Levin","Elon Rimon","Amir Shapiro"],"pdf_url":"https://arxiv.org/pdf/2501.02874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23576v1","updated":"2025-10-27T17:46:43Z","published":"2025-10-27T17:46:43Z","title":"UrbanVLA: A Vision-Language-Action Model for Urban Micromobility","summary":"  Urban micromobility applications, such as delivery robots, demand reliable\nnavigation across large-scale urban environments while following long-horizon\nroute instructions. This task is particularly challenging due to the dynamic\nand unstructured nature of real-world city areas, yet most existing navigation\nmethods remain tailored to short-scale and controllable scenarios. Effective\nurban micromobility requires two complementary levels of navigation skills:\nlow-level capabilities such as point-goal reaching and obstacle avoidance, and\nhigh-level capabilities, such as route-visual alignment. To this end, we\npropose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework\ndesigned for scalable urban navigation. Our method explicitly aligns noisy\nroute waypoints with visual observations during execution, and subsequently\nplans trajectories to drive the robot. To enable UrbanVLA to master both levels\nof navigation, we employ a two-stage training pipeline. The process begins with\nSupervised Fine-Tuning (SFT) using simulated environments and trajectories\nparsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on\na mixture of simulation and real-world data, which enhances the model's safety\nand adaptability in real-world settings. Experiments demonstrate that UrbanVLA\nsurpasses strong baselines by more than 55% in the SocialNav task on MetaUrban.\nFurthermore, UrbanVLA achieves reliable real-world navigation, showcasing both\nscalability to large-scale urban environments and robustness against real-world\nuncertainties.\n","authors":["Anqi Li","Zhiyong Wang","Jiazhao Zhang","Minghan Li","Yunpeng Qi","Zhibo Chen","Zhizheng Zhang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2510.23576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23571v1","updated":"2025-10-27T17:41:38Z","published":"2025-10-27T17:41:38Z","title":"RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim\n  Translation","summary":"  The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape.\n","authors":["Yash Jangir","Yidi Zhang","Kashu Yamazaki","Chenyu Zhang","Kuan-Hsun Tu","Tsung-Wei Ke","Lei Ke","Yonatan Bisk","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2510.23571v1.pdf","comment":"Website: https://robotarenainf.github.io"},{"id":"http://arxiv.org/abs/2503.10919v2","updated":"2025-10-27T17:16:54Z","published":"2025-03-13T22:14:48Z","title":"Data-Driven Soft Robot Control via Adiabatic Spectral Submanifolds","summary":"  The mechanical complexity of soft robots creates significant challenges for\ntheir model-based control. Specifically, linear data-driven models have\nstruggled to control soft robots on complex, spatially extended paths that\nexplore regions with significant nonlinear behavior. To account for these\nnonlinearities, we develop here a model-predictive control strategy based on\nthe recent theory of adiabatic spectral submanifolds (aSSMs). This theory is\napplicable because the internal vibrations of heavily overdamped robots decay\nat a speed that is much faster than the desired speed of the robot along its\nintended path. In that case, low-dimensional attracting invariant manifolds\n(aSSMs) emanate from the path and carry the dominant dynamics of the robot.\nAided by this recent theory, we devise an aSSM-based model-predictive control\nscheme purely from data. We demonstrate our data-driven model's effectiveness\nin tracking dynamic trajectories across diverse tasks, validated on a\nhigh-fidelity, high-dimensional finite-element model of a soft trunk robot and\na Cosserat rod-based elastic soft arm. Notably, we find that five- or\nsix-dimensional aSSM-reduced models outperform the tracking performance of\nother data-driven modeling methods by a factor up to $10$ across all\nclosed-loop control tasks.\n","authors":["Roshan S. Kaundinya","John Irvin Alora","Jonas G. Matt","Luis A. Pabon","Marco Pavone","George Haller"],"pdf_url":"https://arxiv.org/pdf/2503.10919v2.pdf","comment":"41 pages, 24 figures"},{"id":"http://arxiv.org/abs/2510.23525v1","updated":"2025-10-27T17:05:59Z","published":"2025-10-27T17:05:59Z","title":"DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised\n  Domain Adaptation in 3D LiDAR Semantic Segmentation","summary":"  Annotating real-world LiDAR point clouds for use in intelligent autonomous\nsystems is costly. To overcome this limitation, self-training-based\nUnsupervised Domain Adaptation (UDA) has been widely used to improve point\ncloud semantic segmentation by leveraging synthetic point cloud data. However,\nwe argue that existing methods do not effectively utilize unlabeled data, as\nthey either rely on predefined or fixed confidence thresholds, resulting in\nsuboptimal performance. In this paper, we propose a Dynamic Pseudo-Label\nFiltering (DPLF) scheme to enhance real data utilization in point cloud UDA\nsemantic segmentation. Additionally, we design a simple and efficient\nPrior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shift\nbetween synthetic and real-world point clouds. Finally, we utilize data mixing\nconsistency loss to push the model to learn context-free representations. We\nimplement and thoroughly evaluate our approach through extensive comparisons\nwith state-of-the-art methods. Experiments on two challenging synthetic-to-real\npoint cloud semantic segmentation tasks demonstrate that our approach achieves\nsuperior performance. Ablation studies confirm the effectiveness of the DPLF\nand PG-DAP modules. We release the code of our method in this paper.\n","authors":["Wanmeng Li","Simone Mosco","Daniel Fusaro","Alberto Pretto"],"pdf_url":"https://arxiv.org/pdf/2510.23525v1.pdf","comment":"This paper has been accepted for publication at the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2510.23521v1","updated":"2025-10-27T17:00:27Z","published":"2025-10-27T17:00:27Z","title":"Explicit Memory through Online 3D Gaussian Splatting Improves\n  Class-Agnostic Video Segmentation","summary":"  Remembering where object segments were predicted in the past is useful for\nimproving the accuracy and consistency of class-agnostic video segmentation\nalgorithms. Existing video segmentation algorithms typically use either no\nobject-level memory (e.g. FastSAM) or they use implicit memories in the form of\nrecurrent neural network features (e.g. SAM2). In this paper, we augment both\ntypes of segmentation models using an explicit 3D memory and show that the\nresulting models have more accurate and consistent predictions. For this, we\ndevelop an online 3D Gaussian Splatting (3DGS) technique to store predicted\nobject-level segments generated throughout the duration of a video. Based on\nthis 3DGS representation, a set of fusion techniques are developed, named\nFastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improve\ntheir respective foundation models' predictions. Ablation experiments are used\nto validate the proposed techniques' design and hyperparameter settings.\nResults from both real-world and simulated benchmarking experiments show that\nmodels which use explicit 3D memories result in more accurate and consistent\npredictions than those which use no memory or only implicit neural network\nmemories. Project Page: https://topipari.com/projects/FastSAM-Splat/\n","authors":["Anthony Opipari","Aravindhan K Krishnan","Shreekant Gayaka","Min Sun","Cheng-Hao Kuo","Arnie Sen","Odest Chadwicke Jenkins"],"pdf_url":"https://arxiv.org/pdf/2510.23521v1.pdf","comment":"Accepted in IEEE Robotics and Automation Letters September 2025"},{"id":"http://arxiv.org/abs/2510.23512v1","updated":"2025-10-27T16:50:12Z","published":"2025-10-27T16:50:12Z","title":"Localising under the drape: proprioception in the era of distributed\n  surgical robotic system","summary":"  Despite their mechanical sophistication, surgical robots remain blind to\ntheir surroundings. This lack of spatial awareness causes collisions, system\nrecoveries, and workflow disruptions, issues that will intensify with the\nintroduction of distributed robots with independent interacting arms. Existing\ntracking systems rely on bulky infrared cameras and reflective markers,\nproviding only limited views of the surgical scene and adding hardware burden\nin crowded operating rooms. We present a marker-free proprioception method that\nenables precise localisation of surgical robots under their sterile draping\ndespite associated obstruction of visual cues. Our method solely relies on\nlightweight stereo-RGB cameras and novel transformer-based deep learning\nmodels. It builds on the largest multi-centre spatial robotic surgery dataset\nto date (1.4M self-annotated images from human cadaveric and preclinical in\nvivo studies). By tracking the entire robot and surgical scene, rather than\nindividual markers, our approach provides a holistic view robust to occlusions,\nsupporting surgical scene understanding and context-aware control. We\ndemonstrate an example of potential clinical benefits during in vivo breathing\ncompensation with access to tissue dynamics, unobservable under state of the\nart tracking, and accurately locate in multi-robot systems for future\nintelligent interaction. In addition, and compared with existing systems, our\nmethod eliminates markers and improves tracking visibility by 25%. To our\nknowledge, this is the first demonstration of marker-free proprioception for\nfully draped surgical robots, reducing setup complexity, enhancing safety, and\npaving the way toward modular and autonomous robotic surgery.\n","authors":["Martin Huber","Nicola A. Cavalcanti","Ayoob Davoodi","Ruixuan Li","Christopher E. Mower","Fabio Carrillo","Christoph J. Laux","Francois Teyssere","Thibault Chandanson","Antoine Harlé","Elie Saghbiny","Mazda Farshad","Guillaume Morel","Emmanuel Vander Poorten","Philipp Fürnstahl","Sébastien Ourselin","Christos Bergeles","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2510.23512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23511v1","updated":"2025-10-27T16:47:24Z","published":"2025-10-27T16:47:24Z","title":"Dexbotic: Open-Source Vision-Language-Action Toolbox","summary":"  In this paper, we present Dexbotic, an open-source Vision-Language-Action\n(VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLA\nresearch service for professionals in the field of embodied intelligence. It\noffers a codebase that supports multiple mainstream VLA policies\nsimultaneously, allowing users to reproduce various VLA methods with just a\nsingle environment setup. The toolbox is experiment-centric, where the users\ncan quickly develop new VLA experiments by simply modifying the Exp script.\nMoreover, we provide much stronger pretrained models to achieve great\nperformance improvements for state-of-the-art VLA policies. Dexbotic will\ncontinuously update to include more of the latest pre-trained foundation models\nand cutting-edge VLA models in the industry.\n","authors":["Bin Xie","Erjin Zhou","Fan Jia","Hao Shi","Haoqiang Fan","Haowei Zhang","Hebei Li","Jianjian Sun","Jie Bin","Junwen Huang","Kai Liu","Kaixin Liu","Kefan Gu","Lin Sun","Meng Zhang","Peilong Han","Ruitao Hao","Ruitao Zhang","Saike Huang","Songhan Xie","Tiancai Wang","Tianle Liu","Wenbin Tang","Wenqi Zhu","Yang Chen","Yingfei Liu","Yizhuang Zhou","Yu Liu","Yucheng Zhao","Yunchao Ma","Yunfei Wei","Yuxiang Chen","Ze Chen","Zeming Li","Zhao Wu","Ziheng Zhang","Ziming Liu","Ziwei Yan","Ziyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.23511v1.pdf","comment":"Authors are listed in alphabetical order. The official website is\n  located at https://dexbotic.com/. Code is available at\n  https://github.com/Dexmal/dexbotic"},{"id":"http://arxiv.org/abs/2510.23509v1","updated":"2025-10-27T16:47:15Z","published":"2025-10-27T16:47:15Z","title":"Deductive Chain-of-Thought Augmented Socially-aware Robot Navigation\n  World Model","summary":"  Social robot navigation increasingly relies on large language models for\nreasoning, path planning, and enabling movement in dynamic human spaces.\nHowever, relying solely on LLMs for planning often leads to unpredictable and\nunsafe behaviors, especially in dynamic human spaces, due to limited physical\ngrounding and weak logical consistency. In this work, we introduce NaviWM, a\nsocially-aware robot Navigation World Model that augments LLM reasoning with a\nstructured world model and a logic-driven chain-of-thought process. NaviWM\nconsists of two main components: (1) a spatial-temporal world model that\ncaptures the positions, velocities, and activities of agents in the\nenvironment, and (2) a deductive reasoning module that guides LLMs through a\nmulti-step, logic-based inference process. This integration enables the robot\nto generate navigation decisions that are both socially compliant and\nphysically safe, under well-defined constraints such as personal space,\ncollision avoidance, and timing. Unlike previous methods based on prompting or\nfine-tuning, NaviWM encodes social norms as first-order logic, enabling\ninterpretable and verifiable reasoning. Experiments show that NaviWM improves\nsuccess rates and reduces social violations, particularly in crowded\nenvironments. These results demonstrate the benefit of combining formal\nreasoning with LLMs for robust social navigation. Additional experimental\ndetails and demo videos for this work can be found at:\nhttps://sites.google.com/view/NaviWM.\n","authors":["Weizheng Wang","Obi Ike","Soyun Choi","Sungeun Hong","Byung-Cheol Min"],"pdf_url":"https://arxiv.org/pdf/2510.23509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18361v5","updated":"2025-10-27T16:39:17Z","published":"2024-09-27T00:35:21Z","title":"iWalker: Imperative Visual Planning for Walking Humanoid Robot","summary":"  Humanoid robots, designed to operate in human-centric environments, serve as\na fundamental platform for a broad range of tasks. Although humanoid robots\nhave been extensively studied for decades, a majority of existing humanoid\nrobots still heavily rely on complex modular frameworks, leading to\ninflexibility and potential compounded errors from independent sensing,\nplanning, and acting components. In response, we propose an end-to-end humanoid\nsense-plan-act walking system, enabling vision-based obstacle avoidance and\nfootstep planning for whole body balancing simultaneously. We designed two\nimperative learning (IL)-based bilevel optimizations for model-predictive step\nplanning and whole body balancing, respectively, to achieve self-supervised\nlearning for humanoid robot walking. This enables the robot to learn from\narbitrary unlabeled data, improving its adaptability and generalization\ncapabilities. We refer to our method as iWalker and demonstrate its\neffectiveness in both simulated and real-world environments, representing a\nsignificant advancement toward autonomous humanoid robots.\n","authors":["Xiao Lin","Yuhao Huang","Taimeng Fu","Xiaobin Xiong","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2409.18361v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23495v1","updated":"2025-10-27T16:29:11Z","published":"2025-10-27T16:29:11Z","title":"COOPERA: Continual Open-Ended Human-Robot Assistance","summary":"  To understand and collaborate with humans, robots must account for individual\nhuman traits, habits, and activities over time. However, most robotic\nassistants lack these abilities, as they primarily focus on predefined tasks in\nstructured environments and lack a human model to learn from. This work\nintroduces COOPERA, a novel framework for COntinual, OPen-Ended human-Robot\nAssistance, where simulated humans, driven by psychological traits and\nlong-term intentions, interact with robots in complex environments. By\nintegrating continuous human feedback, our framework, for the first time,\nenables the study of long-term, open-ended human-robot collaboration (HRC) in\ndifferent collaborative tasks across various time-scales. Within COOPERA, we\nintroduce a benchmark and an approach to personalize the robot's collaborative\nactions by learning human traits and context-dependent intents. Experiments\nvalidate the extent to which our simulated humans reflect realistic human\nbehaviors and demonstrate the value of inferring and personalizing to human\nintents for open-ended and long-term HRC. Project Page:\nhttps://dannymcy.github.io/coopera/\n","authors":["Chenyang Ma","Kai Lu","Ruta Desai","Xavier Puig","Andrew Markham","Niki Trigoni"],"pdf_url":"https://arxiv.org/pdf/2510.23495v1.pdf","comment":"NeurIPS 2025 (Spotlight); Project Page:\n  https://dannymcy.github.io/coopera/"},{"id":"http://arxiv.org/abs/2506.06094v3","updated":"2025-10-27T15:42:48Z","published":"2025-06-06T13:54:19Z","title":"Onboard Mission Replanning for Adaptive Cooperative Multi-Robot Systems","summary":"  Cooperative autonomous robotic systems have significant potential for\nexecuting complex multi-task missions across space, air, ground, and maritime\ndomains. But they commonly operate in remote, dynamic and hazardous\nenvironments, requiring rapid in-mission adaptation without reliance on fragile\nor slow communication links to centralised compute. Fast, on-board replanning\nalgorithms are therefore needed to enhance resilience. Reinforcement Learning\nshows strong promise for efficiently solving mission planning tasks when\nformulated as Travelling Salesperson Problems (TSPs), but existing methods: 1)\nare unsuitable for replanning, where agents do not start at a single location;\n2) do not allow cooperation between agents; 3) are unable to model tasks with\nvariable durations; or 4) lack practical considerations for on-board\ndeployment. Here we define the Cooperative Mission Replanning Problem as a\nnovel variant of multiple TSP with adaptations to overcome these issues, and\ndevelop a new encoder/decoder-based model using Graph Attention Networks and\nAttention Models to solve it effectively and efficiently. Using a simple\nexample of cooperative drones, we show our replanner consistently (90% of the\ntime) maintains performance within 10% of the state-of-the-art LKH3 heuristic\nsolver, whilst running 85-370 times faster on a Raspberry Pi. This work paves\nthe way for increased resilience in autonomous multi-agent systems.\n","authors":["Elim Kwan","Rehman Qureshi","Liam Fletcher","Colin Laganier","Victoria Nockles","Richard Walters"],"pdf_url":"https://arxiv.org/pdf/2506.06094v3.pdf","comment":"9 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2509.24948v2","updated":"2025-10-27T15:03:09Z","published":"2025-09-29T15:45:19Z","title":"World-Env: Leveraging World Model as a Virtual Environment for VLA\n  Post-Training","summary":"  Vision-Language-Action (VLA) models trained via imitation learning suffer\nfrom significant performance degradation in data-scarce scenarios due to their\nreliance on large-scale demonstration datasets. Although reinforcement learning\n(RL)-based post-training has proven effective in addressing data scarcity, its\napplication to VLA models is hindered by the non-resettable nature of\nreal-world environments. This limitation is particularly critical in high-risk\ndomains such as industrial automation, where interactions often induce state\nchanges that are costly or infeasible to revert. Furthermore, existing VLA\napproaches lack a reliable mechanism for detecting task completion, leading to\nredundant actions that reduce overall task success rates. To address these\nchallenges, we propose World-Env, an RL-based post-training framework that\nreplaces physical interaction with a low-cost, world model-based virtual\nsimulator. World-Env consists of two key components: (1) a video-based world\nsimulator that generates temporally consistent future visual observations, and\n(2) a vision-language model (VLM)-guided instant reflector that provides\ncontinuous reward signals and predicts action termination. This simulated\nenvironment enables VLA models to safely explore and generalize beyond their\ninitial imitation learning distribution. Our method achieves notable\nperformance gains with as few as five expert demonstrations per task.\nExperiments on complex robotic manipulation tasks demonstrate that World-Env\neffectively overcomes the data inefficiency, safety constraints, and\ninefficient execution of conventional VLA models that rely on real-world\ninteraction, offering a practical and scalable solution for post-training in\nresource-constrained settings. Our code is available at\nhttps://github.com/junjxiao/world-env.\n","authors":["Junjin Xiao","Yandan Yang","Xinyuan Chang","Ronghan Chen","Feng Xiong","Mu Xu","Wei-Shi Zheng","Qing Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.24948v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.12206v2","updated":"2025-10-27T14:53:32Z","published":"2025-10-14T06:56:33Z","title":"Controllable Collision Scenario Generation via Collision Pattern\n  Prediction","summary":"  Evaluating the safety of autonomous vehicles (AVs) requires diverse,\nsafety-critical scenarios, with collisions being especially important yet rare\nand unsafe to collect in the real world. Therefore, the community has been\nfocusing on generating safety-critical scenarios in simulation. However,\ncontrolling attributes such as collision type and time-to-accident (TTA)\nremains challenging. We introduce a new task called controllable collision\nscenario generation, where the goal is to produce trajectories that realize a\nuser-specified collision type and TTA, to investigate the feasibility of\nautomatically generating desired collision scenarios. To support this task, we\npresent COLLIDE, a large-scale collision scenario dataset constructed by\ntransforming real-world driving logs into diverse collisions, balanced across\nfive representative collision types and different TTA intervals. We propose a\nframework that predicts Collision Pattern, a compact and interpretable\nrepresentation that captures the spatial configuration of the ego and the\nadversarial vehicles at impact, before rolling out full adversarial\ntrajectories. Experiments show that our approach outperforms strong baselines\nin both collision rate and controllability. Furthermore, generated scenarios\nconsistently induce higher planner failure rates, revealing limitations of\nexisting planners. We demonstrate that these scenarios fine-tune planners for\nrobustness improvements, contributing to safer AV deployment in different\ncollision scenarios. Project page is available at\nhttps://submit-user.github.io/anon2025\n","authors":["Pin-Lun Chen","Chi-Hsi Kung","Che-Han Chang","Wei-Chen Chiu","Yi-Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2510.12206v2.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2510.23386v1","updated":"2025-10-27T14:42:31Z","published":"2025-10-27T14:42:31Z","title":"Full-Dynamics Real-Time Nonlinear Model Predictive Control of Heavy-Duty\n  Hydraulic Manipulator for Trajectory Tracking Tasks","summary":"  Heavy-duty hydraulic manipulators (HHMs) operate under strict physical and\nsafety-critical constraints due to their large size, high power, and complex\nnonlinear dynamics. Ensuring that both joint-level and end-effector\ntrajectories remain compliant with actuator capabilities, such as force,\nvelocity, and position limits, is essential for safe and reliable operation,\nyet remains largely underexplored in real-time control frameworks. This paper\npresents a nonlinear model predictive control (NMPC) framework designed to\nguarantee constraint satisfaction throughout the full nonlinear dynamics of\nHHMs, while running at a real-time control frequency of 1 kHz. The proposed\nmethod combines a multiple-shooting strategy with real-time sensor feedback,\nand is supported by a robust low-level controller based on virtual\ndecomposition control (VDC) for precise joint tracking. Experimental validation\non a full-scale hydraulic manipulator shows that the NMPC framework not only\nenforces actuator constraints at the joint level, but also ensures\nconstraint-compliant motion in Cartesian space for the end-effector. These\nresults demonstrate the method's capability to deliver high-accuracy trajectory\ntracking while strictly respecting safety-critical limits, setting a new\nbenchmark for real-time control in large-scale hydraulic systems.\n","authors":["Alvaro Paz","Mahdi Hejrati","Pauli Mustalahti","Jouni Mattila"],"pdf_url":"https://arxiv.org/pdf/2510.23386v1.pdf","comment":"This work has been submitted for possible publication in IEEE"},{"id":"http://arxiv.org/abs/2510.23359v1","updated":"2025-10-27T14:08:32Z","published":"2025-10-27T14:08:32Z","title":"T-ESKF: Transformed Error-State Kalman Filter for Consistent\n  Visual-Inertial Navigation","summary":"  This paper presents a novel approach to address the inconsistency problem\ncaused by observability mismatch in visual-inertial navigation systems (VINS).\nThe key idea involves applying a linear time-varying transformation to the\nerror-state within the Error-State Kalman Filter (ESKF). This transformation\nensures that \\textrr{the unobservable subspace of the transformed error-state\nsystem} becomes independent of the state, thereby preserving the correct\nobservability of the transformed system against variations in linearization\npoints. We introduce the Transformed ESKF (T-ESKF), a consistent VINS estimator\nthat performs state estimation using the transformed error-state system.\nFurthermore, we develop an efficient propagation technique to accelerate the\ncovariance propagation based on the transformation relationship between the\ntransition and accumulated matrices of T-ESKF and ESKF. We validate the\nproposed method through extensive simulations and experiments, demonstrating\nbetter (or competitive at least) performance compared to state-of-the-art\nmethods. The code is available at github.com/HITCSC/T-ESKF.\n","authors":["Chungeng Tian","Ning Hao","Fenghua He"],"pdf_url":"https://arxiv.org/pdf/2510.23359v1.pdf","comment":"This paper was submitted to IEEE RA-L on July 14, 2024, and accepted\n  on December 18, 2024. This version serves as the 'plus edition' of the\n  accepted paper, incorporating supplementary materials for completeness"},{"id":"http://arxiv.org/abs/2510.23357v1","updated":"2025-10-27T14:06:40Z","published":"2025-10-27T14:06:40Z","title":"Large language model-based task planning for service robots: A review","summary":"  With the rapid advancement of large language models (LLMs) and robotics,\nservice robots are increasingly becoming an integral part of daily life,\noffering a wide range of services in complex environments. To deliver these\nservices intelligently and efficiently, robust and accurate task planning\ncapabilities are essential. This paper presents a comprehensive overview of the\nintegration of LLMs into service robotics, with a particular focus on their\nrole in enhancing robotic task planning. First, the development and\nfoundational techniques of LLMs, including pre-training, fine-tuning,\nretrieval-augmented generation (RAG), and prompt engineering, are reviewed. We\nthen explore the application of LLMs as the cognitive core-`brain'-of service\nrobots, discussing how LLMs contribute to improved autonomy and\ndecision-making. Furthermore, recent advancements in LLM-driven task planning\nacross various input modalities are analyzed, including text, visual, audio,\nand multimodal inputs. Finally, we summarize key challenges and limitations in\ncurrent research and propose future directions to advance the task planning\ncapabilities of service robots in complex, unstructured domestic environments.\nThis review aims to serve as a valuable reference for researchers and\npractitioners in the fields of artificial intelligence and robotics.\n","authors":["Shaohan Bian","Ying Zhang","Guohui Tian","Zhiqiang Miao","Edmond Q. Wu","Simon X. Yang","Changchun Hua"],"pdf_url":"https://arxiv.org/pdf/2510.23357v1.pdf","comment":"Submitted to Biomimetic Intelligence and Robotics for possible\n  publication"},{"id":"http://arxiv.org/abs/2503.00654v2","updated":"2025-10-27T13:47:32Z","published":"2025-03-01T23:02:26Z","title":"ExAMPC: the Data-Driven Explainable and Approximate NMPC with Physical\n  Insights","summary":"  Amidst the surge in the use of Artificial Intelligence (AI) for control\npurposes, classical and model-based control methods maintain their popularity\ndue to their transparency and deterministic nature. However, advanced\ncontrollers like Nonlinear Model Predictive Control (NMPC), despite proven\ncapabilities, face adoption challenges due to their computational complexity\nand unpredictable closed-loop performance in complex validation systems. This\npaper introduces ExAMPC, a methodology bridging classical control and\nexplainable AI by augmenting the NMPC with data-driven insights to improve the\ntrustworthiness and reveal the optimization solution and closed-loop\nperformance's sensitivities to physical variables and system parameters. By\nemploying a low-order spline embedding, we reduce the open-loop trajectory\ndimensionality by over 95%, and integrate it with SHAP and Symbolic Regression\nfrom eXplainable AI (XAI) for an approximate NMPC, enabling intuitive physical\ninsights into the NMPC's optimization routine. The prediction accuracy of the\napproximate NMPC is enhanced through physics-inspired continuous-time\nconstraints penalties, reducing the predicted continuous trajectory violations\nby 93%. ExAMPC also enables accurate forecasting of the NMPC's computational\nrequirements with explainable insights on worst-case scenarios. Experimental\nvalidation on automated valet parking and autonomous racing with lap-time\noptimization, demonstrates the methodology's practical effectiveness for\npotential real-world applications.\n","authors":["Jean Pierre Allamaa","Panagiotis Patrinos","Tong Duy Son"],"pdf_url":"https://arxiv.org/pdf/2503.00654v2.pdf","comment":"This paper has been accepted for publication in the 2025 IEEE/RSJ\n  IROS Conference"},{"id":"http://arxiv.org/abs/2510.23329v1","updated":"2025-10-27T13:45:50Z","published":"2025-10-27T13:45:50Z","title":"Transferable Deep Reinforcement Learning for Cross-Domain Navigation:\n  from Farmland to the Moon","summary":"  Autonomous navigation in unstructured environments is essential for field and\nplanetary robotics, where robots must efficiently reach goals while avoiding\nobstacles under uncertain conditions. Conventional algorithmic approaches often\nrequire extensive environment-specific tuning, limiting scalability to new\ndomains. Deep Reinforcement Learning (DRL) provides a data-driven alternative,\nallowing robots to acquire navigation strategies through direct interactions\nwith their environment. This work investigates the feasibility of DRL policy\ngeneralization across visually and topographically distinct simulated domains,\nwhere policies are trained in terrestrial settings and validated in a zero-shot\nmanner in extraterrestrial environments. A 3D simulation of an agricultural\nrover is developed and trained using Proximal Policy Optimization (PPO) to\nachieve goal-directed navigation and obstacle avoidance in farmland settings.\nThe learned policy is then evaluated in a lunar-like simulated environment to\nassess transfer performance. The results indicate that policies trained under\nterrestrial conditions retain a high level of effectiveness, achieving close to\n50\\% success in lunar simulations without the need for additional training and\nfine-tuning. This underscores the potential of cross-domain DRL-based policy\ntransfer as a promising approach to developing adaptable and efficient\nautonomous navigation for future planetary exploration missions, with the added\nbenefit of minimizing retraining costs.\n","authors":["Shreya Santra","Thomas Robbins","Kazuya Yoshida"],"pdf_url":"https://arxiv.org/pdf/2510.23329v1.pdf","comment":"6 pages, 7 figures. Accepted at IEEE iSpaRo 2025"},{"id":"http://arxiv.org/abs/2510.23296v1","updated":"2025-10-27T13:03:47Z","published":"2025-10-27T13:03:47Z","title":"Payload trajectory tracking control for aerial transportation systems\n  with cable length online optimization","summary":"  Cable-suspended aerial transportation systems are employed extensively across\nvarious industries. The capability to flexibly adjust the relative position\nbetween the multirotor and the payload has spurred growing interest in the\nsystem equipped with variable-length cable, promising broader application\npotential. Compared to systems with fixed-length cables, introducing the\nvariable-length cable adds a new degree of freedom. However, it also results in\nincreased nonlinearity and more complex dynamic coupling among the multirotor,\nthe cable and the payload, posing significant challenges in control design.\nThis paper introduces a backstepping control strategy tailored for aerial\ntransportation systems with variable-length cable, designed to precisely track\nthe payload trajectory while dynamically adjusting cable length. Then, a cable\nlength generator has been developed that achieves online optimization of the\ncable length while satisfying state constraints, thus balancing the\nmultirotor's motion and cable length changes without the need for manual\ntrajectory planning. The asymptotic stability of the closed-loop system is\nguaranteed through Lyapunov techniques and the growth restriction condition.\nFinally, simulation results confirm the efficacy of the proposed method in\nmanaging trajectory tracking and cable length adjustments effectively.\n","authors":["Hai Yu","Zhichao Yang","Wei He","Jianda Han","Yongchun Fang","Xiao Liang"],"pdf_url":"https://arxiv.org/pdf/2510.23296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23286v1","updated":"2025-10-27T12:54:58Z","published":"2025-10-27T12:54:58Z","title":"Precise Time Delay Measurement and Compensation for Tightly Coupled\n  Underwater SINS/piUSBL Navigation","summary":"  In multi-sensor systems, time synchronization between sensors is a\nsignificant challenge, and this issue is particularly pronounced in underwater\nintegrated navigation systems incorporating acoustic positioning. Such systems\nare highly susceptible to time delay, which can significantly degrade accuracy\nwhen measurement and fusion moments are misaligned. To address this challenge,\nthis paper introduces a tightly coupled navigation framework that integrates a\npassive inverted ultra-short baseline (piUSBL) acoustic positioning system, a\nstrapdown inertial navigation system (SINS), and a depth gauge under precise\ntime synchronization. The framework fuses azimuth and slant range from the\npiUSBL with depth data, thereby avoiding poor vertical-angle observability in\nplanar arrays. A novel delay measurement strategy is introduced, combining\nsynchronized timing with acoustic signal processing, which redefines\ndelay-traditionally an unobservable error-into a quantifiable parameter,\nenabling explicit estimation of both acoustic propagation and system processing\ndelays. Simulations and field experiments confirm the feasibility of the\nproposed method, with delay-compensated navigation reducing RMSE by 40.45% and\nmaximum error by 32.55%. These findings show that precise delay measurement and\ncompensation not only enhance underwater navigation accuracy but also establish\na generalizable framework for acoustic positioning integration, offering\nvaluable insights into time alignment and data fusion in latency-sensitive\nmulti-sensor systems.\n","authors":["Jin Huang","Yingqiang Wang","Haoda Li","Zichen Liu","Zhikun Wang","Ying Chen"],"pdf_url":"https://arxiv.org/pdf/2510.23286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23258v1","updated":"2025-10-27T12:21:33Z","published":"2025-10-27T12:21:33Z","title":"Deep Active Inference with Diffusion Policy and Multiple Timescale World\n  Model for Real-World Exploration and Navigation","summary":"  Autonomous robotic navigation in real-world environments requires exploration\nto acquire environmental information as well as goal-directed navigation in\norder to reach specified targets. Active inference (AIF) based on the\nfree-energy principle provides a unified framework for these behaviors by\nminimizing the expected free energy (EFE), thereby combining epistemic and\nextrinsic values. To realize this practically, we propose a deep AIF framework\nthat integrates a diffusion policy as the policy model and a multiple timescale\nrecurrent state-space model (MTRSSM) as the world model. The diffusion policy\ngenerates diverse candidate actions while the MTRSSM predicts their\nlong-horizon consequences through latent imagination, enabling action selection\nthat minimizes EFE. Real-world navigation experiments demonstrated that our\nframework achieved higher success rates and fewer collisions compared with the\nbaselines, particularly in exploration-demanding scenarios. These results\nhighlight how AIF based on EFE minimization can unify exploration and\ngoal-directed navigation in real-world robotic settings.\n","authors":["Riko Yokozawa","Kentaro Fujii","Yuta Nomura","Shingo Murata"],"pdf_url":"https://arxiv.org/pdf/2510.23258v1.pdf","comment":"Preprint version"},{"id":"http://arxiv.org/abs/2405.14144v3","updated":"2025-10-27T11:52:11Z","published":"2024-05-23T03:38:00Z","title":"A Single Motor Nano Aerial Vehicle with Novel Peer-to-Peer Communication\n  and Sensing Mechanism","summary":"  Communication and position sensing are among the most important capabilities\nfor swarm robots to interact with their peers and perform tasks\ncollaboratively. However, the hardware required to facilitate communication and\nposition sensing is often too complicated, expensive, and bulky to be carried\non swarm robots. Here we present Maneuverable Piccolissimo 3 (MP3), a\nminimalist, single motor drone capable of executing inter-robot communication\nvia infrared light and triangulation-based sensing of relative bearing,\ndistance, and elevation using message arrival time. Thanks to its novel design,\nMP3 can communicate with peers and localize itself using simple components,\nkeeping its size and mass small and making it inherently safe for human\ninteraction. We present the hardware and software design of MP3 and demonstrate\nits capability to localize itself, fly stably, and maneuver in the environment\nusing peer-to-peer communication and sensing.\n","authors":["Jingxian Wang","Andrew G. Curtis","Mark Yim","Michael Rubenstein"],"pdf_url":"https://arxiv.org/pdf/2405.14144v3.pdf","comment":"Proceedings of Robotics: Science and Systems (RSS), 2024"},{"id":"http://arxiv.org/abs/2310.01791v5","updated":"2025-10-27T11:41:37Z","published":"2023-10-03T04:40:38Z","title":"Online POMDP Planning with Anytime Deterministic Optimality Guarantees","summary":"  Decision-making under uncertainty is a critical aspect of many practical\nautonomous systems due to incomplete information. Partially Observable Markov\nDecision Processes (POMDPs) offer a mathematically principled framework for\nformulating decision-making problems under such conditions. However, finding an\noptimal solution for a POMDP is generally intractable. In recent years, there\nhas been a significant progress of scaling approximate solvers from small to\nmoderately sized problems, using online tree search solvers. Often, such\napproximate solvers are limited to probabilistic or asymptotic guarantees\ntowards the optimal solution. In this paper, we derive a deterministic\nrelationship for discrete POMDPs between an approximated and the optimal\nsolution. We show that at any time, we can derive bounds that relate between\nthe existing solution and the optimal one. We show that our derivations provide\nan avenue for a new set of algorithms and can be attached to existing\nalgorithms that have a certain structure to provide them with deterministic\nguarantees with marginal computational overhead. In return, not only do we\ncertify the solution quality, but we demonstrate that making a decision based\non the deterministic guarantee may result in superior performance compared to\nthe original algorithm without the deterministic certification.\n","authors":["Moran Barenboim","Vadim Indelman"],"pdf_url":"https://arxiv.org/pdf/2310.01791v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23234v1","updated":"2025-10-27T11:37:50Z","published":"2025-10-27T11:37:50Z","title":"Optimal Dimensioning of Elastic-Link Manipulators regarding Lifetime\n  Estimation","summary":"  Resourceful operation and design of robots is key for sustainable industrial\nautomation. This will be enabled by lightweight design along with time and\nenergy optimal control of robotic manipulators. Design and control of such\nsystems is intertwined as the control must take into account inherent\nmechanical compliance while the design must accommodate the dynamic\nrequirements demanded by the control. As basis for such design optimization, a\nmethod for estimating the lifetime of elastic link robotic manipulators is\npresented. This is applied to the geometry optimization of flexible serial\nmanipulators performing pick-and-place operations, where the optimization\nobjective is a combination of overall weight and vibration amplitudes. The\nlifetime estimation draws from a fatigue analysis combining the rainflow\ncounting algorithm and the method of critical cutting plane. Tresca hypothesis\nis used to formulate an equivalent stress, and linear damage accumulation is\nassumed. The final robot geometry is selected from a Pareto front as a tradeoff\nof lifetime and vibration characteristic. The method is illustrated for a three\ndegrees of freedom articulated robotic manipulator.\n","authors":["Klaus Zauner","Hubert Gattringer","Andreas Mueller"],"pdf_url":"https://arxiv.org/pdf/2510.23234v1.pdf","comment":"Mechanics Based Design of Structures and Machines, December 2024"},{"id":"http://arxiv.org/abs/2510.23227v1","updated":"2025-10-27T11:24:44Z","published":"2025-10-27T11:24:44Z","title":"Workspace Registration and Collision Detection for Industrial Robotics\n  Applications","summary":"  Motion planning for robotic manipulators relies on precise knowledge of the\nenvironment in order to be able to define restricted areas and to take\ncollision objects into account. To capture the workspace, point clouds of the\nenvironment are acquired using various sensors. The collision objects are\nidentified by region growing segmentation and VCCS algorithm. Subsequently the\npoint clusters are approximated. The aim of the present paper is to compare\ndifferent sensors, to illustrate the process from detection to the finished\ncollision environment and to detect collisions between the robot and this\nenvironment.\n","authors":["Klaus Zauner","Josef El Dib","Hubert Gattringer","Andreas Mueller"],"pdf_url":"https://arxiv.org/pdf/2510.23227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13618v3","updated":"2025-10-27T10:58:53Z","published":"2025-04-18T10:48:46Z","title":"On the Importance of Tactile Sensing for Imitation Learning: A Case\n  Study on Robotic Match Lighting","summary":"  The field of robotic manipulation has advanced significantly in recent years.\nAt the sensing level, several novel tactile sensors have been developed,\ncapable of providing accurate contact information. On a methodological level,\nlearning from demonstrations has proven an efficient paradigm to obtain\nperformant robotic manipulation policies. The combination of both holds the\npromise to extract crucial contact-related information from the demonstration\ndata and actively exploit it during policy rollouts. However, this integration\nhas so far been underexplored, most notably in dynamic, contact-rich\nmanipulation tasks where precision and reactivity are essential. This work\ntherefore proposes a multimodal, visuotactile imitation learning framework that\nintegrates a modular transformer architecture with a flow-based generative\nmodel, enabling efficient learning of fast and dexterous manipulation policies.\nWe evaluate our framework on the dynamic, contact-rich task of robotic match\nlighting - a task in which tactile feedback influences human manipulation\nperformance. The experimental results highlight the effectiveness of our\napproach and show that adding tactile information improves policy performance,\nthereby underlining their combined potential for learning dynamic manipulation\nfrom few demonstrations. Project website:\nhttps://sites.google.com/view/tactile-il .\n","authors":["Niklas Funk","Changqi Chen","Tim Schneider","Georgia Chalvatzaki","Roberto Calandra","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2504.13618v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16726v2","updated":"2025-10-27T10:48:41Z","published":"2025-05-22T14:34:32Z","title":"D-LIO: 6DoF Direct LiDAR-Inertial Odometry based on Simultaneous\n  Truncated Distance Field Mapping","summary":"  This paper presents a new approach for 6DoF Direct LiDAR-Inertial Odometry\n(D-LIO) based on the simultaneous mapping of truncated distance fields on CPU.\nSuch continuous representation (in the vicinity of the points) enables working\nwith raw 3D LiDAR data online, avoiding the need of LiDAR feature selection and\ntracking, simplifying the odometry pipeline and easily generalizing to many\nscenarios. The method is based on the proposed Fast Truncated Distance Field\n(Fast-TDF) method as a convenient tool to represent the environment. Such\nrepresentation enables i) solving the LiDAR point-cloud registration as a\nnonlinear optimization process without the need of selecting/tracking LiDAR\nfeatures in the input data, ii) simultaneously producing an accurate truncated\ndistance field map of the environment, and iii) updating such map at constant\ntime independently of its size. The approach is tested using open datasets,\naerial and ground. It is also benchmarked against other state-of-the-art\nodometry approaches, demonstrating the same or better level of accuracy with\nthe added value of an online-generated TDF representation of the environment,\nthat can be used for other robotics tasks as planning or collision avoidance.\nThe source code is publicly available at\nhttps://anonymous.4open.science/r/D-LIO\n","authors":["Lucia Coto-Elena","J. E. Maese","L. Merino","F. Caballero"],"pdf_url":"https://arxiv.org/pdf/2505.16726v2.pdf","comment":"9 pages, 3 figures and 43 references"},{"id":"http://arxiv.org/abs/2510.23204v1","updated":"2025-10-27T10:47:55Z","published":"2025-10-27T10:47:55Z","title":"If They Disagree, Will You Conform? Exploring the Role of Robots' Value\n  Awareness in a Decision-Making Task","summary":"  This study investigates whether the opinions of robotic agents are more\nlikely to influence human decision-making when the robots are perceived as\nvalue-aware (i.e., when they display an understanding of human principles). We\ndesigned an experiment in which participants interacted with two Furhat robots\n- one programmed to be Value-Aware and the other Non-Value-Aware - during a\nlabeling task for images representing human values. Results indicate that\nparticipants distinguished the Value-Aware robot from the Non-Value-Aware one.\nAlthough their explicit choices did not indicate a clear preference for one\nrobot over the other, participants directed their gaze more toward the\nValue-Aware robot. Additionally, the Value-Aware robot was perceived as more\nloyal, suggesting that value awareness in a social robot may enhance its\nperceived commitment to the group. Finally, when both robots disagreed with the\nparticipant, conformity occurred in about one out of four trials, and\nparticipants took longer to confirm their responses, suggesting that two robots\nexpressing dissent may introduce hesitation in decision-making. On one hand,\nthis highlights the potential risk that robots, if misused, could manipulate\nusers for unethical purposes. On the other hand, it reinforces the idea that\nsocial robots might encourage reflection in ambiguous situations and help users\navoid scams.\n","authors":["Giulia Pusceddu","Giulio Antonio Abbo","Francesco Rea","Tony Belpaeme","Alessandra Sciutti"],"pdf_url":"https://arxiv.org/pdf/2510.23204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23176v1","updated":"2025-10-27T10:10:19Z","published":"2025-10-27T10:10:19Z","title":"TARC: Time-Adaptive Robotic Control","summary":"  Fixed-frequency control in robotics imposes a trade-off between the\nefficiency of low-frequency control and the robustness of high-frequency\ncontrol, a limitation not seen in adaptable biological systems. We address this\nwith a reinforcement learning approach in which policies jointly select control\nactions and their application durations, enabling robots to autonomously\nmodulate their control frequency in response to situational demands. We\nvalidate our method with zero-shot sim-to-real experiments on two distinct\nhardware platforms: a high-speed RC car and a quadrupedal robot. Our method\nmatches or outperforms fixed-frequency baselines in terms of rewards while\nsignificantly reducing the control frequency and exhibiting adaptive frequency\ncontrol under real-world conditions.\n","authors":["Arnav Sukhija","Lenart Treven","Jin Cheng","Florian Dörfler","Stelian Coros","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2510.23176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.17335v3","updated":"2025-10-27T09:29:20Z","published":"2025-10-20T09:27:24Z","title":"DDBot: Differentiable Physics-based Digging Robot for Unknown Granular\n  Materials","summary":"  Automating the manipulation of granular materials poses significant\nchallenges due to complex contact dynamics, unpredictable material properties,\nand intricate system states. Existing approaches often fail to achieve\nefficiency and accuracy in such tasks. To fill the research gap, this paper\nstudies the small-scale and high-precision granular material digging task with\nunknown physical properties. A new framework, named differentiable digging\nrobot (DDBot), is proposed to manipulate granular materials, including sand and\nsoil.\n  Specifically, we equip DDBot with a differentiable physics-based simulator,\ntailored for granular material manipulation, powered by GPU-accelerated\nparallel computing and automatic differentiation. DDBot can perform efficient\ndifferentiable system identification and high-precision digging skill\noptimisation for unknown granular materials, which is enabled by a\ndifferentiable skill-to-action mapping, a task-oriented demonstration method,\ngradient clipping and line search-based gradient descent.\n  Experimental results show that DDBot can efficiently (converge within 5 to 20\nminutes) identify unknown granular material dynamics and optimise digging\nskills, with high-precision results in zero-shot real-world deployments,\nhighlighting its practicality. Benchmark results against state-of-the-art\nbaselines also confirm the robustness and efficiency of DDBot in such digging\ntasks.\n","authors":["Xintong Yang","Minglun Wei","Yu-Kun Lai","Ze Ji"],"pdf_url":"https://arxiv.org/pdf/2510.17335v3.pdf","comment":"Accepted as a regular paper by the IEEE Transactions on Robotics"},{"id":"http://arxiv.org/abs/2510.23129v1","updated":"2025-10-27T09:05:27Z","published":"2025-10-27T09:05:27Z","title":"Combining High Level Scheduling and Low Level Control to Manage Fleets\n  of Mobile Robots","summary":"  The deployment of mobile robots for material handling in industrial\nenvironments requires scalable coordination of large fleets in dynamic\nsettings. This paper presents a two-layer framework that combines high-level\nscheduling with low-level control. Tasks are assigned and scheduled using the\ncompositional algorithm ComSat, which generates time-parameterized routes for\neach robot. These schedules are then used by a distributed Model Predictive\nControl (MPC) system in real time to compute local reference trajectories,\naccounting for static and dynamic obstacles. The approach ensures safe,\ncollision-free operation, and supports rapid rescheduling in response to\ndisruptions such as robot failures or environmental changes. We evaluate the\nmethod in simulated 2D environments with varying road capacities and traffic\nconditions, demonstrating high task completion rates and robust behavior even\nunder congestion. The modular structure of the framework allows for\ncomputational tractability and flexibility, making it suitable for deployment\nin complex, real-world industrial scenarios.\n","authors":["Sabino Francesco Roselli","Ze Zhang","Knut Åkesson"],"pdf_url":"https://arxiv.org/pdf/2510.23129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23121v1","updated":"2025-10-27T08:50:27Z","published":"2025-10-27T08:50:27Z","title":"Reliable Robotic Task Execution in the Face of Anomalies","summary":"  Learned robot policies have consistently been shown to be versatile, but they\ntypically have no built-in mechanism for handling the complexity of open\nenvironments, making them prone to execution failures; this implies that\ndeploying policies without the ability to recognise and react to failures may\nlead to unreliable and unsafe robot behaviour. In this paper, we present a\nframework that couples a learned policy with a method to detect visual\nanomalies during policy deployment and to perform recovery behaviours when\nnecessary, thereby aiming to prevent failures. Specifically, we train an\nanomaly detection model using data collected during nominal executions of a\ntrained policy. This model is then integrated into the online policy execution\nprocess, so that deviations from the nominal execution can trigger a\nthree-level sequential recovery process that consists of (i) pausing the\nexecution temporarily, (ii) performing a local perturbation of the robot's\nstate, and (iii) resetting the robot to a safe state by sampling from a learned\nexecution success model. We verify our proposed method in two different\nscenarios: (i) a door handle reaching task with a Kinova Gen3 arm using a\npolicy trained in simulation and transferred to the real robot, and (ii) an\nobject placing task with a UFactory xArm 6 using a general-purpose policy\nmodel. Our results show that integrating policy execution with anomaly\ndetection and recovery increases the execution success rate in environments\nwith various anomalies, such as trajectory deviations and adversarial human\ninterventions.\n","authors":["Bharath Santhanam","Alex Mitrevski","Santosh Thoduka","Sebastian Houben","Teena Hassan"],"pdf_url":"https://arxiv.org/pdf/2510.23121v1.pdf","comment":"Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)"},{"id":"http://arxiv.org/abs/2510.23119v1","updated":"2025-10-27T08:48:41Z","published":"2025-10-27T08:48:41Z","title":"OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and\n  Force Feedback","summary":"  Enabling robots to dexterously grasp and manipulate objects based on human\ncommands is a promising direction in robotics. However, existing approaches are\nchallenging to generalize across diverse objects or tasks due to the limited\nscale of semantic dexterous grasp datasets. Foundation models offer a new way\nto enhance generalization, yet directly leveraging them to generate feasible\nrobotic actions remains challenging due to the gap between abstract model\nknowledge and physical robot execution. To address these challenges, we propose\nOmniDexGrasp, a generalizable framework that achieves omni-capabilities in user\nprompting, dexterous embodiment, and grasping tasks by combining foundation\nmodels with the transfer and control strategies. OmniDexGrasp integrates three\nkey modules: (i) foundation models are used to enhance generalization by\ngenerating human grasp images supporting omni-capability of user prompt and\ntask; (ii) a human-image-to-robot-action transfer strategy converts human\ndemonstrations into executable robot actions, enabling omni dexterous\nembodiment; (iii) force-aware adaptive grasp strategy ensures robust and stable\ngrasp execution. Experiments in simulation and on real robots validate the\neffectiveness of OmniDexGrasp on diverse user prompts, grasp task and dexterous\nhands, and further results show its extensibility to dexterous manipulation\ntasks.\n","authors":["Yi-Lin Wei","Zhexi Luo","Yuhao Lin","Mu Lin","Zhizhao Liang","Shuoyu Chen","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2510.23119v1.pdf","comment":"Project page: https://isee-laboratory.github.io/OmniDexGrasp/"},{"id":"http://arxiv.org/abs/2510.23109v1","updated":"2025-10-27T08:22:26Z","published":"2025-10-27T08:22:26Z","title":"An Automated Tape Laying System Employing a Uniaxial Force Control\n  Device","summary":"  This paper deals with the design of a cost effective automated tape laying\nsystem (ATL system) with integrated uniaxial force control to ensure the\nnecessary compaction forces as well as with an accurate temperature control to\nguarantee the used tape being melted appropriate. It is crucial to control the\nsubstrate and the oncoming tape onto a specific temperature level to ensure an\noptimal consolidation between the different layers of the product. Therefore,\nit takes several process steps from the spooled tape on the coil until it is\nfinally tacked onto the desired mold. The different modules are divided into\nthe tape storage spool, a tape-guiding roller, a tape processing unit, a\nheating zone and the consolidation unit. Moreover, a special robot control\nconcept for testing the ATL system is presented. In contrast to many other\nsystems, with this approach, the tape laying device is spatially fixed and the\nshape is moved accordingly by the robot, which allows for handling of rather\ncompact and complex shapes. The functionality of the subsystems and the taping\nprocess itself was finally approved in experimental results using a carbon\nfiber reinforced HDPE tape.\n","authors":["Bernhard Rameder","Hubert Gattringer","Ronald Naderer","Andreas Mueller"],"pdf_url":"https://arxiv.org/pdf/2510.23109v1.pdf","comment":"Proceedings ECCM21 - 21st European Conference on Composite Materials,\n  Nantes, France, 7-2024"},{"id":"http://arxiv.org/abs/2510.23087v1","updated":"2025-10-27T07:45:17Z","published":"2025-10-27T07:45:17Z","title":"EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic\n  Reconstruction","summary":"  In robot-assisted minimally invasive surgery, accurate 3D reconstruction from\nendoscopic video is vital for downstream tasks and improved outcomes. However,\nendoscopic scenarios present unique challenges, including photometric\ninconsistencies, non-rigid tissue motion, and view-dependent highlights. Most\n3DGS-based methods that rely solely on appearance constraints for optimizing\n3DGS are often insufficient in this context, as these dynamic visual artifacts\ncan mislead the optimization process and lead to inaccurate reconstructions. To\naddress these limitations, we present EndoWave, a unified spatio-temporal\nGaussian Splatting framework by incorporating an optical flow-based geometric\nconstraint and a multi-resolution rational wavelet supervision. First, we adopt\na unified spatio-temporal Gaussian representation that directly optimizes\nprimitives in a 4D domain. Second, we propose a geometric constraint derived\nfrom optical flow to enhance temporal coherence and effectively constrain the\n3D structure of the scene. Third, we propose a multi-resolution rational\northogonal wavelet as a constraint, which can effectively separate the details\nof the endoscope and enhance the rendering performance. Extensive evaluations\non two real surgical datasets, EndoNeRF and StereoMIS, demonstrate that our\nmethod EndoWave achieves state-of-the-art reconstruction quality and visual\naccuracy compared to the baseline method.\n","authors":["Taoyu Wu","Yiyi Miao","Jiaxin Guo","Ziyan Chen","Sihang Zhao","Zhuoxiao Li","Zhe Tang","Baoru Huang","Limin Yu"],"pdf_url":"https://arxiv.org/pdf/2510.23087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.01642v2","updated":"2025-10-27T07:38:07Z","published":"2025-10-02T03:48:07Z","title":"FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action\n  Models","summary":"  Recent advances in robotic manipulation have integrated low-level robotic\ncontrol into Vision-Language Models (VLMs), extending them into\nVision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve\nstrong performance in downstream robotic applications, supported by large-scale\ncrowd-sourced robot training data, they still inevitably encounter failures\nduring execution. Enabling robots to reason and recover from unpredictable and\nabrupt failures remains a critical challenge. Existing robotic manipulation\ndatasets, collected in either simulation or the real world, primarily provide\nonly ground-truth trajectories, leaving robots unable to recover once failures\noccur. Moreover, the few datasets that address failure detection typically\noffer only textual explanations, which are difficult to utilize directly in VLA\nmodels. To address this gap, we introduce FailSafe, a novel failure generation\nand recovery system that automatically produces diverse failure cases paired\nwith executable recovery actions. FailSafe can be seamlessly applied to any\nmanipulation task in any simulator, enabling scalable creation of failure\naction data. To demonstrate its effectiveness, we fine-tune LLaVa-OneVision-7B\n(LLaVa-OV-7B) to build FailSafe-VLM. Experimental results show that\nFailSafe-VLM successfully helps robotic arms detect and recover from potential\nfailures, improving the performance of three state-of-the-art VLA models\n(pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several tasks\nin Maniskill. Furthermore, FailSafe-VLM could generalize across different\nspatial configurations, camera viewpoints, object and robotic embodiments. We\nplan to release the FailSafe code to the community.\n","authors":["Zijun Lin","Jiafei Duan","Haoquan Fang","Dieter Fox","Ranjay Krishna","Cheston Tan","Bihan Wen"],"pdf_url":"https://arxiv.org/pdf/2510.01642v2.pdf","comment":"Project Page: https://jimntu.github.io/FailSafe"},{"id":"http://arxiv.org/abs/2510.23084v1","updated":"2025-10-27T07:37:28Z","published":"2025-10-27T07:37:28Z","title":"Breaking the Circle: An Autonomous Control-Switching Strategy for Stable\n  Orographic Soaring in MAVs","summary":"  Orographic soaring can significantly extend the endurance of micro aerial\nvehicles (MAVs), but circling behavior, arising from control conflicts between\nthe longitudinal and vertical axes, increases energy consumption and the risk\nof divergence. We propose a control switching method, named SAOS: Switched\nControl for Autonomous Orographic Soaring, which mitigates circling behavior by\nselectively controlling either the horizontal or vertical axis, effectively\ntransforming the system from underactuated to fully actuated during soaring.\nAdditionally, the angle of attack is incorporated into the INDI controller to\nimprove force estimation. Simulations with randomized initial positions and\nwind tunnel experiments on two MAVs demonstrate that the SAOS improves position\nconvergence, reduces throttle usage, and mitigates roll oscillations caused by\npitch-roll coupling. These improvements enhance energy efficiency and flight\nstability in constrained soaring environments.\n","authors":["Sunyou Hwang","Christophe De Wagter","Bart Remes","Guido de Croon"],"pdf_url":"https://arxiv.org/pdf/2510.23084v1.pdf","comment":"13 pages, 15 figures"},{"id":"http://arxiv.org/abs/2510.23059v1","updated":"2025-10-27T06:43:20Z","published":"2025-10-27T06:43:20Z","title":"Awakening Facial Emotional Expressions in Human-Robot","summary":"  The facial expression generation capability of humanoid social robots is\ncritical for achieving natural and human-like interactions, playing a vital\nrole in enhancing the fluidity of human-robot interactions and the accuracy of\nemotional expression. Currently, facial expression generation in humanoid\nsocial robots still relies on pre-programmed behavioral patterns, which are\nmanually coded at high human and time costs. To enable humanoid robots to\nautonomously acquire generalized expressive capabilities, they need to develop\nthe ability to learn human-like expressions through self-training. To address\nthis challenge, we have designed a highly biomimetic robotic face with\nphysical-electronic animated facial units and developed an end-to-end learning\nframework based on KAN (Kolmogorov-Arnold Network) and attention mechanisms.\nUnlike previous humanoid social robots, we have also meticulously designed an\nautomated data collection system based on expert strategies of facial motion\nprimitives to construct the dataset. Notably, to the best of our knowledge,\nthis is the first open-source facial dataset for humanoid social robots.\nComprehensive evaluations indicate that our approach achieves accurate and\ndiverse facial mimicry across different test subjects.\n","authors":["Yongtong Zhu","Lei Li","Iggy Qian","WenBin Zhou","Ye Yuan","Qingdu Li","Na Liu","Jianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.23059v1.pdf","comment":"Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS 2025). 8 pages, 7 figures, IEEE two-column format"},{"id":"http://arxiv.org/abs/2510.23057v1","updated":"2025-10-27T06:39:57Z","published":"2025-10-27T06:39:57Z","title":"Seq-DeepIPC: Sequential Sensing for End-to-End Control in Legged Robot\n  Navigation","summary":"  We present Seq-DeepIPC, a sequential end-to-end perception-to-control model\nfor legged robot navigation in realworld environments. Seq-DeepIPC advances\nintelligent sensing for autonomous legged navigation by tightly integrating\nmulti-modal perception (RGB-D + GNSS) with temporal fusion and control. The\nmodel jointly predicts semantic segmentation and depth estimation, giving\nricher spatial features for planning and control. For efficient deployment on\nedge devices, we use EfficientNet-B0 as the encoder, reducing computation while\nmaintaining accuracy. Heading estimation is simplified by removing the noisy\nIMU and instead computing the bearing angle directly from consecutive GNSS\npositions. We collected a larger and more diverse dataset that includes both\nroad and grass terrains, and validated Seq-DeepIPC on a robot dog. Comparative\nand ablation studies show that sequential inputs improve perception and control\nin our models, while other baselines do not benefit. Seq-DeepIPC achieves\ncompetitive or better results with reasonable model size; although GNSS-only\nheading is less reliable near tall buildings, it is robust in open areas.\nOverall, Seq-DeepIPC extends end-to-end navigation beyond wheeled robots to\nmore versatile and temporally-aware systems. To support future research, we\nwill release the codes to our GitHub repository at\nhttps://github.com/oskarnatan/Seq-DeepIPC.\n","authors":["Oskar Natan","Jun Miura"],"pdf_url":"https://arxiv.org/pdf/2510.23057v1.pdf","comment":"Preprint notice, this manuscript has been submitted to IEEE sensors\n  journal for possible publication"},{"id":"http://arxiv.org/abs/2510.23026v1","updated":"2025-10-27T05:45:59Z","published":"2025-10-27T05:45:59Z","title":"Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal\n  Resolution","summary":"  Recent studies demonstrate that diffusion planners benefit from sparse-step\nplanning over single-step planning. Training models to skip steps in their\ntrajectories helps capture long-term dependencies without additional or memory\ncomputational cost. However, predicting excessively sparse plans degrades\nperformance. We hypothesize this temporal density threshold is non-uniform\nacross a temporal horizon and that certain parts of a planned trajectory should\nbe more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion\nplanner where the densities throughout the horizon are tunable hyperparameters.\nMDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL\ntask domains.\n","authors":["Crimson Stambaugh","Rajesh P. N. Rao"],"pdf_url":"https://arxiv.org/pdf/2510.23026v1.pdf","comment":"European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESSAN) (under review)"},{"id":"http://arxiv.org/abs/2510.23021v1","updated":"2025-10-27T05:33:00Z","published":"2025-10-27T05:33:00Z","title":"Planning Oriented Integrated Sensing and Communication","summary":"  Integrated sensing and communication (ISAC) enables simultaneous\nlocalization, environment perception, and data exchange for connected\nautonomous vehicles. However, most existing ISAC designs prioritize sensing\naccuracy and communication throughput, treating all targets uniformly and\noverlooking the impact of critical obstacles on motion efficiency. To overcome\nthis limitation, we propose a planning-oriented ISAC (PISAC) framework that\nreduces the sensing uncertainty of planning-bottleneck obstacles and expands\nthe safe navigable path for the ego-vehicle, thereby bridging the gap between\nphysical-layer optimization and motion-level planning. The core of PISAC lies\nin deriving a closed-form safety bound that explicitly links ISAC transmit\npower to sensing uncertainty, based on the Cram\\'er-Rao Bound and occupancy\ninflation principles. Using this model, we formulate a bilevel power allocation\nand motion planning (PAMP) problem, where the inner layer optimizes the ISAC\nbeam power distribution and the outer layer computes a collision-free\ntrajectory under uncertainty-aware safety constraints. Comprehensive\nsimulations in high-fidelity urban driving environments demonstrate that PISAC\nachieves up to 40% higher success rates and over 5% shorter traversal times\nthan existing ISAC-based and communication-oriented benchmarks, validating its\neffectiveness in enhancing both safety and efficiency.\n","authors":["Xibin Jin","Guoliang Li","Shuai Wang","Fan Liu","Miaowen Wen","Huseyin Arslan","Derrick Wing Kwan Ng","Chengzhong Xu"],"pdf_url":"https://arxiv.org/pdf/2510.23021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23016v1","updated":"2025-10-27T05:23:14Z","published":"2025-10-27T05:23:14Z","title":"ManiDP: Manipulability-Aware Diffusion Policy for Posture-Dependent\n  Bimanual Manipulation","summary":"  Recent work has demonstrated the potential of diffusion models in robot\nbimanual skill learning. However, existing methods ignore the learning of\nposture-dependent task features, which are crucial for adapting dual-arm\nconfigurations to meet specific force and velocity requirements in dexterous\nbimanual manipulation. To address this limitation, we propose\nManipulability-Aware Diffusion Policy (ManiDP), a novel imitation learning\nmethod that not only generates plausible bimanual trajectories, but also\noptimizes dual-arm configurations to better satisfy posture-dependent task\nrequirements. ManiDP achieves this by extracting bimanual manipulability from\nexpert demonstrations and encoding the encapsulated posture features using\nRiemannian-based probabilistic models. These encoded posture features are then\nincorporated into a conditional diffusion process to guide the generation of\ntask-compatible bimanual motion sequences. We evaluate ManiDP on six real-world\nbimanual tasks, where the experimental results demonstrate a 39.33$\\%$ increase\nin average manipulation success rate and a 0.45 improvement in task\ncompatibility compared to baseline methods. This work highlights the importance\nof integrating posture-relevant robotic priors into bimanual skill diffusion to\nenable human-like adaptability and dexterity.\n","authors":["Zhuo Li","Junjia Liu","Dianxi Li","Tao Teng","Miao Li","Sylvain Calinon","Darwin Caldwell","Fei Chen"],"pdf_url":"https://arxiv.org/pdf/2510.23016v1.pdf","comment":"7 pages, 6 figures, Accepted and published in IROS 2025"},{"id":"http://arxiv.org/abs/2510.23003v1","updated":"2025-10-27T04:43:20Z","published":"2025-10-27T04:43:20Z","title":"An Intelligent Water-Saving Irrigation System Based on Multi-Sensor\n  Fusion and Visual Servoing Control","summary":"  This paper introduces an intelligent water-saving irrigation system designed\nto address critical challenges in precision agriculture, such as inefficient\nwater use and poor terrain adaptability. The system integrates advanced\ncomputer vision, robotic control, and real-time stabilization technologies via\na multi-sensor fusion approach. A lightweight YOLO model, deployed on an\nembedded vision processor (K210), enables real-time plant container detection\nwith over 96% accuracy under varying lighting conditions. A simplified hand-eye\ncalibration algorithm-designed for 'handheld camera' robot arm\nconfigurations-ensures that the end effector can be precisely positioned, with\na success rate exceeding 90%. The active leveling system, driven by the\nSTM32F103ZET6 main control chip and JY901S inertial measurement data, can\nstabilize the irrigation platform on slopes up to 10 degrees, with a response\ntime of 1.8 seconds. Experimental results across three simulated agricultural\nenvironments (standard greenhouse, hilly terrain, complex lighting) demonstrate\na 30-50% reduction in water consumption compared to conventional flood\nirrigation, with water use efficiency exceeding 92% in all test cases.\n","authors":["ZhengKai Huang","YiKun Wang","ChenYu Hui"," XiaoCheng"],"pdf_url":"https://arxiv.org/pdf/2510.23003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05552v2","updated":"2025-10-27T04:34:34Z","published":"2025-04-07T23:01:50Z","title":"Lazy-DaSH: Lazy Approach for Hypergraph-based Multi-robot Task and\n  Motion Planning","summary":"  We introduce Lazy-DaSH, an improvement over the recent state of the art\nmulti-robot task and motion planning method DaSH, which scales to more than\ndouble the number of robots and objects compared to the original method and\nachieves an order of magnitude faster planning time when applied to a\nmulti-manipulator object rearrangement problem. We achieve this improvement\nthrough a hierarchical approach, where a high-level task planning layer\nidentifies planning spaces required for task completion, and motion feasibility\nis validated lazily only within these spaces. In contrast, DaSH precomputes the\nmotion feasibility of all possible actions, resulting in higher costs for\nconstructing state space representations. Lazy-DaSH maintains efficient query\nperformance by utilizing a constraint feedback mechanism within its\nhierarchical structure, ensuring that motion feasibility is effectively\nconveyed to the query process. By maintaining smaller state space\nrepresentations, our method significantly reduces both representation\nconstruction time and query time. We evaluate Lazy-DaSH in four distinct\nscenarios, demonstrating its scalability to increasing numbers of robots and\nobjects, as well as its adaptability in resolving conflicts through the\nconstraint feedback mechanism.\n","authors":["Seongwon Lee","James Motes","Isaac Ngui","Marco Morales","Nancy M. Amato"],"pdf_url":"https://arxiv.org/pdf/2504.05552v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05020v3","updated":"2025-10-27T04:26:01Z","published":"2025-06-05T13:27:41Z","title":"Hierarchical Language Models for Semantic Navigation and Manipulation in\n  an Aerial-Ground Robotic System","summary":"  Heterogeneous multirobot systems show great potential in complex tasks\nrequiring coordinated hybrid cooperation. However, existing methods that rely\non static or task-specific models often lack generalizability across diverse\ntasks and dynamic environments. This highlights the need for generalizable\nintelligence that can bridge high-level reasoning with low-level execution\nacross heterogeneous agents. To address this, we propose a hierarchical\nmultimodal framework that integrates a prompted large language model (LLM) with\na fine-tuned vision-language model (VLM). At the system level, the LLM performs\nhierarchical task decomposition and constructs a global semantic map, while the\nVLM provides semantic perception and object localization, where the proposed\nGridMask significantly enhances the VLM's spatial accuracy for reliable\nfine-grained manipulation. The aerial robot leverages this global map to\ngenerate semantic paths and guide the ground robot's local navigation and\nmanipulation, ensuring robust coordination even in target-absent or ambiguous\nscenarios. We validate the framework through extensive simulation and\nreal-world experiments on long-horizon object arrangement tasks, demonstrating\nzero-shot adaptability, robust semantic navigation, and reliable manipulation\nin dynamic environments. To the best of our knowledge, this work presents the\nfirst heterogeneous aerial-ground robotic system that integrates VLM-based\nperception with LLM-driven reasoning for global high-level task planning and\nexecution.\n","authors":["Haokun Liu","Zhaoqi Ma","Yunong Li","Junichiro Sugihara","Yicheng Chen","Jinjie Li","Moju Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.05020v3.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2504.17959v3","updated":"2025-10-27T03:10:19Z","published":"2025-04-24T22:08:29Z","title":"CIVIL: Causal and Intuitive Visual Imitation Learning","summary":"  Today's robots attempt to learn new tasks by imitating human examples. These\nrobots watch the human complete the task, and then try to match the actions\ntaken by the human expert. However, this standard approach to visual imitation\nlearning is fundamentally limited: the robot observes what the human does, but\nnot why the human chooses those behaviors. Without understanding which features\nof the system or environment factor into the human's decisions, robot learners\noften misinterpret the human's examples. In practice, this results in causal\nconfusion, inefficient learning, and robot policies that fail when the\nenvironment changes. We therefore propose a shift in perspective: instead of\nasking human teachers just to show what actions the robot should take, we also\nenable humans to intuitively indicate why they made those decisions. Under our\nparadigm human teachers attach markers to task-relevant objects and use natural\nlanguage prompts to describe their state representation. Our proposed\nalgorithm, CIVIL, leverages this augmented demonstration data to filter the\nrobot's visual observations and extract a feature representation that aligns\nwith the human teacher. CIVIL then applies these causal features to train a\ntransformer-based policy that -- when tested on the robot -- is able to emulate\nhuman behaviors without being confused by visual distractors or irrelevant\nitems. Our simulations and real-world experiments demonstrate that robots\ntrained with CIVIL learn both what actions to take and why to take those\nactions, resulting in better performance than state-of-the-art baselines. From\nthe human's perspective, our user study reveals that this new training paradigm\nactually reduces the total time required for the robot to learn the task, and\nalso improves the robot's performance in previously unseen scenarios. See\nvideos at our project website: https://civil2025.github.io\n","authors":["Yinlong Dai","Robert Ramirez Sanchez","Ryan Jeronimus","Shahabedin Sagheb","Cara M. Nunez","Heramb Nemlekar","Dylan P. Losey"],"pdf_url":"https://arxiv.org/pdf/2504.17959v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22949v1","updated":"2025-10-27T03:07:23Z","published":"2025-10-27T03:07:23Z","title":"End-to-End Design and Validation of a Low-Cost Stewart Platform with\n  Nonlinear Estimation and Control","summary":"  This paper presents the complete design, control, and experimental validation\nof a low-cost Stewart platform prototype developed as an affordable yet capable\nrobotic testbed for research and education. The platform combines off the shelf\ncomponents with 3D printed and custom fabricated parts to deliver full six\ndegrees of freedom motions using six linear actuators connecting a moving\nplatform to a fixed base. The system software integrates dynamic modeling, data\nacquisition, and real time control within a unified framework. A robust\ntrajectory tracking controller based on feedback linearization, augmented with\nan LQR scheme, compensates for the platform's nonlinear dynamics to achieve\nprecise motion control. In parallel, an Extended Kalman Filter fuses IMU and\nactuator encoder feedback to provide accurate and reliable state estimation\nunder sensor noise and external disturbances. Unlike prior efforts that\nemphasize only isolated aspects such as modeling or control, this work delivers\na complete hardware-software platform validated through both simulation and\nexperiments on static and dynamic trajectories. Results demonstrate effective\ntrajectory tracking and real-time state estimation, highlighting the platform's\npotential as a cost effective and versatile tool for advanced research and\neducational applications.\n","authors":["Benedictus C. G. Cinun","Tua A. Tamba","Immanuel R. Santjoko","Xiaofeng Wang","Michael A. Gunarso","Bin Hu"],"pdf_url":"https://arxiv.org/pdf/2510.22949v1.pdf","comment":"24 pages, journal"},{"id":"http://arxiv.org/abs/2505.20455v4","updated":"2025-10-27T02:03:10Z","published":"2025-05-26T18:48:44Z","title":"HAND Me the Data: Fast Robot Adaptation via Hand Path Retrieval","summary":"  We hand the community HAND, a simple and time-efficient method for teaching\nrobots new manipulation tasks through human hand demonstrations. Instead of\nrelying on task-specific robot demonstrations collected via teleoperation, HAND\nuses easy-to-provide hand demonstrations to retrieve relevant behaviors from\ntask-agnostic robot play data. Using a visual tracking pipeline, HAND extracts\nthe motion of the human hand from the hand demonstration and retrieves robot\nsub-trajectories in two stages: first filtering by visual similarity, then\nretrieving trajectories with similar behaviors to the hand. Fine-tuning a\npolicy on the retrieved data enables real-time learning of tasks in under four\nminutes, without requiring calibrated cameras or detailed hand pose estimation.\nExperiments also show that HAND outperforms retrieval baselines by over 2x in\naverage task success rates on real robots. Videos can be found at our project\nwebsite: https://liralab.usc.edu/handretrieval/.\n","authors":["Matthew Hong","Anthony Liang","Kevin Kim","Harshitha Rajaprakash","Jesse Thomason","Erdem Bıyık","Jesse Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.20455v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.23107v2","updated":"2025-10-27T01:43:56Z","published":"2025-09-27T04:31:24Z","title":"Open-Vocabulary Spatio-Temporal Scene Graph for Robot Perception and\n  Teleoperation Planning","summary":"  Teleoperation via natural-language reduces operator workload and enhances\nsafety in high-risk or remote settings. However, in dynamic remote scenes,\ntransmission latency during bidirectional communication creates gaps between\nremote perceived states and operator intent, leading to command\nmisunderstanding and incorrect execution. To mitigate this, we introduce the\nSpatio-Temporal Open-Vocabulary Scene Graph (ST-OVSG), a representation that\nenriches open-vocabulary perception with temporal dynamics and lightweight\nlatency annotations. ST-OVSG leverages LVLMs to construct open-vocabulary 3D\nobject representations, and extends them into the temporal domain via Hungarian\nassignment with our temporal matching cost, yielding a unified spatio-temporal\nscene graph. A latency tag is embedded to enable LVLM planners to\nretrospectively query past scene states, thereby resolving local-remote state\nmismatches caused by transmission delays. To further reduce redundancy and\nhighlight task-relevant cues, we propose a task-oriented subgraph filtering\nstrategy that produces compact inputs for the planner. ST-OVSG generalizes to\nnovel categories and enhances planning robustness against transmission latency\nwithout requiring fine-tuning. Experiments show that our method achieves 74\npercent node accuracy on the Replica benchmark, outperforming ConceptGraph.\nNotably, in the latency-robustness experiment, the LVLM planner assisted by\nST-OVSG achieved a planning success rate of 70.5 percent.\n","authors":["Yi Wang","Zeyu Xue","Mujie Liu","Tongqin Zhang","Yan Hu","Zhou Zhao","Chenguang Yang","Zhenyu Lu"],"pdf_url":"https://arxiv.org/pdf/2509.23107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22913v1","updated":"2025-10-27T01:30:26Z","published":"2025-10-27T01:30:26Z","title":"Clinic-Oriented Feasibility of a Sensor-Fused Wearable for Upper-Limb\n  Function","summary":"  Background: Upper-limb weakness and tremor (4--12 Hz) limit activities of\ndaily living (ADL) and reduce adherence to home rehabilitation. Objective: To\nassess technical feasibility and clinician-relevant signals of a sensor-fused\nwearable targeting the triceps brachii and extensor pollicis brevis. Methods: A\nlightweight node integrates surface EMG (1 kHz), IMU (100--200 Hz), and\nflex/force sensors with on-device INT8 inference (Tiny 1D-CNN/Transformer) and\na safety-bounded assist policy (angle/torque/jerk limits; stall/time-out).\nHealthy adults (n = 12) performed three ADL-like tasks. Primary outcomes:\nTremor Index (TI), range of motion (ROM), repetitions (Reps min$^{-1}$).\nSecondary: EMG median-frequency slope (fatigue trend), closed-loop latency,\nsession completion, and device-related adverse events. Analyses used\nsubject-level paired medians with BCa 95\\% CIs; exact Wilcoxon $p$-values are\nreported in the Results. Results: Assistance was associated with lower tremor\nprominence and improved task throughput: TI decreased by $-0.092$ (95\\% CI\n[$-0.102$, $-0.079$]), ROM increased by $+12.65\\%$ (95\\% CI [$+8.43$,\n$+13.89$]), and Reps rose by $+2.99$ min$^{-1}$ (95\\% CI [$+2.61$, $+3.35$]).\nMedian on-device latency was 8.7 ms at a 100 Hz loop rate; all sessions were\ncompleted with no device-related adverse events. Conclusions: Multimodal\nsensing with low-latency, safety-bounded assistance produced improved movement\nquality (TI $\\downarrow$) and throughput (ROM, Reps $\\uparrow$) in a pilot\ntechnical-feasibility setting, supporting progression to IRB-approved patient\nstudies. Trial registration: Not applicable (pilot non-clinical).\n","authors":["Thanyanee Srichaisak","Arissa Ieochai","Aueaphum Aueawattthanaphisut"],"pdf_url":"https://arxiv.org/pdf/2510.22913v1.pdf","comment":"19 pages, 7 figures, 5 Tables"},{"id":"http://arxiv.org/abs/2506.12851v2","updated":"2025-10-27T01:13:54Z","published":"2025-06-15T13:58:53Z","title":"KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning\n  Highly-Dynamic Skills","summary":"  Humanoid robots are promising to acquire various skills by imitating human\nbehaviors. However, existing algorithms are only capable of tracking smooth,\nlow-speed human motions, even with delicate reward and curriculum design. This\npaper presents a physics-based humanoid control framework, aiming to master\nhighly-dynamic human behaviors such as Kungfu and dancing through multi-steps\nmotion processing and adaptive motion tracking. For motion processing, we\ndesign a pipeline to extract, filter out, correct, and retarget motions, while\nensuring compliance with physical constraints to the maximum extent. For motion\nimitation, we formulate a bi-level optimization problem to dynamically adjust\nthe tracking accuracy tolerance based on the current tracking error, creating\nan adaptive curriculum mechanism. We further construct an asymmetric\nactor-critic framework for policy training. In experiments, we train whole-body\ncontrol policies to imitate a set of highly-dynamic motions. Our method\nachieves significantly lower tracking errors than existing approaches and is\nsuccessfully deployed on the Unitree G1 robot, demonstrating stable and\nexpressive behaviors. The project page is https://kungfu-bot.github.io.\n","authors":["Weiji Xie","Jinrui Han","Jiakun Zheng","Huanyu Li","Xinzhe Liu","Jiyuan Shi","Weinan Zhang","Chenjia Bai","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2506.12851v2.pdf","comment":"NeurIPS 2025. Project Page: https://kungfu-bot.github.io/"},{"id":"http://arxiv.org/abs/2510.22892v1","updated":"2025-10-27T00:46:43Z","published":"2025-10-27T00:46:43Z","title":"Never Too Rigid to Reach: Adaptive Virtual Model Control with LLM- and\n  Lyapunov-Based Reinforcement Learning","summary":"  Robotic arms are increasingly deployed in uncertain environments, yet\nconventional control pipelines often become rigid and brittle when exposed to\nperturbations or incomplete information. Virtual Model Control (VMC) enables\ncompliant behaviors by embedding virtual forces and mapping them into joint\ntorques, but its reliance on fixed parameters and limited coordination among\nvirtual components constrains adaptability and may undermine stability as task\nobjectives evolve. To address these limitations, we propose Adaptive VMC with\nLarge Language Model (LLM)- and Lyapunov-Based Reinforcement Learning (RL),\nwhich preserves the physical interpretability of VMC while supporting\nstability-guaranteed online adaptation. The LLM provides structured priors and\nhigh-level reasoning that enhance coordination among virtual components,\nimprove sample efficiency, and facilitate flexible adjustment to varying task\nrequirements. Complementarily, Lyapunov-based RL enforces theoretical stability\nconstraints, ensuring safe and reliable adaptation under uncertainty. Extensive\nsimulations on a 7-DoF Panda arm demonstrate that our approach effectively\nbalances competing objectives in dynamic tasks, achieving superior performance\nwhile highlighting the synergistic benefits of LLM guidance and\nLyapunov-constrained adaptation.\n","authors":["Jingzehua Xu","Yangyang Li","Yangfei Chen","Guanwen Xie","Shuai Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22892v1.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2504.03099v3","updated":"2025-10-27T23:53:18Z","published":"2025-04-04T00:57:48Z","title":"Capturing Non-Linear Human Perspective in Line Drawings","summary":"  Artist-drawn sketches only loosely conform to analytical models of\nperspective projection; the deviation of human-drawn perspective from\nanalytical perspective models is persistent and well documented, but has yet to\nbe algorithmically replicated. We encode this deviation between human and\nanalytic perspectives as a continuous function in 3D space and develop a method\nto learn it. We seek deviation functions that (i)mimic artist deviation on our\ntraining data; (ii)generalize to other shapes; (iii)are consistent across\ndifferent views of the same shape; and (iv)produce outputs that appear\nhuman-drawn. The natural data for learning this deviation is pairs of artist\nsketches of 3D shapes and best-matching analytical camera views of the same\nshapes. However, a core challenge in learning perspective deviation is the\nheterogeneity of human drawing choices, combined with relative data paucity\n(the datasets we rely on have only a few dozen training pairs). We sidestep\nthis challenge by learning perspective deviation from an individual pair of an\nartist sketch of a 3D shape and the contours of the same shape rendered from a\nbest-matching analytical camera view. We first match contours of the depicted\nshape to artist strokes, then learn a spatially continuous local perspective\ndeviation function that modifies the camera perspective projecting the contours\nto their corresponding strokes. This function retains key geometric properties\nthat artists strive to preserve when depicting 3D content, thus satisfying (i)\nand (iv) above. We generalize our method to alternative shapes and views (ii,\niii) via a self-augmentation approach that algorithmically generates training\ndata for nearby views, and enforces spatial smoothness and consistency across\nall views. We compare our results to potential alternatives, demonstrating the\nsuperiority of the proposed approach.\n","authors":["Jinfan Yang","Leo Foord-Kelcey","Suzuran Takikawa","Nicholas Vining","Niloy Mitra","Alla Sheffer"],"pdf_url":"https://arxiv.org/pdf/2504.03099v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23880v1","updated":"2025-10-27T21:40:31Z","published":"2025-10-27T21:40:31Z","title":"TRELLISWorld: Training-Free World Generation from Object Generators","summary":"  Text-driven 3D scene generation holds promise for a wide range of\napplications, from virtual prototyping to AR/VR and simulation. However,\nexisting methods are often constrained to single-object generation, require\ndomain-specific training, or lack support for full 360-degree viewability. In\nthis work, we present a training-free approach to 3D scene synthesis by\nrepurposing general-purpose text-to-3D object diffusion models as modular tile\ngenerators. We reformulate scene generation as a multi-tile denoising problem,\nwhere overlapping 3D regions are independently generated and seamlessly blended\nvia weighted averaging. This enables scalable synthesis of large, coherent\nscenes while preserving local semantic control. Our method eliminates the need\nfor scene-level datasets or retraining, relies on minimal heuristics, and\ninherits the generalization capabilities of object-level priors. We demonstrate\nthat our approach supports diverse scene layouts, efficient generation, and\nflexible editing, establishing a simple yet powerful foundation for\ngeneral-purpose, language-driven 3D scene construction.\n","authors":["Hanke Chen","Yuan Liu","Minchen Li"],"pdf_url":"https://arxiv.org/pdf/2510.23880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.23605v1","updated":"2025-10-27T17:59:51Z","published":"2025-10-27T17:59:51Z","title":"Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with\n  Progressive Texture Infilling","summary":"  Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/.\n","authors":["Shuhong Zheng","Ashkan Mirzaei","Igor Gilitschenski"],"pdf_url":"https://arxiv.org/pdf/2510.23605v1.pdf","comment":"NeurIPS 2025, 38 pages, 22 figures"},{"id":"http://arxiv.org/abs/2510.23494v1","updated":"2025-10-27T16:28:55Z","published":"2025-10-27T16:28:55Z","title":"Yesnt: Are Diffusion Relighting Models Ready for Capture Stage\n  Compositing? A Hybrid Alternative to Bridge the Gap","summary":"  Volumetric video relighting is essential for bringing captured performances\ninto virtual worlds, but current approaches struggle to deliver temporally\nstable, production-ready results. Diffusion-based intrinsic decomposition\nmethods show promise for single frames, yet suffer from stochastic noise and\ninstability when extended to sequences, while video diffusion models remain\nconstrained by memory and scale. We propose a hybrid relighting framework that\ncombines diffusion-derived material priors with temporal regularization and\nphysically motivated rendering. Our method aggregates multiple stochastic\nestimates of per-frame material properties into temporally consistent shading\ncomponents, using optical-flow-guided regularization. For indirect effects such\nas shadows and reflections, we extract a mesh proxy from Gaussian Opacity\nFields and render it within a standard graphics pipeline. Experiments on real\nand synthetic captures show that this hybrid strategy achieves substantially\nmore stable relighting across sequences than diffusion-only baselines, while\nscaling beyond the clip lengths feasible for video diffusion. These results\nindicate that hybrid approaches, which balance learned priors with physically\ngrounded constraints, are a practical step toward production-ready volumetric\nvideo relighting.\n","authors":["Elisabeth Jüttner","Leona Krath","Stefan Korfhage","Hannah Dröge","Matthias B. Hullin","Markus Plack"],"pdf_url":"https://arxiv.org/pdf/2510.23494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.14081v3","updated":"2025-10-27T13:30:00Z","published":"2025-10-15T20:36:28Z","title":"Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from\n  Unstructured Phone Images","summary":"  We present a novel, zero-shot pipeline for creating hyperrealistic,\nidentity-preserving 3D avatars from a few unstructured phone images. Existing\nmethods face several challenges: single-view approaches suffer from geometric\ninconsistencies and hallucinations, degrading identity preservation, while\nmodels trained on synthetic data fail to capture high-frequency details like\nskin wrinkles and fine hair, limiting realism. Our method introduces two key\ncontributions: (1) a generative canonicalization module that processes multiple\nunstructured views into a standardized, consistent representation, and (2) a\ntransformer-based model trained on a new, large-scale dataset of high-fidelity\nGaussian splatting avatars derived from dome captures of real people. This\n\"Capture, Canonicalize, Splat\" pipeline produces static quarter-body avatars\nwith compelling realism and robust identity preservation from unstructured\nphotos.\n","authors":["Emanuel Garbin","Guy Adam","Oded Krams","Zohar Barzelay","Eran Guendelman","Michael Schwarz","Matteo Presutto","Moran Vatelmacher","Yigal Shenkman","Eli Peker","Itai Druker","Uri Patish","Yoav Blum","Max Bluvstein","Junxuan Li","Rawal Khirodkar","Shunsuke Saito"],"pdf_url":"https://arxiv.org/pdf/2510.14081v3.pdf","comment":"This work received the Best Paper Honorable Mention at the AMFG\n  Workshop, ICCV 2025"},{"id":"http://arxiv.org/abs/2510.23122v1","updated":"2025-10-27T08:55:50Z","published":"2025-10-27T08:55:50Z","title":"FlowCapX: Physics-Grounded Flow Capture with Long-Term Consistency","summary":"  We present FlowCapX, a physics-enhanced framework for flow reconstruction\nfrom sparse video inputs, addressing the challenge of jointly optimizing\ncomplex physical constraints and sparse observational data over long time\nhorizons. Existing methods often struggle to capture turbulent motion while\nmaintaining physical consistency, limiting reconstruction quality and\ndownstream tasks. Focusing on velocity inference, our approach introduces a\nhybrid framework that strategically separates representation and supervision\nacross spatial scales. At the coarse level, we resolve sparse-view ambiguities\nvia a novel optimization strategy that aligns long-term observation with\nphysics-grounded velocity fields. By emphasizing vorticity-based physical\nconstraints, our method enhances physical fidelity and improves optimization\nstability. At the fine level, we prioritize observational fidelity to preserve\ncritical turbulent structures. Extensive experiments demonstrate\nstate-of-the-art velocity reconstruction, enabling velocity-aware downstream\ntasks, e.g., accurate flow analysis, scene augmentation with tracer\nvisualization and re-simulation.\n","authors":["Ningxiao Tao","Liru Zhang","Xingyu Ni","Mengyu Chu","Baoquan Chen"],"pdf_url":"https://arxiv.org/pdf/2510.23122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13918v2","updated":"2025-10-27T08:22:57Z","published":"2025-01-23T18:55:41Z","title":"Improving Video Generation with Human Feedback","summary":"  Video generation has achieved significant advances through rectified flow\ntechniques, but issues like unsmooth motion and misalignment between videos and\nprompts persist. In this work, we develop a systematic pipeline that harnesses\nhuman feedback to mitigate these problems and refine the video generation\nmodel. Specifically, we begin by constructing a large-scale human preference\ndataset focused on modern video generation models, incorporating pairwise\nannotations across multi-dimensions. We then introduce VideoReward, a\nmulti-dimensional video reward model, and examine how annotations and various\ndesign choices impact its rewarding efficacy. From a unified reinforcement\nlearning perspective aimed at maximizing reward with KL regularization, we\nintroduce three alignment algorithms for flow-based models. These include two\ntraining-time strategies: direct preference optimization for flow (Flow-DPO)\nand reward weighted regression for flow (Flow-RWR), and an inference-time\ntechnique, Flow-NRG, which applies reward guidance directly to noisy videos.\nExperimental results indicate that VideoReward significantly outperforms\nexisting reward models, and Flow-DPO demonstrates superior performance compared\nto both Flow-RWR and supervised fine-tuning methods. Additionally, Flow-NRG\nlets users assign custom weights to multiple objectives during inference,\nmeeting personalized video quality needs.\n","authors":["Jie Liu","Gongye Liu","Jiajun Liang","Ziyang Yuan","Xiaokun Liu","Mingwu Zheng","Xiele Wu","Qiulin Wang","Menghan Xia","Xintao Wang","Xiaohong Liu","Fei Yang","Pengfei Wan","Di Zhang","Kun Gai","Yujiu Yang","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2501.13918v2.pdf","comment":"https://github.com/KwaiVGI/VideoAlign"},{"id":"http://arxiv.org/abs/2510.22975v1","updated":"2025-10-27T03:56:25Z","published":"2025-10-27T03:56:25Z","title":"VoMP: Predicting Volumetric Mechanical Property Fields","summary":"  Physical simulation relies on spatially-varying mechanical properties, often\nlaboriously hand-crafted. VoMP is a feed-forward method trained to predict\nYoung's modulus ($E$), Poisson's ratio ($\\nu$), and density ($\\rho$) throughout\nthe volume of 3D objects, in any representation that can be rendered and\nvoxelized. VoMP aggregates per-voxel multi-view features and passes them to our\ntrained Geometry Transformer to predict per-voxel material latent codes. These\nlatents reside on a manifold of physically plausible materials, which we learn\nfrom a real-world dataset, guaranteeing the validity of decoded per-voxel\nmaterials. To obtain object-level training data, we propose an annotation\npipeline combining knowledge from segmented 3D datasets, material databases,\nand a vision-language model, along with a new benchmark. Experiments show that\nVoMP estimates accurate volumetric properties, far outperforming prior art in\naccuracy and speed.\n","authors":["Rishit Dagli","Donglai Xiang","Vismay Modi","Charles Loop","Clement Fuji Tsang","Anka He Chen","Anita Hu","Gavriel State","David I. W. Levin","Maria Shugrina"],"pdf_url":"https://arxiv.org/pdf/2510.22975v1.pdf","comment":"hi-res paper and other details at:\n  https://research.nvidia.com/labs/sil/projects/vomp"}]},"2025-10-26T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2510.22865v1","updated":"2025-10-26T23:16:51Z","published":"2025-10-26T23:16:51Z","title":"Civic Ground Truth in News Recommenders: A Method for Public Value\n  Scoring","summary":"  Research in news recommendation systems (NRS) continues to explore the best\nways to integrate normative goals such as editorial objectives and public\nservice values into existing systems. Prior efforts have incorporated expert\ninput or audience feedback to quantify these values, laying the groundwork for\nmore civic-minded recommender systems. This paper contributes to that\ntrajectory, introducing a method for embedding civic values into NRS through\nlarge-scale, structured audience evaluations. The proposed civic ground truth\napproach aims to generate value-based labels through a nationally\nrepresentative survey that are generalisable across a wider news corpus, using\nautomated metadata enrichment.\n","authors":["James Meese","Kyle Herbertson"],"pdf_url":"https://arxiv.org/pdf/2510.22865v1.pdf","comment":"Presented at NORMalize 2025: The Third Workshop on the Normative\n  Design and Evaluation of Recommender Systems, co-located with the ACM\n  Conference on Recommender Systems 2025 (RecSys 2025), Prague"},{"id":"http://arxiv.org/abs/2502.14305v2","updated":"2025-10-26T23:07:43Z","published":"2025-02-20T06:40:12Z","title":"Scaling Down, Serving Fast: Compressing and Deploying Efficient LLMs for\n  Recommendation Systems","summary":"  Large language models (LLMs) have demonstrated remarkable performance across\na wide range of industrial applications, from search and recommendation systems\nto generative tasks. Although scaling laws indicate that larger models\ngenerally yield better generalization and performance, their substantial\ncomputational requirements often render them impractical for many real-world\nscenarios at scale. In this paper, we present a comprehensive set of insights\nfor training and deploying small language models (SLMs) that deliver high\nperformance for a variety of industry use cases. We focus on two key\ntechniques: (1) knowledge distillation and (2) model compression via structured\npruning and quantization. These approaches enable SLMs to retain much of the\nquality of their larger counterparts while significantly reducing\ntraining/serving costs and latency. We detail the impact of these techniques on\na variety of use cases in a large professional social network platform and\nshare deployment lessons, including hardware optimization strategies that\nimprove speed and throughput for both predictive and reasoning-based\napplications in Recommendation Systems.\n","authors":["Kayhan Behdin","Ata Fatahibaarzi","Qingquan Song","Yun Dai","Aman Gupta","Zhipeng Wang","Shao Tang","Hejian Sang","Gregory Dexter","Sirou Zhu","Siyu Zhu","Tejas Dharamsi","Vignesh Kothapalli","Zhoutong Fu","Yihan Cao","Pin-Lun Hsu","Fedor Borisyuk","Natesh Pillai","Luke Simon","Rahul Mazumder"],"pdf_url":"https://arxiv.org/pdf/2502.14305v2.pdf","comment":"Accepted to EMNLP 2025 Industry Track - Oral Presentation"},{"id":"http://arxiv.org/abs/2510.02656v2","updated":"2025-10-26T21:25:59Z","published":"2025-10-03T01:21:55Z","title":"A Simple but Effective Elaborative Query Reformulation Approach for\n  Natural Language Recommendation","summary":"  Natural Language (NL) recommender systems aim to retrieve relevant items from\nfree-form user queries and item descriptions. Existing systems often rely on\ndense retrieval (DR), which struggles to interpret challenging queries that\nexpress broad (e.g., \"cities for youth friendly activities\") or indirect (e.g.,\n\"cities for a high school graduation trip\") user intents. While query\nreformulation (QR) has been widely adopted to improve such systems, existing QR\nmethods tend to focus only on expanding the range of query subtopics (breadth)\nor elaborating on the potential meaning of a query (depth), but not both. In\nthis paper, we propose EQR (Elaborative Subtopic Query Reformulation), a large\nlanguage model-based QR method that combines both breadth and depth by\ngenerating potential query subtopics with information-rich elaborations. We\nalso introduce three new natural language recommendation benchmarks in travel,\nhotel, and restaurant domains to establish evaluation of NL recommendation with\nchallenging queries. Experiments show EQR substantially outperforms\nstate-of-the-art QR methods in various evaluation metrics, highlighting that a\nsimple yet effective QR approach can significantly improve NL recommender\nsystems for queries with broad and indirect user intents.\n","authors":["Qianfeng Wen","Yifan Liu","Justin Cui","Joshua Zhang","Anton Korikov","George-Kirollos Saad","Scott Sanner"],"pdf_url":"https://arxiv.org/pdf/2510.02656v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2510.22739v1","updated":"2025-10-26T16:15:50Z","published":"2025-10-26T16:15:50Z","title":"REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for\n  E-commerce Visual Search System Optimization","summary":"  In Taobao e-commerce visual search, user behavior analysis reveals a large\nproportion of no-click requests, suggesting diverse and implicit user intents.\nThese intents are expressed in various forms and are difficult to mine and\ndiscover, thereby leading to the limited adaptability and lag in platform\nstrategies. This greatly restricts users' ability to express diverse intents\nand hinders the scalability of the visual search system. This mismatch between\nuser implicit intent expression and system response defines the User-SearchSys\nIntent Discrepancy. To alleviate the issue, we propose a novel framework\nREVISION. This framework integrates offline reasoning mining with online\ndecision-making and execution, enabling adaptive strategies to solve implicit\nuser demands. In the offline stage, we construct a periodic pipeline to mine\ndiscrepancies from historical no-click requests. Leveraging large models, we\nanalyze implicit intent factors and infer optimal suggestions by jointly\nreasoning over query and product metadata. These inferred suggestions serve as\nactionable insights for refining platform strategies. In the online stage,\nREVISION-R1-3B, trained on the curated offline data, performs holistic analysis\nover query images and associated historical products to generate optimization\nplans and adaptively schedule strategies across the search pipeline. Our\nframework offers a streamlined paradigm for integrating large models with\ntraditional search systems, enabling end-to-end intelligent optimization across\ninformation aggregation and user interaction. Experimental results demonstrate\nthat our approach improves the efficiency of implicit intent mining from\nlarge-scale search logs and significantly reduces the no-click rate.\n","authors":["Yiwen Tang","Qiuyu Zhao","Zenghui Sun","Jinsong Lan","Xiaoyong Zhu","Bo Zheng","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22733v1","updated":"2025-10-26T16:04:48Z","published":"2025-10-26T16:04:48Z","title":"$\\text{E}^2\\text{Rank}$: Your Text Embedding can Also be an Effective\n  and Efficient Listwise Reranker","summary":"  Text embedding models serve as a fundamental component in real-world search\napplications. By mapping queries and documents into a shared embedding space,\nthey deliver competitive retrieval performance with high efficiency. However,\ntheir ranking fidelity remains limited compared to dedicated rerankers,\nespecially recent LLM-based listwise rerankers, which capture fine-grained\nquery-document and document-document interactions. In this paper, we propose a\nsimple yet effective unified framework $\\text{E}^2\\text{Rank}$, means Efficient\nEmbedding-based Ranking (also means Embedding-to-Rank), which extends a single\ntext embedding model to perform both high-quality retrieval and listwise\nreranking through continued training under a listwise ranking objective,\nthereby achieving strong effectiveness with remarkable efficiency. By applying\ncosine similarity between the query and document embeddings as a unified\nranking function, the listwise ranking prompt, which is constructed from the\noriginal query and its candidate documents, serves as an enhanced query\nenriched with signals from the top-K documents, akin to pseudo-relevance\nfeedback (PRF) in traditional retrieval models. This design preserves the\nefficiency and representational quality of the base embedding model while\nsignificantly improving its reranking performance. Empirically,\n$\\textrm{E}^2\\text{Rank}$ achieves state-of-the-art results on the BEIR\nreranking benchmark and demonstrates competitive performance on the\nreasoning-intensive BRIGHT benchmark, with very low reranking latency. We also\nshow that the ranking training process improves embedding performance on the\nMTEB benchmark. Our findings indicate that a single embedding model can\neffectively unify retrieval and reranking, offering both computational\nefficiency and competitive ranking accuracy.\n","authors":["Qi Liu","Yanzhao Zhang","Mingxin Li","Dingkun Long","Pengjun Xie","Jiaxin Mao"],"pdf_url":"https://arxiv.org/pdf/2510.22733v1.pdf","comment":"Code and models are avaliable at https://alibaba-nlp.github.io/E2Rank"},{"id":"http://arxiv.org/abs/2510.22732v1","updated":"2025-10-26T16:03:39Z","published":"2025-10-26T16:03:39Z","title":"ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation","summary":"  We observe that current state-of-the-art web-agents are unable to effectively\nadapt to new environments without neural network fine-tuning, without which\nthey produce inefficient execution plans due to a lack of awareness of the\nstructure and dynamics of the new environment. To address this limitation, we\nintroduce ATLAS (Actor-Critic Task-completion with Look-ahead Action\nSimulation), a memory-augmented agent that is able to make plans grounded in a\nmodel of the environment by simulating the consequences of those actions in\ncognitive space. Our agent starts by building a \"cognitive map\" by performing a\nlightweight curiosity driven exploration of the environment. The planner\nproposes candidate actions; the simulator predicts their consequences in\ncognitive space; a critic analyzes the options to select the best roll-out and\nupdate the original plan; and a browser executor performs the chosen action. On\nthe WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%\nsuccess rate for the previously published state-of-the-art. Unlike previous\nsystems, our modular architecture requires no website-specific LLM fine-tuning.\nAblations show sizable drops without the world-model, hierarchical planner, and\nlook-ahead-based replanner confirming their complementary roles within the\ndesign of our system\n","authors":["Jiali Cheng","Anjishnu Kumar","Roshan Lal","Rishi Rajasekaran","Hani Ramezani","Omar Zia Khan","Oleg Rokhlenko","Sunny Chiu-Webster","Gang Hua","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2510.22732v1.pdf","comment":"9 pages, NeurIPS 2025 Workshop on Language Agents and World Models"},{"id":"http://arxiv.org/abs/2510.22694v1","updated":"2025-10-26T14:36:16Z","published":"2025-10-26T14:36:16Z","title":"Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation","summary":"  Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising\nmethod to generate factual and up-to-date responses of Multimodal Large\nLanguage Models (MLLMs) by incorporating non-parametric knowledge from external\nknowledge bases. However, existing MRAG approaches suffer from static retrieval\nstrategies, inflexible modality selection, and suboptimal utilization of\nretrieved information, leading to three critical challenges: determining when\nto retrieve, what modality to incorporate, and how to utilize retrieved\ninformation effectively. To address these challenges, we introduce Windsock, a\nquery-dependent module making decisions on retrieval necessity and modality\nselection, effectively reducing computational overhead and improving response\nquality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction\nTuning, an adaptive training strategy that enhances MLLMs' ability to utilize\nretrieved information while maintaining robustness against noise. Moreover, we\nadopt a self-assessment approach leveraging knowledge within MLLMs to convert\nquestion-answering datasets to MRAG training datasets. Extensive experiments\ndemonstrate that our proposed method significantly improves the generation\nquality by 17.07% while reducing 8.95% retrieval times.\n","authors":["Shu Zhao","Tianyi Shen","Nilesh Ahuja","Omesh Tickoo","Vijaykrishnan Narayanan"],"pdf_url":"https://arxiv.org/pdf/2510.22694v1.pdf","comment":"Accepted at NeurIPS 2025 UniReps Workshop"},{"id":"http://arxiv.org/abs/2510.22681v1","updated":"2025-10-26T13:51:45Z","published":"2025-10-26T13:51:45Z","title":"Diversification as Risk Minimization","summary":"  Users tend to remember failures of a search session more than its many\nsuccesses. This observation has led to work on search robustness, where systems\nare penalized if they perform very poorly on some queries. However, this\nprinciple of robustness has been overlooked within a single query. An ambiguous\nor underspecified query (e.g., ``jaguar'') can have several user intents, where\npopular intents often dominate the ranking, leaving users with minority intents\nunsatisfied. Although the diversification literature has long recognized this\nissue, existing metrics only model the average relevance across intents and\nprovide no robustness guarantees. More surprisingly, we show theoretically and\nempirically that many well-known diversification algorithms are no more robust\nthan a naive, non-diversified algorithm. To address this critical gap, we\npropose to frame diversification as a risk-minimization problem. We introduce\nVRisk, which measures the expected risk faced by the least-served fraction of\nintents in a query. Optimizing VRisk produces a robust ranking, reducing the\nlikelihood of poor user experiences. We then propose VRisker, a fast greedy\nre-ranker with provable approximation guarantees. Finally, experiments on NTCIR\nINTENT-2, TREC Web 2012, and MovieLens show the vulnerability of existing\nmethods. VRisker reduces worst-case intent failures by up to 33% with a minimal\n2% drop in average performance.\n","authors":["Rikiya Takehi","Fernando Diaz","Tetsuya Sakai"],"pdf_url":"https://arxiv.org/pdf/2510.22681v1.pdf","comment":"Preprint, accepted at WSDM 2026 (Full Paper). 16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2510.22670v1","updated":"2025-10-26T13:17:01Z","published":"2025-10-26T13:17:01Z","title":"Tools are under-documented: Simple Document Expansion Boosts Tool\n  Retrieval","summary":"  Large Language Models (LLMs) have recently demonstrated strong capabilities\nin tool use, yet progress in tool retrieval remains hindered by incomplete and\nheterogeneous tool documentation. To address this challenge, we introduce\nTool-DE, a new benchmark and framework that systematically enriches tool\ndocumentation with structured fields to enable more effective tool retrieval,\ntogether with two dedicated models, Tool-Embed and Tool-Rank. We design a\nscalable document expansion pipeline that leverages both open- and\nclosed-source LLMs to generate, validate, and refine enriched tool profiles at\nlow cost, producing large-scale corpora with 50k instances for embedding-based\nretrievers and 200k for rerankers. On top of this data, we develop two models\nspecifically tailored for tool retrieval: Tool-Embed, a dense retriever, and\nTool-Rank, an LLM-based reranker. Extensive experiments on ToolRet and Tool-DE\ndemonstrate that document expansion substantially improves retrieval\nperformance, with Tool-Embed and Tool-Rank achieving new state-of-the-art\nresults on both benchmarks. We further analyze the contribution of individual\nfields to retrieval effectiveness, as well as the broader impact of document\nexpansion on both training and evaluation. Overall, our findings highlight both\nthe promise and limitations of LLM-driven document expansion, positioning\nTool-DE, along with the proposed Tool-Embed and Tool-Rank, as a foundation for\nfuture research in tool retrieval.\n","authors":["Xuan Lu","Haohang Huang","Rui Meng","Yaohui Jin","Wenjun Zeng","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2510.22670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22590v1","updated":"2025-10-26T09:10:26Z","published":"2025-10-26T09:10:26Z","title":"ATOM: AdapTive and OptiMized dynamic temporal knowledge graph\n  construction using LLMs","summary":"  In today's rapidly expanding data landscape, knowledge extraction from\nunstructured text is vital for real-time analytics, temporal inference, and\ndynamic memory frameworks. However, traditional static knowledge graph (KG)\nconstruction often overlooks the dynamic and time-sensitive nature of\nreal-world data, limiting adaptability to continuous changes. Moreover, recent\nzero- or few-shot approaches that avoid domain-specific fine-tuning or reliance\non prebuilt ontologies often suffer from instability across multiple runs, as\nwell as incomplete coverage of key facts. To address these challenges, we\nintroduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that\nbuilds and continuously updates Temporal Knowledge Graphs (TKGs) from\nunstructured texts. ATOM splits input documents into minimal, self-contained\n\"atomic\" facts, improving extraction exhaustivity and stability. Then, it\nconstructs atomic TKGs from these facts while employing a dual-time modeling\nthat distinguishes when information is observed from when it is valid. The\nresulting atomic TKGs are subsequently merged in parallel. Empirical\nevaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%\nbetter stability, and over 90% latency reduction compared to baseline methods,\ndemonstrating a strong scalability potential for dynamic TKG construction.\n","authors":["Yassir Lairgi","Ludovic Moncla","Khalid Benabdeslem","Rémy Cazabet","Pierre Cléau"],"pdf_url":"https://arxiv.org/pdf/2510.22590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16101v3","updated":"2025-10-26T06:07:13Z","published":"2025-02-22T05:50:15Z","title":"Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the\n  Robustness of RAG Against Misleading Retrievals","summary":"  Retrieval-augmented generation (RAG) has shown impressive capabilities in\nmitigating hallucinations in large language models (LLMs). However, LLMs\nstruggle to maintain consistent reasoning when exposed to misleading or\nconflicting evidence, especially in real-world domains such as politics, where\ninformation is polarized or selectively framed. Mainstream RAG benchmarks\nevaluate models under clean retrieval settings, where systems generate answers\nfrom gold-standard documents, or under synthetically perturbed settings, where\ndocuments are artificially injected with noise. These assumptions fail to\nreflect real-world conditions, often leading to an overestimation of RAG system\nperformance. To address this gap, we introduce RAGuard, the first benchmark to\nevaluate the robustness of RAG systems against misleading retrievals. Unlike\nprior benchmarks that rely on synthetic noise, our fact-checking dataset\ncaptures naturally occurring misinformation by constructing its retrieval\ncorpus from Reddit discussions. It categorizes retrieved evidence into three\ntypes: supporting, misleading, and unrelated, providing a realistic and\nchallenging testbed for assessing how well RAG systems navigate different types\nof evidence. Our experiments reveal that, when exposed to potentially\nmisleading retrievals, all tested LLM-powered RAG systems perform worse than\ntheir zero-shot baselines (i.e., no retrieval at all), while human annotators\nconsistently perform better, highlighting LLMs' susceptibility to noisy\nenvironments. To our knowledge, RAGuard is the first benchmark to\nsystematically assess the robustness of the RAG against misleading evidence. We\nexpect this benchmark to drive future research toward improving RAG systems\nbeyond idealized datasets, making them more reliable for real-world\napplications. The dataset is available at\nhttps://huggingface.co/datasets/UCSC-IRKM/RAGuard.\n","authors":["Linda Zeng","Rithwik Gupta","Divij Motwani","Diji Yang","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.16101v3.pdf","comment":"Advances in Neural Information Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2510.22521v1","updated":"2025-10-26T04:13:31Z","published":"2025-10-26T04:13:31Z","title":"Open Multimodal Retrieval-Augmented Factual Image Generation","summary":"  Large Multimodal Models (LMMs) have achieved remarkable progress in\ngenerating photorealistic and prompt-aligned images, but they often produce\noutputs that contradict verifiable knowledge, especially when prompts involve\nfine-grained attributes or time-sensitive events. Conventional\nretrieval-augmented approaches attempt to address this issue by introducing\nexternal information, yet they are fundamentally incapable of grounding\ngeneration in accurate and evolving knowledge due to their reliance on static\nsources and shallow evidence integration. To bridge this gap, we introduce\nORIG, an agentic open multimodal retrieval-augmented framework for Factual\nImage Generation (FIG), a new task that requires both visual realism and\nfactual grounding. ORIG iteratively retrieves and filters multimodal evidence\nfrom the web and incrementally integrates the refined knowledge into enriched\nprompts to guide generation. To support systematic evaluation, we build\nFIG-Eval, a benchmark spanning ten categories across perceptual, compositional,\nand temporal dimensions. Experiments demonstrate that ORIG substantially\nimproves factual consistency and overall image quality over strong baselines,\nhighlighting the potential of open multimodal retrieval for factual image\ngeneration.\n","authors":["Yang Tian","Fan Liu","Jingyuan Zhang","Wei Bi","Yupeng Hu","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2510.22521v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.12925v2","updated":"2025-10-26T03:34:29Z","published":"2025-05-19T10:07:51Z","title":"CPRet: A Dataset, Benchmark, and Model for Retrieval in Competitive\n  Programming","summary":"  Competitive programming benchmarks are widely used in scenarios such as\nprogramming contests and large language model assessments. However, the growing\npresence of duplicate or highly similar problems raises concerns not only about\ncompetition fairness, but also about the validity of competitive programming as\na benchmark for model evaluation. In this paper, we propose a new problem,\nsimilar question retrieval, to tackle this issue. Due to the lack of both data\nand models, solving this problem is challenging. To this end, we introduce\nCPRet, a retrieval-oriented benchmark suite for competitive programming,\ncovering four retrieval tasks: two code-centric (i.e., Text-to-Code,\nCode-to-Code) and two newly proposed problem-centric tasks (i.e.,\nProblem-to-Duplicate, Simplified-to-Full) built from a combination of\nautomatically crawled problem-solution data and manually curated annotations.\nOur contribution includes both high-quality training data and temporally\nseparated test sets for reliable evaluation. Besides, we further develop two\ntask-specialized retrievers based on this dataset: CPRetriever-Code, trained\nwith a novel Group-InfoNCE loss for problem-code alignment, and\nCPRetriever-Prob, fine-tuned for identifying problem-level similarity. Both\nmodels achieve strong results and are open-sourced for local use. Finally, we\nanalyze LiveCodeBench and find that high-similarity problems inflate model pass\nrates and reduce differentiation, underscoring the need for similarity-aware\nevaluation in future benchmarks.\n  Github: https://github.com/coldchair/CPRet\n  Online Demo: https://www.cpret.online/\n","authors":["Han Deng","Yuan Meng","Shixiang Tang","Wanli Ouyang","Xinzhu Ma"],"pdf_url":"https://arxiv.org/pdf/2505.12925v2.pdf","comment":"Accepted by NeurIPS 2025 Dataset and Benchmark Track"}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.16421v3","updated":"2025-10-26T22:58:59Z","published":"2025-03-20T17:59:42Z","title":"MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance","summary":"  Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.\n","authors":["Quanhao Li","Zhen Xing","Rui Wang","Hui Zhang","Qi Dai","Zuxuan Wu"],"pdf_url":"https://arxiv.org/pdf/2503.16421v3.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2510.22829v1","updated":"2025-10-26T20:51:52Z","published":"2025-10-26T20:51:52Z","title":"LLM-based Fusion of Multi-modal Features for Commercial Memorability\n  Prediction","summary":"  This paper addresses the prediction of commercial (brand) memorability as\npart of \"Subtask 2: Commercial/Ad Memorability\" within the \"Memorability:\nPredicting movie and commercial memorability\" task at the MediaEval 2025\nworkshop competition. We propose a multimodal fusion system with a Gemma-3 LLM\nbackbone that integrates pre-computed visual (ViT) and textual (E5) features by\nmulti-modal projections. The model is adapted using Low-Rank Adaptation (LoRA).\nA heavily-tuned ensemble of gradient boosted trees serves as a baseline. A key\ncontribution is the use of LLM-generated rationale prompts, grounded in\nexpert-derived aspects of memorability, to guide the fusion model. The results\ndemonstrate that the LLM-based system exhibits greater robustness and\ngeneralization performance on the final test set, compared to the baseline.\n  The paper's codebase can be found at\nhttps://github.com/dsgt-arc/mediaeval-2025-memorability\n","authors":["Aleksandar Pramov"],"pdf_url":"https://arxiv.org/pdf/2510.22829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22812v1","updated":"2025-10-26T19:55:57Z","published":"2025-10-26T19:55:57Z","title":"Region-Adaptive Learned Hierarchical Encoding for 3D Gaussian Splatting\n  Data","summary":"  We introduce Region-Adaptive Learned Hierarchical Encoding (RALHE) for 3D\nGaussian Splatting (3DGS) data. While 3DGS has recently become popular for\nnovel view synthesis, the size of trained models limits its deployment in\nbandwidth-constrained applications such as volumetric media streaming. To\naddress this, we propose a learned hierarchical latent representation that\nbuilds upon the principles of \"overfitted\" learned image compression (e.g.,\nCool-Chic and C3) to efficiently encode 3DGS attributes. Unlike images, 3DGS\ndata have irregular spatial distributions of Gaussians (geometry) and consist\nof multiple attributes (signals) defined on the irregular geometry. Our codec\nis designed to account for these differences between images and 3DGS.\nSpecifically, we leverage the octree structure of the voxelized 3DGS geometry\nto obtain a hierarchical multi-resolution representation. Our approach overfits\nlatents to each Gaussian attribute under a global rate constraint. These\nlatents are decoded independently through a lightweight decoder network. To\nestimate the bitrate during training, we employ an autoregressive probability\nmodel that leverages octree-derived contexts from the 3D point structure. The\nmulti-resolution latents, decoder, and autoregressive entropy coding networks\nare jointly optimized for each Gaussian attribute. Experiments demonstrate that\nthe proposed RALHE compression framework achieves a rendering PSNR gain of up\nto 2dB at low bitrates (less than 1 MB) compared to the baseline 3DGS\ncompression methods.\n","authors":["Shashank N. Sridhara","Birendra Kathariya","Fangjun Pu","Peng Yin","Eduardo Pavez","Antonio Ortega"],"pdf_url":"https://arxiv.org/pdf/2510.22812v1.pdf","comment":"10 Pages, 5 Figures"},{"id":"http://arxiv.org/abs/2510.22760v1","updated":"2025-10-26T17:18:48Z","published":"2025-10-26T17:18:48Z","title":"Understanding What Is Not Said:Referring Remote Sensing Image\n  Segmentation with Scarce Expressions","summary":"  Referring Remote Sensing Image Segmentation (RRSIS) aims to segment instances\nin remote sensing images according to referring expressions. Unlike Referring\nImage Segmentation on general images, acquiring high-quality referring\nexpressions in the remote sensing domain is particularly challenging due to the\nprevalence of small, densely distributed objects and complex backgrounds. This\npaper introduces a new learning paradigm, Weakly Referring Expression Learning\n(WREL) for RRSIS, which leverages abundant class names as weakly referring\nexpressions together with a small set of accurate ones to enable efficient\ntraining under limited annotation conditions. Furthermore, we provide a\ntheoretical analysis showing that mixed-referring training yields a provable\nupper bound on the performance gap relative to training with fully annotated\nreferring expressions, thereby establishing the validity of this new setting.\nWe also propose LRB-WREL, which integrates a Learnable Reference Bank (LRB) to\nrefine weakly referring expressions through sample-specific prompt embeddings\nthat enrich coarse class-name inputs. Combined with a teacher-student\noptimization framework using dynamically scheduled EMA updates, LRB-WREL\nstabilizes training and enhances cross-modal generalization under noisy weakly\nreferring supervision. Extensive experiments on our newly constructed benchmark\nwith varying weakly referring data ratios validate both the theoretical\ninsights and the practical effectiveness of WREL and LRB-WREL, demonstrating\nthat they can approach or even surpass models trained with fully annotated\nreferring expressions.\n","authors":["Kai Ye","Bowen Liu","Jianghang Lin","Jiayi Ji","Pingyang Dai","Liujuan Cao"],"pdf_url":"https://arxiv.org/pdf/2510.22760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17343v2","updated":"2025-10-26T14:53:32Z","published":"2025-07-23T09:12:25Z","title":"Principled Multimodal Representation Learning","summary":"  Multimodal representation learning seeks to create a unified representation\nspace by integrating diverse data modalities to improve multimodal\nunderstanding. Traditional methods often depend on pairwise contrastive\nlearning, which relies on a predefined anchor modality, restricting alignment\nacross all modalities. Recent advances have investigated the simultaneous\nalignment of multiple modalities, yet several challenges remain, such as\nlimitations imposed by fixed anchor points and instability arising from\noptimizing the product of singular values. To address the challenges, in this\npaper, we propose Principled Multimodal Representation Learning (PMRL), a novel\nframework that achieves simultaneous alignment of multiple modalities without\nanchor dependency in a more stable manner. Specifically, grounded in the\ntheoretical insight that full alignment corresponds to a rank-1 Gram matrix,\nPMRL optimizes the dominant singular value of the representation matrix to\nalign modalities along a shared leading direction. We propose a softmax-based\nloss function that treats singular values as logits to prioritize the largest\nsingular value. Besides, instance-wise contrastive regularization on the\nleading eigenvectors maintains inter-instance separability and prevents\nrepresentation collapse. Extensive experiments across diverse tasks demonstrate\nPMRL's superiority compared to baseline methods. The source code will be\npublicly available.\n","authors":["Xiaohao Liu","Xiaobo Xia","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2507.17343v2.pdf","comment":"Corrected typos and updated experimental results. 32 pages, 9\n  figures, 10 tables"},{"id":"http://arxiv.org/abs/2510.22646v1","updated":"2025-10-26T12:17:33Z","published":"2025-10-26T12:17:33Z","title":"TVMC: Time-Varying Mesh Compression via Multi-Stage Anchor Mesh\n  Generation","summary":"  Time-varying meshes, characterized by dynamic connectivity and varying vertex\ncounts, hold significant promise for applications such as augmented reality.\nHowever, their practical utilization remains challenging due to the substantial\ndata volume required for high-fidelity representation. While various\ncompression methods attempt to leverage temporal redundancy between consecutive\nmesh frames, most struggle with topological inconsistency and motion-induced\nartifacts. To address these issues, we propose Time-Varying Mesh Compression\n(TVMC), a novel framework built on multi-stage coarse-to-fine anchor mesh\ngeneration for inter-frame prediction. Specifically, the anchor mesh is\nprogressively constructed in three stages: initial, coarse, and fine. The\ninitial anchor mesh is obtained through fast topology alignment to exploit\ntemporal coherence. A Kalman filter-based motion estimation module then\ngenerates a coarse anchor mesh by accurately compensating inter-frame motions.\nSubsequently, a Quadric Error Metric-based refinement step optimizes vertex\npositions to form a fine anchor mesh with improved geometric fidelity. Based on\nthe refined anchor mesh, the inter-frame motions relative to the reference base\nmesh are encoded, while the residual displacements between the subdivided fine\nanchor mesh and the input mesh are adaptively quantized and compressed. This\nhierarchical strategy preserves consistent connectivity and high-quality\nsurface approximation, while achieving an efficient and compact representation\nof dynamic geometry. Extensive experiments on standard MPEG dynamic mesh\nsequences demonstrate that TVMC achieves state-of-the-art compression\nperformance. Compared to the latest V-DMC standard, it delivers a significant\nBD-rate gain of 10.2% ~ 16.9%, while preserving high reconstruction quality.\nThe code is available at https://github.com/H-Huang774/TVMC.\n","authors":["He Huang","Qi Yang","Yiling Xu","Zhu Li","Jenq-Neng Hwang"],"pdf_url":"https://arxiv.org/pdf/2510.22646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22622v1","updated":"2025-10-26T10:40:52Z","published":"2025-10-26T10:40:52Z","title":"DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake\n  Detection","summary":"  The misuse of advanced generative AI models has resulted in the widespread\nproliferation of falsified data, particularly forged human-centric audiovisual\ncontent, which poses substantial societal risks (e.g., financial fraud and\nsocial instability). In response to this growing threat, several works have\npreliminarily explored countermeasures. However, the lack of sufficient and\ndiverse training data, along with the absence of a standardized benchmark,\nhinder deeper exploration. To address this challenge, we first build Mega-MMDF,\na large-scale, diverse, and high-quality dataset for multimodal deepfake\ndetection. Specifically, we employ 21 forgery pipelines through the combination\nof 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face\nreenactment methods. Mega-MMDF currently contains 0.1 million real samples and\n1.1 million forged samples, making it one of the largest and most diverse\nmultimodal deepfake datasets, with plans for continuous expansion. Building on\nit, we present DeepfakeBench-MM, the first unified benchmark for multimodal\ndeepfake detection. It establishes standardized protocols across the entire\ndetection pipeline and serves as a versatile platform for evaluating existing\nmethods as well as exploring novel approaches. DeepfakeBench-MM currently\nsupports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our\ncomprehensive evaluations and in-depth analyses uncover several key findings\nfrom multiple perspectives (e.g., augmentation, stacked forgery). We believe\nthat DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as\nfoundational infrastructures for advancing multimodal deepfake detection.\n","authors":["Kangran Zhao","Yupeng Chen","Xiaoyu Zhang","Yize Chen","Weinan Guan","Baicheng Chen","Chengzhe Sun","Soumyya Kanti Datta","Qingshan Liu","Siwei Lyu","Baoyuan Wu"],"pdf_url":"https://arxiv.org/pdf/2510.22622v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2501.06488v3","updated":"2025-10-26T10:20:24Z","published":"2025-01-11T09:12:43Z","title":"NVS-SQA: Exploring Self-Supervised Quality Representation Learning for\n  Neurally Synthesized Scenes without References","summary":"  Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting,\neffectively creates photorealistic scenes from sparse viewpoints, typically\nevaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However,\nthese full-reference methods, which compare synthesized views to reference\nviews, may not fully capture the perceptual quality of neurally synthesized\nscenes (NSS), particularly due to the limited availability of dense reference\nviews. Furthermore, the challenges in acquiring human perceptual labels hinder\nthe creation of extensive labeled datasets, risking model overfitting and\nreduced generalizability. To address these issues, we propose NVS-SQA, a NSS\nquality assessment method to learn no-reference quality representations through\nself-supervision without reliance on human labels. Traditional self-supervised\nlearning predominantly relies on the \"same instance, similar representation\"\nassumption and extensive datasets. However, given that these conditions do not\napply in NSS quality assessment, we employ heuristic cues and quality scores as\nlearning objectives, along with a specialized contrastive pair preparation\nprocess to improve the effectiveness and efficiency of learning. The results\nshow that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e.,\non average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second\nbest) and even exceeds 16 full-reference methods across all evaluation metrics\n(i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).\n","authors":["Qiang Qu","Yiran Shen","Xiaoming Chen","Yuk Ying Chung","Weidong Cai","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.06488v3.pdf","comment":"Accepted by TPAMI"},{"id":"http://arxiv.org/abs/2510.22571v1","updated":"2025-10-26T08:04:28Z","published":"2025-10-26T08:04:28Z","title":"STATUS Bench: A Rigorous Benchmark for Evaluating Object State\n  Understanding in Vision-Language Models","summary":"  Object state recognition aims to identify the specific condition of objects,\nsuch as their positional states (e.g., open or closed) and functional states\n(e.g., on or off). While recent Vision-Language Models (VLMs) are capable of\nperforming a variety of multimodal tasks, it remains unclear how precisely they\ncan identify object states. To alleviate this issue, we introduce the STAte and\nTransition UnderStanding Benchmark (STATUS Bench), the first benchmark for\nrigorously evaluating the ability of VLMs to understand subtle variations in\nobject states in diverse situations. Specifically, STATUS Bench introduces a\nnovel evaluation scheme that requires VLMs to perform three tasks\nsimultaneously: object state identification (OSI), image retrieval (IR), and\nstate change identification (SCI). These tasks are defined over our fully\nhand-crafted dataset involving image pairs, their corresponding object state\ndescriptions and state change descriptions. Furthermore, we introduce a\nlarge-scale training dataset, namely STATUS Train, which consists of 13 million\nsemi-automatically created descriptions. This dataset serves as the largest\nresource to facilitate further research in this area. In our experiments, we\ndemonstrate that STATUS Bench enables rigorous consistency evaluation and\nreveal that current state-of-the-art VLMs still significantly struggle to\ncapture subtle object state distinctions. Surprisingly, under the proposed\nrigorous evaluation scheme, most open-weight VLMs exhibited chance-level\nzero-shot performance. After fine-tuning on STATUS Train, Qwen2.5-VL achieved\nperformance comparable to Gemini 2.0 Flash. These findings underscore the\nnecessity of STATUS Bench and Train for advancing object state recognition in\nVLM research.\n","authors":["Mahiro Ukai","Shuhei Kurita","Nakamasa Inoue"],"pdf_url":"https://arxiv.org/pdf/2510.22571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22565v1","updated":"2025-10-26T07:44:26Z","published":"2025-10-26T07:44:26Z","title":"Learning Event-guided Exposure-agnostic Video Frame Interpolation via\n  Adaptive Feature Blending","summary":"  Exposure-agnostic video frame interpolation (VFI) is a challenging task that\naims to recover sharp, high-frame-rate videos from blurry, low-frame-rate\ninputs captured under unknown and dynamic exposure conditions. Event cameras\nare sensors with high temporal resolution, making them especially advantageous\nfor this task. However, existing event-guided methods struggle to produce\nsatisfactory results on severely low-frame-rate blurry videos due to the lack\nof temporal constraints. In this paper, we introduce a novel event-guided\nframework for exposure-agnostic VFI, addressing this limitation through two key\ncomponents: a Target-adaptive Event Sampling (TES) and a Target-adaptive\nImportance Mapping (TIM). Specifically, TES samples events around the target\ntimestamp and the unknown exposure time to better align them with the\ncorresponding blurry frames. TIM then generates an importance map that\nconsiders the temporal proximity and spatial relevance of consecutive features\nto the target. Guided by this map, our framework adaptively blends consecutive\nfeatures, allowing temporally aligned features to serve as the primary cues\nwhile spatially relevant ones offer complementary support. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of our approach in exposure-agnostic VFI scenarios.\n","authors":["Junsik Jung","Yoonki Cho","Woo Jae Kim","Lin Wang","Sune-eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2510.22565v1.pdf","comment":"Accepted for BMVC2025"},{"id":"http://arxiv.org/abs/2412.11762v2","updated":"2025-10-26T05:03:24Z","published":"2024-12-16T13:26:52Z","title":"GS-ProCams: Gaussian Splatting-based Projector-Camera Systems","summary":"  We present GS-ProCams, the first Gaussian Splatting-based framework for\nprojector-camera systems (ProCams). GS-ProCams is not only view-agnostic but\nalso significantly enhances the efficiency of projection mapping (PM) that\nrequires establishing geometric and radiometric mappings between the projector\nand the camera. Previous CNN-based ProCams are constrained to a specific\nviewpoint, limiting their applicability to novel perspectives. In contrast,\nNeRF-based ProCams support view-agnostic projection mapping, however, they\nrequire an additional co-located light source and demand significant\ncomputational and memory resources. To address this issue, we propose\nGS-ProCams that employs 2D Gaussian for scene representations, and enables\nefficient view-agnostic ProCams applications. In particular, we explicitly\nmodel the complex geometric and photometric mappings of ProCams using projector\nresponses, the projection surface's geometry and materials represented by\nGaussians, and the global illumination component. Then, we employ\ndifferentiable physically-based rendering to jointly estimate them from\ncaptured multi-view projections. Compared to state-of-the-art NeRF-based\nmethods, our GS-ProCams eliminates the need for additional devices, achieving\nsuperior ProCams simulation quality. It also uses only 1/10 of the GPU memory\nfor training and is 900 times faster in inference speed. Please refer to our\nproject page for the code and dataset:\nhttps://realqingyue.github.io/GS-ProCams/.\n","authors":["Qingyue Deng","Jijiang Li","Haibin Ling","Bingyao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11762v2.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2507.15857v7","updated":"2025-10-26T22:38:20Z","published":"2025-07-21T17:59:57Z","title":"Diffusion Beats Autoregressive in Data-Constrained Settings","summary":"  Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings where\ntraining involves repeated passes over limited data and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We find new scaling laws for\ndiffusion models and derive a closed-form expression for the critical compute\nthreshold at which diffusion begins to outperform AR. Finally, we explain why\ndiffusion models excel in this regime: their randomized masking objective\nimplicitly trains over a rich distribution of token orderings, acting as an\nimplicit data augmentation that AR's fixed left-to-right factorization lacks.\nOur results suggest that when data, not compute, is the bottleneck, diffusion\nmodels offer a compelling alternative to the standard AR paradigm. Our code is\navailable at: https://diffusion-scaling.github.io.\n","authors":["Mihir Prabhudesai","Mengning Wu","Amir Zadeh","Katerina Fragkiadaki","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2507.15857v7.pdf","comment":"Project Webpage: https://diffusion-scaling.github.io"},{"id":"http://arxiv.org/abs/2510.22846v1","updated":"2025-10-26T21:38:12Z","published":"2025-10-26T21:38:12Z","title":"Drone Carry-on Weight and Wind Flow Assessment via Micro-Doppler\n  Analysis","summary":"  Remote monitoring of drones has become a global objective due to emerging\napplications in national security and managing aerial delivery traffic. Despite\ntheir relatively small size, drones can carry significant payloads, which\nrequire monitoring, especially in cases of unauthorized transportation of\ndangerous goods. A drone's flight dynamics heavily depend on outdoor wind\nconditions and the carry-on weight, which affect the tilt angle of a drone's\nbody and the rotation velocity of the blades. A surveillance radar can capture\nboth effects, provided a sufficient signal-to-noise ratio for the received\nechoes and an adjusted postprocessing detection algorithm. Here, we conduct a\nsystematic study to demonstrate that micro-Doppler analysis enables the\ndisentanglement of the impacts of wind and weight on a hovering drone. The\nphysics behind the effect is related to the flight controller, as the way the\ndrone counteracts weight and wind differs. When the payload is balanced, it\nimposes an additional load symmetrically on all four rotors, causing them to\nrotate faster, thereby generating a blade-related micro-Doppler shift at a\nhigher frequency. However, the impact of the wind is different. The wind\nattempts to displace the drone, and to counteract this, the drone tilts to the\nside. As a result, the forward and rear rotors rotate at different velocities\nto maintain the tilt angle of the drone body relative to the airflow direction.\nThis causes the splitting in the micro-Doppler spectra. By performing a set of\nexperiments in a controlled environment, specifically, an anechoic chamber for\nelectromagnetic isolation and a wind tunnel for imposing deterministic wind\nconditions, we demonstrate that both wind and payload details can be extracted\nusing a simple deterministic algorithm based on branching in the micro-Doppler\nspectra.\n","authors":["Dmytro Vovchuk","Oleg Torgovitsky","Mykola Khobzei","Vladyslav Tkach","Sergey Geyman","Anton Kharchevskii","Andrey Sheleg","Toms Salgals","Vjaceslavs Bobrovs","Shai Gizach","Aviel Glam","Niv Haim Mizrahi","Alexander Liberzon","Pavel Ginzburg"],"pdf_url":"https://arxiv.org/pdf/2510.22846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22825v1","updated":"2025-10-26T20:38:26Z","published":"2025-10-26T20:38:26Z","title":"Kinematically Controllable Cable Robots with Reconfigurable\n  End-effectors","summary":"  To enlarge the translational workspace of cable-driven robots, one common\napproach is to increase the number of cables. However, this introduces two\nchallenges: (1) cable interference significantly reduces the rotational\nworkspace, and (2) the solution of tensions in cables becomes non-unique,\nresulting in difficulties for kinematic control of the robot. In this work, we\ndesign structurally simple reconfigurable end-effectors for cable robots. By\nincorporating a spring, a helical-grooved shaft, and a matching nut, relative\nlinear motions between end-effector components are converted into relative\nrotations, thereby expanding the rotational workspace of the mechanism.\nMeanwhile, a bearing is introduced to provide an additional rotational degree\nof freedom, making the mechanism non-redundant. As a result, the robot's motion\ncan be controlled purely through kinematics without additional tension sensing\nand control.\n","authors":["Nan Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22825v1.pdf","comment":"7 pages, 12 figures, Technical Report"},{"id":"http://arxiv.org/abs/2510.22821v1","updated":"2025-10-26T20:31:03Z","published":"2025-10-26T20:31:03Z","title":"Analytical Swarm Chemistry: Characterization and Analysis of Emergent\n  Swarm Behaviors","summary":"  Swarm robotics has potential for a wide variety of applications, but\nreal-world deployments remain rare due to the difficulty of predicting emergent\nbehaviors arising from simple local interactions. Traditional engineering\napproaches design controllers to achieve desired macroscopic outcomes under\nidealized conditions, while agent-based and artificial life studies explore\nemergent phenomena in a bottom-up, exploratory manner. In this work, we\nintroduce Analytical Swarm Chemistry, a framework that integrates concepts from\nengineering, agent-based and artificial life research, and chemistry. This\nframework combines macrostate definitions with phase diagram analysis to\nsystematically explore how swarm parameters influence emergent behavior.\nInspired by concepts from chemistry, the framework treats parameters like\nthermodynamic variables, enabling visualization of regions in parameter space\nthat give rise to specific behaviors. Applying this framework to agents with\nminimally viable capabilities, we identify sufficient conditions for behaviors\nsuch as milling and diffusion and uncover regions of the parameter space that\nreliably produce these behaviors. Preliminary validation on real robots\ndemonstrates that these regions correspond to observable behaviors in practice.\nBy providing a principled, interpretable approach, this framework lays the\ngroundwork for predictable and reliable emergent behavior in real-world swarm\nsystems.\n","authors":["Ricardo Vega","Connor Mattson","Kevin Zhu","Daniel S. Brown","Cameron Nowzari"],"pdf_url":"https://arxiv.org/pdf/2510.22821v1.pdf","comment":"9 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2510.22789v1","updated":"2025-10-26T18:46:46Z","published":"2025-10-26T18:46:46Z","title":"Learning Neural Observer-Predictor Models for Limb-level Sampling-based\n  Locomotion Planning","summary":"  Accurate full-body motion prediction is essential for the safe, autonomous\nnavigation of legged robots, enabling critical capabilities like limb-level\ncollision checking in cluttered environments. Simplified kinematic models often\nfail to capture the complex, closed-loop dynamics of the robot and its\nlow-level controller, limiting their predictions to simple planar motion. To\naddress this, we present a learning-based observer-predictor framework that\naccurately predicts this motion. Our method features a neural observer with\nprovable UUB guarantees that provides a reliable latent state estimate from a\nhistory of proprioceptive measurements. This stable estimate initializes a\ncomputationally efficient predictor, designed for the rapid, parallel\nevaluation of thousands of potential trajectories required by modern\nsampling-based planners. We validated the system by integrating our neural\npredictor into an MPPI-based planner on a Vision 60 quadruped. Hardware\nexperiments successfully demonstrated effective, limb-aware motion planning in\na challenging, narrow passage and over small objects, highlighting our system's\nability to provide a robust foundation for high-performance, collision-aware\nplanning on dynamic robotic platforms.\n","authors":["Abhijeet M. Kulkarni","Ioannis Poulakakis","Guoquan Huang"],"pdf_url":"https://arxiv.org/pdf/2510.22789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22784v1","updated":"2025-10-26T18:37:00Z","published":"2025-10-26T18:37:00Z","title":"PIP-LLM: Integrating PDDL-Integer Programming with LLMs for Coordinating\n  Multi-Robot Teams Using Natural Language","summary":"  Enabling robot teams to execute natural language commands requires\ntranslating high-level instructions into feasible, efficient multi-robot plans.\nWhile Large Language Models (LLMs) combined with Planning Domain Description\nLanguage (PDDL) offer promise for single-robot scenarios, existing approaches\nstruggle with multi-robot coordination due to brittle task decomposition, poor\nscalability, and low coordination efficiency.\n  We introduce PIP-LLM, a language-based coordination framework that consists\nof PDDL-based team-level planning and Integer Programming (IP) based\nrobot-level planning. PIP-LLMs first decomposes the command by translating the\ncommand into a team-level PDDL problem and solves it to obtain a team-level\nplan, abstracting away robot assignment. Each team-level action represents a\nsubtask to be finished by the team. Next, this plan is translated into a\ndependency graph representing the subtasks' dependency structure. Such a\ndependency graph is then used to guide the robot-level planning, in which each\nsubtask node will be formulated as an IP-based task allocation problem,\nexplicitly optimizing travel costs and workload while respecting robot\ncapabilities and user-defined constraints. This separation of planning from\nassignment allows PIP-LLM to avoid the pitfalls of syntax-based decomposition\nand scale to larger teams. Experiments across diverse tasks show that PIP-LLM\nimproves plan success rate, reduces maximum and average travel costs, and\nachieves better load balancing compared to state-of-the-art baselines.\n","authors":["Guangyao Shi","Yuwei Wu","Vijay Kumar","Gaurav S. Sukhatme"],"pdf_url":"https://arxiv.org/pdf/2510.22784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22754v1","updated":"2025-10-26T17:07:47Z","published":"2025-10-26T17:07:47Z","title":"TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi\n  Features Integration for Similar Indoor Environments","summary":"  Multi-agent cooperative SLAM often encounters challenges in similar indoor\nenvironments characterized by repetitive structures, such as corridors and\nrooms. These challenges can lead to significant inaccuracies in shared location\nidentification when employing point cloud-based techniques. To mitigate these\nissues, we introduce TWC-SLAM, a multi-agent cooperative SLAM framework that\nintegrates text semantics and WiFi signal features to enhance location\nidentification and loop closure detection. TWC-SLAM comprises a single-agent\nfront-end odometry module based on FAST-LIO2, a location identification and\nloop closure detection module that leverages text semantics and WiFi features,\nand a global mapping module. The agents are equipped with sensors capable of\ncapturing textual information and detecting WiFi signals. By correlating these\ndata sources, TWC-SLAM establishes a common location, facilitating point cloud\nalignment across different agents' maps. Furthermore, the system employs loop\nclosure detection and optimization modules to achieve global optimization and\ncohesive mapping. We evaluated our approach using an indoor dataset featuring\nsimilar corridors, rooms, and text signs. The results demonstrate that TWC-SLAM\nsignificantly improves the performance of cooperative SLAM systems in complex\nenvironments with repetitive architectural features.\n","authors":["Chunyu Li","Shoubin Chen","Dong Li","Weixing Xue","Qingquan Li"],"pdf_url":"https://arxiv.org/pdf/2510.22754v1.pdf","comment":"Accepted by the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2510.22740v1","updated":"2025-10-26T16:21:24Z","published":"2025-10-26T16:21:24Z","title":"Policies over Poses: Reinforcement Learning based Distributed Pose-Graph\n  Optimization for Multi-Robot SLAM","summary":"  We consider the distributed pose-graph optimization (PGO) problem, which is\nfundamental in accurate trajectory estimation in multi-robot simultaneous\nlocalization and mapping (SLAM). Conventional iterative approaches linearize a\nhighly non-convex optimization objective, requiring repeated solving of normal\nequations, which often converge to local minima and thus produce suboptimal\nestimates. We propose a scalable, outlier-robust distributed planar PGO\nframework using Multi-Agent Reinforcement Learning (MARL). We cast distributed\nPGO as a partially observable Markov game defined on local pose-graphs, where\neach action refines a single edge's pose estimate. A graph partitioner\ndecomposes the global pose graph, and each robot runs a recurrent\nedge-conditioned Graph Neural Network (GNN) encoder with adaptive edge-gating\nto denoise noisy edges. Robots sequentially refine poses through a hybrid\npolicy that utilizes prior action memory and graph embeddings. After local\ngraph correction, a consensus scheme reconciles inter-robot disagreements to\nproduce a globally consistent estimate. Our extensive evaluations on a\ncomprehensive suite of synthetic and real-world datasets demonstrate that our\nlearned MARL-based actors reduce the global objective by an average of 37.5%\nmore than the state-of-the-art distributed PGO framework, while enhancing\ninference efficiency by at least 6X. We also demonstrate that actor replication\nallows a single learned policy to scale effortlessly to substantially larger\nrobot teams without any retraining. Code is publicly available at\nhttps://github.com/herolab-uga/policies-over-poses.\n","authors":["Sai Krishna Ghanta","Ramviyas Parasuraman"],"pdf_url":"https://arxiv.org/pdf/2510.22740v1.pdf","comment":"IEEE International Symposium on Multi-Robot & Multi-Agent Systems\n  (MRS) 2025"},{"id":"http://arxiv.org/abs/2510.22738v1","updated":"2025-10-26T16:12:27Z","published":"2025-10-26T16:12:27Z","title":"SCAL for Pinch-Lifting: Complementary Rotational and Linear Prototypes\n  for Environment-Adaptive Grasping","summary":"  This paper presents environment-adaptive pinch-lifting built on a\nslot-constrained adaptive linkage (SCAL) and instantiated in two complementary\nfingers: SCAL-R, a rotational-drive design with an active fingertip that folds\ninward after contact to form an envelope, and SCAL-L, a linear-drive design\nthat passively opens on contact to span wide or weak-feature objects. Both\nfingers convert surface following into an upward lifting branch while\nmaintaining fingertip orientation, enabling thin or low-profile targets to be\nraised from supports with minimal sensing and control. Two-finger grippers are\nfabricated via PLA-based 3D printing. Experiments evaluate (i)\ncontact-preserving sliding and pinch-lifting on tabletops, (ii) ramp\nnegotiation followed by lift, and (iii) handling of bulky objects via active\nenveloping (SCAL-R) or contact-triggered passive opening (SCAL-L). Across\ndozens of trials on small parts, boxes, jars, and tape rolls, both designs\nachieve consistent grasps with limited tuning. A quasi-static analysis provides\nclosed-form fingertip-force models for linear parallel pinching and two-point\nenveloping, offering geometry-aware guidance for design and operation. Overall,\nthe results indicate complementary operating regimes and a practical path to\nrobust, environment-adaptive grasping with simple actuation.\n","authors":["Wentao Guo","Wenzeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22738v1.pdf","comment":"Preliminary version presented at the IROS 2025 CIM Workshop, where it\n  was selected as a Best Demo Award (Finalist) and subsequently received the\n  Best Demo Award after oral presentation"},{"id":"http://arxiv.org/abs/2510.22732v1","updated":"2025-10-26T16:03:39Z","published":"2025-10-26T16:03:39Z","title":"ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation","summary":"  We observe that current state-of-the-art web-agents are unable to effectively\nadapt to new environments without neural network fine-tuning, without which\nthey produce inefficient execution plans due to a lack of awareness of the\nstructure and dynamics of the new environment. To address this limitation, we\nintroduce ATLAS (Actor-Critic Task-completion with Look-ahead Action\nSimulation), a memory-augmented agent that is able to make plans grounded in a\nmodel of the environment by simulating the consequences of those actions in\ncognitive space. Our agent starts by building a \"cognitive map\" by performing a\nlightweight curiosity driven exploration of the environment. The planner\nproposes candidate actions; the simulator predicts their consequences in\ncognitive space; a critic analyzes the options to select the best roll-out and\nupdate the original plan; and a browser executor performs the chosen action. On\nthe WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%\nsuccess rate for the previously published state-of-the-art. Unlike previous\nsystems, our modular architecture requires no website-specific LLM fine-tuning.\nAblations show sizable drops without the world-model, hierarchical planner, and\nlook-ahead-based replanner confirming their complementary roles within the\ndesign of our system\n","authors":["Jiali Cheng","Anjishnu Kumar","Roshan Lal","Rishi Rajasekaran","Hani Ramezani","Omar Zia Khan","Oleg Rokhlenko","Sunny Chiu-Webster","Gang Hua","Hadi Amiri"],"pdf_url":"https://arxiv.org/pdf/2510.22732v1.pdf","comment":"9 pages, NeurIPS 2025 Workshop on Language Agents and World Models"},{"id":"http://arxiv.org/abs/2510.14511v2","updated":"2025-10-26T15:08:06Z","published":"2025-10-16T09:55:33Z","title":"Stability Criteria and Motor Performance in Delayed Haptic Dyadic\n  Interactions Mediated by Robots","summary":"  This paper establishes analytical stability criteria for robot-mediated\nhuman-human (dyadic) interaction systems, focusing on haptic communication\nunder network-induced time delays. Through frequency-domain analysis supported\nby numerical simulations, we identify both delay-independent and\ndelay-dependent stability criteria. The delay-independent criterion guarantees\nstability irrespective of the delay, whereas the delay-dependent criterion is\ncharacterised by a maximum tolerable delay before instability occurs. The\ncriteria demonstrate dependence on controller and robot dynamic parameters,\nwhere increasing stiffness reduces the maximum tolerable delay in a non-linear\nmanner, thereby heightening system vulnerability. The proposed criteria can be\ngeneralised to a wide range of robot-mediated interactions and serve as design\nguidelines for stable remote dyadic systems. Experiments with robots performing\nhuman-like movements further illustrate the correlation between stability and\nmotor performance. The findings of this paper suggest the prerequisites for\neffective delay-compensation strategies.\n","authors":["Mingtian Du","Suhas Raghavendra Kulkarni","Simone Kager","Domenico Campolo"],"pdf_url":"https://arxiv.org/pdf/2510.14511v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00060v2","updated":"2025-10-26T14:50:22Z","published":"2025-08-25T17:54:24Z","title":"Correspondence-Free, Function-Based Sim-to-Real Learning for Deformable\n  Surface Control","summary":"  This paper presents a correspondence-free, function-based sim-to-real\nlearning method for controlling deformable freeform surfaces. Unlike\ntraditional sim-to-real transfer methods that strongly rely on marker points\nwith full correspondences, our approach simultaneously learns a deformation\nfunction space and a confidence map -- both parameterized by a neural network\n-- to map simulated shapes to their real-world counterparts. As a result, the\nsim-to-real learning can be conducted by input from either a 3D scanner as\npoint clouds (without correspondences) or a motion capture system as marker\npoints (tolerating missed markers). The resultant sim-to-real transfer can be\nseamlessly integrated into a neural network-based computational pipeline for\ninverse kinematics and shape control. We demonstrate the versatility and\nadaptability of our method on both vision devices and across four pneumatically\nactuated soft robots: a deformable membrane, a robotic mannequin, and two soft\nmanipulators.\n","authors":["Yingjun Tian","Guoxin Fang","Renbo Su","Aoran Lyu","Neelotpal Dutta","Weiming Wang","Simeon Gill","Andrew Weightman","Charlie C. L. Wang"],"pdf_url":"https://arxiv.org/pdf/2509.00060v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.08935"},{"id":"http://arxiv.org/abs/2510.22699v1","updated":"2025-10-26T14:48:34Z","published":"2025-10-26T14:48:34Z","title":"RL-AVIST: Reinforcement Learning for Autonomous Visual Inspection of\n  Space Targets","summary":"  The growing need for autonomous on-orbit services such as inspection,\nmaintenance, and situational awareness calls for intelligent spacecraft capable\nof complex maneuvers around large orbital targets. Traditional control systems\noften fall short in adaptability, especially under model uncertainties,\nmulti-spacecraft configurations, or dynamically evolving mission contexts. This\npaper introduces RL-AVIST, a Reinforcement Learning framework for Autonomous\nVisual Inspection of Space Targets. Leveraging the Space Robotics Bench (SRB),\nwe simulate high-fidelity 6-DOF spacecraft dynamics and train agents using\nDreamerV3, a state-of-the-art model-based RL algorithm, with PPO and TD3 as\nmodel-free baselines. Our investigation focuses on 3D proximity maneuvering\ntasks around targets such as the Lunar Gateway and other space assets. We\nevaluate task performance under two complementary regimes: generalized agents\ntrained on randomized velocity vectors, and specialized agents trained to\nfollow fixed trajectories emulating known inspection orbits. Furthermore, we\nassess the robustness and generalization of policies across multiple spacecraft\nmorphologies and mission domains. Results demonstrate that model-based RL\noffers promising capabilities in trajectory fidelity, and sample efficiency,\npaving the way for scalable, retrainable control solutions for future space\noperations\n","authors":["Matteo El-Hariry","Andrej Orsula","Matthieu Geist","Miguel Olivares-Mendez"],"pdf_url":"https://arxiv.org/pdf/2510.22699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22680v1","updated":"2025-10-26T13:49:38Z","published":"2025-10-26T13:49:38Z","title":"Uncertainty-Aware Autonomous Vehicles: Predicting the Road Ahead","summary":"  Autonomous Vehicle (AV) perception systems have advanced rapidly in recent\nyears, providing vehicles with the ability to accurately interpret their\nenvironment. Perception systems remain susceptible to errors caused by\noverly-confident predictions in the case of rare events or out-of-sample data.\nThis study equips an autonomous vehicle with the ability to 'know when it is\nuncertain', using an uncertainty-aware image classifier as part of the AV\nsoftware stack. Specifically, the study exploits the ability of Random-Set\nNeural Networks (RS-NNs) to explicitly quantify prediction uncertainty. Unlike\ntraditional CNNs or Bayesian methods, RS-NNs predict belief functions over sets\nof classes, allowing the system to identify and signal uncertainty clearly in\nnovel or ambiguous scenarios. The system is tested in a real-world autonomous\nracing vehicle software stack, with the RS-NN classifying the layout of the\nroad ahead and providing the associated uncertainty of the prediction.\nPerformance of the RS-NN under a range of road conditions is compared against\ntraditional CNN and Bayesian neural networks, with the RS-NN achieving\nsignificantly higher accuracy and superior uncertainty calibration. This\nintegration of RS-NNs into Robot Operating System (ROS)-based vehicle control\npipeline demonstrates that predictive uncertainty can dynamically modulate\nvehicle speed, maintaining high-speed performance under confident predictions\nwhile proactively improving safety through speed reductions in uncertain\nscenarios. These results demonstrate the potential of uncertainty-aware neural\nnetworks - in particular RS-NNs - as a practical solution for safer and more\nrobust autonomous driving.\n","authors":["Shireen Kudukkil Manchingal","Armand Amaritei","Mihir Gohad","Maryam Sultana","Julian F. P. Kooij","Fabio Cuzzolin","Andrew Bradley"],"pdf_url":"https://arxiv.org/pdf/2510.22680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14305v3","updated":"2025-10-26T11:20:38Z","published":"2025-04-19T14:03:57Z","title":"Adversarial Locomotion and Motion Imitation for Humanoid Policy Learning","summary":"  Humans exhibit diverse and expressive whole-body movements. However,\nattaining human-like whole-body coordination in humanoid robots remains\nchallenging, as conventional approaches that mimic whole-body motions often\nneglect the distinct roles of upper and lower body. This oversight leads to\ncomputationally intensive policy learning and frequently causes robot\ninstability and falls during real-world execution. To address these issues, we\npropose Adversarial Locomotion and Motion Imitation (ALMI), a novel framework\nthat enables adversarial policy learning between upper and lower body.\nSpecifically, the lower body aims to provide robust locomotion capabilities to\nfollow velocity commands while the upper body tracks various motions.\nConversely, the upper-body policy ensures effective motion tracking when the\nrobot executes velocity-based movements. Through iterative updates, these\npolicies achieve coordinated whole-body control, which can be extended to\nloco-manipulation tasks with teleoperation systems. Extensive experiments\ndemonstrate that our method achieves robust locomotion and precise motion\ntracking in both simulation and on the full-size Unitree H1 robot.\nAdditionally, we release a large-scale whole-body motion control dataset\nfeaturing high-quality episodic trajectories from MuJoCo simulations deployable\non real robots. The project page is https://almi-humanoid.github.io.\n","authors":["Jiyuan Shi","Xinzhe Liu","Dewei Wang","Ouyang Lu","Sören Schwertfeger","Chi Zhang","Fuchun Sun","Chenjia Bai","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2504.14305v3.pdf","comment":"NeurIPS 2025. Code: https://github.com/TeleHuman/ALMI-Open, Dataset:\n  https://huggingface.co/datasets/TeleEmbodied/ALMI-X"},{"id":"http://arxiv.org/abs/2507.21545v2","updated":"2025-10-26T10:02:59Z","published":"2025-07-29T07:20:49Z","title":"Pretraining a Unified PDDL Domain from Real-World Demonstrations for\n  Generalizable Robot Task Planning","summary":"  Robotic task planning in real-world environments requires reasoning over\nimplicit constraints from language and vision. While LLMs and VLMs offer strong\npriors, they struggle with long-horizon structure and symbolic grounding.\nExisting methods that combine LLMs with symbolic planning often rely on\nhandcrafted or narrow domains, limiting generalization. We propose UniDomain, a\nframework that pre-trains a PDDL domain from robot manipulation demonstrations\nand applies it for online robotic task planning. It extracts atomic domains\nfrom 12,393 manipulation videos to form a unified domain with 3137 operators,\n2875 predicates, and 16481 causal edges. Given a target class of tasks, it\nretrieves relevant atomics from the unified domain and systematically fuses\nthem into high-quality meta-domains to support compositional generalization in\nplanning. Experiments on diverse real-world tasks show that UniDomain solves\ncomplex, unseen tasks in a zero-shot manner, achieving up to 58% higher task\nsuccess and 160% improvement in plan optimality over state-of-the-art LLM and\nLLM-PDDL baselines.\n","authors":["Haoming Ye","Yunxiao Xiao","Cewu Lu","Panpan Cai"],"pdf_url":"https://arxiv.org/pdf/2507.21545v2.pdf","comment":"Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.22600v1","updated":"2025-10-26T09:32:43Z","published":"2025-10-26T09:32:43Z","title":"RoGER-SLAM: A Robust Gaussian Splatting SLAM System for Noisy and\n  Low-light Environment Resilience","summary":"  The reliability of Simultaneous Localization and Mapping (SLAM) is severely\nconstrained in environments where visual inputs suffer from noise and low\nillumination. Although recent 3D Gaussian Splatting (3DGS) based SLAM\nframeworks achieve high-fidelity mapping under clean conditions, they remain\nvulnerable to compounded degradations that degrade mapping and tracking\nperformance. A key observation underlying our work is that the original 3DGS\nrendering pipeline inherently behaves as an implicit low-pass filter,\nattenuating high-frequency noise but also risking over-smoothing. Building on\nthis insight, we propose RoGER-SLAM, a robust 3DGS SLAM system tailored for\nnoise and low-light resilience. The framework integrates three innovations: a\nStructure-Preserving Robust Fusion (SP-RoFusion) mechanism that couples\nrendered appearance, depth, and edge cues; an adaptive tracking objective with\nresidual balancing regularization; and a Contrastive Language-Image Pretraining\n(CLIP)-based enhancement module, selectively activated under compounded\ndegradations to restore semantic and structural fidelity. Comprehensive\nexperiments on Replica, TUM, and real-world sequences show that RoGER-SLAM\nconsistently improves trajectory accuracy and reconstruction quality compared\nwith other 3DGS-SLAM systems, especially under adverse imaging conditions.\n","authors":["Huilin Yin","Zhaolin Yang","Linchuan Zhang","Gerhard Rigoll","Johannes Betz"],"pdf_url":"https://arxiv.org/pdf/2510.22600v1.pdf","comment":"13 pages, 11 figures, under review"},{"id":"http://arxiv.org/abs/2507.12744v3","updated":"2025-10-26T08:49:12Z","published":"2025-07-17T02:59:35Z","title":"ASC-SW: Atrous strip convolution network with sliding windows","summary":"  With the rapid development of lightweight visual neural network\narchitectures, traditional high-performance vision models have undergone\nsignificant compression, enhancing their computational and energy efficiency\nand enabling deployment on resource-constrained edge devices. In order to\nenable the mobile robot to avoid the ground wires, we propose a visual-assisted\nnavigation framework called Atrous Strip Convolution Sliding Window (ASC-SW).\nThis framework compensates for the limitations of traditional light detection\nand range (LiDAR) sensors to detect ground-level obstacles such as wires. A\nlightweight and efficient segmentation model, Atrous Strip Convolution Network\n(ASCnet) was proposed, for detecting deformable linear objects (DLOs). Atrous\nStrip Convolution Spatial Pyramid Pooling (ASCSPP) is designed to extract DLOs\nfeatures effectively. Atrous Strip Convolution is integrated into ASCSPP to\naccurately identify the linear structure of DLOs with low computational cost.\nAdditionally, a Sliding Window (SW) post processing module is proposed to\ndenoise the output in complex environments, improving recognition accuracy.\nASC-SW achieves 75.3% MIoU at 217 FPS on a self-built real world dataset and\nreal-robot experiment was demonstrated that our proposed framework. It can be\nsuccessfully verified on the real-robot on the edge device(Jetson platform) at\nthat were originally inoperable.\n","authors":["Cheng Liu","Fan Zhu","Yifeng Xu","Baoru Huang","Mohd Rizal Arshad"],"pdf_url":"https://arxiv.org/pdf/2507.12744v3.pdf","comment":"The data of model comparsion in chapter 4 need to be modified"},{"id":"http://arxiv.org/abs/2510.22570v1","updated":"2025-10-26T08:03:06Z","published":"2025-10-26T08:03:06Z","title":"Curriculum-Based Iterative Self-Play for Scalable Multi-Drone Racing","summary":"  The coordination of multiple autonomous agents in high-speed, competitive\nenvironments represents a significant engineering challenge. This paper\npresents CRUISE (Curriculum-Based Iterative Self-Play for Scalable Multi-Drone\nRacing), a reinforcement learning framework designed to solve this challenge in\nthe demanding domain of multi-drone racing. CRUISE overcomes key scalability\nlimitations by synergistically combining a progressive difficulty curriculum\nwith an efficient self-play mechanism to foster robust competitive behaviors.\nValidated in high-fidelity simulation with realistic quadrotor dynamics, the\nresulting policies significantly outperform both a standard reinforcement\nlearning baseline and a state-of-the-art game-theoretic planner. CRUISE\nachieves nearly double the planner's mean racing speed, maintains high success\nrates, and demonstrates robust scalability as agent density increases. Ablation\nstudies confirm that the curriculum structure is the critical component for\nthis performance leap. By providing a scalable and effective training\nmethodology, CRUISE advances the development of autonomous systems for dynamic,\ncompetitive tasks and serves as a blueprint for future real-world deployment.\n","authors":["Onur Akgün"],"pdf_url":"https://arxiv.org/pdf/2510.22570v1.pdf","comment":"13 pages, 5 figures. This paper is currently under review at the\n  journal Engineering Applications of Artificial Intelligence. Supplementary\n  video: https://drive.google.com/file/d/1k7necen2DgIxaYT2alKK8-b20sE_AyDA/view\n  Source code and models: https://doi.org/10.5281/zenodo.17256943"},{"id":"http://arxiv.org/abs/2510.22568v1","updated":"2025-10-26T07:59:44Z","published":"2025-10-26T07:59:44Z","title":"SPIRAL: Self-Play Incremental Racing Algorithm for Learning in\n  Multi-Drone Competitions","summary":"  This paper introduces SPIRAL (Self-Play Incremental Racing Algorithm for\nLearning), a novel approach for training autonomous drones in multi-agent\nracing competitions. SPIRAL distinctively employs a self-play mechanism to\nincrementally cultivate complex racing behaviors within a challenging, dynamic\nenvironment. Through this self-play core, drones continuously compete against\nincreasingly proficient versions of themselves, naturally escalating the\ndifficulty of competitive interactions. This progressive learning journey\nguides agents from mastering fundamental flight control to executing\nsophisticated cooperative multi-drone racing strategies. Our method is designed\nfor versatility, allowing integration with any state-of-the-art Deep\nReinforcement Learning (DRL) algorithms within its self-play framework.\nSimulations demonstrate the significant advantages of SPIRAL and benchmark the\nperformance of various DRL algorithms operating within it. Consequently, we\ncontribute a versatile, scalable, and self-improving learning framework to the\nfield of autonomous drone racing. SPIRAL's capacity to autonomously generate\nappropriate and escalating challenges through its self-play dynamic offers a\npromising direction for developing robust and adaptive racing strategies in\nmulti-agent environments. This research opens new avenues for enhancing the\nperformance and reliability of autonomous racing drones in increasingly complex\nand competitive scenarios.\n","authors":["Onur Akgün"],"pdf_url":"https://arxiv.org/pdf/2510.22568v1.pdf","comment":"\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2501.02630v2","updated":"2025-10-26T04:45:05Z","published":"2025-01-05T19:21:16Z","title":"Soft and Compliant Contact-Rich Hair Manipulation and Care","summary":"  Hair care robots can help address labor shortages in elderly care while\nenabling those with limited mobility to maintain their hair-related identity.\nWe present MOE-Hair, a soft robot system that performs three hair-care tasks:\nhead patting, finger combing, and hair grasping. The system features a\ntendon-driven soft robot end-effector (MOE) with a wrist-mounted RGBD camera,\nleveraging both mechanical compliance for safety and visual force sensing\nthrough deformation. In testing with a force-sensorized mannequin head, MOE\nachieved comparable hair-grasping effectiveness while applying significantly\nless force than rigid grippers. Our novel force estimation method combines\nvisual deformation data and tendon tensions from actuators to infer applied\nforces, reducing sensing errors by up to 60.1% and 20.3% compared to actuator\ncurrent load-only and depth image-only baselines, respectively. A user study\nwith 12 participants demonstrated statistically significant preferences for\nMOE-Hair over a baseline system in terms of comfort, effectiveness, and\nappropriate force application. These results demonstrate the unique advantages\nof soft robots in contact-rich hair-care tasks, while highlighting the\nimportance of precise force control despite the inherent compliance of the\nsystem.\n","authors":["Uksang Yoo","Nathaniel Dennler","Eliot Xing","Maja Matarić","Stefanos Nikolaidis","Jeffrey Ichnowski","Jean Oh"],"pdf_url":"https://arxiv.org/pdf/2501.02630v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.25756v2","updated":"2025-10-26T04:37:16Z","published":"2025-09-30T04:21:20Z","title":"SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies\n  via Velocity-Reparameterized Sequential Modeling","summary":"  Training expressive flow-based policies with off-policy reinforcement\nlearning is notoriously unstable due to gradient pathologies in the multi-step\naction sampling process. We trace this instability to a fundamental connection:\nthe flow rollout is algebraically equivalent to a residual recurrent\ncomputation, making it susceptible to the same vanishing and exploding\ngradients as RNNs. To address this, we reparameterize the velocity network\nusing principles from modern sequential models, introducing two stable\narchitectures: Flow-G, which incorporates a gated velocity, and Flow-T, which\nutilizes a decoded velocity. We then develop a practical SAC-based algorithm,\nenabled by a noise-augmented rollout, that facilitates direct end-to-end\ntraining of these policies. Our approach supports both from-scratch and\noffline-to-online learning and achieves state-of-the-art performance on\ncontinuous control and robotic manipulation benchmarks, eliminating the need\nfor common workarounds like policy distillation or surrogate objectives.\n","authors":["Yixian Zhang","Shu'ang Yu","Tonghe Zhang","Mo Guang","Haojia Hui","Kaiwen Long","Yu Wang","Chao Yu","Wenbo Ding"],"pdf_url":"https://arxiv.org/pdf/2509.25756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22529v1","updated":"2025-10-26T04:31:01Z","published":"2025-10-26T04:31:01Z","title":"Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection\n  Method Under Perceptual Aliasing","summary":"  Loop closure is critical in Simultaneous Localization and Mapping (SLAM)\nsystems to reduce accumulative drift and ensure global mapping consistency.\nHowever, conventional methods struggle in perceptually aliased environments,\nsuch as narrow pipes, due to vector quantization, feature sparsity, and\nrepetitive textures, while existing solutions often incur high computational\ncosts. This paper presents Bag-of-Word-Groups (BoWG), a novel loop closure\ndetection method that achieves superior precision-recall, robustness, and\ncomputational efficiency. The core innovation lies in the introduction of word\ngroups, which captures the spatial co-occurrence and proximity of visual words\nto construct an online dictionary. Additionally, drawing inspiration from\nprobabilistic transition models, we incorporate temporal consistency directly\ninto similarity computation with an adaptive scheme, substantially improving\nprecision-recall performance. The method is further strengthened by a feature\ndistribution analysis module and dedicated post-verification mechanisms. To\nevaluate the effectiveness of our method, we conduct experiments on both public\ndatasets and a confined-pipe dataset we constructed. Results demonstrate that\nBoWG surpasses state-of-the-art methods, including both traditional and\nlearning-based approaches, in terms of precision-recall and computational\nefficiency. Our approach also exhibits excellent scalability, achieving an\naverage processing time of 16 ms per image across 17,565 images in the\nBicocca25b dataset.\n","authors":["Xiang Fei","Tina Tian","Howie Choset","Lu Li"],"pdf_url":"https://arxiv.org/pdf/2510.22529v1.pdf","comment":"This paper has been accepted by IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2510.22524v1","updated":"2025-10-26T04:18:40Z","published":"2025-10-26T04:18:40Z","title":"Ant-inspired Walling Strategies for Scalable Swarm Separation:\n  Reinforcement Learning Approaches Based on Finite State Machines","summary":"  In natural systems, emergent structures often arise to balance competing\ndemands. Army ants, for example, form temporary \"walls\" that prevent\ninterference between foraging trails. Inspired by this behavior, we developed\ntwo decentralized controllers for heterogeneous robotic swarms to maintain\nspatial separation while executing concurrent tasks. The first is a\nfinite-state machine (FSM)-based controller that uses encounter-triggered\ntransitions to create rigid, stable walls. The second integrates FSM states\nwith a Deep Q-Network (DQN), dynamically optimizing separation through emergent\n\"demilitarized zones.\" In simulation, both controllers reduce mixing between\nsubgroups, with the DQN-enhanced controller improving adaptability and reducing\nmixing by 40-50% while achieving faster convergence.\n","authors":["Shenbagaraj Kannapiran","Elena Oikonomou","Albert Chu","Spring Berman","Theodore P. Pavlic"],"pdf_url":"https://arxiv.org/pdf/2510.22524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.20414v2","updated":"2025-10-26T04:10:24Z","published":"2025-09-24T09:06:41Z","title":"SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and\n  Self-Reflective Agent","summary":"  Indoor scene synthesis has become increasingly important with the rise of\nEmbodied AI, which requires 3D environments that are not only visually\nrealistic but also physically plausible and functionally diverse. While recent\napproaches have advanced visual fidelity, they often remain constrained to\nfixed scene categories, lack sufficient object-level detail and physical\nconsistency, and struggle to align with complex user instructions. In this\nwork, we present SceneWeaver, a reflective agentic framework that unifies\ndiverse scene synthesis paradigms through tool-based iterative refinement. At\nits core, SceneWeaver employs a language model-based planner to select from a\nsuite of extensible scene generation tools, ranging from data-driven generative\nmodels to visual- and LLM-based methods, guided by self-evaluation of physical\nplausibility, visual realism, and semantic alignment with user input. This\nclosed-loop reason-act-reflect design enables the agent to identify semantic\ninconsistencies, invoke targeted tools, and update the environment over\nsuccessive iterations. Extensive experiments on both common and open-vocabulary\nroom types demonstrate that SceneWeaver not only outperforms prior methods on\nphysical, visual, and semantic metrics, but also generalizes effectively to\ncomplex scenes with diverse instructions, marking a step toward general-purpose\n3D environment generation. Project website: https://scene-weaver.github.io/.\n","authors":["Yandan Yang","Baoxiong Jia","Shujie Zhang","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2509.20414v2.pdf","comment":"Accepted by NeurIPS 2025, 26 pages"},{"id":"http://arxiv.org/abs/2410.15068v4","updated":"2025-10-26T03:39:20Z","published":"2024-10-19T11:11:58Z","title":"A Cycle Ride to HDR: Semantics Aware Self-Supervised Framework for\n  Unpaired LDR-to-HDR Image Reconstruction","summary":"  Reconstruction of High Dynamic Range (HDR) from Low Dynamic Range (LDR)\nimages is an important computer vision task. There is a significant amount of\nresearch utilizing both conventional non-learning methods and modern\ndata-driven approaches, focusing on using both single-exposed and multi-exposed\nLDR for HDR image reconstruction. However, most current state-of-the-art\nmethods require high-quality paired {LDR;HDR} datasets with limited literature\nuse of unpaired datasets, that is, methods that learn the LDR-HDR mapping\nbetween domains. This paper proposes CycleHDR, a method that integrates\nself-supervision into a modified semantic- and cycle-consistent adversarial\narchitecture that utilizes unpaired LDR and HDR datasets for training. Our\nmethod introduces novel artifact- and exposure-aware generators to address\nvisual artifact removal. It also puts forward an encoder and loss to address\nsemantic consistency, another under-explored topic. CycleHDR is the first to\nuse semantic and contextual awareness for the LDR-HDR reconstruction task in a\nself-supervised setup. The method achieves state-of-the-art performance across\nseveral benchmark datasets and reconstructs high-quality HDR images. The\nofficial website of this work is available at:\nhttps://github.com/HrishavBakulBarua/Cycle-HDR\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","Lemuel Lai En Che","Abhinav Dhall","KokSheik Wong","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2410.15068v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22504v1","updated":"2025-10-26T03:00:02Z","published":"2025-10-26T03:00:02Z","title":"On Steerability Factors for Growing Vine Robots","summary":"  Vine robots extend their tubular bodies by everting material from the tip,\nenabling navigation in complex environments with a minimalist soft body.\nDespite their promise for field applications, especially in the urban search\nand rescue domain, performance is constrained by the weight of attached sensors\nor tools, as well as other design and control choices. This work investigates\nhow tip load, pressure, length, diameter, and fabrication method shape vine\nrobot steerability--the ability to maneuver with controlled curvature--for\nrobots that steer with series pouch motor-style pneumatic actuators. We conduct\ntwo groups of experiments: (1) studying tip load, chamber pressure, length, and\ndiameter in a robot supporting itself against gravity, and (2) studying\nfabrication method and ratio of actuator to chamber pressure in a robot\nsupported on the ground. Results show that steerability decreases with\nincreasing tip load, is best at moderate chamber pressure, increases with\nlength, and is largely unaffected by diameter. Robots with actuators attached\non their exterior begin curving at low pressure ratios, but curvature saturates\nat high pressure ratios; those with actuators integrated into the robot body\nrequire higher pressure ratios to begin curving but achieve higher curvature\noverall. We demonstrate that robots optimized with these principles outperform\nthose with ad hoc parameters in a mobility task that involves maximizing upward\nand horizontal curvatures.\n","authors":["Ciera McFarland","Antonio Alvarez","Sarah Taher","Nathaniel Hanson","Margaret McGuinness"],"pdf_url":"https://arxiv.org/pdf/2510.22504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13457v2","updated":"2025-10-26T02:13:53Z","published":"2025-01-23T08:15:52Z","title":"Zero-Shot Trajectory Planning for Signal Temporal Logic Tasks","summary":"  Signal Temporal Logic (STL) is a powerful specification language for\ndescribing complex temporal behaviors of continuous signals, making it\nwell-suited for high-level robotic task descriptions. However, generating\nexecutable plans for STL tasks is challenging, as it requires consideration of\nthe coupling between the task specification and the system dynamics. Existing\napproaches either follow a model-based setting that explicitly requires\nknowledge of the system dynamics or adopt a task-oriented data-driven approach\nto learn plans for specific tasks. In this work, we address the problem of\ngenerating executable STL plans for systems with unknown dynamics. We propose a\nhierarchical planning framework that enables zero-shot generalization to new\nSTL tasks by leveraging only task-agnostic trajectory data during offline\ntraining. The framework consists of three key components: (i) decomposing the\nSTL specification into several progresses and time constraints, (ii) searching\nfor timed waypoints that satisfy all progresses under time constraints, and\n(iii) generating trajectory segments using a pre-trained diffusion model and\nstitching them into complete trajectories. We formally prove that our method\nguarantees STL satisfaction, and simulation results demonstrate its\neffectiveness in generating dynamically feasible trajectories across diverse\nlong-horizon STL tasks.\n","authors":["Ruijia Liu","Ancheng Hou","Xiao Yu","Xiang Yin"],"pdf_url":"https://arxiv.org/pdf/2501.13457v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22465v1","updated":"2025-10-26T00:46:17Z","published":"2025-10-26T00:46:17Z","title":"Forward Kinematics Solution For A General Stewart Platform Through\n  Iteration Based Simulation","summary":"  This paper presents a method to generate feasible, unique forward-kinematic\nsolutions for a general Stewart platform. This is done by using inverse\nkinematics to obtain valid workspace data and corresponding actuator lengths\nfor the moving platform. For parallel kinematic machines, such as the Stewart\nPlatform, inverse kinematics are straight forward, but the forward kinematics\nare complex and generates multiple solutions due to the closed loop structure\nof the kinematic links. In this research, a simple iterative algorithm has been\nused employing modified Denavit-Hartenberg convention. The outcome is\nencouraging as this method generates a single feasible forward kinematic\nsolution for each valid pose with the solved DH parameters and unlike earlier\nforward kinematics solutions, this unique solution does not need to be manually\nverified. Therefore, the forward kinematic solutions can be used directly for\nfurther calculations without the need for manual pose verification. This\ncapability is essential for the six degree of freedom materials testing system\ndeveloped by the authors in their laboratory. The developed system is aimed at\ncharacterizing additively manufactured materials under complex combined\nmultiple loading conditions. The material characterization is done by enabling\nhigh precision force control on the moving platform via in situ calibration of\nthe as-built kinematics of the Stewart Gough Platform.\n","authors":["Sourabh Karmakar","Cameron J. Turner"],"pdf_url":"https://arxiv.org/pdf/2510.22465v1.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2510.22712v1","updated":"2025-10-26T15:12:02Z","published":"2025-10-26T15:12:02Z","title":"Step2Motion: Locomotion Reconstruction from Pressure Sensing Insoles","summary":"  Human motion is fundamentally driven by continuous physical interaction with\nthe environment. Whether walking, running, or simply standing, the forces\nexchanged between our feet and the ground provide crucial insights for\nunderstanding and reconstructing human movement. Recent advances in wearable\ninsole devices offer a compelling solution for capturing these forces in\ndiverse, real-world scenarios. Sensor insoles pose no constraint on the users'\nmotion (unlike mocap suits) and are unaffected by line-of-sight limitations (in\ncontrast to optical systems). These qualities make sensor insoles an ideal\nchoice for robust, unconstrained motion capture, particularly in outdoor\nenvironments. Surprisingly, leveraging these devices with recent motion\nreconstruction methods remains largely unexplored. Aiming to fill this gap, we\npresent Step2Motion, the first approach to reconstruct human locomotion from\nmulti-modal insole sensors. Our method utilizes pressure and inertial\ndata-accelerations and angular rates-captured by the insoles to reconstruct\nhuman motion. We evaluate the effectiveness of our approach across a range of\nexperiments to show its versatility for diverse locomotion styles, from simple\nones like walking or jogging up to moving sideways, on tiptoes, slightly\ncrouching, or dancing.\n","authors":["Jose Luis Ponton","Eduardo Alvarado","Lin Geng Foo","Nuria Pelechano","Carlos Andujar","Marc Habermann"],"pdf_url":"https://arxiv.org/pdf/2510.22712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22632v1","updated":"2025-10-26T11:28:50Z","published":"2025-10-26T11:28:50Z","title":"Environment-aware Motion Matching","summary":"  Interactive applications demand believable characters that respond naturally\nto dynamic environments. Traditional character animation techniques often\nstruggle to handle arbitrary situations, leading to a growing trend of\ndynamically selecting motion-captured animations based on predefined features.\nWhile Motion Matching has proven effective for locomotion by aligning to target\ntrajectories, animating environment interactions and crowd behaviors remains\nchallenging due to the need to consider surrounding elements. Existing\napproaches often involve manual setup or lack the naturalism of motion capture.\nFurthermore, in crowd animation, body animation is frequently treated as a\nseparate process from trajectory planning, leading to inconsistencies between\nbody pose and root motion. To address these limitations, we present\nEnvironment-aware Motion Matching, a novel real-time system for full-body\ncharacter animation that dynamically adapts to obstacles and other agents,\nemphasizing the bidirectional relationship between pose and trajectory. In a\npreprocessing step, we extract shape, pose, and trajectory features from a\nmotion capture database. At runtime, we perform an efficient search that\nmatches user input and current pose while penalizing collisions with a dynamic\nenvironment. Our method allows characters to naturally adjust their pose and\ntrajectory to navigate crowded scenes.\n","authors":["Jose Luis Ponton","Sheldon Andrews","Carlos Andujar","Nuria Pelechano"],"pdf_url":"https://arxiv.org/pdf/2510.22632v1.pdf","comment":"Published in ACM TOG and presented in SIGGRAPH ASIA 2025. Project\n  webpage: https://upc-virvig.github.io/Environment-aware-Motion-Matching/"},{"id":"http://arxiv.org/abs/2412.11762v2","updated":"2025-10-26T05:03:24Z","published":"2024-12-16T13:26:52Z","title":"GS-ProCams: Gaussian Splatting-based Projector-Camera Systems","summary":"  We present GS-ProCams, the first Gaussian Splatting-based framework for\nprojector-camera systems (ProCams). GS-ProCams is not only view-agnostic but\nalso significantly enhances the efficiency of projection mapping (PM) that\nrequires establishing geometric and radiometric mappings between the projector\nand the camera. Previous CNN-based ProCams are constrained to a specific\nviewpoint, limiting their applicability to novel perspectives. In contrast,\nNeRF-based ProCams support view-agnostic projection mapping, however, they\nrequire an additional co-located light source and demand significant\ncomputational and memory resources. To address this issue, we propose\nGS-ProCams that employs 2D Gaussian for scene representations, and enables\nefficient view-agnostic ProCams applications. In particular, we explicitly\nmodel the complex geometric and photometric mappings of ProCams using projector\nresponses, the projection surface's geometry and materials represented by\nGaussians, and the global illumination component. Then, we employ\ndifferentiable physically-based rendering to jointly estimate them from\ncaptured multi-view projections. Compared to state-of-the-art NeRF-based\nmethods, our GS-ProCams eliminates the need for additional devices, achieving\nsuperior ProCams simulation quality. It also uses only 1/10 of the GPU memory\nfor training and is 900 times faster in inference speed. Please refer to our\nproject page for the code and dataset:\nhttps://realqingyue.github.io/GS-ProCams/.\n","authors":["Qingyue Deng","Jijiang Li","Haibin Ling","Bingyao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.20414v2","updated":"2025-10-26T04:10:24Z","published":"2025-09-24T09:06:41Z","title":"SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and\n  Self-Reflective Agent","summary":"  Indoor scene synthesis has become increasingly important with the rise of\nEmbodied AI, which requires 3D environments that are not only visually\nrealistic but also physically plausible and functionally diverse. While recent\napproaches have advanced visual fidelity, they often remain constrained to\nfixed scene categories, lack sufficient object-level detail and physical\nconsistency, and struggle to align with complex user instructions. In this\nwork, we present SceneWeaver, a reflective agentic framework that unifies\ndiverse scene synthesis paradigms through tool-based iterative refinement. At\nits core, SceneWeaver employs a language model-based planner to select from a\nsuite of extensible scene generation tools, ranging from data-driven generative\nmodels to visual- and LLM-based methods, guided by self-evaluation of physical\nplausibility, visual realism, and semantic alignment with user input. This\nclosed-loop reason-act-reflect design enables the agent to identify semantic\ninconsistencies, invoke targeted tools, and update the environment over\nsuccessive iterations. Extensive experiments on both common and open-vocabulary\nroom types demonstrate that SceneWeaver not only outperforms prior methods on\nphysical, visual, and semantic metrics, but also generalizes effectively to\ncomplex scenes with diverse instructions, marking a step toward general-purpose\n3D environment generation. Project website: https://scene-weaver.github.io/.\n","authors":["Yandan Yang","Baoxiong Jia","Shujie Zhang","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2509.20414v2.pdf","comment":"Accepted by NeurIPS 2025, 26 pages"},{"id":"http://arxiv.org/abs/2410.15068v4","updated":"2025-10-26T03:39:20Z","published":"2024-10-19T11:11:58Z","title":"A Cycle Ride to HDR: Semantics Aware Self-Supervised Framework for\n  Unpaired LDR-to-HDR Image Reconstruction","summary":"  Reconstruction of High Dynamic Range (HDR) from Low Dynamic Range (LDR)\nimages is an important computer vision task. There is a significant amount of\nresearch utilizing both conventional non-learning methods and modern\ndata-driven approaches, focusing on using both single-exposed and multi-exposed\nLDR for HDR image reconstruction. However, most current state-of-the-art\nmethods require high-quality paired {LDR;HDR} datasets with limited literature\nuse of unpaired datasets, that is, methods that learn the LDR-HDR mapping\nbetween domains. This paper proposes CycleHDR, a method that integrates\nself-supervision into a modified semantic- and cycle-consistent adversarial\narchitecture that utilizes unpaired LDR and HDR datasets for training. Our\nmethod introduces novel artifact- and exposure-aware generators to address\nvisual artifact removal. It also puts forward an encoder and loss to address\nsemantic consistency, another under-explored topic. CycleHDR is the first to\nuse semantic and contextual awareness for the LDR-HDR reconstruction task in a\nself-supervised setup. The method achieves state-of-the-art performance across\nseveral benchmark datasets and reconstructs high-quality HDR images. The\nofficial website of this work is available at:\nhttps://github.com/HrishavBakulBarua/Cycle-HDR\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","Lemuel Lai En Che","Abhinav Dhall","KokSheik Wong","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2410.15068v4.pdf","comment":null}]},"2025-10-25T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2510.22344v1","updated":"2025-10-25T15:59:33Z","published":"2025-10-25T15:59:33Z","title":"FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented\n  Generation","summary":"  While Retrieval-Augmented Generation (RAG) mitigates hallucination and\nknowledge staleness in Large Language Models (LLMs), existing frameworks often\nfalter on complex, multi-hop queries that require synthesizing information from\ndisparate sources. Current advanced RAG methods, employing iterative or\nadaptive strategies, lack a robust mechanism to systematically identify and\nfill evidence gaps, often propagating noise or failing to gather a\ncomprehensive context. We introduce FAIR-RAG, a novel agentic framework that\ntransforms the standard RAG pipeline into a dynamic, evidence-driven reasoning\nprocess. At its core is an Iterative Refinement Cycle governed by a module we\nterm Structured Evidence Assessment (SEA). The SEA acts as an analytical gating\nmechanism: it deconstructs the initial query into a checklist of required\nfindings and audits the aggregated evidence to identify confirmed facts and,\ncritically, explicit informational gaps. These gaps provide a precise signal to\nan Adaptive Query Refinement agent, which generates new, targeted sub-queries\nto retrieve missing information. This cycle repeats until the evidence is\nverified as sufficient, ensuring a comprehensive context for a final, strictly\nfaithful generation. We conducted experiments on challenging multi-hop QA\nbenchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified\nexperimental setup, FAIR-RAG significantly outperforms strong baselines. On\nHotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3\npoints over the strongest iterative baseline -- establishing a new\nstate-of-the-art for this class of methods on these benchmarks. Our work\ndemonstrates that a structured, evidence-driven refinement process with\nexplicit gap analysis is crucial for unlocking reliable and accurate reasoning\nin advanced RAG systems for complex, knowledge-intensive tasks.\n","authors":["Mohammad Aghajani Asl","Majid Asgari-Bidhendi","Behrooz Minaei-Bidgoli"],"pdf_url":"https://arxiv.org/pdf/2510.22344v1.pdf","comment":"30 pages, 5 figures, 5 tables. Keywords: Retrieval-Augmented\n  Generation (RAG), Large Language Models (LLMs), Agentic AI, Multi-hop\n  Question Answering, Faithfulness"},{"id":"http://arxiv.org/abs/2509.07759v2","updated":"2025-10-25T13:44:18Z","published":"2025-09-09T13:57:53Z","title":"A Survey of Long-Document Retrieval in the PLM and LLM Era","summary":"  The proliferation of long-form documents presents a fundamental challenge to\ninformation retrieval (IR), as their length, dispersed evidence, and complex\nstructures demand specialized methods beyond standard passage-level techniques.\nThis survey provides the first comprehensive treatment of long-document\nretrieval (LDR), consolidating methods, challenges, and applications across\nthree major eras. We systematize the evolution from classical lexical and early\nneural models to modern pre-trained (PLM) and large language models (LLMs),\ncovering key paradigms like passage aggregation, hierarchical encoding,\nefficient attention, and the latest LLM-driven re-ranking and retrieval\ntechniques. Beyond the models, we review domain-specific applications,\nspecialized evaluation resources, and outline critical open challenges such as\nefficiency trade-offs, multimodal alignment, and faithfulness. This survey aims\nto provide both a consolidated reference and a forward-looking agenda for\nadvancing long-document retrieval in the era of foundation models.\n","authors":["Minghan Li","Miyang Luo","Tianrui Lv","Yishuai Zhang","Siqi Zhao","Ercong Nie","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2509.07759v2.pdf","comment":"32 pages, 6 figures"},{"id":"http://arxiv.org/abs/2509.07794v2","updated":"2025-10-25T13:13:22Z","published":"2025-09-09T14:31:11Z","title":"Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey","summary":"  Modern information retrieval (IR) must reconcile short, ambiguous queries\nwith increasingly diverse and dynamic corpora. Query expansion (QE) remains\ncentral to alleviating vocabulary mismatch, yet the design space has shifted\nwith pre-trained and large language models (PLMs, LLMs). In this survey, we\norganize recent work along four complementary dimensions: the point of\ninjection (implicit/embedding vs. selection-based explicit), grounding and\ninteraction (from zero-grounding prompts to multi-round retrieve-expand loops),\nlearning and alignment (SFT/PEFT/DPO), and knowledge-graph integration. A\nmodel-centric taxonomy is also outlined, spanning encoder-only,\nencoder-decoder, decoder-only, instruction-tuned, and domain or multilingual\nvariants, with affordances for QE such as contextual disambiguation,\ncontrollable generation, and zero-shot or few-shot reasoning. Practice-oriented\nguidance specifies where neural QE helps most: first-stage retrieval,\nmulti-query fusion, re-ranking, and retrieval-augmented generation (RAG). The\nsurvey compares traditional and neural QE across seven aspects and maps\napplications in web search, biomedicine, e-commerce, open-domain question\nanswering/RAG, conversational and code search, and cross-lingual settings. The\nsurvey concludes with an agenda focused on reliable, safe, efficient, and\nadaptive QE, offering a principled blueprint for deploying and combining\ntechniques under real-world constraints.\n","authors":["Minghan Li","Xinxuan Lv","Junjie Zou","Tongna Chen","Chao Zhang","Suchao An","Ercong Nie","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2509.07794v2.pdf","comment":"36 pages,3 figures,3 tables"},{"id":"http://arxiv.org/abs/2504.06714v3","updated":"2025-10-25T12:48:03Z","published":"2025-04-09T09:15:37Z","title":"Unifying Search and Recommendation with Dual-View Representation\n  Learning in a Generative Paradigm","summary":"  Recommender systems and search engines serve as foundational elements of\nonline platforms, with the former delivering information proactively and the\nlatter enabling users to seek information actively. Unifying both tasks in a\nshared model is promising since it can enhance user modeling and item\nunderstanding. Previous approaches mainly follow a discriminative paradigm,\nutilizing shared encoders to process input features and task-specific heads to\nperform each task. However, this paradigm encounters two key challenges:\ngradient conflict and manual design complexity. From the information theory\nperspective, these challenges potentially both stem from the same issue -- low\nmutual information between the input features and task-specific outputs during\nthe optimization process.\n  To tackle these issues, we propose GenSR, a novel generative paradigm for\nunifying search and recommendation (S&R), which leverages task-specific prompts\nto partition the model's parameter space into subspaces, thereby enhancing\nmutual information. To construct effective subspaces for each task, GenSR first\nprepares informative representations for each subspace and then optimizes both\nsubspaces in one unified model. Specifically, GenSR consists of two main\nmodules: (1) Dual Representation Learning, which independently models\ncollaborative and semantic historical information to derive expressive item\nrepresentations; and (2) S&R Task Unifying, which utilizes contrastive learning\ntogether with instruction tuning to generate task-specific outputs effectively.\nExtensive experiments on two public datasets show GenSR outperforms\nstate-of-the-art methods across S&R tasks. Our work introduces a new generative\nparadigm compared with previous discriminative methods and establishes its\nsuperiority from the mutual information perspective.\n","authors":["Jujia Zhao","Wenjie Wang","Chen Xu","Xiuying Chen","Zhaochun Ren","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2504.06714v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22264v1","updated":"2025-10-25T12:01:46Z","published":"2025-10-25T12:01:46Z","title":"PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text\n  Embedding","summary":"  Patent text embeddings enable prior art search, technology landscaping, and\npatent analysis, yet existing benchmarks inadequately capture patent-specific\nchallenges. We introduce PatenTEB, a comprehensive benchmark comprising 15\ntasks across retrieval, classification, paraphrase, and clustering, with 2.06\nmillion examples. PatenTEB employs domain-stratified splits, domain specific\nhard negative mining, and systematic coverage of asymmetric\nfragment-to-document matching scenarios absent from general embedding\nbenchmarks. We develop the patembed model family through multi-task training,\nspanning 67M to 344M parameters with context lengths up to 4096 tokens.\nExternal validation shows strong generalization: patembed-base achieves\nstate-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445\nprevious best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.\nSystematic ablations reveal that multi-task training improves external\ngeneralization despite minor benchmark costs, and that domain-pretrained\ninitialization provides consistent advantages across task families. All\nresources will be made available at https://github.com/iliass-y/patenteb.\nKeywords: patent retrieval, sentence embeddings, multi-task learning,\nasymmetric retrieval, benchmark evaluation, contrastive learning.\n","authors":["Iliass Ayaou","Denis Cavallucci"],"pdf_url":"https://arxiv.org/pdf/2510.22264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22242v1","updated":"2025-10-25T10:11:29Z","published":"2025-10-25T10:11:29Z","title":"PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search\n  and Reading","summary":"  Large Language Models (LLMs) increasingly serve as research assistants, yet\ntheir reliability in scholarly tasks remains under-evaluated. In this work, we\nintroduce PaperAsk, a benchmark that systematically evaluates LLMs across four\nkey research tasks: citation retrieval, content extraction, paper discovery,\nand claim verification. We evaluate GPT-4o, GPT-5, and Gemini-2.5-Flash under\nrealistic usage conditions-via web interfaces where search operations are\nopaque to the user. Through controlled experiments, we find consistent\nreliability failures: citation retrieval fails in 48-98% of multi-reference\nqueries, section-specific content extraction fails in 72-91% of cases, and\ntopical paper discovery yields F1 scores below 0.32, missing over 60% of\nrelevant literature. Further human analysis attributes these failures to the\nuncontrolled expansion of retrieved context and the tendency of LLMs to\nprioritize semantically relevant text over task instructions. Across basic\ntasks, the LLMs display distinct failure behaviors: ChatGPT often withholds\nresponses rather than risk errors, whereas Gemini produces fluent but\nfabricated answers. To address these issues, we develop lightweight reliability\nclassifiers trained on PaperAsk data to identify unreliable outputs. PaperAsk\nprovides a reproducible and diagnostic framework for advancing the reliability\nevaluation of LLM-based scholarly assistance systems.\n","authors":["Yutao Wu","Xiao Liu","Yunhao Feng","Jiale Ding","Xingjun Ma"],"pdf_url":"https://arxiv.org/pdf/2510.22242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22215v1","updated":"2025-10-25T08:27:37Z","published":"2025-10-25T08:27:37Z","title":"Hybrid-Vector Retrieval for Visually Rich Documents: Combining\n  Single-Vector Efficiency and Multi-Vector Accuracy","summary":"  Retrieval over visually rich documents is essential for tasks such as legal\ndiscovery, scientific search, and enterprise knowledge management. Existing\napproaches fall into two paradigms: single-vector retrieval, which is efficient\nbut coarse, and multi-vector retrieval, which is accurate but computationally\nexpensive. To address this trade-off, we propose HEAVEN, a two-stage\nhybrid-vector framework. In the first stage, HEAVEN efficiently retrieves\ncandidate pages using a single-vector method over Visually-Summarized Pages\n(VS-Pages), which assemble representative visual layouts from multiple pages.\nIn the second stage, it reranks candidates with a multi-vector method while\nfiltering query tokens by linguistic importance to reduce redundant\ncomputations. To evaluate retrieval systems under realistic conditions, we also\nintroduce ViMDOC, the first benchmark for visually rich, multi-document, and\nlong-document retrieval. Across four benchmarks, HEAVEN attains 99.87% of the\nRecall@1 performance of multi-vector models on average while reducing per-query\ncomputation by 99.82%, achieving efficiency and accuracy. Our code and datasets\nare available at: https://github.com/juyeonnn/HEAVEN\n","authors":["Juyeon Kim","Geon Lee","Dongwon Choi","Taeuk Kim","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2510.22215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.17442v2","updated":"2025-10-25T05:48:12Z","published":"2025-09-22T07:32:06Z","title":"WildClaims: Information Access Conversations in the Wild(Chat)","summary":"  The rapid advancement of Large Language Models (LLMs) has transformed\nconversational systems into practical tools used by millions. However, the\nnature and necessity of information retrieval in real-world conversations\nremain largely unexplored, as research has focused predominantly on\ntraditional, explicit information access conversations. The central question\nis: What do real-world information access conversations look like? To this end,\nwe first conduct an observational study on the WildChat dataset, large-scale\nuser-ChatGPT conversations, finding that users' access to information occurs\nimplicitly as check-worthy factual assertions made by the system, even when the\nconversation's primary intent is non-informational, such as creative writing.\nTo enable the systematic study of this phenomenon, we release the WildClaims\ndataset, a novel resource consisting of 121,905 extracted factual claims from\n7,587 utterances in 3,000 WildChat conversations, each annotated for\ncheck-worthiness. Our preliminary analysis of this resource reveals that\nconservatively 18% to 51% of conversations contain check-worthy assertions,\ndepending on the methods employed, and less conservatively, as many as 76% may\ncontain such assertions. This high prevalence underscores the importance of\nmoving beyond the traditional understanding of explicit information access, to\naddress the implicit information access that arises in real-world user-system\nconversations.\n","authors":["Hideaki Joko","Shakiba Amirshahi","Charles L. A. Clarke","Faegheh Hasibi"],"pdf_url":"https://arxiv.org/pdf/2509.17442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00314v2","updated":"2025-10-25T05:28:21Z","published":"2025-05-30T23:54:13Z","title":"FACE: A Fine-grained Reference Free Evaluator for Conversational\n  Recommender Systems","summary":"  A systematic, reliable, and low-cost evaluation of Conversational Recommender\nSystems (CRSs) remains an open challenge. Existing automatic CRS evaluation\nmethods are proven insufficient for evaluating the dynamic nature of\nrecommendation conversations. This work proposes FACE: a Fine-grained,\nAspect-based Conversation Evaluation method that provides evaluation scores for\ndiverse turn and dialogue level qualities of recommendation conversations. FACE\nis reference-free and shows strong correlation with human judgments, achieving\nsystem correlation of 0.9 and turn/dialogue-level of 0.5, outperforming\nstate-of-the-art CRS evaluation methods by a large margin. Additionally, unlike\nexisting LLM-based methods that provide single uninterpretable scores, FACE\nprovides insights into the system performance and enables identifying and\nlocating problems within conversations.\n","authors":["Hideaki Joko","Faegheh Hasibi"],"pdf_url":"https://arxiv.org/pdf/2506.00314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.04273v3","updated":"2025-10-25T03:36:49Z","published":"2025-08-06T09:58:43Z","title":"Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video\n  Moment Retrieval","summary":"  Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically\nrelated to the given query. To tackle this task, most existing VMR methods\nsolely focus on the visual and textual modalities while neglecting the\ncomplementary but important audio modality. Although a few recent works try to\ntackle the joint audio-vision-text reasoning, they treat all modalities equally\nand simply embed them without fine-grained interaction for moment retrieval.\nThese designs are counter-practical as: Not all audios are helpful for video\nmoment retrieval, and the audio of some videos may be complete noise or\nbackground sound that is meaningless to the moment determination. To this end,\nwe propose a novel Importance-aware Multi-Granularity fusion model (IMG), which\nlearns to dynamically and selectively aggregate the audio-vision-text contexts\nfor VMR. Specifically, after integrating the textual guidance with vision and\naudio separately, we first design a pseudo-label-supervised audio importance\npredictor that predicts the importance score of the audio, and accordingly\nassigns weights to mitigate the interference caused by noisy audio. Then, we\ndesign a multi-granularity audio fusion module that adaptively fuses audio and\nvisual modalities at local-, event-, and global-level, fully capturing their\ncomplementary contexts. We further propose a cross-modal knowledge distillation\nstrategy to address the challenge of missing audio modality during inference.\nTo evaluate our method, we further construct a new VMR dataset, i.e.,\nCharades-AudioMatter, where audio-related samples are manually selected and\nre-organized from the original Charades-STA to validate the model's capability\nin utilizing audio modality. Extensive experiments validate the effectiveness\nof our method, achieving state-of-the-art with audio-video fusion in VMR\nmethods. Our code is available at https://github.com/HuiGuanLab/IMG.\n","authors":["Junan Lin","Daizong Liu","Xianke Chen","Xiaoye Qu","Xun Yang","Jixiang Zhu","Sanyuan Zhang","Jianfeng Dong"],"pdf_url":"https://arxiv.org/pdf/2508.04273v3.pdf","comment":"Accepted to ACM MM 2025"},{"id":"http://arxiv.org/abs/2510.22101v1","updated":"2025-10-25T00:56:06Z","published":"2025-10-25T00:56:06Z","title":"Scaling Up Efficient Small Language Models Serving and Deployment for\n  Semantic Job Search","summary":"  Large Language Models (LLMs) have demonstrated impressive quality when\napplied to predictive tasks such as relevance ranking and semantic search.\nHowever, deployment of such LLMs remains prohibitively expensive for industry\napplications with strict latency and throughput requirements. In this work, we\npresent lessons and efficiency insights from developing a purely text-based\ndecoder-only Small Language Model (SLM) for a semantic search application at\nLinkedIn. Particularly, we discuss model compression techniques such as pruning\nthat allow us to reduce the model size by up to $40\\%$ while maintaining the\naccuracy. Additionally, we present context compression techniques that allow us\nto reduce the input context length by up to $10$x with minimal loss of\naccuracy. Finally, we present practical lessons from optimizing the serving\ninfrastructure for deploying such a system on GPUs at scale, serving millions\nof requests per second. Taken together, this allows us to increase our system's\nthroughput by $10$x in a real-world deployment, while meeting our quality bar.\n","authors":["Kayhan Behdin","Qingquan Song","Sriram Vasudevan","Jian Sheng","Xiaojing Ma","Z Zhou","Chuanrui Zhu","Guoyao Li","Chanh Nguyen","Sayan Ghosh","Hejian Sang","Ata Fatahi Baarzi","Sundara Raman Ramachandran","Xiaoqing Wang","Qing Lan","Vinay Y S","Qi Guo","Caleb Johnson","Zhipeng Wang","Fedor Borisyuk"],"pdf_url":"https://arxiv.org/pdf/2510.22101v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2510.22154v1","updated":"2025-10-25T04:17:50Z","published":"2025-10-25T04:17:50Z","title":"Frequency-Spatial Interaction Driven Network for Low-Light Image\n  Enhancement","summary":"  Low-light image enhancement (LLIE) aims at improving the perception or\ninterpretability of an image captured in an environment with poor illumination.\nWith the advent of deep learning, the LLIE technique has achieved significant\nbreakthroughs. However, existing LLIE methods either ignore the important role\nof frequency domain information or fail to effectively promote the propagation\nand flow of information, limiting the LLIE performance. In this paper, we\ndevelop a novel frequency-spatial interaction-driven network (FSIDNet) for LLIE\nbased on two-stage architecture. To be specific, the first stage is designed to\nrestore the amplitude of low-light images to improve the lightness, and the\nsecond stage devotes to restore phase information to refine fine-grained\nstructures. Considering that Frequency domain and spatial domain information\nare complementary and both favorable for LLIE, we further develop two\nfrequency-spatial interaction blocks which mutually amalgamate the\ncomplementary spatial and frequency information to enhance the capability of\nthe model. In addition, we construct the Information Exchange Module (IEM) to\nassociate two stages by adequately incorporating cross-stage and cross-scale\nfeatures to effectively promote the propagation and flow of information in the\ntwo-stage network structure. Finally, we conduct experiments on several widely\nused benchmark datasets (i.e., LOL-Real, LSRW-Huawei, etc.), which demonstrate\nthat our method achieves the excellent performance in terms of visual results\nand quantitative metrics while preserving good model efficiency.\n","authors":["Yunhong Tao","Wenbing Tao","Xiang Xiang"],"pdf_url":"https://arxiv.org/pdf/2510.22154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.04273v3","updated":"2025-10-25T03:36:49Z","published":"2025-08-06T09:58:43Z","title":"Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video\n  Moment Retrieval","summary":"  Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically\nrelated to the given query. To tackle this task, most existing VMR methods\nsolely focus on the visual and textual modalities while neglecting the\ncomplementary but important audio modality. Although a few recent works try to\ntackle the joint audio-vision-text reasoning, they treat all modalities equally\nand simply embed them without fine-grained interaction for moment retrieval.\nThese designs are counter-practical as: Not all audios are helpful for video\nmoment retrieval, and the audio of some videos may be complete noise or\nbackground sound that is meaningless to the moment determination. To this end,\nwe propose a novel Importance-aware Multi-Granularity fusion model (IMG), which\nlearns to dynamically and selectively aggregate the audio-vision-text contexts\nfor VMR. Specifically, after integrating the textual guidance with vision and\naudio separately, we first design a pseudo-label-supervised audio importance\npredictor that predicts the importance score of the audio, and accordingly\nassigns weights to mitigate the interference caused by noisy audio. Then, we\ndesign a multi-granularity audio fusion module that adaptively fuses audio and\nvisual modalities at local-, event-, and global-level, fully capturing their\ncomplementary contexts. We further propose a cross-modal knowledge distillation\nstrategy to address the challenge of missing audio modality during inference.\nTo evaluate our method, we further construct a new VMR dataset, i.e.,\nCharades-AudioMatter, where audio-related samples are manually selected and\nre-organized from the original Charades-STA to validate the model's capability\nin utilizing audio modality. Extensive experiments validate the effectiveness\nof our method, achieving state-of-the-art with audio-video fusion in VMR\nmethods. Our code is available at https://github.com/HuiGuanLab/IMG.\n","authors":["Junan Lin","Daizong Liu","Xianke Chen","Xiaoye Qu","Xun Yang","Jixiang Zhu","Sanyuan Zhang","Jianfeng Dong"],"pdf_url":"https://arxiv.org/pdf/2508.04273v3.pdf","comment":"Accepted to ACM MM 2025"}],"Robotics":[{"id":"http://arxiv.org/abs/2510.22448v1","updated":"2025-10-25T22:31:06Z","published":"2025-10-25T22:31:06Z","title":"A short methodological review on social robot navigation benchmarking","summary":"  Social Robot Navigation is the skill that allows robots to move efficiently\nin human-populated environments while ensuring safety, comfort, and trust.\nUnlike other areas of research, the scientific community has not yet achieved\nan agreement on how Social Robot Navigation should be benchmarked. This is\nnotably important, as the lack of a de facto standard to benchmark Social Robot\nNavigation can hinder the progress of the field and may lead to contradicting\nconclusions. Motivated by this gap, we contribute with a short review focused\nexclusively on benchmarking trends in the period from January 2020 to July\n2025. Of the 130 papers identified by our search using IEEE Xplore, we analysed\nthe 85 papers that met the criteria of the review. This review addresses the\nmetrics used in the literature for benchmarking purposes, the algorithms\nemployed in such benchmarks, the use of human surveys for benchmarking, and how\nconclusions are drawn from the benchmarking results, when applicable.\n","authors":["Pranup Chhetri","Alejandro Torrejon","Sergio Eslava","Luis J. Manso"],"pdf_url":"https://arxiv.org/pdf/2510.22448v1.pdf","comment":"18 pages, 14 of which references. 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2510.22434v1","updated":"2025-10-25T21:03:27Z","published":"2025-10-25T21:03:27Z","title":"Separation of Unconscious Robots with Obstructed Visibility","summary":"  We study a recently introduced \\textit{unconscious} mobile robot model, where\neach robot is associated with a \\textit{color}, which is visible to other\nrobots but not to itself. The robots are autonomous, anonymous, oblivious and\nsilent, operating in the Euclidean plane under the conventional\n\\textit{Look-Compute-Move} cycle. A primary task in this model is the\n\\textit{separation problem}, where unconscious robots sharing the same color\nmust separate from others, forming recognizable geometric shapes such as\ncircles, points, or lines. All prior works model the robots as\n\\textit{transparent}, enabling each to know the positions and colors of all\nother robots. In contrast, we model the robots as \\textit{opaque}, where a\nrobot can obstruct the visibility of two other robots, if it lies on the line\nsegment between them. Under this obstructed visibility, we consider a variant\nof the separation problem in which robots, starting from any arbitrary initial\nconfiguration, are required to separate into concentric semicircles. We present\na collision-free algorithm that solves the separation problem under a\nsemi-synchronous scheduler in $O(n)$ epochs, where $n$ is the number of robots.\nThe robots agree on one coordinate axis but have no knowledge of $n$.\n","authors":["Prajyot Pyati","Navjot Kaur","Saswata Jana","Adri Bhattacharya","Partha Sarathi Mandal"],"pdf_url":"https://arxiv.org/pdf/2510.22434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22420v1","updated":"2025-10-25T19:40:55Z","published":"2025-10-25T19:40:55Z","title":"A Novel Multi-Timescale Stability-Preserving Hierarchical Reinforcement\n  Learning Controller Framework for Adaptive Control in High-Dimensional\n  Dynamical Systems","summary":"  Controlling high-dimensional stochastic systems, critical in robotics,\nautonomous vehicles, and hyperchaotic systems, faces the curse of\ndimensionality, lacks temporal abstraction, and often fails to ensure\nstochastic stability. To overcome these limitations, this study introduces the\nMulti-Timescale Lyapunov-Constrained Hierarchical Reinforcement Learning\n(MTLHRL) framework. MTLHRL integrates a hierarchical policy within a\nsemi-Markov Decision Process (SMDP), featuring a high-level policy for\nstrategic planning and a low-level policy for reactive control, which\neffectively manages complex, multi-timescale decision-making and reduces\ndimensionality overhead. Stability is rigorously enforced using a neural\nLyapunov function optimized via Lagrangian relaxation and multi-timescale\nactor-critic updates, ensuring mean-square boundedness or asymptotic stability\nin the face of stochastic dynamics. The framework promotes efficient and\nreliable learning through trust-region constraints and decoupled optimization.\nExtensive simulations on an 8D hyperchaotic system and a 5-DOF robotic\nmanipulator demonstrate MTLHRL's empirical superiority. It significantly\noutperforms baseline methods in both stability and performance, recording the\nlowest error indices (e.g., Integral Absolute Error (IAE): 3.912 in\nhyperchaotic control and IAE: 1.623 in robotics), achieving faster convergence,\nand exhibiting superior disturbance rejection. MTLHRL offers a theoretically\ngrounded and practically viable solution for robust control of complex\nstochastic systems.\n","authors":["Mohammad Ali Labbaf Khaniki","Fateme Taroodi","Benyamin Safizadeh"],"pdf_url":"https://arxiv.org/pdf/2510.22420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18796v3","updated":"2025-10-25T18:51:14Z","published":"2024-02-29T01:56:41Z","title":"MOSAIC: Modular Foundation Models for Assistive and Interactive Cooking","summary":"  We present MOSAIC, a modular architecture for coordinating multiple robots to\n(a) interact with users using natural language and (b) manipulate an open\nvocabulary of everyday objects. MOSAIC employs modularity at several levels: it\nleverages multiple large-scale pre-trained models for high-level tasks like\nlanguage and image recognition, while using streamlined modules designed for\nlow-level task-specific control. This decomposition allows us to reap the\ncomplementary benefits of foundation models as well as precise, more\nspecialized models. Pieced together, our system is able to scale to complex\ntasks that involve coordinating multiple robots and humans. First, we unit-test\nindividual modules with 180 episodes of visuomotor picking, 60 episodes of\nhuman motion forecasting, and 46 online user evaluations of the task planner.\nWe then extensively evaluate MOSAIC with 60 end-to-end trials. We discuss\ncrucial design decisions, limitations of the current system, and open\nchallenges in this domain. The project's website is at\nhttps://portal-cornell.github.io/MOSAIC/\n","authors":["Huaxiaoyue Wang","Kushal Kedia","Juntao Ren","Rahma Abdullah","Atiksh Bhardwaj","Angela Chao","Kelly Y Chen","Nathaniel Chin","Prithwish Dan","Xinyi Fan","Gonzalo Gonzalez-Pumariega","Aditya Kompella","Maximus Adrian Pace","Yash Sharma","Xiangwan Sun","Neha Sunkara","Sanjiban Choudhury"],"pdf_url":"https://arxiv.org/pdf/2402.18796v3.pdf","comment":"22 pages, 13 figures; CoRL 2024"},{"id":"http://arxiv.org/abs/2510.22370v1","updated":"2025-10-25T17:27:08Z","published":"2025-10-25T17:27:08Z","title":"BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework\n  for Lane Keeping in Autonomous Vehicles","summary":"  In this paper, we propose Bootstrapped Language-Image Pretraining-driven\nFused State Representation in Proximal Policy Optimization (BLIP-FusePPO), a\nnovel multimodal reinforcement learning (RL) framework for autonomous\nlane-keeping (LK), in which semantic embeddings generated by a vision-language\nmodel (VLM) are directly fused with geometric states, LiDAR observations, and\nProportional-Integral-Derivative-based (PID) control feedback within the agent\nobservation space. The proposed method lets the agent learn driving rules that\nare aware of their surroundings and easy to understand by combining high-level\nscene understanding from the VLM with low-level control and spatial signals.\nOur architecture brings together semantic, geometric, and control-aware\nrepresentations to make policy learning more robust. A hybrid reward function\nthat includes semantic alignment, LK accuracy, obstacle avoidance, and speed\nregulation helps learning to be more efficient and generalizable. Our method is\ndifferent from the approaches that only use semantic models to shape rewards.\nInstead, it directly embeds semantic features into the state representation.\nThis cuts down on expensive runtime inference and makes sure that semantic\nguidance is always available. The simulation results show that the proposed\nmodel is better at LK stability and adaptability than the best vision-based and\nmultimodal RL baselines in a wide range of difficult driving situations. We\nmake our code publicly available.\n","authors":["Seyed Ahmad Hosseini Miangoleh","Amin Jalal Aghdasian","Farzaneh Abdollahi"],"pdf_url":"https://arxiv.org/pdf/2510.22370v1.pdf","comment":"https://github.com/Amin-A96/BLIP-FusePPO-A-Vision-Language-Deep-Reinforcement-Learning-Framework-for-Lane-Keeping-in-Autonomous.git"},{"id":"http://arxiv.org/abs/2510.14627v2","updated":"2025-10-25T16:54:00Z","published":"2025-10-16T12:38:14Z","title":"GOPLA: Generalizable Object Placement Learning via Synthetic\n  Augmentation of Human Arrangement","summary":"  Robots are expected to serve as intelligent assistants, helping humans with\neveryday household organization. A central challenge in this setting is the\ntask of object placement, which requires reasoning about both semantic\npreferences (e.g., common-sense object relations) and geometric feasibility\n(e.g., collision avoidance). We present GOPLA, a hierarchical framework that\nlearns generalizable object placement from augmented human demonstrations. A\nmulti-modal large language model translates human instructions and visual\ninputs into structured plans that specify pairwise object relationships. These\nplans are then converted into 3D affordance maps with geometric common sense by\na spatial mapper, while a diffusion-based planner generates placement poses\nguided by test-time costs, considering multi-plan distributions and collision\navoidance. To overcome data scarcity, we introduce a scalable pipeline that\nexpands human placement demonstrations into diverse synthetic training data.\nExtensive experiments show that our approach improves placement success rates\nby 30.04 percentage points over the runner-up, evaluated on positioning\naccuracy and physical plausibility, demonstrating strong generalization across\na wide range of real-world robotic placement scenarios.\n","authors":["Yao Zhong","Hanzhi Chen","Simon Schaefer","Anran Zhang","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2510.14627v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22339v1","updated":"2025-10-25T15:49:06Z","published":"2025-10-25T15:49:06Z","title":"Estimating Continuum Robot Shape under External Loading using\n  Spatiotemporal Neural Networks","summary":"  This paper presents a learning-based approach for accurately estimating the\n3D shape of flexible continuum robots subjected to external loads. The proposed\nmethod introduces a spatiotemporal neural network architecture that fuses\nmulti-modal inputs, including current and historical tendon displacement data\nand RGB images, to generate point clouds representing the robot's deformed\nconfiguration. The network integrates a recurrent neural module for temporal\nfeature extraction, an encoding module for spatial feature extraction, and a\nmulti-modal fusion module to combine spatial features extracted from visual\ndata with temporal dependencies from historical actuator inputs. Continuous 3D\nshape reconstruction is achieved by fitting B\\'ezier curves to the predicted\npoint clouds. Experimental validation demonstrates that our approach achieves\nhigh precision, with mean shape estimation errors of 0.08 mm (unloaded) and\n0.22 mm (loaded), outperforming state-of-the-art methods in shape sensing for\nTDCRs. The results validate the efficacy of deep learning-based spatiotemporal\ndata fusion for precise shape estimation under loading conditions.\n","authors":["Enyi Wang","Zhen Deng","Chuanchuan Pan","Bingwei He","Jianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2510.22339v1.pdf","comment":"2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)"},{"id":"http://arxiv.org/abs/2510.22336v1","updated":"2025-10-25T15:40:18Z","published":"2025-10-25T15:40:18Z","title":"Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and\n  Morphology for Fall Recovery","summary":"  Humanoid robots represent a central frontier in embodied intelligence, as\ntheir anthropomorphic form enables natural deployment in humans' workspace.\nBrain-body co-design for humanoids presents a promising approach to realizing\nthis potential by jointly optimizing control policies and physical morphology.\nWithin this context, fall recovery emerges as a critical capability. It not\nonly enhances safety and resilience but also integrates naturally with\nlocomotion systems, thereby advancing the autonomy of humanoids. In this paper,\nwe propose RoboCraft, a scalable humanoid co-design framework for fall recovery\nthat iteratively improves performance through the coupled updates of control\npolicy and morphology. A shared policy pretrained across multiple designs is\nprogressively finetuned on high-performing morphologies, enabling efficient\nadaptation without retraining from scratch. Concurrently, morphology search is\nguided by human-inspired priors and optimization algorithms, supported by a\npriority buffer that balances reevaluation of promising candidates with the\nexploration of novel designs. Experiments show that \\ourmethod{} achieves an\naverage performance gain of 44.55% on seven public humanoid robots, with\nmorphology optimization drives at least 40% of improvements in co-designing\nfour humanoid robots, underscoring the critical role of humanoid co-design.\n","authors":["Bo Yue","Sheng Xu","Kui Jia","Guiliang Liu"],"pdf_url":"https://arxiv.org/pdf/2510.22336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18253v3","updated":"2025-10-25T15:34:56Z","published":"2025-04-25T10:56:56Z","title":"Depth-Constrained ASV Navigation with Deep RL and Limited Sensing","summary":"  Autonomous Surface Vehicles (ASVs) play a crucial role in maritime\noperations, yet their navigation in shallow-water environments remains\nchallenging due to dynamic disturbances and depth constraints. Traditional\nnavigation strategies struggle with limited sensor information, making safe and\nefficient operation difficult. In this paper, we propose a reinforcement\nlearning (RL) framework for ASV navigation under depth constraints, where the\nvehicle must reach a target while avoiding unsafe areas with only a single\ndepth measurement per timestep from a downward-facing Single Beam Echosounder\n(SBES). To enhance environmental awareness, we integrate Gaussian Process (GP)\nregression into the RL framework, enabling the agent to progressively estimate\na bathymetric depth map from sparse sonar readings. This approach improves\ndecision-making by providing a richer representation of the environment.\nFurthermore, we demonstrate effective sim-to-real transfer, ensuring that\ntrained policies generalize well to real-world aquatic conditions. Experimental\nresults validate our method's capability to improve ASV navigation performance\nwhile maintaining safety in challenging shallow-water environments.\n","authors":["Amirhossein Zhalehmehrabi","Daniele Meli","Francesco Dal Santo","Francesco Trotti","Alessandro Farinelli"],"pdf_url":"https://arxiv.org/pdf/2504.18253v3.pdf","comment":"8 pages, 8 figures, Accepted to IEEE Robotics and Automation Letters\n  (this is not the final version)"},{"id":"http://arxiv.org/abs/2510.22313v1","updated":"2025-10-25T14:28:12Z","published":"2025-10-25T14:28:12Z","title":"Breaking the Static Assumption: A Dynamic-Aware LIO Framework Via\n  Spatio-Temporal Normal Analysis","summary":"  This paper addresses the challenge of Lidar-Inertial Odometry (LIO) in\ndynamic environments, where conventional methods often fail due to their\nstatic-world assumptions. Traditional LIO algorithms perform poorly when\ndynamic objects dominate the scenes, particularly in geometrically sparse\nenvironments. Current approaches to dynamic LIO face a fundamental challenge:\naccurate localization requires a reliable identification of static features,\nyet distinguishing dynamic objects necessitates precise pose estimation. Our\nsolution breaks this circular dependency by integrating dynamic awareness\ndirectly into the point cloud registration process. We introduce a novel\ndynamic-aware iterative closest point algorithm that leverages spatio-temporal\nnormal analysis, complemented by an efficient spatial consistency verification\nmethod to enhance static map construction. Experimental evaluations demonstrate\nsignificant performance improvements over state-of-the-art LIO systems in\nchallenging dynamic environments with limited geometric structure. The code and\ndataset are available at https://github.com/thisparticle/btsa.\n","authors":["Chen Zhiqiang","Le Gentil Cedric","Lin Fuling","Lu Minghao","Qiao Qiyuan","Xu Bowen","Qi Yuhua","Lu Peng"],"pdf_url":"https://arxiv.org/pdf/2510.22313v1.pdf","comment":"8 pages, 7 figures, Accepted to IEEE Robotics and Automation Letters\n  (RA-L)"},{"id":"http://arxiv.org/abs/2404.17922v2","updated":"2025-10-25T13:48:58Z","published":"2024-04-27T14:20:46Z","title":"Open-Set 3D Semantic Instance Maps for Vision Language Navigation --\n  O3D-SIM","summary":"  Humans excel at forming mental maps of their surroundings, equipping them to\nunderstand object relationships and navigate based on language queries. Our\nprevious work, SI Maps (Nanwani L, Agarwal A, Jain K, et al. Instance-level\nsemantic maps for vision language navigation. In: 2023 32nd IEEE International\nConference on Robot and Human Interactive Communication (RO-MAN). IEEE; 2023\nAug.), showed that having instance-level information and the semantic\nunderstanding of an environment helps significantly improve performance for\nlanguage-guided tasks. We extend this instance-level approach to 3D while\nincreasing the pipeline's robustness and improving quantitative and qualitative\nresults. Our method leverages foundational models for object recognition, image\nsegmentation, and feature extraction. We propose a representation that results\nin a 3D point cloud map with instance-level embeddings, which bring in the\nsemantic understanding that natural language commands can query.\nQuantitatively, the work improves upon the success rate of language-guided\ntasks. At the same time, we qualitatively observe the ability to identify\ninstances more clearly and leverage the foundational models and language and\nimage-aligned embeddings to identify objects that, otherwise, a closed-set\napproach wouldn't be able to identify.\n  Project Page - https://smart-wheelchair-rrc.github.io/o3d-sim-webpage\n","authors":["Laksh Nanwani","Kumaraditya Gupta","Aditya Mathur","Swayam Agrawal","A. H. Abdul Hafez","K. Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2404.17922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.13579v4","updated":"2025-10-25T12:00:58Z","published":"2025-09-16T22:37:37Z","title":"TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement\n  Learning","summary":"  We present TreeIRL, a novel planner for autonomous driving that combines\nMonte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to\nachieve state-of-the-art performance in simulation and in real-world driving.\nThe core idea is to use MCTS to find a promising set of safe candidate\ntrajectories and a deep IRL scoring function to select the most human-like\namong them. We evaluate TreeIRL against both classical and state-of-the-art\nplanners in large-scale simulations and on 500+ miles of real-world autonomous\ndriving in the Las Vegas metropolitan area. Test scenarios include dense urban\ntraffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves\nthe best overall performance, striking a balance between safety, progress,\ncomfort, and human-likeness. To our knowledge, our work is the first\ndemonstration of MCTS-based planning on public roads and underscores the\nimportance of evaluating planners across a diverse set of metrics and in\nreal-world environments. TreeIRL is highly extensible and could be further\nimproved with reinforcement learning and imitation learning, providing a\nframework for exploring different combinations of classical and learning-based\napproaches to solve the planning bottleneck in autonomous driving.\n","authors":["Momchil S. Tomov","Sang Uk Lee","Hansford Hendrago","Jinwook Huh","Teawon Han","Forbes Howington","Rafael da Silva","Gianmarco Bernasconi","Marc Heim","Samuel Findler","Xiaonan Ji","Alexander Boule","Michael Napoli","Kuo Chen","Jesse Miller","Boaz Floor","Yunqing Hu"],"pdf_url":"https://arxiv.org/pdf/2509.13579v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22235v1","updated":"2025-10-25T09:39:39Z","published":"2025-10-25T09:39:39Z","title":"CGoT: A Novel Inference Mechanism for Embodied Multi-Agent Systems Using\n  Composable Graphs of Thoughts","summary":"  The integration of self-driving cars and service robots is becoming\nincreasingly prevalent across a wide array of fields, playing a crucial and\nexpanding role in both industrial applications and everyday life. In parallel,\nthe rapid advancements in Large Language Models (LLMs) have garnered\nsubstantial attention and interest within the research community. This paper\nintroduces a novel vehicle-robot system that leverages the strengths of both\nautonomous vehicles and service robots. In our proposed system, two autonomous\nego-vehicles transports service robots to locations within an office park,\nwhere they perform a series of tasks. The study explores the feasibility and\npotential benefits of incorporating LLMs into this system, with the aim of\nenhancing operational efficiency and maximizing the potential of the\ncooperative mechanisms between the vehicles and the robots. This paper proposes\na novel inference mechanism which is called CGOT toward this type of system\nwhere an agent can carry another agent. Experimental results are presented to\nvalidate the performance of the proposed method.\n","authors":["Yixiao Nie","Yang Zhang","Yingjie Jin","Zhepeng Wang","Xiu Li","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2510.22235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00538v3","updated":"2025-10-25T09:30:10Z","published":"2024-11-30T17:09:18Z","title":"Prognostic Framework for Robotic Manipulators Operating Under Dynamic\n  Task Severities","summary":"  Robotic manipulators are critical in many applications but are known to\ndegrade over time. This degradation is influenced by the nature of the tasks\nperformed by the robot. Tasks with higher severity, such as handling heavy\npayloads, can accelerate the degradation process. One way this degradation is\nreflected is in the position accuracy of the robot's end-effector. In this\npaper, we present a prognostic modeling framework that predicts a robotic\nmanipulator's Remaining Useful Life (RUL) while accounting for the effects of\ntask severity. Our framework represents the robot's position accuracy as a\nBrownian motion process with a random drift parameter that is influenced by\ntask severity. The dynamic nature of task severity is modeled using a\ncontinuous-time Markov chain (CTMC). To evaluate RUL, we discuss two approaches\n-- (1) a novel closed-form expression for Remaining Lifetime Distribution\n(RLD), and (2) Monte Carlo simulations, commonly used in prognostics\nliterature. Theoretical results establish the equivalence between these RUL\ncomputation approaches. We validate our framework through experiments using two\ndistinct physics-based simulators for planar and spatial robot fleets. Our\nfindings show that robots in both fleets experience shorter RUL when handling a\nhigher proportion of high-severity tasks.\n","authors":["Ayush Mohanty","Jason Dekarske","Stephen K. Robinson","Sanjay Joshi","Nagi Gebraeel"],"pdf_url":"https://arxiv.org/pdf/2412.00538v3.pdf","comment":"Accepted for Publication in IEEE Transactions on Systems, Man, and\n  Cybernetics: Systems"},{"id":"http://arxiv.org/abs/2504.04516v3","updated":"2025-10-25T09:11:42Z","published":"2025-04-06T15:24:29Z","title":"DexSinGrasp: Learning a Unified Policy for Dexterous Object Singulation\n  and Grasping in Densely Cluttered Environments","summary":"  Grasping objects in cluttered environments remains a fundamental yet\nchallenging problem in robotic manipulation. While prior works have explored\nlearning-based synergies between pushing and grasping for two-fingered\ngrippers, few have leveraged the high degrees of freedom (DoF) in dexterous\nhands to perform efficient singulation for grasping in cluttered settings. In\nthis work, we introduce DexSinGrasp, a unified policy for dexterous object\nsingulation and grasping. DexSinGrasp enables high-dexterity object singulation\nto facilitate grasping, significantly improving efficiency and effectiveness in\ncluttered environments. We incorporate clutter arrangement curriculum learning\nto enhance success rates and generalization across diverse clutter conditions,\nwhile policy distillation enables a deployable vision-based grasping strategy.\nTo evaluate our approach, we introduce a set of cluttered grasping tasks with\nvarying object arrangements and occlusion levels. Experimental results show\nthat our method outperforms baselines in both efficiency and grasping success\nrate, particularly in dense clutter. Codes, appendix, and videos are available\non our website https://nus-lins-lab.github.io/dexsingweb/.\n","authors":["Lixin Xu","Zixuan Liu","Zhewei Gui","Jingxiang Guo","Zeyu Jiang","Tongzhou Zhang","Zhixuan Xu","Chongkai Gao","Lin Shao"],"pdf_url":"https://arxiv.org/pdf/2504.04516v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22204v1","updated":"2025-10-25T08:08:04Z","published":"2025-10-25T08:08:04Z","title":"Bridging Perception and Reasoning: Dual-Pipeline Neuro-Symbolic Landing\n  for UAVs in Cluttered Environments","summary":"  Autonomous landing in unstructured (cluttered, uneven, and map-poor)\nenvironments is a core requirement for Unmanned Aerial Vehicles (UAVs), yet\npurely vision-based or deep learning models often falter under covariate shift\nand provide limited interpretability. We propose NeuroSymLand, a neuro-symbolic\nframework that tightly couples two complementary pipelines: (i) an offline\npipeline, where Large Language Models (LLMs) and human-in-the-loop refinement\nsynthesize Scallop code from diverse landing scenarios, distilling\ngeneralizable and verifiable symbolic knowledge; and (ii) an online pipeline,\nwhere a compact foundation-based semantic segmentation model generates\nprobabilistic Scallop facts that are composed into semantic scene graphs for\nreal-time deductive reasoning. This design combines the perceptual strengths of\nlightweight foundation models with the interpretability and verifiability of\nsymbolic reasoning. Node attributes (e.g., flatness, area) and edge relations\n(adjacency, containment, proximity) are computed with geometric routines rather\nthan learned, avoiding the data dependence and latency of train-time graph\nbuilders. The resulting Scallop program encodes landing principles (avoid water\nand obstacles; prefer large, flat, accessible regions) and yields calibrated\nsafety scores with ranked Regions of Interest (ROIs) and human-readable\njustifications. Extensive evaluations across datasets, diverse simulation maps,\nand real UAV hardware show that NeuroSymLand achieves higher accuracy, stronger\nrobustness to covariate shift, and superior efficiency compared with\nstate-of-the-art baselines, while advancing UAV safety and reliability in\nemergency response, surveillance, and delivery missions.\n","authors":["Weixian Qian","Sebastian Schroder","Yao Deng","Jiaohong Yao","Linfeng Liang","Xiao Cheng","Richard Han","Xi Zheng"],"pdf_url":"https://arxiv.org/pdf/2510.22204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22201v1","updated":"2025-10-25T07:44:33Z","published":"2025-10-25T07:44:33Z","title":"ACG: Action Coherence Guidance for Flow-based VLA models","summary":"  Diffusion and flow matching models have emerged as powerful robot policies,\nenabling Vision-Language-Action (VLA) models to generalize across diverse\nscenes and instructions. Yet, when trained via imitation learning, their high\ngenerative capacity makes them sensitive to noise in human demonstrations:\njerks, pauses, and jitter which reduce action coherence. Reduced action\ncoherence causes instability and trajectory drift during deployment, failures\nthat are catastrophic in fine-grained manipulation where precision is crucial.\nIn this paper, we present Action Coherence Guidance (ACG) for VLA models, a\ntraining-free test-time guidance algorithm that improves action coherence and\nthereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and\nreal-world SO-101 tasks, ACG consistently improves action coherence and boosts\nsuccess rates across diverse manipulation tasks. Code and project page are\navailable at https://github.com/DAVIAN-Robotics/ACG and\nhttps://DAVIAN-Robotics.github.io/ACG , respectively.\n","authors":["Minho Park","Kinam Kim","Junha Hyung","Hyojin Jang","Hoiyeong Jin","Jooyeol Yun","Hojoon Lee","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2510.22201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22199v1","updated":"2025-10-25T07:39:02Z","published":"2025-10-25T07:39:02Z","title":"MOGRAS: Human Motion with Grasping in 3D Scenes","summary":"  Generating realistic full-body motion interacting with objects is critical\nfor applications in robotics, virtual reality, and human-computer interaction.\nWhile existing methods can generate full-body motion within 3D scenes, they\noften lack the fidelity for fine-grained tasks like object grasping.\nConversely, methods that generate precise grasping motions typically ignore the\nsurrounding 3D scene. This gap, generating full-body grasping motions that are\nphysically plausible within a 3D scene, remains a significant challenge. To\naddress this, we introduce MOGRAS (Human MOtion with GRAsping in 3D Scenes), a\nlarge-scale dataset that bridges this gap. MOGRAS provides pre-grasping\nfull-body walking motions and final grasping poses within richly annotated 3D\nindoor scenes. We leverage MOGRAS to benchmark existing full-body grasping\nmethods and demonstrate their limitations in scene-aware generation.\nFurthermore, we propose a simple yet effective method to adapt existing\napproaches to work seamlessly within 3D scenes. Through extensive quantitative\nand qualitative experiments, we validate the effectiveness of our dataset and\nhighlight the significant improvements our proposed method achieves, paving the\nway for more realistic human-scene interactions.\n","authors":["Kunal Bhosikar","Siddharth Katageri","Vivek Madhavaram","Kai Han","Charu Sharma"],"pdf_url":"https://arxiv.org/pdf/2510.22199v1.pdf","comment":"British Machine Vision Conference Workshop - From Scene Understanding\n  to Human Modeling"},{"id":"http://arxiv.org/abs/2510.13443v2","updated":"2025-10-25T06:24:34Z","published":"2025-10-15T11:41:40Z","title":"Real-Time Knee Angle Prediction Using EMG and Kinematic Data with an\n  Attention-Based CNN-LSTM Network and Transfer Learning Across Multiple\n  Datasets","summary":"  Electromyography (EMG) signals are widely used for predicting body joint\nangles through machine learning (ML) and deep learning (DL) methods. However,\nthese approaches often face challenges such as limited real-time applicability,\nnon-representative test conditions, and the need for large datasets to achieve\noptimal performance. This paper presents a transfer-learning framework for knee\njoint angle prediction that requires only a few gait cycles from new subjects.\nThree datasets - Georgia Tech, the University of California Irvine (UCI), and\nthe Sharif Mechatronic Lab Exoskeleton (SMLE) - containing four EMG channels\nrelevant to knee motion were utilized. A lightweight attention-based CNN-LSTM\nmodel was developed and pre-trained on the Georgia Tech dataset, then\ntransferred to the UCI and SMLE datasets. The proposed model achieved\nNormalized Mean Absolute Errors (NMAE) of 6.8 percent and 13.7 percent for\none-step and 50-step predictions on abnormal subjects using EMG inputs alone.\nIncorporating historical knee angles reduced the NMAE to 3.1 percent and 3.5\npercent for normal subjects, and to 2.8 percent and 7.5 percent for abnormal\nsubjects. When further adapted to the SMLE exoskeleton with EMG, kinematic, and\ninteraction force inputs, the model achieved 1.09 percent and 3.1 percent NMAE\nfor one- and 50-step predictions, respectively. These results demonstrate\nrobust performance and strong generalization for both short- and long-term\nrehabilitation scenarios.\n","authors":["Mojtaba Mollahossein","Gholamreza Vossoughi","Mohammad Hossein Rohban"],"pdf_url":"https://arxiv.org/pdf/2510.13443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22164v1","updated":"2025-10-25T05:23:50Z","published":"2025-10-25T05:23:50Z","title":"LT-Exosense: A Vision-centric Multi-session Mapping System for Lifelong\n  Safe Navigation of Exoskeletons","summary":"  Self-balancing exoskeletons offer a promising mobility solution for\nindividuals with lower-limb disabilities. For reliable long-term operation,\nthese exoskeletons require a perception system that is effective in changing\nenvironments. In this work, we introduce LT-Exosense, a vision-centric,\nmulti-session mapping system designed to support long-term (semi)-autonomous\nnavigation for exoskeleton users. LT-Exosense extends single-session mapping\ncapabilities by incrementally fusing spatial knowledge across multiple\nsessions, detecting environmental changes, and updating a persistent global\nmap. This representation enables intelligent path planning, which can adapt to\nnewly observed obstacles and can recover previous routes when obstructions are\nremoved. We validate LT-Exosense through several real-world experiments,\ndemonstrating a scalable multi-session map that achieves an average\npoint-to-point error below 5 cm when compared to ground-truth laser scans. We\nalso illustrate the potential application of adaptive path planning in\ndynamically changing indoor environments.\n","authors":["Jianeng Wang","Matias Mattamala","Christina Kassab","Nived Chebrolu","Guillaume Burger","Fabio Elnecave","Marine Petriaux","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2510.22164v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.06278v2","updated":"2025-10-25T05:16:48Z","published":"2025-05-06T19:52:18Z","title":"Robust Understanding of Human-Robot Social Interactions through\n  Multimodal Distillation","summary":"  There is a growing need for social robots and intelligent agents that can\neffectively interact with and support users. For the interactions to be\nseamless, the agents need to analyse social scenes and behavioural cues from\ntheir (robot's) perspective. Works that model human-agent interactions in\nsocial situations are few; and even those existing ones are computationally too\nintensive to be deployed in real time or perform poorly in real-world scenarios\nwhen only limited information is available. We propose a knowledge distillation\nframework that models social interactions through various multimodal cues, and\nyet is robust against incomplete and noisy information during inference. We\ntrain a teacher model with multimodal input (body, face and hand gestures,\ngaze, raw images) that transfers knowledge to a student model which relies\nsolely on body pose. Extensive experiments on two publicly available\nhuman-robot interaction datasets demonstrate that our student model achieves an\naverage accuracy gain of 14.75% over competitive baselines on multiple\ndownstream social understanding tasks, even with up to 51% of its input being\ncorrupted. The student model is also highly efficient - less than 1% in size of\nthe teacher model in terms of parameters and its latency is 11.9% of the\nteacher model. Our code and related data are available at\ngithub.com/biantongfei/SocialEgoMobile.\n","authors":["Tongfei Bian","Mathieu Chollet","Tanaya Guha"],"pdf_url":"https://arxiv.org/pdf/2505.06278v2.pdf","comment":"Accepted by ACM Multimedia 2025, camera-ready version"},{"id":"http://arxiv.org/abs/2410.23643v4","updated":"2025-10-25T04:49:01Z","published":"2024-10-31T05:29:30Z","title":"SceneComplete: Open-World 3D Scene Completion in Cluttered Real World\n  Environments for Robot Manipulation","summary":"  Careful robot manipulation in every-day cluttered environments requires an\naccurate understanding of the 3D scene, in order to grasp and place objects\nstably and reliably and to avoid colliding with other objects. In general, we\nmust construct such a 3D interpretation of a complex scene based on limited\ninput, such as a single RGB-D image. We describe SceneComplete, a system for\nconstructing a complete, segmented, 3D model of a scene from a single view.\nSceneComplete is a novel pipeline for composing general-purpose pretrained\nperception modules (vision-language, segmentation, image-inpainting,\nimage-to-3D, visual-descriptors and pose-estimation) to obtain highly accurate\nresults. We demonstrate its accuracy and effectiveness with respect to\nground-truth models in a large benchmark dataset and show that its accurate\nwhole-object reconstruction enables robust grasp proposal generation, including\nfor a dexterous hand. We release the code and additional results on our\nwebsite.\n","authors":["Aditya Agarwal","Gaurav Singh","Bipasha Sen","Tomás Lozano-Pérez","Leslie Pack Kaelbling"],"pdf_url":"https://arxiv.org/pdf/2410.23643v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15386v3","updated":"2025-10-25T04:31:05Z","published":"2025-03-19T16:24:55Z","title":"CCDP: Composition of Conditional Diffusion Policies with Guided Sampling","summary":"  Imitation Learning offers a promising approach to learn directly from data\nwithout requiring explicit models, simulations, or detailed task definitions.\nDuring inference, actions are sampled from the learned distribution and\nexecuted on the robot. However, sampled actions may fail for various reasons,\nand simply repeating the sampling step until a successful action is obtained\ncan be inefficient. In this work, we propose an enhanced sampling strategy that\nrefines the sampling distribution to avoid previously unsuccessful actions. We\ndemonstrate that by solely utilizing data from successful demonstrations, our\nmethod can infer recovery actions without the need for additional exploratory\nbehavior or a high-level controller. Furthermore, we leverage the concept of\ndiffusion model decomposition to break down the primary problem, which may\nrequire long-horizon history to manage failures, into multiple smaller, more\nmanageable sub-problems in learning, data collection, and inference, thereby\nenabling the system to adapt to variable failure counts. Our approach yields a\nlow-level controller that dynamically adjusts its sampling space to improve\nefficiency when prior samples fall short. We validate our method across several\ntasks, including door opening with unknown directions, object manipulation, and\nbutton-searching scenarios, demonstrating that our approach outperforms\ntraditional baselines.\n","authors":["Amirreza Razmjoo","Sylvain Calinon","Michael Gienger","Fan Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.15386v3.pdf","comment":"Accepted to IROS 2025"},{"id":"http://arxiv.org/abs/2510.22141v1","updated":"2025-10-25T03:27:19Z","published":"2025-10-25T03:27:19Z","title":"LOC: A General Language-Guided Framework for Open-Set 3D Occupancy\n  Prediction","summary":"  Vision-Language Models (VLMs) have shown significant progress in open-set\nchallenges. However, the limited availability of 3D datasets hinders their\neffective application in 3D scene understanding. We propose LOC, a general\nlanguage-guided framework adaptable to various occupancy networks, supporting\nboth supervised and self-supervised learning paradigms. For self-supervised\ntasks, we employ a strategy that fuses multi-frame LiDAR points for\ndynamic/static scenes, using Poisson reconstruction to fill voids, and\nassigning semantics to voxels via K-Nearest Neighbor (KNN) to obtain\ncomprehensive voxel representations. To mitigate feature over-homogenization\ncaused by direct high-dimensional feature distillation, we introduce Densely\nContrastive Learning (DCL). DCL leverages dense voxel semantic information and\npredefined textual prompts. This efficiently enhances open-set recognition\nwithout dense pixel-level supervision, and our framework can also leverage\nexisting ground truth to further improve performance. Our model predicts dense\nvoxel features embedded in the CLIP feature space, integrating textual and\nimage pixel information, and classifies based on text and semantic similarity.\nExperiments on the nuScenes dataset demonstrate the method's superior\nperformance, achieving high-precision predictions for known classes and\ndistinguishing unknown classes without additional training data.\n","authors":["Yuhang Gao","Xiang Xiang","Sheng Zhong","Guoyou Wang"],"pdf_url":"https://arxiv.org/pdf/2510.22141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04308v3","updated":"2025-10-25T03:07:10Z","published":"2025-06-04T17:59:27Z","title":"RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for Robotics","summary":"  Spatial referring is a fundamental capability of embodied robots to interact\nwith the 3D physical world. However, even with the powerful pretrained vision\nlanguage models (VLMs), recent approaches are still not qualified to accurately\nunderstand the complex 3D scenes and dynamically reason about the\ninstruction-indicated locations for interaction. To this end, we propose\nRoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding\nby integrating a disentangled but dedicated depth encoder via supervised\nfine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial\nreasoning via reinforcement fine-tuning (RFT), with metric-sensitive process\nreward functions tailored for spatial referring tasks. To support SFT and RFT\ntraining, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x\nprior), covering 31 spatial relations (vs. 15 prior) and supporting complex\nreasoning processes (up to 5 steps). In addition, we introduce\nRefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial\nreferring with multi-step reasoning. Experiments show that SFT-trained\nRoboRefer achieves state-of-the-art spatial understanding, with an average\nsuccess rate of 89.6%. RFT-trained RoboRefer further outperforms all other\nbaselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average\naccuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various\ncontrol policies to execute long-horizon, dynamic tasks across diverse robots\n(e,g., UR5, G1 humanoid) in cluttered real-world scenes. Please see the project\npage at https://zhoues.github.io/RoboRefer.\n","authors":["Enshen Zhou","Jingkun An","Cheng Chi","Yi Han","Shanyu Rong","Chi Zhang","Pengwei Wang","Zhongyuan Wang","Tiejun Huang","Lu Sheng","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.04308v3.pdf","comment":"Accepted by NeurIPS 2025. Project page:\n  https://zhoues.github.io/RoboRefer/"},{"id":"http://arxiv.org/abs/2505.16394v2","updated":"2025-10-25T02:57:21Z","published":"2025-05-22T08:46:53Z","title":"Raw2Drive: Reinforcement Learning with Aligned World Models for\n  End-to-End Autonomous Driving (in CARLA v2)","summary":"  Reinforcement Learning (RL) can mitigate the causal confusion and\ndistribution shift inherent to imitation learning (IL). However, applying RL to\nend-to-end autonomous driving (E2E-AD) remains an open problem for its training\ndifficulty, and IL is still the mainstream paradigm in both academia and\nindustry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated\npromising results in neural planning; however, these methods typically require\nprivileged information as input rather than raw sensor data. We fill this gap\nby designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently\ntrain an auxiliary privileged world model paired with a neural planner that\nuses privileged information as input. Subsequently, we introduce a raw sensor\nworld model trained via our proposed Guidance Mechanism, which ensures\nconsistency between the raw sensor world model and the privileged world model\nduring rollouts. Finally, the raw sensor world model combines the prior\nknowledge embedded in the heads of the privileged world model to effectively\nguide the training of the raw sensor policy. Raw2Drive is so far the only RL\nbased end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it\nachieves state-of-the-art performance.\n","authors":["Zhenjie Yang","Xiaosong Jia","Qifeng Li","Xue Yang","Maoqing Yao","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2505.16394v2.pdf","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2510.22126v1","updated":"2025-10-25T02:55:02Z","published":"2025-10-25T02:55:02Z","title":"EasyUUV: An LLM-Enhanced Universal and Lightweight Sim-to-Real\n  Reinforcement Learning Framework for UUV Attitude Control","summary":"  Despite recent advances in Unmanned Underwater Vehicle (UUV) attitude\ncontrol, existing methods still struggle with generalizability, robustness to\nreal-world disturbances, and efficient deployment. To address the above\nchallenges, this paper presents EasyUUV, a Large Language Model (LLM)-enhanced,\nuniversal, and lightweight simulation-to-reality reinforcement learning (RL)\nframework for robust attitude control of UUVs. EasyUUV combines parallelized RL\ntraining with a hybrid control architecture, where a learned policy outputs\nhigh-level attitude corrections executed by an adaptive S-Surface controller. A\nmultimodal LLM is further integrated to adaptively tune controller parameters\nat runtime using visual and textual feedback, enabling training-free adaptation\nto unmodeled dynamics. Also, we have developed a low-cost 6-DoF UUV platform\nand applied an RL policy trained through efficient parallelized simulation.\nExtensive simulation and real-world experiments validate the effectiveness and\noutstanding performance of EasyUUV in achieving robust and adaptive UUV\nattitude control across diverse underwater conditions. The source code is\navailable from the following website: https://360zmem.github.io/easyuuv/\n","authors":["Guanwen Xie","Jingzehua Xu","Jiwei Tang","Yubo Huang","Shuai Zhang","Xiaofan Li"],"pdf_url":"https://arxiv.org/pdf/2510.22126v1.pdf","comment":"8 pages, 15 figures"},{"id":"http://arxiv.org/abs/2510.22113v1","updated":"2025-10-25T01:38:38Z","published":"2025-10-25T01:38:38Z","title":"RaycastGrasp: Eye-Gaze Interaction with Wearable Devices for Robotic\n  Manipulation","summary":"  Robotic manipulators are increasingly used to assist individuals with\nmobility impairments in object retrieval. However, the predominant\njoystick-based control interfaces can be challenging due to high precision\nrequirements and unintuitive reference frames. Recent advances in human-robot\ninteraction have explored alternative modalities, yet many solutions still rely\non external screens or restrictive control schemes, limiting their\nintuitiveness and accessibility. To address these challenges, we present an\negocentric, gaze-guided robotic manipulation interface that leverages a\nwearable Mixed Reality (MR) headset. Our system enables users to interact\nseamlessly with real-world objects using natural gaze fixation from a\nfirst-person perspective, while providing augmented visual cues to confirm\nintent and leveraging a pretrained vision model and robotic arm for intent\nrecognition and object manipulation. Experimental results demonstrate that our\napproach significantly improves manipulation accuracy, reduces system latency,\nand achieves single-pass intention and object recognition accuracy greater than\n88% across multiple real-world scenarios. These results demonstrate the\nsystem's effectiveness in enhancing intuitiveness and accessibility,\nunderscoring its practical significance for assistive robotics applications.\n","authors":["Zitiantao Lin","Yongpeng Sang","Yang Ye"],"pdf_url":"https://arxiv.org/pdf/2510.22113v1.pdf","comment":"5 pages, 5 figures; Accepted to: 2025 IEEE 4th International\n  Conference on Intelligent Reality (ICIR 2025); Zitiantao Lin and Yongpeng\n  Sang contributed equally to this work (co-first authors). Corresponding\n  author: Yang Ye (y.ye@northeastern.edu)"},{"id":"http://arxiv.org/abs/2510.19495v2","updated":"2025-10-25T01:18:43Z","published":"2025-10-22T11:43:39Z","title":"Using Non-Expert Data to Robustify Imitation Learning via Offline\n  Reinforcement Learning","summary":"  Imitation learning has proven effective for training robots to perform\ncomplex tasks from expert human demonstrations. However, it remains limited by\nits reliance on high-quality, task-specific data, restricting adaptability to\nthe diverse range of real-world object configurations and scenarios. In\ncontrast, non-expert data -- such as play data, suboptimal demonstrations,\npartial task completions, or rollouts from suboptimal policies -- can offer\nbroader coverage and lower collection costs. However, conventional imitation\nlearning approaches fail to utilize this data effectively. To address these\nchallenges, we posit that with right design decisions, offline reinforcement\nlearning can be used as a tool to harness non-expert data to enhance the\nperformance of imitation learning policies. We show that while standard offline\nRL approaches can be ineffective at actually leveraging non-expert data under\nthe sparse data coverage settings typically encountered in the real world,\nsimple algorithmic modifications can allow for the utilization of this data,\nwithout significant additional assumptions. Our approach shows that broadening\nthe support of the policy distribution can allow imitation algorithms augmented\nby offline RL to solve tasks robustly, showing considerably enhanced recovery\nand generalization behavior. In manipulation tasks, these innovations\nsignificantly increase the range of initial conditions where learned policies\nare successful when non-expert data is incorporated. Moreover, we show that\nthese methods are able to leverage all collected data, including partial or\nsuboptimal demonstrations, to bolster task-directed policy performance. This\nunderscores the importance of algorithmic techniques for using non-expert data\nfor robust policy learning in robotics. Website:\nhttps://uwrobotlearning.github.io/RISE-offline/\n","authors":["Kevin Huang","Rosario Scalise","Cleah Winston","Ayush Agrawal","Yunchu Zhang","Rohan Baijal","Markus Grotz","Byron Boots","Benjamin Burchfiel","Masha Itkina","Paarth Shah","Abhishek Gupta"],"pdf_url":"https://arxiv.org/pdf/2510.19495v2.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2510.22199v1","updated":"2025-10-25T07:39:02Z","published":"2025-10-25T07:39:02Z","title":"MOGRAS: Human Motion with Grasping in 3D Scenes","summary":"  Generating realistic full-body motion interacting with objects is critical\nfor applications in robotics, virtual reality, and human-computer interaction.\nWhile existing methods can generate full-body motion within 3D scenes, they\noften lack the fidelity for fine-grained tasks like object grasping.\nConversely, methods that generate precise grasping motions typically ignore the\nsurrounding 3D scene. This gap, generating full-body grasping motions that are\nphysically plausible within a 3D scene, remains a significant challenge. To\naddress this, we introduce MOGRAS (Human MOtion with GRAsping in 3D Scenes), a\nlarge-scale dataset that bridges this gap. MOGRAS provides pre-grasping\nfull-body walking motions and final grasping poses within richly annotated 3D\nindoor scenes. We leverage MOGRAS to benchmark existing full-body grasping\nmethods and demonstrate their limitations in scene-aware generation.\nFurthermore, we propose a simple yet effective method to adapt existing\napproaches to work seamlessly within 3D scenes. Through extensive quantitative\nand qualitative experiments, we validate the effectiveness of our dataset and\nhighlight the significant improvements our proposed method achieves, paving the\nway for more realistic human-scene interactions.\n","authors":["Kunal Bhosikar","Siddharth Katageri","Vivek Madhavaram","Kai Han","Charu Sharma"],"pdf_url":"https://arxiv.org/pdf/2510.22199v1.pdf","comment":"British Machine Vision Conference Workshop - From Scene Understanding\n  to Human Modeling"}]},"2025-10-24T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2506.01659v2","updated":"2025-10-24T23:14:58Z","published":"2025-06-02T13:30:39Z","title":"Engram Memory Encoding and Retrieval: A Neurocomputational Perspective","summary":"  Despite substantial research into the biological basis of memory, the precise\nmechanisms by which experiences are encoded, stored, and retrieved in the brain\nremain incompletely understood. A growing body of evidence supports the engram\ntheory, which posits that sparse populations of neurons undergo lasting\nphysical and biochemical changes to support long-term memory. Yet, a\ncomprehensive computational framework that integrates biological findings with\nmechanistic models remains elusive. This work synthesizes insights from\ncellular neuroscience and computational modeling to address key challenges in\nengram research: how engram neurons are identified and manipulated; how\nsynaptic plasticity mechanisms contribute to stable memory traces; and how\nsparsity promotes efficient, interference-resistant representations. Relevant\ncomputational approaches -- such as sparse regularization, engram gating, and\nbiologically inspired architectures like Sparse Distributed Memory and spiking\nneural networks -- are also examined. Together, these findings suggest that\nmemory efficiency, capacity, and stability emerge from the interaction of\nplasticity and sparsity constraints. By integrating neurobiological and\ncomputational perspectives, this paper provides a comprehensive theoretical\nfoundation for engram research and proposes a roadmap for future inquiry into\nthe mechanisms underlying memory, with implications for the diagnosis and\ntreatment of memory-related disorders.\n","authors":["Daniel Szelogowski"],"pdf_url":"https://arxiv.org/pdf/2506.01659v2.pdf","comment":"18 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2510.22055v1","updated":"2025-10-24T22:37:13Z","published":"2025-10-24T22:37:13Z","title":"A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim\n  Decomposition","summary":"  Fact-checking numerical claims is critical as the presence of numbers provide\nmirage of veracity despite being fake potentially causing catastrophic impacts\non society. The prior works in automatic fact verification do not primarily\nfocus on natural numerical claims. A typical human fact-checker first retrieves\nrelevant evidence addressing the different numerical aspects of the claim and\nthen reasons about them to predict the veracity of the claim. Hence, the search\nprocess of a human fact-checker is a crucial skill that forms the foundation of\nthe verification process. Emulating a real-world setting is essential to aid in\nthe development of automated methods that encompass such skills. However,\nexisting benchmarks employ heuristic claim decomposition approaches augmented\nwith weakly supervised web search to collect evidences for verifying claims.\nThis sometimes results in less relevant evidences and noisy sources with\ntemporal leakage rendering a less realistic retrieval setting for claim\nverification. Hence, we introduce QuanTemp++: a dataset consisting of natural\nnumerical claims, an open domain corpus, with the corresponding relevant\nevidence for each claim. The evidences are collected through a claim\ndecomposition process approximately emulating the approach of human\nfact-checker and veracity labels ensuring there is no temporal leakage. Given\nthis dataset, we also characterize the retrieval performance of key claim\ndecomposition paradigms. Finally, we observe their effect on the outcome of the\nverification pipeline and draw insights. The code for data pipeline along with\nlink to data can be found at https://github.com/VenkteshV/QuanTemp_Plus\n","authors":["V Venktesh","Deepali Prabhu","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2510.22055v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2510.22049v1","updated":"2025-10-24T22:17:49Z","published":"2025-10-24T22:17:49Z","title":"Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders","summary":"  Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users.\n","authors":["Zhimin Chen","Chenyu Zhao","Ka Chun Mo","Yunjiang Jiang","Jane H. Lee","Shouwei Chen","Khushhall Chandra Mahajan","Ning Jiang","Kai Ren","Jinhui Li","Wen-Yun Yang"],"pdf_url":"https://arxiv.org/pdf/2510.22049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22023v1","updated":"2025-10-24T21:03:20Z","published":"2025-10-24T21:03:20Z","title":"Multimodal Item Scoring for Natural Language Recommendation via Gaussian\n  Process Regression with LLM Relevance Judgments","summary":"  Natural Language Recommendation (NLRec) generates item suggestions based on\nthe relevance between user-issued NL requests and NL item description passages.\nExisting NLRec approaches often use Dense Retrieval (DR) to compute item\nrelevance scores from aggregation of inner products between user request\nembeddings and relevant passage embeddings. However, DR views the request as\nthe sole relevance label, thus leading to a unimodal scoring function centered\non the query embedding that is often a weak proxy for query relevance. To\nbetter capture the potential multimodal distribution of the relevance scoring\nfunction that may arise from complex NLRec data, we propose GPR-LLM that uses\nGaussian Process Regression (GPR) with LLM relevance judgments for a subset of\ncandidate passages. Experiments on four NLRec datasets and two LLM backbones\ndemonstrate that GPR-LLM with an RBF kernel, capable of modeling multimodal\nrelevance scoring functions, consistently outperforms simpler unimodal kernels\n(dot product, cosine similarity), as well as baseline methods including DR,\ncross-encoder, and pointwise LLM-based relevance scoring by up to 65%. Overall,\nGPR-LLM provides an efficient and effective approach to NLRec within a minimal\nLLM labeling budget.\n","authors":["Yifan Liu","Qianfeng Wen","Jiazhou Liang","Mark Zhao","Justin Cui","Anton Korikov","Armin Torogh","Junyoung Kim","Scott Sanner"],"pdf_url":"https://arxiv.org/pdf/2510.22023v1.pdf","comment":"16 pages,20 figures"},{"id":"http://arxiv.org/abs/2510.21962v1","updated":"2025-10-24T18:41:36Z","published":"2025-10-24T18:41:36Z","title":"Temporal Graph Theoretic Analysis of Geopolitical Dynamics in the U.S.\n  Entity List","summary":"  Export controls have become one of America's most prominent tools of economic\nstatecraft. They aim to block rival countries' access to sensitive\ntechnologies, safeguard U.S. supply chains, protect national security, and\nshape geopolitical competition. Among various instruments, the U.S. Entity List\nhas emerged as the most salient, yet its dynamics remain underexplored. This\npaper introduces a novel temporal graph framework that transforms the Entity\nList documents from a static registry of foreign entities of concern into a\ndynamic representation of geopolitical strategy. We construct the first\nevent-based dataset of U.S. government foreign entity designations and model\nthem as a temporal bipartite graph. Building on this representation, we develop\na multi-level analytical approach that reveals shifting roles, enforcement\nstrategy, and broader sanction ecosystems. Applied to 25 years of data, the\nframework uncovers dynamic patterns of escalation, persistence, and\ncoordination that static views cannot capture. More broadly, our study\ndemonstrates how temporal graph analysis offers systematic computational\ninsights into the geopolitical dynamics of export controls.\n","authors":["Yunsen Lei","Kexin Bai","Quan Li","H. Howie Huang"],"pdf_url":"https://arxiv.org/pdf/2510.21962v1.pdf","comment":"13 pages, 9 figures. Under review"},{"id":"http://arxiv.org/abs/2510.21671v1","updated":"2025-10-24T17:27:35Z","published":"2025-10-24T17:27:35Z","title":"A Data-Centric Approach to Multilingual E-Commerce Product Search: Case\n  Study on Query-Category and Query-Item Relevance","summary":"  Multilingual e-commerce search suffers from severe data imbalance across\nlanguages, label noise, and limited supervision for low-resource\nlanguages--challenges that impede the cross-lingual generalization of relevance\nmodels despite the strong capabilities of large language models (LLMs). In this\nwork, we present a practical, architecture-agnostic, data-centric framework to\nenhance performance on two core tasks: Query-Category (QC) relevance (matching\nqueries to product categories) and Query-Item (QI) relevance (matching queries\nto product titles). Rather than altering the model, we redesign the training\ndata through three complementary strategies: (1) translation-based augmentation\nto synthesize examples for languages absent in training, (2) semantic negative\nsampling to generate hard negatives and mitigate class imbalance, and (3)\nself-validation filtering to detect and remove likely mislabeled instances.\nEvaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields\nsubstantial F1 score improvements over strong LLM baselines, achieving\ncompetitive results in the official competition. Our findings demonstrate that\nsystematic data engineering can be as impactful as--and often more deployable\nthan--complex model modifications, offering actionable guidance for building\nrobust multilingual search systems in the real-world e-commerce settings.\n","authors":["Yabo Yin","Yang Xi","Jialong Wang","Shanqi Wang","Jiateng Hu"],"pdf_url":"https://arxiv.org/pdf/2510.21671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21618v1","updated":"2025-10-24T16:24:01Z","published":"2025-10-24T16:24:01Z","title":"DeepAgent: A General Reasoning Agent with Scalable Toolsets","summary":"  Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent.\n","authors":["Xiaoxi Li","Wenxiang Jiao","Jiarui Jin","Guanting Dong","Jiajie Jin","Yinuo Wang","Hao Wang","Yutao Zhu","Ji-Rong Wen","Yuan Lu","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2510.21618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09682v2","updated":"2025-10-24T16:19:07Z","published":"2025-08-13T15:03:38Z","title":"Faster and Memory-Efficient Training of Sequential Recommendation Models\n  for Large Catalogs","summary":"  Sequential recommendations (SR) with transformer-based architectures are\nwidely adopted in real-world applications, where SR models require frequent\nretraining to adapt to ever-changing user preferences. However, training\ntransformer-based SR models often encounters a high computational cost\nassociated with scoring extensive item catalogs, often exceeding thousands of\nitems. This occurs mainly due to the use of cross-entropy loss, where peak\nmemory scales proportionally to catalog size, batch size, and sequence length.\nRecognizing this, practitioners in the field of recommendation systems\ntypically address memory consumption by integrating the cross-entropy (CE) loss\nwith negative sampling, thereby reducing the explicit memory demands of the\nfinal layer. However, a small number of negative samples would degrade model\nperformance, and as we demonstrate in our work, increasing the number of\nnegative samples and the batch size further improves the model's performance,\nbut rapidly starts to exceed industrial GPUs' size (~40Gb).\n  In this work, we introduce the CCE- method, which offers a GPU-efficient\nimplementation of the CE loss with negative sampling. Our method accelerates\ntraining by up to two times while reducing memory consumption by more than 10\ntimes. Leveraging the memory savings afforded by using CCE- for model training,\nit becomes feasible to enhance its accuracy on datasets with a large item\ncatalog compared to those trained with original PyTorch-implemented loss\nfunctions. Finally, we perform an analysis of key memory-related\nhyperparameters and highlight the necessity of a delicate balance among these\nfactors. We demonstrate that scaling both the number of negative samples and\nbatch size leads to better results rather than maximizing only one of them. To\nfacilitate further adoption of CCE-, we release a Triton kernel that\nefficiently implements the proposed method.\n","authors":["Maxim Zhelnin","Dmitry Redko","Volkov Daniil","Anna Volodkevich","Petr Sokerin","Valeriy Shevchenko","Egor Shvetsov","Alexey Vasilev","Darya Denisova","Ruslan Izmailov","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2509.09682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21603v1","updated":"2025-10-24T16:07:54Z","published":"2025-10-24T16:07:54Z","title":"Doc-Researcher: A Unified System for Multimodal Document Parsing and\n  Deep Research","summary":"  Deep Research systems have revolutionized how LLMs solve complex questions\nthrough iterative reasoning and evidence gathering. However, current systems\nremain fundamentally constrained to textual web data, overlooking the vast\nknowledge embedded in multimodal documents Processing such documents demands\nsophisticated parsing to preserve visual semantics (figures, tables, charts,\nand equations), intelligent chunking to maintain structural coherence, and\nadaptive retrieval across modalities, which are capabilities absent in existing\nsystems. In response, we present Doc-Researcher, a unified system that bridges\nthis gap through three integrated components: (i) deep multimodal parsing that\npreserves layout structure and visual semantics while creating multi-granular\nrepresentations from chunk to document level, (ii) systematic retrieval\narchitecture supporting text-only, vision-only, and hybrid paradigms with\ndynamic granularity selection, and (iii) iterative multi-agent workflows that\ndecompose complex queries, progressively accumulate evidence, and synthesize\ncomprehensive answers across documents and modalities. To enable rigorous\nevaluation, we introduce M4DocBench, the first benchmark for Multi-modal,\nMulti-hop, Multi-document, and Multi-turn deep research. Featuring 158\nexpert-annotated questions with complete evidence chains across 304 documents,\nM4DocBench tests capabilities that existing benchmarks cannot assess.\nExperiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter\nthan state-of-the-art baselines, validating that effective document research\nrequires not just better retrieval, but fundamentally deep parsing that\npreserve multimodal integrity and support iterative research. Our work\nestablishes a new paradigm for conducting deep research on multimodal document\ncollections.\n","authors":["Kuicai Dong","Shurui Huang","Fangda Ye","Wei Han","Zhi Zhang","Dexun Li","Wenjun Li","Qu Yang","Gang Wang","Yichao Wang","Chen Zhang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2510.21603v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2510.21440v1","updated":"2025-10-24T13:17:00Z","published":"2025-10-24T13:17:00Z","title":"Redefining Retrieval Evaluation in the Era of LLMs","summary":"  Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,\nassume that human users sequentially examine documents with diminishing\nattention to lower ranks. This assumption breaks down in Retrieval Augmented\nGeneration (RAG) systems, where search results are consumed by Large Language\nModels (LLMs), which, unlike humans, process all retrieved documents as a whole\nrather than sequentially. Additionally, traditional IR metrics do not account\nfor related but irrelevant documents that actively degrade generation quality,\nrather than merely being ignored. Due to these two major misalignments, namely\nhuman vs. machine position discount and human relevance vs. machine utility,\nclassical IR metrics do not accurately predict RAG performance. We introduce a\nutility-based annotation schema that quantifies both the positive contribution\nof relevant passages and the negative impact of distracting ones. Building on\nthis foundation, we propose UDCG (Utility and Distraction-aware Cumulative\nGain), a metric using an LLM-oriented positional discount to directly optimize\nthe correlation with the end-to-end answer accuracy. Experiments on five\ndatasets and six LLMs demonstrate that UDCG improves correlation by up to 36%\ncompared to traditional metrics. Our work provides a critical step toward\naligning IR evaluation with LLM consumers and enables more reliable assessment\nof RAG components\n","authors":["Giovanni Trappolini","Florin Cuconasu","Simone Filice","Yoelle Maarek","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2510.21440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21352v1","updated":"2025-10-24T11:28:08Z","published":"2025-10-24T11:28:08Z","title":"SciNUP: Natural Language User Interest Profiles for Scientific\n  Literature Recommendation","summary":"  The use of natural language (NL) user profiles in recommender systems offers\ngreater transparency and user control compared to traditional representations.\nHowever, there is scarcity of large-scale, publicly available test collections\nfor evaluating NL profile-based recommendation. To address this gap, we\nintroduce SciNUP, a novel synthetic dataset for scholarly recommendation that\nleverages authors' publication histories to generate NL profiles and\ncorresponding ground truth items. We use this dataset to conduct a comparison\nof baseline methods, ranging from sparse and dense retrieval approaches to\nstate-of-the-art LLM-based rerankers. Our results show that while baseline\nmethods achieve comparable performance, they often retrieve different items,\nindicating complementary behaviors. At the same time, considerable headroom for\nimprovement remains, highlighting the need for effective NL-based\nrecommendation approaches. The SciNUP dataset thus serves as a valuable\nresource for fostering future research and development in this area.\n","authors":["Mariam Arustashvili","Krisztian Balog"],"pdf_url":"https://arxiv.org/pdf/2510.21352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21333v1","updated":"2025-10-24T10:49:50Z","published":"2025-10-24T10:49:50Z","title":"CausalRec: A CausalBoost Attention Model for Sequential Recommendation","summary":"  Recent advances in correlation-based sequential recommendation systems have\ndemonstrated substantial success. Specifically, the attention-based model\noutperforms other RNN-based and Markov chains-based models by capturing both\nshort- and long-term dependencies more effectively. However, solely focusing on\nitem co-occurrences overlooks the underlying motivations behind user behaviors,\nleading to spurious correlations and potentially inaccurate recommendations. To\naddress this limitation, we present a novel framework that integrates causal\nattention for sequential recommendation, CausalRec. It incorporates a causal\ndiscovery block and a CausalBooster. The causal discovery block learns the\ncausal graph in user behavior sequences, and we provide a theory to guarantee\nthe identifiability of the learned causal graph. The CausalBooster utilizes the\ndiscovered causal graph to refine the attention mechanism, prioritizing\nbehaviors with causal significance. Experimental evaluations on real-world\ndatasets indicate that CausalRec outperforms several state-of-the-art methods,\nwith average improvements of 7.21% in Hit Rate (HR) and 8.65% in Normalized\nDiscounted Cumulative Gain (NDCG). To the best of our knowledge, this is the\nfirst model to incorporate causality through the attention mechanism in\nsequential recommendation, demonstrating the value of causality in generating\nmore accurate and reliable recommendations.\n","authors":["Yunbo Hou","Tianle Yang","Ruijie Li","Li He","Liang Wang","Weiping Li","Bo Zheng","Guojie Song"],"pdf_url":"https://arxiv.org/pdf/2510.21333v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.04888v2","updated":"2025-10-24T10:07:26Z","published":"2025-07-07T11:19:28Z","title":"SimLab: A Platform for Simulation-based Evaluation of Conversational\n  Information Access Systems","summary":"  Progress in conversational information access (CIA) systems has been hindered\nby the difficulty of evaluating such systems with reproducible experiments.\nWhile user simulation offers a promising solution, the lack of infrastructure\nand tooling to support this evaluation paradigm remains a significant barrier.\nTo address this gap, we introduce SimLab, the first cloud-based platform\nproviding a centralized solution for the community to benchmark both\nconversational systems and user simulators in a controlled and reproducible\nsetting. We articulate the requirements for such a platform and propose a\ngeneral infrastructure to meet them. We then present the design and\nimplementation of an initial version of SimLab and showcase its features\nthrough an initial simulation-based evaluation task in conversational movie\nrecommendation. Furthermore, we discuss the platform's sustainability and\nfuture opportunities for development, inviting the community to drive further\nprogress in the fields of CIA and user simulation.\n","authors":["Nolwenn Bernard","Sharath Chandra Etagi Suresh","Krisztian Balog","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2507.04888v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21276v1","updated":"2025-10-24T09:22:04Z","published":"2025-10-24T09:22:04Z","title":"Pctx: Tokenizing Personalized Context for Generative Recommendation","summary":"  Generative recommendation (GR) models tokenize each action into a few\ndiscrete tokens (called semantic IDs) and autoregressively generate the next\ntokens as predictions, showing advantages such as memory efficiency,\nscalability, and the potential to unify retrieval and ranking. Despite these\nbenefits, existing tokenization methods are static and non-personalized. They\ntypically derive semantic IDs solely from item features, assuming a universal\nitem similarity that overlooks user-specific perspectives. However, under the\nautoregressive paradigm, semantic IDs with the same prefixes always receive\nsimilar probabilities, so a single fixed mapping implicitly enforces a\nuniversal item similarity standard across all users. In practice, the same item\nmay be interpreted differently depending on user intentions and preferences. To\naddress this issue, we propose a personalized context-aware tokenizer that\nincorporates a user's historical interactions when generating semantic IDs.\nThis design allows the same item to be tokenized into different semantic IDs\nunder different user contexts, enabling GR models to capture multiple\ninterpretive standards and produce more personalized predictions. Experiments\non three public datasets demonstrate up to 11.44% improvement in NDCG@10 over\nnon-personalized action tokenization baselines. Our code is available at\nhttps://github.com/YoungZ365/Pctx.\n","authors":["Qiyong Zhong","Jiajie Su","Yunshan Ma","Julian McAuley","Yupeng Hou"],"pdf_url":"https://arxiv.org/pdf/2510.21276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21242v1","updated":"2025-10-24T08:25:56Z","published":"2025-10-24T08:25:56Z","title":"Bi-Level Optimization for Generative Recommendation: Bridging\n  Tokenization and Generation","summary":"  Generative recommendation is emerging as a transformative paradigm by\ndirectly generating recommended items, rather than relying on matching.\nBuilding such a system typically involves two key components: (1) optimizing\nthe tokenizer to derive suitable item identifiers, and (2) training the\nrecommender based on those identifiers. Existing approaches often treat these\ncomponents separately--either sequentially or in alternation--overlooking their\ninterdependence. This separation can lead to misalignment: the tokenizer is\ntrained without direct guidance from the recommendation objective, potentially\nyielding suboptimal identifiers that degrade recommendation performance.\n  To address this, we propose BLOGER, a Bi-Level Optimization for GEnerative\nRecommendation framework, which explicitly models the interdependence between\nthe tokenizer and the recommender in a unified optimization process. The lower\nlevel trains the recommender using tokenized sequences, while the upper level\noptimizes the tokenizer based on both the tokenization loss and recommendation\nloss. We adopt a meta-learning approach to solve this bi-level optimization\nefficiently, and introduce gradient surgery to mitigate gradient conflicts in\nthe upper-level updates, thereby ensuring that item identifiers are both\ninformative and recommendation-aligned. Extensive experiments on real-world\ndatasets demonstrate that BLOGER consistently outperforms state-of-the-art\ngenerative recommendation methods while maintaining practical efficiency with\nno significant additional computational overhead, effectively bridging the gap\nbetween item tokenization and autoregressive generation.\n","authors":["Yimeng Bai","Chang Liu","Yang Zhang","Dingxian Wang","Frank Yang","Andrew Rabinovich","Wenge Rong","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2510.21242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18512v2","updated":"2025-10-24T06:55:54Z","published":"2025-05-24T05:15:49Z","title":"AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking","summary":"  Listwise reranking with large language models (LLMs) enhances top-ranked\nresults in retrieval-based applications. Due to the limit in context size and\nhigh inference cost of long context, reranking is typically performed over a\nfixed size of small subsets, with the final ranking aggregated from these\npartial results. This fixed computation disregards query difficulty and\ndocument distribution, leading to inefficiencies. We propose AcuRank, an\nadaptive reranking framework that dynamically adjusts both the amount and\ntarget of computation based on uncertainty estimates over document relevance.\nUsing a Bayesian TrueSkill model, we iteratively refine relevance estimates\nuntil reaching sufficient confidence levels, and our explicit modeling of\nranking uncertainty enables principled control over reranking behavior and\navoids unnecessary updates to confident predictions. Results on the TREC-DL and\nBEIR benchmarks show that our method consistently achieves a superior\naccuracy-efficiency trade-off and scales better with compute than\nfixed-computation baselines. These results highlight the effectiveness and\ngeneralizability of our method across diverse retrieval tasks and LLM-based\nreranking models.\n","authors":["Soyoung Yoon","Gyuwan Kim","Gyu-Hwung Cho","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2505.18512v2.pdf","comment":"Accepted at NeurIPS 2025. The first two authors contributed equally.\n  Author order is randomly determined via coin toss"},{"id":"http://arxiv.org/abs/2510.21151v1","updated":"2025-10-24T04:45:29Z","published":"2025-10-24T04:45:29Z","title":"VOGUE: A Multimodal Dataset for Conversational Recommendation in Fashion","summary":"  Multimodal conversational recommendation has emerged as a promising paradigm\nfor delivering personalized experiences through natural dialogue enriched by\nvisual and contextual grounding. Yet, current multimodal conversational\nrecommendation datasets remain limited: existing resources either simulate\nconversations, omit user history, or fail to collect sufficiently detailed\nfeedback, all of which constrain the types of research and evaluation they\nsupport.\n  To address these gaps, we introduce VOGUE, a novel dataset of 60 humanhuman\ndialogues in realistic fashion shopping scenarios. Each dialogue is paired with\na shared visual catalogue, item metadata, user fashion profiles and histories,\nand post-conversation ratings from both Seekers and Assistants. This design\nenables rigorous evaluation of conversational inference, including not only\nalignment between predicted and ground-truth preferences, but also calibration\nagainst full rating distributions and comparison with explicit and implicit\nuser satisfaction signals.\n  Our initial analyses of VOGUE reveal distinctive dynamics of visually\ngrounded dialogue. For example, recommenders frequently suggest items\nsimultaneously in feature-based groups, which creates distinct conversational\nphases bridged by Seeker critiques and refinements. Benchmarking multimodal\nlarge language models against human recommenders shows that while MLLMs\napproach human-level alignment in aggregate, they exhibit systematic\ndistribution errors in reproducing human ratings and struggle to generalize\npreference inference beyond explicitly discussed items. These findings\nestablish VOGUE as both a unique resource for studying multimodal\nconversational systems and as a challenge dataset beyond the current\nrecommendation capabilities of existing top-tier multimodal foundation models\nsuch as GPT-4o-mini, GPT-5-mini, and Gemini-2.5-Flash.\n","authors":["David Guo","Minqi Sun","Yilun Jiang","Jiazhou Liang","Scott Sanner"],"pdf_url":"https://arxiv.org/pdf/2510.21151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.16925v2","updated":"2025-10-24T03:04:35Z","published":"2025-10-19T16:46:11Z","title":"Towards Context-aware Reasoning-enhanced Generative Searching in\n  E-commerce","summary":"  Search-based recommendation is one of the most critical application scenarios\nin e-commerce platforms. Users' complex search contexts--such as spatiotemporal\nfactors, historical interactions, and current query's information--constitute\nan essential part of their decision-making, reflecting implicit preferences\nthat complement explicit query terms. Modeling such rich contextual signals and\ntheir intricate associations with candidate items remains a key challenge.\nAlthough numerous efforts have been devoted to building more effective search\nmethods, existing approaches still show limitations in integrating contextual\ninformation, which hinders their ability to fully capture user intent.\n  To address these challenges, we propose a context-aware reasoning-enhanced\ngenerative search framework for better \\textbf{understanding the complicated\ncontext}. Specifically, the framework first unifies heterogeneous user and item\ncontexts into textual representations or text-based semantic identifiers and\naligns them. To overcome the lack of explicit reasoning trajectories, we\nintroduce a self-evolving post-training paradigm that iteratively combines\nsupervised fine-tuning and reinforcement learning to progressively enhance the\nmodel's reasoning capability. In addition, we identify potential biases in\nexisting RL algorithms when applied to search scenarios and present a debiased\nvariant of GRPO to improve ranking performance. Extensive experiments on search\nlog data collected from a real-world e-commerce platform demonstrate that our\napproach achieves superior performance compared with strong baselines,\nvalidating its effectiveness for search-based recommendation.\n","authors":["Zhiding Liu","Ben Chen","Mingyue Cheng","Enhong Chen","Li Li","Chenyi Lei","Wenwu Ou","Han Li","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2510.16925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.20150v2","updated":"2025-10-24T02:11:31Z","published":"2025-10-23T02:56:00Z","title":"Rank-GRPO: Training LLM-based Conversational Recommender Systems with\n  Reinforcement Learning","summary":"  Large language models (LLMs) are reshaping the recommender system paradigm by\nenabling users to express preferences and receive recommendations through\nconversations. Yet, aligning LLMs to the recommendation task remains\nchallenging: pretrained LLMs often generate out-of-catalog items, violate\nrequired output formats, and their ranking quality degrades sharply toward the\nend of the generated list. To this end, we propose ConvRec-R1, a two-stage\nframework for end-to-end training of LLM-based conversational recommender\nsystems. In Stage 1, we construct a behavioral-cloning dataset with a\nRemap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded\ndemonstrations from powerful blackbox LLMs to warm-start the RL training. In\nStage 2, we propose Rank-GRPO, a principled extension of group relative policy\noptimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats\neach rank in the recommendation list as the unit instead of token (too\nfine-grained) or sequence (too coarse), redefining rewards to remove non-causal\ncredit assignment and introducing a rank-level importance ratio based on the\ngeometric mean of rank-wise token probabilities to stabilize policy updates.\nExperiments on the public Reddit-v2 dataset show that ConvRec-R1 converges\nfaster and achieves higher Recall and NDCG than GRPO-style baselines. Code and\ndatasets are released at https://github.com/yaochenzhu/Rank-GRPO.\n","authors":["Yaochen Zhu","Harald Steck","Dawen Liang","Yinhan He","Vito Ostuni","Jundong Li","Nathan Kallus"],"pdf_url":"https://arxiv.org/pdf/2510.20150v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.11217v2","updated":"2025-10-24T14:21:57Z","published":"2025-05-16T13:13:25Z","title":"Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI\n  models in Sound Localization","summary":"  Imagine hearing a dog bark and turning toward the sound only to see a parked\ncar, while the real, silent dog sits elsewhere. Such sensory conflicts test\nperception, yet humans reliably resolve them by prioritizing sound over\nmisleading visuals. Despite advances in multimodal AI integrating vision and\naudio, little is known about how these systems handle cross-modal conflicts or\nwhether they favor one modality. In this study, we systematically examine\nmodality bias and conflict resolution in AI sound localization. We assess\nleading multimodal models and benchmark them against human performance in\npsychophysics experiments across six audiovisual conditions, including\ncongruent, conflicting, and absent cues. Humans consistently outperform AI,\ndemonstrating superior resilience to conflicting or missing visuals by relying\non auditory information. In contrast, AI models often default to visual input,\ndegrading performance to near chance levels. To address this, we propose a\nneuroscience-inspired model, EchoPin, which uses a stereo audio-image dataset\ngenerated via 3D simulations. Even with limited training data, EchoPin\nsurpasses existing benchmarks. Notably, it also mirrors human-like horizontal\nlocalization bias favoring left-right precision-likely due to the stereo audio\nstructure reflecting human ear placement. These findings underscore how sensory\ninput quality and system architecture shape multimodal representation accuracy.\n","authors":["Yanhao Jia","Ji Xie","S Jivaganesh","Hao Li","Xu Wu","Mengmi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11217v2.pdf","comment":"NeurIPS 2025, Spotlight"},{"id":"http://arxiv.org/abs/2510.24769v1","updated":"2025-10-24T11:27:14Z","published":"2025-10-24T11:27:14Z","title":"YTLive: A Dataset of Real-World YouTube Live Streaming Sessions","summary":"  Live streaming plays a major role in today's digital platforms, supporting\nentertainment, education, social media, etc. However, research in this field is\nlimited by the lack of large, publicly available datasets that capture\nreal-time viewer behavior at scale. To address this gap, we introduce YTLive, a\npublic dataset focused on YouTube Live. Collected through the YouTube\nResearcher Program over May and June 2024, YTLive includes more than 507000\nrecords from 12156 live streams, tracking concurrent viewer counts at\nfive-minute intervals along with precise broadcast durations. We describe the\ndataset design and collection process and present an initial analysis of\ntemporal viewing patterns. Results show that viewer counts are higher and more\nstable on weekends, especially during afternoon hours. Shorter streams attract\nlarger and more consistent audiences, while longer streams tend to grow slowly\nand exhibit greater variability. These insights have direct implications for\nadaptive streaming, resource allocation, and Quality of Experience (QoE)\nmodeling. YTLive offers a timely, open resource to support reproducible\nresearch and system-level innovation in live streaming. The dataset is publicly\navailable at github.\n","authors":["Mojtaba Mozhganfar","Pooya Jamshidi","Seyyed Ali Aghamiri","Mohsen Ghasemi","Mahdi Dolati","Farzad Tashtarian","Ahmad Khonsari","Christian Timmerer"],"pdf_url":"https://arxiv.org/pdf/2510.24769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03317v3","updated":"2025-10-24T02:20:55Z","published":"2024-06-05T14:29:44Z","title":"Save It for the \"Hot\" Day: An LLM-Empowered Visual Analytics System for\n  Heat Risk Management","summary":"  The escalating frequency and intensity of heat-related climate events,\nparticularly heatwaves, emphasize the pressing need for advanced heat risk\nmanagement strategies. Current approaches, primarily relying on numerical\nmodels, face challenges in spatial-temporal resolution and in capturing the\ndynamic interplay of environmental, social, and behavioral factors affecting\nheat risks. This has led to difficulties in translating risk assessments into\neffective mitigation actions. Recognizing these problems, we introduce a novel\napproach leveraging the burgeoning capabilities of Large Language Models (LLMs)\nto extract rich and contextual insights from news reports. We hence propose an\nLLM-empowered visual analytics system, Havior, that integrates the precise,\ndata-driven insights of numerical models with nuanced news report information.\nThis hybrid approach enables a more comprehensive assessment of heat risks and\nbetter identification, assessment, and mitigation of heat-related threats. The\nsystem incorporates novel visualization designs, such as \"thermoglyph\" and news\nglyph, enhancing intuitive understanding and analysis of heat risks. The\nintegration of LLM-based techniques also enables advanced information retrieval\nand semantic knowledge extraction that can be guided by experts' analytics\nneeds. Our case studies on two cities that faced significant heatwave events\nand interviews with five experts have demonstrated the usefulness of our system\nin providing in-depth and actionable insights for heat risk management.\n","authors":["Haobo Li","Wong Kam-Kwai","Yan Luo","Juntong Chen","Chengzhong Liu","Yaxuan Zhang","Alexis Kai Hon Lau","Huamin Qu","Dongyu Liu"],"pdf_url":"https://arxiv.org/pdf/2406.03317v3.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2510.24773v1","updated":"2025-10-24T21:30:52Z","published":"2025-10-24T21:30:52Z","title":"Point-level Uncertainty Evaluation of Mobile Laser Scanning Point Clouds","summary":"  Reliable quantification of uncertainty in Mobile Laser Scanning (MLS) point\nclouds is essential for ensuring the accuracy and credibility of downstream\napplications such as 3D mapping, modeling, and change analysis. Traditional\nbackward uncertainty modeling heavily rely on high-precision reference data,\nwhich are often costly or infeasible to obtain at large scales. To address this\nissue, this study proposes a machine learning-based framework for point-level\nuncertainty evaluation that learns the relationship between local geometric\nfeatures and point-level errors. The framework is implemented using two\nensemble learning models, Random Forest (RF) and XGBoost, which are trained and\nvalidated on a spatially partitioned real-world dataset to avoid data leakage.\nExperimental results demonstrate that both models can effectively capture the\nnonlinear relationships between geometric characteristics and uncertainty,\nachieving mean ROC-AUC values above 0.87. The analysis further reveals that\ngeometric features describing elevation variation, point density, and local\nstructural complexity play a dominant role in predicting uncertainty. The\nproposed framework offers a data-driven perspective of uncertainty evaluation,\nproviding a scalable and adaptable foundation for future quality control and\nerror analysis of large-scale point clouds.\n","authors":["Ziyang Xu","Olaf Wysocki","Christoph Holst"],"pdf_url":"https://arxiv.org/pdf/2510.24773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.22030v1","updated":"2025-10-24T21:28:36Z","published":"2025-10-24T21:28:36Z","title":"Estimation of Minimum Stride Frequency for the Frontal Plane Stability\n  of Bipedal Systems","summary":"  Stability of bipedal systems in frontal plane is affected by the hip offset,\nto the extent that adjusting stride time using feedforward retraction and\nextension of the legs can lead to stable oscillations without feedback control.\nThis feedforward stabilization can be leveraged to reduce the control effort\nand energy expenditure and increase the locomotion robustness. However, there\nis limited understanding of how key parameters, such as mass, stiffness, leg\nlength, and hip width, affect stability and the minimum stride frequency needed\nto maintain it. This study aims to address these gaps through analyzing how\nindividual model parameters and the system's natural frequency influence the\nminimum stride frequency required to maintain a stable cycle. We propose a\nmethod to predict the minimum stride frequency, and compare the predicted\nstride frequencies with actual values for randomly generated models. The\nfindings of this work provide a better understanding of the frontal plane\nstability mechanisms and how feedforward stabilization can be leveraged to\nreduce the control effort.\n","authors":["Harsha Karunanayaka","Siavash Rezazadeh"],"pdf_url":"https://arxiv.org/pdf/2510.22030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21991v1","updated":"2025-10-24T19:52:41Z","published":"2025-10-24T19:52:41Z","title":"Two-Steps Diffusion Policy for Robotic Manipulation via Genetic\n  Denoising","summary":"  Diffusion models, such as diffusion policy, have achieved state-of-the-art\nresults in robotic manipulation by imitating expert demonstrations. While\ndiffusion models were originally developed for vision tasks like image and\nvideo generation, many of their inference strategies have been directly\ntransferred to control domains without adaptation. In this work, we show that\nby tailoring the denoising process to the specific characteristics of embodied\nAI tasks -- particularly structured, low-dimensional nature of action\ndistributions -- diffusion policies can operate effectively with as few as 5\nneural function evaluations (NFE).\n  Building on this insight, we propose a population-based sampling strategy,\ngenetic denoising, which enhances both performance and stability by selecting\ndenoising trajectories with low out-of-distribution risk. Our method solves\nchallenging tasks with only 2 NFE while improving or matching performance. We\nevaluate our approach across 14 robotic manipulation tasks from D4RL and\nRobomimic, spanning multiple action horizons and inference budgets. In over 2\nmillion evaluations, our method consistently outperforms standard\ndiffusion-based policies, achieving up to 20\\% performance gains with\nsignificantly fewer inference steps.\n","authors":["Mateo Clemente","Leo Brunswic","Rui Heng Yang","Xuan Zhao","Yasser Khalil","Haoyu Lei","Amir Rasouli","Yinchuan Li"],"pdf_url":"https://arxiv.org/pdf/2510.21991v1.pdf","comment":"16 pages, 11 figure, 2 tables, accepted at Neurips 2025"},{"id":"http://arxiv.org/abs/2504.15863v2","updated":"2025-10-24T19:00:19Z","published":"2025-04-22T12:58:05Z","title":"DERD-Net: Learning Depth from Event-based Ray Densities","summary":"  Event cameras offer a promising avenue for multi-view stereo depth estimation\nand Simultaneous Localization And Mapping (SLAM) due to their ability to detect\nblur-free 3D edges at high-speed and over broad illumination conditions.\nHowever, traditional deep learning frameworks designed for conventional cameras\nstruggle with the asynchronous, stream-like nature of event data, as their\narchitectures are optimized for discrete, image-like inputs. We propose a\nscalable, flexible and adaptable framework for pixel-wise depth estimation with\nevent cameras in both monocular and stereo setups. The 3D scene structure is\nencoded into disparity space images (DSIs), representing spatial densities of\nrays obtained by back-projecting events into space via known camera poses. Our\nneural network processes local subregions of the DSIs combining 3D convolutions\nand a recurrent structure to recognize valuable patterns for depth prediction.\nLocal processing enables fast inference with full parallelization and ensures\nconstant ultra-low model complexity and memory costs, regardless of camera\nresolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)\ndemonstrate unprecedented effectiveness: (i) using purely monocular data, our\nmethod achieves comparable results to existing stereo methods; (ii) when\napplied to stereo data, it strongly outperforms all state-of-the-art (SOTA)\napproaches, reducing the mean absolute error by at least 42%; (iii) our method\nalso allows for increases in depth completeness by more than 3-fold while still\nyielding a reduction in median absolute error of at least 30%. Given its\nremarkable performance and effective processing of event-data, our framework\nholds strong potential to become a standard approach for using deep learning\nfor event-based depth estimation and SLAM. Project page:\nhttps://github.com/tub-rip/DERD-Net\n","authors":["Diego Hitzges","Suman Ghosh","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2504.15863v2.pdf","comment":"17 pages, 3 figures, 15 tables. Project page:\n  https://github.com/tub-rip/DERD-Net. 39th Conference on Neural Information\n  Processing Systems (NeurIPS), San Diego, 2025"},{"id":"http://arxiv.org/abs/2506.00138v2","updated":"2025-10-24T17:52:29Z","published":"2025-05-30T18:21:40Z","title":"Intrinsic Goals for Autonomous Agents: Model-Based Exploration in\n  Virtual Zebrafish Predicts Ethological Behavior and Whole-Brain Dynamics","summary":"  Autonomy is a hallmark of animal intelligence, enabling adaptive and\nintelligent behavior in complex environments without relying on external reward\nor task structure. Existing reinforcement learning approaches to exploration in\nreward-free environments, including a class of methods known as model-based\nintrinsic motivation, exhibit inconsistent exploration patterns and do not\nconverge to an exploratory policy, thus failing to capture robust autonomous\nbehaviors observed in animals. Moreover, systems neuroscience has largely\noverlooked the neural basis of autonomy, focusing instead on experimental\nparadigms where animals are motivated by external reward rather than engaging\nin ethological, naturalistic and task-independent behavior. To bridge these\ngaps, we introduce a novel model-based intrinsic drive explicitly designed\nafter the principles of autonomous exploration in animals. Our method\n(3M-Progress) achieves animal-like exploration by tracking divergence between\nan online world model and a fixed prior learned from an ecological niche. To\nthe best of our knowledge, we introduce the first autonomous embodied agent\nthat predicts brain data entirely from self-supervised optimization of an\nintrinsic goal -- without any behavioral or neural training data --\ndemonstrating that 3M-Progress agents capture the explainable variance in\nbehavioral patterns and whole-brain neural-glial dynamics recorded from\nautonomously behaving larval zebrafish, thereby providing the first\ngoal-driven, population-level model of neural-glial computation. Our findings\nestablish a computational framework connecting model-based intrinsic motivation\nto naturalistic behavior, providing a foundation for building artificial agents\nwith animal-like autonomy.\n","authors":["Reece Keller","Alyn Kirsch","Felix Pei","Xaq Pitkow","Leo Kozachkov","Aran Nayebi"],"pdf_url":"https://arxiv.org/pdf/2506.00138v2.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2507.23773v2","updated":"2025-10-24T17:44:52Z","published":"2025-07-31T17:57:20Z","title":"SimuRA: A World-Model-Driven Simulative Reasoning Architecture for\n  General Goal-Oriented Agents","summary":"  AI agents built on foundation models hold enormous promise. Current practice,\nhowever, focuses on a one-task-one-agent approach, which not only falls short\nof scalability and generality, but also faces practical limitations from\nblack-box autoregressive reasoning, where decisions unfold token by token\nwithout explicit simulation or counterfactual evaluation of outcomes. Humans,\non the other hand, reason and plan by mentally simulating the consequences of\nactions within an internal model of the world -- a capability that supports\nflexible, goal-directed behavior across diverse contexts. Moving towards a more\ngeneral and powerful AI agent, we introduce SimuRA, a goal-oriented\narchitecture for generalized agentic reasoning. Based on a principled\nformulation of an optimal agent in any general environment, SimuRA addresses\nthe limitations of black-box autoregressive reasoning by incorporating the\nworld model for planning via simulation. Our prototype world model is\nimplemented using LLMs as a substrate, leveraging the natural language as a\ndiscrete, hierarchical representation grounded in concepts for planning, while\nremaining model-agnostic. On complex web-browsing tasks such as flight search,\nSimuRA improves the success rate from 0% to 32.2% compared to a representative\nopen-web agent baseline. Across tasks, world-model-based planning achieves up\nto 124% higher task completion rates than a matched black-box autoregressive\nbaseline, demonstrating the advantages of simulative reasoning. We release\nReasonerAgent-Web, a web-browsing agent built on SimuRA, as an open-source\nresearch demo.\n","authors":["Mingkai Deng","Jinyu Hou","Zhiting Hu","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2507.23773v2.pdf","comment":"This submission has been updated to adjust the scope and presentation\n  of the work"},{"id":"http://arxiv.org/abs/2509.23801v2","updated":"2025-10-24T17:39:01Z","published":"2025-09-28T10:55:23Z","title":"High-Precision Climbing Robot Localization Using Planar Array\n  UWB/GPS/IMU/Barometer Integration","summary":"  To address the need for high-precision localization of climbing robots in\ncomplex high-altitude environments, this paper proposes a multi-sensor fusion\nsystem that overcomes the limitations of single-sensor approaches. Firstly, the\nlocalization scenarios and the problem model are analyzed. An integrated\narchitecture of Attention Mechanism-based Fusion Algorithm (AMFA) incorporating\nplanar array Ultra-Wideband (UWB), GPS, Inertial Measurement Unit (IMU), and\nbarometer is designed to handle challenges such as GPS occlusion and UWB\nNon-Line-of-Sight (NLOS) problem. Then, End-to-end neural network inference\nmodels for UWB and barometer are developed, along with a multimodal attention\nmechanism for adaptive data fusion. An Unscented Kalman Filter (UKF) is applied\nto refine the trajectory, improving accuracy and robustness. Finally,\nreal-world experiments show that the method achieves 0.48 m localization\naccuracy and lower MAX error of 1.50 m, outperforming baseline algorithms such\nas GPS/INS-EKF and demonstrating stronger robustness.\n","authors":["Shuning Zhang","Zhanchen Zhu","Xiangyu Chen","Yunheng Wang","Xu Jiang","Peibo Duan","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2509.23801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07969v3","updated":"2025-10-24T17:37:23Z","published":"2025-07-10T17:48:03Z","title":"Reinforcement Learning with Action Chunking","summary":"  We present Q-chunking, a simple yet effective recipe for improving\nreinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.\nOur recipe is designed for the offline-to-online RL setting, where the goal is\nto leverage an offline prior dataset to maximize the sample-efficiency of\nonline learning. Effective exploration and sample-efficient learning remain\ncentral challenges in this setting, as it is not obvious how the offline data\nshould be utilized to acquire a good exploratory policy. Our key insight is\nthat action chunking, a technique popularized in imitation learning where\nsequences of future actions are predicted rather than a single action at each\ntimestep, can be applied to temporal difference (TD)-based RL methods to\nmitigate the exploration challenge. Q-chunking adopts action chunking by\ndirectly running RL in a 'chunked' action space, enabling the agent to (1)\nleverage temporally consistent behaviors from offline data for more effective\nonline exploration and (2) use unbiased $n$-step backups for more stable and\nefficient TD learning. Our experimental results demonstrate that Q-chunking\nexhibits strong offline performance and online sample efficiency, outperforming\nprior best offline-to-online methods on a range of long-horizon, sparse-reward\nmanipulation tasks.\n","authors":["Qiyang Li","Zhiyuan Zhou","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2507.07969v3.pdf","comment":"The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems (NeurIPS 2025); 36 pages, 17 figures"},{"id":"http://arxiv.org/abs/2510.21648v1","updated":"2025-10-24T17:05:19Z","published":"2025-10-24T17:05:19Z","title":"Design and Structural Validation of a Micro-UAV with On-Board Dynamic\n  Route Planning","summary":"  Micro aerial vehicles are becoming increasingly important in search and\nrescue operations due to their agility, speed, and ability to access confined\nspaces or hazardous areas. However, designing lightweight aerial systems\npresents significant structural, aerodynamic, and computational challenges.\nThis work addresses two key limitations in many low-cost aerial systems under\ntwo kilograms: their lack of structural durability during flight through rough\nterrains and inability to replan paths dynamically when new victims or\nobstacles are detected. We present a fully customised drone built from scratch\nusing only commonly available components and materials, emphasising modularity,\nlow cost, and ease of assembly. The structural frame is reinforced with\nlightweight yet durable materials to withstand impact, while the onboard\ncontrol system is powered entirely by free, open-source software solutions. The\nproposed system demonstrates real-time perception and adaptive navigation\ncapabilities without relying on expensive hardware accelerators, offering an\naffordable and practical solution for real-world search and rescue missions.\n","authors":["Inbazhagan Ravikumar","Ram Sundhar","Narendhiran Vijayakumar"],"pdf_url":"https://arxiv.org/pdf/2510.21648v1.pdf","comment":"8 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.20496v2","updated":"2025-10-24T16:53:48Z","published":"2024-10-27T16:24:06Z","title":"Trust-Aware Assistance Seeking in Human-Supervised Autonomy","summary":"  Our goal is to model and experimentally assess trust evolution to predict\nfuture beliefs and behaviors of human-robot teams in dynamic environments.\nResearch suggests that maintaining trust among team members in a human-robot\nteam is vital for successful team performance. Research suggests that trust is\na multi-dimensional and latent entity that relates to past experiences and\nfuture actions in a complex manner. Employing a human-robot collaborative task,\nwe design an optimal assistance-seeking strategy for the robot using a POMDP\nframework. In the task, the human supervises an autonomous mobile manipulator\ncollecting objects in an environment. The supervisor's task is to ensure that\nthe robot safely executes its task. The robot can either choose to attempt to\ncollect the object or seek human assistance. The human supervisor actively\nmonitors the robot's activities, offering assistance upon request, and\nintervening if they perceive the robot may fail. In this setting, human trust\nis the hidden state, and the primary objective is to optimize team performance.\nWe execute two sets of human-robot interaction experiments. The data from the\nfirst experiment are used to estimate POMDP parameters, which are used to\ncompute an optimal assistance-seeking policy evaluated in the second\nexperiment. The estimated POMDP parameters reveal that, for most participants,\nhuman intervention is more probable when trust is low, particularly in\nhigh-complexity tasks. Our estimates suggest that the robot's action of asking\nfor assistance in high-complexity tasks can positively impact human trust. Our\nexperimental results show that the proposed trust-aware policy is better than\nan optimal trust-agnostic policy. By comparing model estimates of human trust,\nobtained using only behavioral data, with the collected self-reported trust\nvalues, we show that model estimates are isomorphic to self-reported responses.\n","authors":["Dong Hae Mangalindan","Ericka Rovira","Vaibhav Srivastava"],"pdf_url":"https://arxiv.org/pdf/2410.20496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21609v1","updated":"2025-10-24T16:15:05Z","published":"2025-10-24T16:15:05Z","title":"Enhancing Tactile-based Reinforcement Learning for Robotic Control","summary":"  Achieving safe, reliable real-world robotic manipulation requires agents to\nevolve beyond vision and incorporate tactile sensing to overcome sensory\ndeficits and reliance on idealised state information. Despite its potential,\nthe efficacy of tactile sensing in reinforcement learning (RL) remains\ninconsistent. We address this by developing self-supervised learning (SSL)\nmethodologies to more effectively harness tactile observations, focusing on a\nscalable setup of proprioception and sparse binary contacts. We empirically\ndemonstrate that sparse binary tactile signals are critical for dexterity,\nparticularly for interactions that proprioceptive control errors do not\nregister, such as decoupled robot-object motions. Our agents achieve superhuman\ndexterity in complex contact tasks (ball bouncing and Baoding ball rotation).\nFurthermore, we find that decoupling the SSL memory from the on-policy memory\ncan improve performance. We release the Robot Tactile Olympiad (RoTO) benchmark\nto standardise and promote future research in tactile-based manipulation.\nProject page: https://elle-miller.github.io/tactile_rl\n","authors":["Elle Miller","Trevor McInroe","David Abel","Oisin Mac Aodha","Sethu Vijayakumar"],"pdf_url":"https://arxiv.org/pdf/2510.21609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21586v1","updated":"2025-10-24T15:54:05Z","published":"2025-10-24T15:54:05Z","title":"MATrack: Efficient Multiscale Adaptive Tracker for Real-Time Nighttime\n  UAV Operations","summary":"  Nighttime UAV tracking faces significant challenges in real-world robotics\noperations. Low-light conditions not only limit visual perception capabilities,\nbut cluttered backgrounds and frequent viewpoint changes also cause existing\ntrackers to drift or fail during deployment. To address these difficulties,\nresearchers have proposed solutions based on low-light enhancement and domain\nadaptation. However, these methods still have notable shortcomings in actual\nUAV systems: low-light enhancement often introduces visual artifacts, domain\nadaptation methods are computationally expensive and existing lightweight\ndesigns struggle to fully leverage dynamic object information. Based on an\nin-depth analysis of these key issues, we propose MATrack-a multiscale adaptive\nsystem designed specifically for nighttime UAV tracking. MATrack tackles the\nmain technical challenges of nighttime tracking through the collaborative work\nof three core modules: Multiscale Hierarchy Blende (MHB) enhances feature\nconsistency between static and dynamic templates. Adaptive Key Token Gate\naccurately identifies object information within complex backgrounds. Nighttime\nTemplate Calibrator (NTC) ensures stable tracking performance over long\nsequences. Extensive experiments show that MATrack achieves a significant\nperformance improvement. On the UAVDark135 benchmark, its precision, normalized\nprecision and AUC surpass state-of-the-art (SOTA) methods by 5.9%, 5.4% and\n4.2% respectively, while maintaining a real-time processing speed of 81 FPS.\nFurther tests on a real-world UAV platform validate the system's reliability,\ndemonstrating that MATrack can provide stable and effective nighttime UAV\ntracking support for critical robotics applications such as nighttime search\nand rescue and border patrol.\n","authors":["Xuzhao Li","Xuchen Li","Shiyu Hu"],"pdf_url":"https://arxiv.org/pdf/2510.21586v1.pdf","comment":"Preprint, Under Review"},{"id":"http://arxiv.org/abs/2510.21571v1","updated":"2025-10-24T15:39:31Z","published":"2025-10-24T15:39:31Z","title":"Scalable Vision-Language-Action Model Pretraining for Robotic\n  Manipulation with Real-Life Human Activity Videos","summary":"  This paper presents a novel approach for pretraining robotic manipulation\nVision-Language-Action (VLA) models using a large corpus of unscripted\nreal-life video recordings of human hand activities. Treating human hand as\ndexterous robot end-effector, we show that \"in-the-wild\" egocentric human\nvideos without any annotations can be transformed into data formats fully\naligned with existing robotic V-L-A training data in terms of task granularity\nand labels. This is achieved by the development of a fully-automated holistic\nhuman activity analysis approach for arbitrary human hand videos. This approach\ncan generate atomic-level hand activity segments and their language\ndescriptions, each accompanied with framewise 3D hand motion and camera motion.\nWe process a large volume of egocentric videos and create a hand-VLA training\ndataset containing 1M episodes and 26M frames. This training data covers a wide\nrange of objects and concepts, dexterous manipulation tasks, and environment\nvariations in real life, vastly exceeding the coverage of existing robot data.\nWe design a dexterous hand VLA model architecture and pretrain the model on\nthis dataset. The model exhibits strong zero-shot capabilities on completely\nunseen real-world observations. Additionally, fine-tuning it on a small amount\nof real robot action data significantly improves task success rates and\ngeneralization to novel objects in real robotic experiments. We also\ndemonstrate the appealing scaling behavior of the model's task performance with\nrespect to pretraining data scale. We believe this work lays a solid foundation\nfor scalable VLA pretraining, advancing robots toward truly generalizable\nembodied intelligence.\n","authors":["Qixiu Li","Yu Deng","Yaobo Liang","Lin Luo","Lei Zhou","Chengtang Yao","Lingqi Zeng","Zhiyuan Feng","Huizhi Liang","Sicheng Xu","Yizhong Zhang","Xi Chen","Hao Chen","Lily Sun","Dong Chen","Jiaolong Yang","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2510.21571v1.pdf","comment":"Project page: https://microsoft.github.io/VITRA/"},{"id":"http://arxiv.org/abs/2510.21560v1","updated":"2025-10-24T15:20:34Z","published":"2025-10-24T15:20:34Z","title":"Learning Neural Control Barrier Functions from Expert Demonstrations\n  using Inverse Constraint Learning","summary":"  Safety is a fundamental requirement for autonomous systems operating in\ncritical domains. Control barrier functions (CBFs) have been used to design\nsafety filters that minimally alter nominal controls for such systems to\nmaintain their safety. Learning neural CBFs has been proposed as a data-driven\nalternative for their computationally expensive optimization-based synthesis.\nHowever, it is often the case that the failure set of states that should be\navoided is non-obvious or hard to specify formally, e.g., tailgating in\nautonomous driving, while a set of expert demonstrations that achieve the task\nand avoid the failure set is easier to generate. We use ICL to train a\nconstraint function that classifies the states of the system under\nconsideration to safe, i.e., belong to a controlled forward invariant set that\nis disjoint from the unspecified failure set, and unsafe ones, i.e., belong to\nthe complement of that set. We then use that function to label a new set of\nsimulated trajectories to train our neural CBF. We empirically evaluate our\napproach in four different environments, demonstrating that it outperforms\nexisting baselines and achieves comparable performance to a neural CBF trained\nwith the same data but annotated with ground-truth safety labels.\n","authors":["Yuxuan Yang","Hussein Sibai"],"pdf_url":"https://arxiv.org/pdf/2510.21560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21536v1","updated":"2025-10-24T15:01:18Z","published":"2025-10-24T15:01:18Z","title":"AURASeg: Attention Guided Upsampling with Residual Boundary-Assistive\n  Refinement for Drivable-Area Segmentation","summary":"  Free space ground segmentation is essential to navigate robots and autonomous\nvehicles, recognize drivable zones, and traverse efficiently. Fine-grained\nfeatures remain challenging for existing segmentation models, particularly for\nrobots in indoor and structured environments. These difficulties arise from\nineffective multi-scale processing, suboptimal boundary refinement, and limited\nfeature representation. In order to overcome these limitations, we propose\nAttention-Guided Upsampling with Residual Boundary-Assistive Refinement\n(AURASeg), a ground-plane semantic segmentation model that maintains high\nsegmentation accuracy while improving border precision. Our method uses\nCSP-Darknet backbone by adding a Residual Border Refinement Module (RBRM) for\naccurate edge delineation and an Attention Progressive Upsampling Decoder\n(APUD) for strong feature integration. We also incorporate a lightweight Atrous\nSpatial Pyramid Pooling (ASPP-Lite) module to ensure multi-scale context\nextraction without compromising real-time performance. The proposed model beats\nbenchmark segmentation architectures in mIoU and F1 metrics when tested on the\nGround Mobile Robot Perception (GMRP) Dataset and a custom Gazebo indoor\ndataset. Our approach achieves an improvement in mean Intersection-over-Union\n(mIoU) of +1.26% and segmentation precision of +1.65% compared to\nstate-of-the-art models. These results show that our technique is feasible for\nautonomous perception in both indoor and outdoor environments, enabling precise\nborder refinement with minimal effect on inference speed.\n","authors":["Narendhiran Vijayakumar","Sridevi. M"],"pdf_url":"https://arxiv.org/pdf/2510.21536v1.pdf","comment":"10 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2510.21469v1","updated":"2025-10-24T13:55:45Z","published":"2025-10-24T13:55:45Z","title":"Enhancing Social Robots through Resilient AI","summary":"  As artificial intelligence continues to advance and becomes more integrated\ninto sensitive areas like healthcare, education, and everyday life, it's\ncrucial for these systems to be both resilient and robust. This paper shows how\nresilience is a fundamental characteristic of social robots, which, through it,\nensure trust in the robot itself-an essential element especially when operating\nin contexts with elderly people, who often have low trust in these systems.\nResilience is therefore the ability to operate under adverse or stressful\nconditions, even when degraded or weakened, while maintaining essential\noperational capabilities.\n","authors":["Domenico Palmisano","Giuseppe Palestra","Berardina Nadja De Carolis"],"pdf_url":"https://arxiv.org/pdf/2510.21469v1.pdf","comment":"8 pages, Workshop on Adaptive Social Interaction based on user's\n  Mental mOdels and behaVior in HRI, The 17th International Conference on\n  Social Robotics, 10-12 September 2025, Naples (IT)"},{"id":"http://arxiv.org/abs/2510.13626v2","updated":"2025-10-24T13:50:04Z","published":"2025-10-15T14:51:36Z","title":"LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action\n  Models","summary":"  Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation.\n","authors":["Senyu Fei","Siyin Wang","Junhao Shi","Zihao Dai","Jikun Cai","Pengfang Qian","Li Ji","Xinzhe He","Shiduo Zhang","Zhaoye Fei","Jinlan Fu","Jingjing Gong","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2510.13626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21043v2","updated":"2025-10-24T13:49:54Z","published":"2025-05-27T11:24:38Z","title":"Visual Cues Enhance Predictive Turn-Taking for Two-Party Human\n  Interaction","summary":"  Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs)\nfacilitate naturalistic human-robot interaction, yet most rely solely on\nspeech. We introduce MM-VAP, a multimodal PTTM which combines speech with\nvisual cues including facial expression, head pose and gaze. We find that it\noutperforms the state-of-the-art audio-only in videoconferencing interactions\n(84% vs. 79% hold/shift prediction accuracy). Unlike prior work which\naggregates all holds and shifts, we group by duration of silence between turns.\nThis reveals that through the inclusion of visual features, MM-VAP outperforms\na state-of-the-art audio-only turn-taking model across all durations of speaker\ntransitions. We conduct a detailed ablation study, which reveals that facial\nexpression features contribute the most to model performance. Thus, our working\nhypothesis is that when interlocutors can see one another, visual cues are\nvital for turn-taking and must therefore be included for accurate turn-taking\nprediction. We additionally validate the suitability of automatic speech\nalignment for PTTM training using telephone speech. This work represents the\nfirst comprehensive analysis of multimodal PTTMs. We discuss implications for\nfuture work and make all code publicly available.\n","authors":["Sam O'Connor Russell","Naomi Harte"],"pdf_url":"https://arxiv.org/pdf/2505.21043v2.pdf","comment":"Accepted to ACL 2025, Findings of the Association for Computational\n  Linguistics"},{"id":"http://arxiv.org/abs/2510.21447v1","updated":"2025-10-24T13:25:39Z","published":"2025-10-24T13:25:39Z","title":"PhysWorld: From Real Videos to World Models of Deformable Objects via\n  Physics-Aware Demonstration Synthesis","summary":"  Interactive world models that simulate object dynamics are crucial for\nrobotics, VR, and AR. However, it remains a significant challenge to learn\nphysics-consistent dynamics models from limited real-world video data,\nespecially for deformable objects with spatially-varying physical properties.\nTo overcome the challenge of data scarcity, we propose PhysWorld, a novel\nframework that utilizes a simulator to synthesize physically plausible and\ndiverse demonstrations to learn efficient world models. Specifically, we first\nconstruct a physics-consistent digital twin within MPM simulator via\nconstitutive model selection and global-to-local optimization of physical\nproperties. Subsequently, we apply part-aware perturbations to the physical\nproperties and generate various motion patterns for the digital twin,\nsynthesizing extensive and diverse demonstrations. Finally, using these\ndemonstrations, we train a lightweight GNN-based world model that is embedded\nwith physical properties. The real video can be used to further refine the\nphysical properties. PhysWorld achieves accurate and fast future predictions\nfor various deformable objects, and also generalizes well to novel\ninteractions. Experiments show that PhysWorld has competitive performance while\nenabling inference speeds 47 times faster than the recent state-of-the-art\nmethod, i.e., PhysTwin.\n","authors":["Yu Yang","Zhilu Zhang","Xiang Zhang","Yihan Zeng","Hui Li","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2510.21447v1.pdf","comment":"17 pages, 5 figures"}],"Graphics":[{"id":"http://arxiv.org/abs/2510.21682v1","updated":"2025-10-24T17:39:52Z","published":"2025-10-24T17:39:52Z","title":"WorldGrow: Generating Infinite 3D World","summary":"  We tackle the challenge of generating the infinitely extendable 3D world --\nlarge, continuous environments with coherent geometry and realistic appearance.\nExisting methods face key challenges: 2D-lifting approaches suffer from\ngeometric and appearance inconsistencies across views, 3D implicit\nrepresentations are hard to scale up, and current 3D foundation models are\nmostly object-centric, limiting their applicability to scene-level generation.\nOur key insight is leveraging strong generation priors from pre-trained 3D\nmodels for structured scene block generation. To this end, we propose\nWorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our\nmethod features three core components: (1) a data curation pipeline that\nextracts high-quality scene blocks for training, making the 3D structured\nlatent representations suitable for scene generation; (2) a 3D block inpainting\nmechanism that enables context-aware scene extension; and (3) a coarse-to-fine\ngeneration strategy that ensures both global layout plausibility and local\ngeometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset,\nWorldGrow achieves SOTA performance in geometry reconstruction, while uniquely\nsupporting infinite scene generation with photorealistic and structurally\nconsistent outputs. These results highlight its capability for constructing\nlarge-scale virtual environments and potential for building future world\nmodels.\n","authors":["Sikuang Li","Chen Yang","Jiemin Fang","Taoran Yi","Jia Lu","Jiazhong Cen","Lingxi Xie","Wei Shen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2510.21682v1.pdf","comment":"Project page: https://world-grow.github.io/ Code:\n  https://github.com/world-grow/WorldGrow"},{"id":"http://arxiv.org/abs/2510.21654v1","updated":"2025-10-24T17:11:50Z","published":"2025-10-24T17:11:50Z","title":"Group Inertial Poser: Multi-Person Pose and Global Translation from\n  Sparse Inertial Sensors and Ultra-Wideband Ranging","summary":"  Tracking human full-body motion using sparse wearable inertial measurement\nunits (IMUs) overcomes the limitations of occlusion and instrumentation of the\nenvironment inherent in vision-based approaches. However, purely IMU-based\ntracking compromises translation estimates and accurate relative positioning\nbetween individuals, as inertial cues are inherently self-referential and\nprovide no direct spatial reference for others. In this paper, we present a\nnovel approach for robustly estimating body poses and global translation for\nmultiple individuals by leveraging the distances between sparse wearable\nsensors - both on each individual and across multiple individuals. Our method\nGroup Inertial Poser estimates these absolute distances between pairs of\nsensors from ultra-wideband ranging (UWB) and fuses them with inertial\nobservations as input into structured state-space models to integrate temporal\nmotion patterns for precise 3D pose estimation. Our novel two-step optimization\nfurther leverages the estimated distances for accurately tracking people's\nglobal trajectories through the world. We also introduce GIP-DB, the first\nIMU+UWB dataset for two-person tracking, which comprises 200 minutes of motion\nrecordings from 14 participants. In our evaluation, Group Inertial Poser\noutperforms previous state-of-the-art methods in accuracy and robustness across\nsynthetic and real-world data, showing the promise of IMU+UWB-based multi-human\nmotion capture in the wild. Code, models, dataset:\nhttps://github.com/eth-siplab/GroupInertialPoser\n","authors":["Ying Xue","Jiaxi Jiang","Rayan Armani","Dominik Hollidt","Yi-Chi Liao","Christian Holz"],"pdf_url":"https://arxiv.org/pdf/2510.21654v1.pdf","comment":"Accepted by ICCV 2025, Code:\n  https://github.com/eth-siplab/GroupInertialPoser"},{"id":"http://arxiv.org/abs/2510.21432v1","updated":"2025-10-24T13:08:15Z","published":"2025-10-24T13:08:15Z","title":"ArtiLatent: Realistic Articulated 3D Object Generation via Structured\n  Latents","summary":"  We propose ArtiLatent, a generative framework that synthesizes human-made 3D\nobjects with fine-grained geometry, accurate articulation, and realistic\nappearance. Our approach jointly models part geometry and articulation dynamics\nby embedding sparse voxel representations and associated articulation\nproperties, including joint type, axis, origin, range, and part category, into\na unified latent space via a variational autoencoder. A latent diffusion model\nis then trained over this space to enable diverse yet physically plausible\nsampling. To reconstruct photorealistic 3D shapes, we introduce an\narticulation-aware Gaussian decoder that accounts for articulation-dependent\nvisibility changes (e.g., revealing the interior of a drawer when opened). By\nconditioning appearance decoding on articulation state, our method assigns\nplausible texture features to regions that are typically occluded in static\nposes, significantly improving visual realism across articulation\nconfigurations. Extensive experiments on furniture-like objects from\nPartNet-Mobility and ACD datasets demonstrate that ArtiLatent outperforms\nexisting approaches in geometric consistency and appearance fidelity. Our\nframework provides a scalable solution for articulated 3D object synthesis and\nmanipulation.\n","authors":["Honghua Chen","Yushi Lan","Yongwei Chen","Xingang Pan"],"pdf_url":"https://arxiv.org/pdf/2510.21432v1.pdf","comment":"accepted to SIGGRAPH Asia; Project page:\n  https://chenhonghua.github.io/MyProjects/ArtiLatent/"},{"id":"http://arxiv.org/abs/2510.21404v1","updated":"2025-10-24T12:47:56Z","published":"2025-10-24T12:47:56Z","title":"PC-NCLaws: Physics-Embedded Conditional Neural Constitutive Laws for\n  Elastoplastic Materials","summary":"  While data-driven methods offer significant promise for modeling complex\nmaterials, they often face challenges in generalizing across diverse physical\nscenarios and maintaining physical consistency. To address these limitations,\nwe propose a generalizable framework called Physics-Embedded Conditional Neural\nConstitutive Laws for Elastoplastic Materials, which combines the partial\ndifferential equations with neural networks. Specifically, the model employs\ntwo separate neural networks to model elastic and plastic constitutive laws.\nSimultaneously, the model incorporates physical parameters as conditional\ninputs and is trained on comprehensive datasets encompassing multiple scenarios\nwith varying physical parameters, thereby enabling generalization across\ndifferent properties without requiring retraining for each individual case.\nFurthermore, the differentiable architecture of our model, combined with its\nexplicit parameter inputs, enables the inverse estimation of physical\nparameters from observed motion sequences. This capability extends our\nframework to objects with unknown or unmeasured properties. Experimental\nresults demonstrate state-of-the-art performance in motion reconstruction,\nrobust long-term prediction, geometry generalization, and precise parameters\nestimation for elastoplastic materials, highlighting its versatility as a\nunified simulator and inverse analysis tool.\n","authors":["Xueguang Xie","Shu Yan","Shiwen Jia","Siyu Yang","Aimin Hao","Yang Gao","Peng Yu"],"pdf_url":"https://arxiv.org/pdf/2510.21404v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2505.02350v3","updated":"2025-10-24T06:52:52Z","published":"2025-05-05T04:16:16Z","title":"Approximating Signed Distance Fields of Implicit Surfaces with Sparse\n  Ellipsoidal Radial Basis Function Networks","summary":"  Accurate and compact representation of signed distance functions (SDFs) of\nimplicit surfaces is crucial for efficient storage, computation, and downstream\nprocessing of 3D geometry. In this work, we propose a general learning method\nfor approximating precomputed SDF fields of implicit surfaces by a relatively\nsmall number of ellipsoidal radial basis functions (ERBFs). The SDF values\ncould be computed from various sources, including point clouds, triangle\nmeshes, analytical expressions, pretrained neural networks, etc. Given SDF\nvalues on spatial grid points, our method approximates the SDF using as few\nERBFs as possible, achieving a compact representation while preserving the\ngeometric shape of the corresponding implicit surface. To balance sparsity and\napproximation precision, we introduce a dynamic multi-objective optimization\nstrategy, which adaptively incorporates regularization to enforce sparsity and\njointly optimizes the weights, centers, shapes, and orientations of the ERBFs.\nFor computational efficiency, a nearest-neighbor-based data structure restricts\ncomputations to points near each kernel center, and CUDA-based parallelism\nfurther accelerates the optimization. Furthermore, a hierarchical refinement\nstrategy based on SDF spatial grid points progressively incorporates\ncoarse-to-fine samples for parameter initialization and optimization, improving\nconvergence and training efficiency. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method can represent SDF fields with\nsignificantly fewer parameters than existing sparse implicit representation\napproaches, achieving better accuracy, robustness, and computational\nefficiency. The corresponding executable program is publicly available at\nhttps://github.com/lianbobo/SE-RBFNet.git\n","authors":["Bobo Lian","Dandan Wang","Chenjian Wu","Minxin Chen"],"pdf_url":"https://arxiv.org/pdf/2505.02350v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2510.21890v1","updated":"2025-10-24T02:29:02Z","published":"2025-10-24T02:29:02Z","title":"The Principles of Diffusion Models","summary":"  This monograph presents the core principles that have guided the development\nof diffusion models, tracing their origins and showing how diverse formulations\narise from shared mathematical ideas. Diffusion modeling starts by defining a\nforward process that gradually corrupts data into noise, linking the data\ndistribution to a simple prior through a continuum of intermediate\ndistributions. The goal is to learn a reverse process that transforms noise\nback into data while recovering the same intermediates. We describe three\ncomplementary views. The variational view, inspired by variational\nautoencoders, sees diffusion as learning to remove noise step by step. The\nscore-based view, rooted in energy-based modeling, learns the gradient of the\nevolving data distribution, indicating how to nudge samples toward more likely\nregions. The flow-based view, related to normalizing flows, treats generation\nas following a smooth path that moves samples from noise to data under a\nlearned velocity field. These perspectives share a common backbone: a\ntime-dependent velocity field whose flow transports a simple prior to the data.\nSampling then amounts to solving a differential equation that evolves noise\ninto data along a continuous trajectory. On this foundation, the monograph\ndiscusses guidance for controllable generation, efficient numerical solvers,\nand diffusion-motivated flow-map models that learn direct mappings between\narbitrary times. It provides a conceptual and mathematically grounded\nunderstanding of diffusion models for readers with basic deep-learning\nknowledge.\n","authors":["Chieh-Hsin Lai","Yang Song","Dongjun Kim","Yuki Mitsufuji","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2510.21890v1.pdf","comment":null}]}}